id,url,title,date,author,text
1,https://redis.com/blog/thank-you-salvatore-sanfilippo/,"Thank You, Salvatore Sanfilippo","June 30, 2020",Ofer Bengal and Yiftach Shoolman,"Today marks another milestone in the history of Redis. After maintaining the open source Redis project for 11 years, Salvatore Sanfilippo (a.k.a. antirez) has decided to take a step back.We certainly understand Salvatore’s rationale—single-handedly overseeing an open source project of this scale for more than a decade is a huge task. And while no transition of this size is ever stress free, with the release of Redis 6 and a pair of homegrown technical leaders from within the Redis community ready to step up (meet Yossi Gottlieb and Oran Agra), we’re all confident that the Redis project will be in great hands in a new, community-driven model.Critically, Salvatore is not leaving the Redis project. He will continue to be a vital member of the Redis community, and serve as a member of the technical advisory board of Redis. But by stepping away from the day-to-day work of maintaining Redis—bug fixes, version releases, and contributions, and much more—Salvatore is gaining the ability to focus on new ways to advance Redis.As Salvatore has said, the Redis project is in an exciting new stage, welcoming many new individual contributors to the core project and creating modules to expand the ways Redis can be used as a primary database. And as active members of the Redis community working closely with Salvatore for more than ten years, Yossi (who recently released the RedisRaft project) and Oran have made many vital contributions in this evolution. They also share our belief in collaboration and transparency with the Redis community—working to create software that offers the most significant benefits to developers with the greatest simplicity.Furthermore, the core of the open source Redis project will remain under the 3-Clause BSD license, and as expressed by the new governance structure for Redis, the project will be guided by a core team of contributors from the community selected according to their involvement and contributions. We believe this moment is an excellent opportunity for the community to contribute, give feedback, and help improve the Redis project.In the coming weeks, Yossi and Oran, together with the other members of this new core team, will begin sharing what’s ahead for Redis. All of us are committed to making this transition as seamless as possible for the Redis community.But before we look ahead to the future, let’s take a moment to give Salvatore our heartfelt thanks for everything he’s done for Redis since that fateful first link was posted on Hacker News way back in 2009.Thanks to Salvatore, Redis has become known for its unique and superior software design based on simple but smart architecture principles. Redis is widely loved because it is easy to learn and simple to deploy, with well-supported documentation. And, of course, Redis is very, very fast.These days Redis powers almost any online application you can think of, across all industries and market segments. It has been voted the Most Loved Database for four consecutive years in Stack Overflow’s global developer survey, named the most widely used database in the Amazon Web Services cloud, and is the most-launched database daily from Docker Hub.Working closely with Salvatore over the past five years has been a true privilege, and we’re incredibly proud that our collaboration has not only made Redis the success it is today, but that it’s positioned to continue into the next decade of Redis. We can’t thank Salvatore enough for what he has done to grow Redis and advance new and better ways to develop software.Together with Salvatore, we’ve never felt more optimistic about the future of the Redis community. The Redis community has always been the primary driver of Redis’s growth and success, and as we enter this new stage together, we believe the best is yet to come!"
2,https://redis.com/blog/new-governance-for-redis/,New Governance for Redis,"June 30, 2020",Yossi Gottlieb and Oran Agra,"Today Salvatore Sanfilippo (a.k.a. antirez) announced that he’s stepping back from being the maintainer of the Redis project. We are honored and humbled that he requested us to succeed him as Redis project leads. With this change, we are excited to offer a new “community-driven” governing structure. Let’s look inside this new approach and see how we arrived at this decision.In the 11 years since Salvatore created Redis, it has become extremely popular and a standard tool in practically every modern application stack. During this time Salvatore has been, for the most part, the BDFL of the Redis project.It was Salvatore’s final call on what went into or stayed out of Redis, how bugs should be fixed, what features were added, and what design tradeoffs were accepted. Basically, he was the only one to commit or, occasionally, press “Merge.” So, as you can imagine, his stepping back is a big deal for Redis.The change in Salvatore’s role is also a very big deal for the two of us, because he has asked us to pick up Redis and take it forward.Fortunately, Redis is well-charted territory for us. Together, our journey with Redis development spans more than 15 years. Some of this time we’ve been busy creating Redis Enterprise and its unique features like Redis on Flash and CRDTs-based Active-Active replication. Building these capabilities required in-depth involvement with the Redis core and working closely with Salvatore.We’ve also been collaborating with Salvatore on many other core open source Redis initiatives: the modules API, diskless replicas, active memory defragmentation, TLS support, and many other optimizations, bug fixes, and general design discussions. Most recently, we’ve been busy with RedisRaft, a new open source project that is part of the Redis ecosystem.But having a good knowledge of the code base is not enough. When it comes to the dynamics of how the project is going to run in its new settings, that’s a new thing for us and for the Redis community in general.When faced with such a big change, we feel it is important to clearly identify two main things: the qualities of the project we want to preserve and the opportunities to change and improve as the community continues to grow.Redis has its own unique DNA. It’s hard to define or quantify, but it involves ideas like striving for simplicity, solving fewer problems but in a better way, and doing the right thing by default. All in the pursuit of speed and efficiency. Preserving and refining Redis’s unique DNA, even as Redis continues to evolve, will remain a priority for us.As Salvatore steps back from maintaining Redis, the project’s scale can no longer be managed as a BDFL-style project. We see this as an opportunity for Redis to adopt a new model that, hopefully, will promote more teamwork and structure and let us scale up its development and maintenance processes.Salvatore has always been very open and collaborative with the Redis community. It was a common practice for him to listen to what users are asking, and to share his thoughts and ask for user feedback. This is something we’re going to work hard to preserve. Taking it a step further, we want to make Redis more approachable, and make it easier for community members to become effective contributors taking a more active and significant part in its development.To facilitate this vision we are proposing a new light-governance model for Redis, which is described on the project’s site. The new model is based around forming a small core team of developers—individuals we will gather based on their demonstrated Redis familiarity, contributions, and commitment.The first person to join the team will be Itamar Haber, who is known to many in the Redis community. In the coming days and weeks, we’ll be working to make this core team a reality and reflect the community’s contributions to Redis. We look forward to announcing more core team members soon.We wish to thank Redis for supporting us in this process and for its ongoing commitment to the open source Redis project.Last and definitely not least, we wish to thank Salvatore for all his hard work, for his wonderful company on this Redis journey, and for his trust."
3,https://redis.com/blog/sending-redis-cluster-alerts-to-slack-with-syslog/,Sending Redis Cluster Alerts to Slack with Syslog,"June 18, 2020",Tugdual Grall,"When running in production, a system logs many events and alerts, helping administrators monitor what is happening in a system and be notified automatically when they need to respond to an issue. For example:This post will cover how Redis Enterprise uses the syslog service to log events and alerts, and will explain how to:To put this in context, Redis Enterprise manages many logs, held by default in the /var/opt/redis/log directory, but we’re going to focus on the use of syslog as laid out in the Redis documentation. Hopefully, by the time you finish reading, you will know how to:Before we start, you’ll need to have a Redis Enterprise Software cluster up and running as well as a Linux instance to run syslog server. (Note that for this post, I used a 3-node cluster of Redis Enterprise 5.4.14-28, running on CentOS 7.x., and another Linux instance to run syslog server.)On all nodes, you will see a /var/opt/redis/log directory in which Redis Enterprise writes all the logs. Let’s focus on the event_log. This file is managed by the Redis Enterprise alert manager service, which generates entries based on the configuration of the cluster and database alerts, and logs various administrative actions, such as database creation, configuration, and deletion.If you open `/var/opt/redis/log/event_log.log` file, you will see entries that look like this:The log entries have the following basic structure:timestamp severity event_log  EventLog:{<list of key value pairs in any order>}These events are managed at the cluster level, so only the master node of the cluster is writing in events in the event_log file at a specific moment in time. Let’s run the `rladmin` command on the cluster:As you can see, the node:1 is the master of the cluster. This means that the event_log will be populated there. If the master moves to another node, the events will be saved on the new master.These logs are managed by Redis cluster logging infrastructure. However, Redis Enterprise also uses syslog. You will find the same log entries in the /var/log/messages file:Note that the same entry is present with the name of the host and process as a prefix:(ip-100-00-00-000 event_log[2015])Let’s now look at the configuration of Redis Enterprise to see how and why the logs are used by rsyslog. Redis Enterprise logging is configured in the /opt/redis/config/logging.conf file:This file contains two interesting sections for this post:loggers.event_log defines which handler(s) are used to log eventshandlers.syslog defines the handler. The default configuration uses the “daemon” “facility”.Keep in mind that if you change the /opt/redis/config/logging.conf it will be overridden when you update Redis Enterprise. In the following sections, I will explain how to control logging using syslog without touching any Redis Enterprise configurations. If you do change the logging configuration, you must run the following command to make it active:$ supervisorctl restart alert_mgrWhat is a syslog facility?Syslog facility is a configuration entry located in /etc/rsyslog.conf that defines where a syslog message will be written, based on the origin of the message. You can find more information about facilities in the syslog Wikipedia page, including the list of available facilities.As noted above, by default Redis Enterprise uses the daemon facility that is used to log system daemons. Syslog also defines the facilities local0 to local7 to let administrators configure a custom logging strategy.We’re now ready to configure syslog. Let’s start by creating a new configuration file to contain all the specific information for Redis event logging.Create the file as shown here:/etc/rsyslog.d/redis.confAnd add the following configuration:With this configuration, the syslog service will:You can learn more about the template syntax in the syslog documentation.Finally, restart syslog:systemctl restart rsyslogTesting the new configurationGo to the Redis Enterprise web console and create a new database (or edit an existing database). You should see a new /var/log/redis.log file and the event you generated. Here’s an example of what a log entry might look like:In this example, the first part of the log entry (info) is the severity, that you can use later to capture specific events.Configuring the full clusterAs mentioned earlier, you are configuring Redis Enterprise and syslog on only one node (the master node), so you have to repeat these steps on each node in the cluster.Redis Enterprise cluster is now configured to use a custom syslog configuration to capture events and alerts; it is possible now to use the syslog capabilities.A common syslog use case is to aggregate all your logs into a single location for processing. Syslog provides a server capability to do this,  you just need to add a new action in the syslog configuration to send the messages on the network.To send all messages to a centralized logging server, let’s add a new action in the /etc/rsyslog.d/redis.conf file:Restart the syslog service:$ systemctl restart rsyslogIf you generate an event from Redis Enterprise, for example by editing a database, you should see a new event in the /var/log/messages file in the syslog server you are pointing to.All Redis cluster events are sent to a centralized syslog server, writing these events in the /var/log/messages file.You can now use this server to parse and integrate these messages with any other tool such as Splunk, Syslog Watcher, or Datadog. For example, a nice and easy flow would be to parse the messages from the log file and send a message to Slack when a specific pattern is met. Here’s how to do that:Creating incoming Slack webhooksThe Slack incoming webhook is a simple REST endpoint that you can use to post messages to Slack.Writing the Python message parserNow that the Slack webhook is in place, let’s use a simple Python script to tail this file, and check if the new line comes from Redis event_log and contains the string failed. When it does, the script will push the message to Slack using a simple webhook.The redis-alert.py script looks like this:Here is the script on the syslog server that aggregates all Redis events:$ python redis-alert.py /var/log/messages failedTo see how it works, generate a node failure, for example by forcing a reboot one of the nodes. You should see a notification in your Slack channel:This Python script is very simple, it sends the message exactly as it is received.Syslog is a powerful tool to capture, organize, filter, and move log entries in various places. In this post we configured Redis Enterprise and syslog to capture cluster events and alerts in a custom format and sent these entries to a central syslog server. If you want to learn more about using syslog, go to the official RSylog documentation."
4,https://redis.com/blog/redis-core-team-update/,Redis Core Team Update,"July 9, 2020",Yossi Gottlieb and Oran Agra,"Last week we announced a new light-governance model for the open source Redis project, following Salvatore Sanfilippo’s stepping back from being Redis’s BDFL and sole maintainer.We believe that defining the right model for Redis was an important step, but it’s just as important to get the right people on board the new core team. In order to preserve the spirit of Redis and its unique DNA, we wanted the core team to consist of people who are already committed to and involved with the project.To that end, last week we announced that Redis Technology Evangelist Itamar Haber, a veteran of the Redis community, will be joining the team as a community lead. Today, we’re very happy to announce that Madelyn Olson, Senior Software Development Engineer at Amazon Web Services (AWS) and Zhao Zhao, Senior Engineer at Alibaba Cloud, have also accepted our invitation and have joined the core team.Madelyn and Zhao have been actively involved in Redis development for several years, contributing numerous changes throughout Redis, including bug fixes and features. They have also spent countless hours collaborating with us, along with Salvatore, on core Redis topics. Some of these collaborations have already proved to be productive, resulting in, among other things, Redis 6.0 TLS support.Madelyn Olson is a Senior Software Development Engineer at Amazon ElastiCache, the managed Redis and Memcached service. She focuses on building secure and highly reliable features for Redis including numerous contributions to the Redis Project.Zhao Zhao is a Senior Engineer at Alibaba Cloud NoSQL Database, from Hangzhou, China. He also manages Redis and Memcached services. As one of the top contributors to Redis 6, he focuses on improving Redis performance and data replication consistency.We are confident that beyond the experience and skills of the team members as a set of individuals, this team has the capacity to work together and produce something greater than the sum of its parts.We’d like to thank Madelyn and Zhao for accepting the challenge and joining the Redis Core Team! We know we have a big mission ahead of us, but assembling this talented, diverse, and committed Redis Core Team definitely gives us confidence we’re on the right track."
5,https://redis.com/blog/getting-started-with-knowledge-graphs-in-redisgraph/,Getting Started with Knowledge Graphs in RedisGraph,"June 22, 2020",Alex Milowski,"A knowledge graph can be a simple data structure that represents what we know about part of the real world. For example, imagine searching for a product on your favorite shopping site. You might search for “glitter shoelaces” and expect to see whatever shoe-lacing products that match that specific term.But how does the site know to display other shoe parts, or shoes themselves? The simple answer would be products that match a single term (e.g., “shoe”). But as people with knowledge of the world around us, we know a lot about this particular search choice that can help provide better responses.This blog post explores how knowledge graphs work, how they’re used in computing, and how to use them with Redis Enterprise’s RedisGraph module. We’ll explore briefly how you can use Cypher queries to access information in a knowledge graph. Finally, we’ll talk about working with knowledge graphs at scale and discuss their future uses. If you’re interested in building a knowledge graph to enhance your machine learning applications, this is a great place to start.The search term is a particular kind of product—a shoelace with a coating—that we can represent in an ontology of objects in a knowledge graph. While there might appear to be a lot of things going on in the graph above, I’ve simplified it into two kinds of relations: “is a” (a category subtype) and “has a” (a part of something). For example, a shoelace is a lacing, a part of a shoe, and has a coating, or a laced shoe has a shoelace. The algorithm has no a priori knowledge of the world aside from what we’ve encoded in the graph.When a user searches for “glitter shoelaces,” a search algorithm can identify a direct match in the ontology. With the knowledge graph, the algorithm can now understand that such an item has a glitter coating and is part of a larger category of items called “shoelaces” or even “lacing” (the purple arrows). It also can follow the “has a” relationship with “shoe” to understand that it is part of a larger item in the product catalog.When we market products, we have choices as to what we can do with the relationships. While the simple choice is to display various categories of shoelaces, maybe we want to show an anti-pattern: shoes with no laces! We can follow the “has a” path back to shoes and query the graph for products that have no shoelace part (the magenta arrows) and display that as an alternative result.Similarly, perhaps the consumer is trying to replace broken laces and we’d be better off showing them new shoes that match their preferences. In our ontology, shoes have coatings as well (the orange arrows) and we can find products that have both glitter shoelaces and a glitter coating (also in orange). Maybe they’d prefer a fabulous new shoe over replacing those broken glitter shoelaces?While the technique of using graph data structures has been around in computing for a long time, the term “knowledge graph” was popularized by Google in 2012. The use of a graph as basis for representing knowledge has a long history, from the early days of the Web with RDF (1997) to now, where it’s often used in various areas of machine learning (ML), natural language processing (NLP), and search.Knowledge graphs are often conceptualized as a way to capture what we know about a particular domain. The graph represents the terms, relationships, and instances of facts and concepts. What we know is represented as data that can be processed by algorithms, rather than algorithms that encode knowledge as rules written in code.This representation of knowledge as data in a graph is an important distinction that makes using knowledge graphs important to search and model inference in machine learning. What we can understand about the world can be encoded and changed over time as we learn, without necessarily rewriting all our software components. In turn, models can learn by being re-trained continuously on the information encoded in the knowledge graphs. This cycle has become increasingly important for applications where the scale of data is massive and ever increasing.Redis Enterprise’s RedisGraph module can help load and use ontologies when building knowledge graph infrastructures. The ontology’s structure becomes a set of graph nodes and edges with labels that can be queried via Cypher and searched via RediSearch. A simple ontology use case—such as looking up a referenced term and using the relationships in the graph to follow graph edges to find synonyms, superclasses, or other related terms—can be as simple as a single Cypher query.For biology/biomedical domains, for example, there are community-developed ontologies available at the Open Biological and Biomedical Ontology (OBO) Foundry. The ontologies are available in the OBO and OWL formats and cover a variety of domains. Given the current pandemic, let’s look at the Coronavirus Infectious Disease Ontology.There is also a Python library, called pygobo, that I developed for reading and transforming the OBO ontologies into a property graph that can be used with RedisGraph. The Coronavirus Infectious Disease Ontology is distributed in OWL format and so, once we convert it into OBO format, we can load the ontology and take a look at its structure.Let’s take a look at this ontology by starting a local RedisGraph instance:% docker run -p 6379:6379 redis/redisgraphWe can then load the ontology, here locally called “cido.obo”:% pip install pygobo
% python -m pygobo load cido.obo --graph cidoWith this completed, the ontology has been loaded into a graph called “cido.” The structure of this graph is documented on GitHub, but here’s a simple summary: There are nodes labeled Ontology and Term, ontologies have a “term” relationship, and terms relate to terms with a “is_a” relationship. For example, we can query for the term “antiviral drug” with:MATCH (r:Term) WHERE r.name='antiviral drug' RETURN rA more interesting query would be to retrieve the whole hierarchy of terms related to antiviral drugs that are within a certain depth of the “is_a” relationship.MATCH (r:Term) WHERE r.name='antiviral drug'
MATCH p=(:Ontology)-[:term]->(:Term)<-[:is_a*1..3]-(r)
RETURN pThe RedisInsight visualization below shows the relationships of all the paths returned in this query. In the right bottom of the image is the “antiviral drug” node that we chose as a root. In the middle is the ontology node. The yellow arrows between nodes show the “is_a” relationship between the terms. Finally, the gray arrows show the term relationship between the ontology node and the term nodes.In the visualization, we can follow “is a” relationships from the “antiviral drug” node in the right corner to “antiviral agent” and  “antimicrobial drug.” These both lead to “antimicrobial agent.” This subgraph retrieved by a simple query gives us an expanding set of categories and an understanding of the “is a” relationship. If we would like to search for alternative treatment for a particular drug, traversing these relationships allows us to discover a larger category or an alternative category.Note that the ontology itself does not have instance data. That is, there are no drugs with the label “antiviral drug.” If we want to use this ontology with a database of drugs, we would simply include the identifier of the term node (i.e., the “id” property), which is guaranteed to be a unique value. Then queries can be used to navigate from a result in the drug database into the ontology.For example, if a drug database is tagged with “CHEBI:36044” (antiviral drug), this simple query will discover the superclass terms “CHEBI:22587” (antiviral agent) and “CHEBI:36043” (antimicrobial drug):MATCH (r:Term) WHERE r.id='CHEBI:36044'
MATCH (t:Term)<-[:is_a]-(r)
RETURN t.name,t.idThese terms can be used as additional search parameters that can expand the search results from the drug database. While very simple, this kind of ontological reasoning allows the database search algorithms to be independent of the term organization.Knowledge graphs are an essential information resource for various scalable applications of machine learning in search and inferencing, as we’ve shown with retail and medical examples. A knowledge graph can be used to extend the hand-curated aspects of an ontology into the instances of data that relate to items within the ontology. Instances of data are often annotated, by hand or automatically by algorithms, with terms from the ontologies.For example, the Gene Ontology has been used to annotate articles in PubMed so that search algorithms can apply logical reasoning to the ontology terms about gene function. The labeling of abstracts within PubMed with ontology terms extends the classifications from the ontology into metadata about the articles. Yet, this was initially largely done by human annotation and curation of labels.The result of extending the ontology into instance data is what makes the result a knowledge graph—it includes both the data and information about the data in one large graph. The challenge here is to do this kind of analysis at scale. Human curation of annotation of data, while very important in some cases, will not scale to the vast amounts of data now being created.When processing a large amount of data, inferencing via machine learning, text analysis via natural language processing, and harvesting data off the Web via human-curated information annotations (i.e., schema.org annotations) all allow a wide variety of data to be stored and categorized in a knowledge graph. The large scale of the data processed enables algorithms to navigate from a particular instance to a very large set of similar items by traversing from the instance data in the graph through the ontology terms to another set of related data. The terms and relations in the ontology allow us to know something about how two specific items relate to each other as part of a very large set of information.The critical point here for scalability is that the ontology part of the knowledge graph is much smaller than the instance data. We can easily store the ontologies in RedisGraph, either as a single graph or multiple graphs, and use the other data structures within Redis or elsewhere to store instance data. It is not necessary to use graph-based representations for instance data, which may already be tabular.From these building blocks we can build algorithms for search, provide recommendations to users of a service, or provide ways for experts to summarize and follow relationships within large amounts of information. Careful construction of the knowledge graph lets systems support professionals (e.g., in clinical settings or for drug development) where the classification of information is important to the correct outcome.The scale of instance data may be too large to practically store in a graph structure. Fortunately, knowledge graphs do not have to be completely realized as graphs. For example, instance data can be stored in a scalable tabular data store where it can be easily augmented with term annotations. In particular, the tabular data can also be stored in Redis alongside the related ontologies stored via RedisGraph.A query result from the tabular data with term annotations can be used by algorithms to find matching terms in the ontology. As we saw previously, the matches in the ontology form a subgraph of information. We can use this subgraph as model inference input. Subsequently, the output of the model inference can be used directly or for additional queries. The composition of the various queries and inference creates a powerful and flexible system with atomic query properties.We can also query the ontology using the RediSearch features of RedisGraph to perform full-text searches of term names, descriptions, and relations. Once candidate terms are identified, we can use the same techniques with the subgraph from the ontology as a mechanism to query data previously stored in Redis. For example, we can find a tabular row annotated with one of the matching terms from the full-text search. The ontology provides us a domain-specific way to filter search results.The techniques discussed here all use a single ontology. RedisGraph can store multiple ontologies and other graph structures in a single larger graph or as separate graphs under separate keys. This flexibility allows algorithms to be created by combinations of different graph components in a larger data pipeline.The possibilities for implementing and using knowledge graphs within Redis are widespread. You can try out ontologies and knowledge graphs on RedisGraph via Docker, with Redis Enterprise, or on our Redis Cloud Pro service."
6,https://redis.com/blog/announcing-a-new-technology-collaboration-with-amd/,Announcing a New Technology Collaboration with AMD,"August 21, 2020",Yiftach Shoolman and Filipe Oliveira,"We’re excited to announce a new technology collaboration with AMD, which is using Redis Enterprise to benchmark Redis database on publicly launched and available AMD EPYCTM 7002 series processor powered systems, and instances in the cloud. The first series of cloud instances tested in this collaboration are the AWS EC2 C5a instances, which are powered by 2nd generation AMD EPYCTM processors. The 2nd generation AMD EPYCTM processors used in the AWS C5a instances run at frequencies up to 3.3 GHz[i] and are available in eight virtualized sizes—ranging from 2 vCPUs to 96 vCPUs—and offer up to 192GB of memory. These C5a instances provide customers with options to optimize cost and performance for a variety of compute-intensive workloads and can be up to 10% less expensive than comparable instances.“We’re excited to be working with AMD to benchmark the Amazon EC2 C5a instances, and pleased the results clearly demonstrate cost and performance advantages for our customers while highlighting how Redis can contribute to benchmarking the next-generation cloud infrastructure. We are looking forward to continuing this collaboration with the next in line AMD EPYC processor-based cloud instances as well as with other infrastructure innovations from AMD,” said Yiftach Shoolman, Co-Founder and CTO at Redis.“We are thrilled to team with Redis to showcase how AMD EPYC processors can deliver high performance for the Redis database solutions used by a variety of demanding enterprise applications. The core density and large memory capacity of AMD EPYC™ processors can optimally scale and serve the growing needs of multi-model based NoSQL databases in real-time,” said Raghu Nambiar, Corporate Vice President, Data Center Ecosystems and Applications, AMD. “By collaborating with Redis we want to showcase how enterprises can get breakthrough database performance by optimizing Redis and Redis Enterprise to take advantage of innovations in AMD EPYC™ processors.”The AMD and Redis collaboration agreement adds AMD to the list of prestigious tech companies—including Intel and Samsung, as well as the major cloud service providers and many innovative startups—that utilize Redis Enterprise for testing and benchmarking next-generation infrastructure products in the areas of persistent memory, SSDs, storage engines, and network adaptors.For this first engagement, AMD engineers used the memtier_benchmark, an open source load-generation tool developed by Redis, for benchmarking NoSQL datastores like Redis and Memcached.The goal of this benchmark was to see how many operations per second a single C5a instance, across multiple instance sizes, can perform while keeping latency at sub-millisecond levels. Redis Enterprise’s shared-nothing architecture allows running a Redis cluster across all cores of a cloud instance, for achieving maximum throughput at a linear scale. That is, the throughput grows linearly as more cores are added and without affecting the sub-millisecond latency.To validate this level of performance and scalability with the new C5a instance, AMD engineers set-up memtier_benchmark with five runs on each C5a instance under test, and the scores reflect the medians of all runs, as shown in the chart below:These results demonstrate the exceptional throughput performance and linear scalability among the different instances, making it clear that Redis running on AWS C5a instances can help boost application performance and can easily scale with application growth.Going forward, AMD engineers plan to perform a scale-out benchmark to demonstrate that linear scalability can also be achieved by adding more instances to the cluster, not just by scaling up the instances. Stay tuned: We look forward to sharing the results as they become available.To learn more, visit:[i] Max boost for AMD EPYC processors is the maximum frequency achievable by any single core on the processor under normal operating conditions for server systems. EPYC-18AMD, the AMD logo, EPYC, and combinations thereof are trademarks of Advanced Micro Devices, Inc."
7,https://redis.com/blog/how-to-use-the-new-redis-data-source-for-grafana-plug-in/,How to Use the New Redis Data Source for Grafana Plug-in,"September 2, 2020",Mikhail Volkov,"Earlier this month, Redis released the new Redis Data Source for Grafana plug-in, which connects the widely used open source application monitoring tool to Redis. To give you an idea of how it all works, let’s take a look at a self-referential example: using the plug-in to see how many times it has been downloaded over time. (The Grafana plug-in repository itself does not provide such statistics out of the box.)Want to learn more? Read Introducing the Redis Data Source Plug-in for GrafanaIf you’re not familiar with Grafana, it’s a very popular tool used to build dashboards to monitor applications, infrastructures, and software components. The Redis Data Source for Grafana is a plug-in that allows users to connect to the Redis database and build dashboards in Grafana to easily monitor Redis data. It provides an out-of-the-box predefined dashboard, but also lets you build customized dashboards tuned to your specific needs.The Redis Data Source for Grafana plug-in can be installed using grafana-cli, Docker, or used in the Grafana Cloud. Alternatively the plug-in can be built from scratch following the instructions on GitHub.grafana-cli plugins install redis-datasourceThis demo uses:Information about any registered plug-in in a Grafana repository can be retrieved using the API in JSON format:For this example, I wanted to find out how many times Redis Data Source for Grafana plug-in was downloaded per day, and to look for spikes after we tweeted or posted on the Redis blog about it. I decided to use RedisTimeSeries (a Redis module that adds a time-series data structure to Redis) to track the number of downloads every hour.To populate the data I used the TS.ADD command with an automatic timestamp and labels `plugin` and `version`. X is a number of downloads and the latest version `1.1.2` retrieved from API. Labels will be used later to query the time series.I wrote a simple script using ioredis and Axios libraries to call the API and use plug-in information to add time-series samples:I used a package.json file to install dependencies and ran commands using `npm` as shown here:To orchestrate Docker containers, I used docker-compose:To run the script every hour and collect the download data, I used crontab on the Linux server in the cloud:To run the script and collect data, you need to install Node.js, Docker, and Docker Compose, following the instructions for your operating system:After running the script, we can check the RedisTimeSeries data using the TS.MRANGE command. You can query a range across multiple time-series by using filters in forward or reverse directions:The command TS.MRANGE with filter `plugin` retrieves samples only for the `redis-datasource` plug-in. Use the option WITHLABELS to return labels.Open Grafana in a web browser using `http://localhost:3000` and create the data source by selecting Configuration -> Data Sources. Redis Data Source for Grafana supports transport layer security (TLS) and can connect to open source Redis OSS, Redis Enterprise, and Redis Enterprise Cloud databases anywhere using a direct connection.The next step is to create a dashboard with a graph panel to visualize data. Select “Redis Datasource” and “RedisTimeSeries commands” in the query editor. Use the command TS.MRANGE with a plug-in name filter.Finally, I named the plug-in Legend Labels and set the version as Value Label, which will make it easier to display the series for later versions of Redis Data Source for Grafana.Use the command TS.INFO to see the information and statistics for the time series. So far I have collected download data for 250 hours and can see how much memory (in bytes) was allocated to store time-series and other information.At the time of publication, Redis Data Source for Grafana plug-in has been downloaded more than 3500 times! We have received valuable feedback from the community and continue developing new features for the data source.For more information, look at the GitHub repository for the project and let us know if you have any questions in the issues.I hope this post, and my example using the Redis Data Source for Grafana to track downloads of the plug-in over time, has demonstrated the power and ease of use of this new tool and inspires your to monitor your application data (transactions, streams, queues, etc.) using RedisTimeSeries. Stay tuned for more posts on how and why to use the Redis Data Source for Grafana plug-in."
8,https://redis.com/blog/top-10-remote-work-tips-redis-labs-employees/,Top 10 Remote Work Tips From Redis Labs Employees,"October 2, 2020",Haley Kim,"While the last few months have been full of excitement here at Redis—including our first fully virtual RedisConf in May and the announcement of a new funding round in August—needless to say, having the entire team work remotely has been a big adjustment for many staffers.That’s why for the premiere issue of Rediscover Magazine, we asked leaders at top tech companies to share their lessons learned on managing remote workers. And it’s why Ofer Bengal, CEO and Co-Founder of Redis, wrote about rediscovering the benefits of collaboration in the age of COVID-19. But of course the story doesn’t end there. Many companies, including Redis, won’t be fully back in the office until at least 2021—if then. So we decided to poll Redis employees on their must-have remote-work tools and insights. Here are their top 10 tips:At the office, all Redis employees had adjustable desks and most had extra screens. Consider investing in these amenities when you’re building out your home office space to help with productivity and comfort. A few extra tools you might consider:We primarily use Zoom for meetings at Redis, but it’s worth understanding how Google Hangouts, Microsoft Teams, BlueJeans, and Webex work too (you never know when you’ll have a meeting with a customer or provider who uses a different platform).Whether you’re working at home or in the office, your eyes are glued to screens for most of the day. To reduce eye strain, every hour, lift your eyes away from your monitor and look into the distance for at least 30 seconds. Additionally, if you find yourself having trouble sleeping and/or experiencing migraines, invest in a pair of blue-light blocking glasses. And when you can, try to avoid using your phone outside of work hours.Five minutes of meditation (especially in sunlight) is an easy way to give yourself a mental reset (check out meditation apps Calm or Headspace to get started). If you can, take a walk outside at least once a day, even if you have only a few minutes.Remote work reduces our opportunities to bump into coworkers (especially those from other departments) at the coffee machine or at happy hour. Building strong relationships with coworkers is one of the most important parts of creating effective teams, so make sure to ask people what they’re doing outside of work, too!Whether daily or weekly, short “stand-up” syncs are a great way to stay in touch. That’s always been true for remote team members, and now applies to everyone. (Ironically, those previously remote workers might not be feeling as left out as they did before COVID-19.)Schedule a lunch break or other needed personal time on your calendar to avoid being co-opted by work. Try to avoid eating your meals at your desk, too.Create a designated “work area” and try not to work outside of it (so you can enjoy the rest of your home for what it is, a home!). That can also help create a mental boundary between work and life. That being said, you might get sick of the same office environment—so feel free to consciously choose to work for a bit on the living room sofa when you need to change things up a bit. Similarly, try to set consistent start and stop times for your work day, so you’re not always on the clock.One perk of working from home: no more commute! Many of our Mountain View, Calif., employees have taken advantage of the office-less lifestyle and are working all over the U.S. now, including New England, Seattle, and Portland, Oregon. If you like to be out in the world, try spending some time on an extended stay in a city or other location you’ve always wanted to visit for a change of scenery.Our furry friends can make long days at home just a little sweeter.Do you have some must-have tech tools and techniques you’ve been relying on to make it through coronavirus quarantines? Share them with us on Twitter @Redis with the hashtag #RedisWFH!Cover image via Grovemade on Unsplash"
9,https://redis.com/blog/introducing-the-redis-data-source-plug-in-for-grafana/,Introducing the Redis Data Source Plug-in for Grafana,"August 25, 2020",Alexey Smolyanyy and Mikhail Volkov,"Grafana is a well-known and widely used open source application monitoring tool. And now, thanks to the new Redis Data Source for Grafana plug-in, it works with Redis!With this new capability, DevOps practitioners and database admins can use a tool they are already familiar with to easily create dashboards to monitor their Redis databases and application data. The new Grafana Redis Data Source plug-in allows you to visualize RedisTimeSeries data and core Redis data types like Strings, Hashes, Sets, and more. Also, it can parse and display the output of Redis admin commands, such as SLOWLOG GET, INFO, and CLIENT LIST.Don’t miss the other blogs in this series: How to Use the New Redis Data Source for Grafana Plug-in and 3 Real-Life Apps Built with Redis Data Source for GrafanaThe new Redis Data Source for Grafana can connect to any Redis database—including open source Redis, Redis Enterprise, Redis Enterprise Cloud—and works with Grafana 7.0 and later. If you already have Grafana 7.0, you can install the Data Source plug-in with this grafana-cli command:grafana-cli plugins install redis-datasourceIf you don’t have Grafana installed, or just want to try the new data source, you can easily get started with Grafana in a Docker container:docker run -d -p 3000:3000 --name=grafana -e ""GF_INSTALL_PLUGINS=redis-datasource"" grafana/grafanaSetting up Redis Data Source for Grafana is just as easy as working with any other Grafana data source. There are additional configuration options available, besides the server address and port, including database password and Transport Layer Security (TLS) connection.After you complete the initial configuration, you can start to create panels displaying Redis data! The Redis Data Source plug-in supports three different command types: Redis commands, RedisTimeSeries commands, and universal inputs.1. Redis commands comprise a number of predefined commands to retrieve core Redis data types, such as Hashes, Sets, Strings, Streams, etc. The command’s output is pre-formatted for easy use in the Grafana interface. This mode also allows you to execute Redis admin commands: SLOWLOG GET, INFO, CLIENT LIST. Their output comes in newly introduced data frames, so you can apply Grafana transformations to modify the standard output.2. RedisTimeSeries commands offer an interface to let you work with the RedisTimeSeries module. Currently, it supports two commands: TS.RANGE and TS.MRANGE, which let you query a range from one or more time series. The example below shows the number of downloads of the Redis Data Source from the Grafana repository.3. Universal input allows you to use other commands, not supported by the first two modes. Please keep in mind that:To get started, install the Redis Monitoring Dashboard, built for the new Grafana Data Source, and play with it.The monitoring dashboard uses various sections of the INFO command with the relevant Grafana transformation. Additionally, it has a SLOWLOG panel, so you can quickly identify your slowest queries (which can impact the performance of your Redis database), and a CLIENT LIST panel displaying the information about client connections.There are endless possibilities to use the new Redis Data Source Plug-in for Grafana; we plan to share more example dashboards, including a fun application for weather geeks, in the coming weeks. So please stay tuned!"
10,https://redis.com/blog/real-time-observability-with-redis-and-grafana/,Real-Time Observability with Redis and Grafana,"October 28, 2020",Mikhail Volkov and Alexey Smolyanyy,"On Monday, October 26, 2020, Grafana kicked off ObservabilityCON. The virtual conference included live talks and demos on the newest features, functionality, use cases, and just about everything else in the open source observability ecosystem. After the keynote session, we presented our session on Real-time Observability with Redis and Grafana.We talked about the integration of Grafana and Redis and presented three real-life applications built with Redis Data Source for Grafana, including monitoring COVID-19 cases, weather, and a pop-up store app that uses RedisGears to process incoming data. Additionally, we showed how to use the Redis Data Source and the three command types it supports: Redis commands, RedisTimeSeries, and universal inputs. Finally, we demonstrated the new Redis Application Plug-in for Grafana with a custom Redis CLI panel.Want to see for yourself? Flip through the slide deck below:"
11,https://redis.com/blog/how-to-lead-a-great-software-engineering-team/,How to Lead a Great Software Engineering Team,"November 13, 2020",Haley Kim,"In our Advice to Our Younger Selves series, Redis women tech staffers share insights they wish they knew when they were starting their careers.Since a very young age, Adi Godkin loved pets, and believed she would grow up to be a veterinarian. Eventually, though, she learned being a vet required doing surgeries—and she realized this may not be the role for her.At the age of 18, Adi was chosen for Mamram, a technical unit in the Israel Defense Forces. Even though she didn’t have much experience working with computers, she found that she really enjoyed software engineering and programming, so naturally she decided to study computer science when she went to university.After several years in a variety of engineering roles, from software engineer to R&D manager, Adi joined Redis’ R&D team as a Director of Software Engineering, where she manages a group of 14 engineers with 3 direct reports. We asked Adi to share her tips for becoming a software engineering manager and what she wished she knew when she was younger.Redis: What was your first job? Does it correlate to what you do now?Adi Godkin: During my studies, I was teaching many kids, some of whom came from poor neighborhoods and distressed families—I saw their smiles each time we met, and while spending time together, saw how proud they were to have a student mentor them. This felt very satisfying, and it also taught me something about myself: I really like mentoring others, and I should look for a role where that was a major part of my job. But my first real job after finishing university was as a software engineer in a big telecommunications company.Redis: What’s the best part of your job now?Adi Godkin: Today, I’m leading young managers and software engineers, and no doubt the best part of my work is seeing how they evolve and grow and what they learn along the way.Each day at work is dedicated to listening to others, and understanding the things that do not work well and need to be improved. I’m constantly looking for ways to empower others, encourage them to voice their thoughts and share their ideas, and push them to have good collaborations with others, while also doing what they are best at.When I worked as a software engineer, I was very dedicated and a hard worker, but didn’t really understand what was different about being a manager and what they’re doing on a daily basis. When I became one, I realized that managers work even harder. Managers have bigger responsibilities for both product quality and delivery, but also for the career paths of many individuals.Redis: Have you had any mentors, yourself?Adi Godkin: I had several mentors along my career, and one of them was a former manager of mine. Among many other things, he taught me that good collaboration with your colleagues is key to success and that your relationship with your manager has a great impact on your personal growth.When I was a young manager I made some mistakes—I was very execution-oriented, and therefore I sometimes found myself doing things my team should have done, mostly because I wanted the job to to be delivered in the best and fastest way. I learned that though it may speed delivery in the short term, it is not the best way to grow your team. Making mistakes and learning from them is one of the best ways to improve, and I still make my share, and on some occasions I let my direct reports make their own mistakes when I see they can learn from it—and of course, when I know it will not result in a disaster.Redis: What do you think young people and/or women who want to work in tech should know about the industry?Adi Godkin: This high-tech industry is demanding, with lots of working hours and stress. While Redis emphasizes work/life balance, there’s always plenty of work to be done, so you really need to love it. On the other hand, it can be very satisfying and rewarding if you put your heart in it and are able to make an impact.It is also important to see things in proportion and perspective and not get too anxious if something is not working as planned. This is all part of the journey—when you fail, the point is to recover fast and move forward.Redis: Can you name three skills (technical or soft) that people who want to become software engineering managers should make sure they have?Adi Godkin: If you want to become a software engineering manager, you need to have strong technical and architecture skills, along with the ability to make hard decisions. You should have empathy for your employees, find ways to help them when they are facing personal issues, and also care for their personal growth. This job also requires lots of sensitivity and high emotional intelligence to understand your employees’ inner drives and learn what motivates them.Redis: Do you have any advice for your teenage self?Adi Godkin: I would say never stop learning. When you graduate from university and start your career, you realize there is lots to learn, so at first you work on educating yourself, asking questions, and learning from others. But once you gain the relevant competences in your job, it’s easy to make learning less of a priority. But the most successful people, in work and in life, never stop learning and improving."
12,https://redis.com/blog/what-is-a-solution-architect-and-how-to-become-one/,"What Is a Solution Architect, and How to Become One","September 25, 2020",Haley Kim,"In our Advice to Our Younger Selves series, Redis women tech staffers share insights they wish they knew when they were starting their careers.Like many kids, young Jane Paek dreamed of becoming an astronaut. Once she started high school—and learned astronauts can’t wear glasses in space—she considered a career in medicine before eventually choosing to pursue electrical engineering.Fast forward more than 20 years, and Jane is currently Director of Regional Solution Architects at Redis, managing a team of 11 Solution Architects, or SAs. So how did an electrical engineering major from Canada make her way to the technical sales department of a database startup in Silicon Valley? In this Q&A, Jane explains what a solution architect is, the perks of being a technical person in sales, and how she overcame imposter syndrome.Redis: Let’s start from the beginning: how did you move from engineering to sales?Jane Paek: Through many co-op opportunities, I realized by the time I graduated university that I was not destined to be an electrical/hardware engineer. So the first company I joined post-graduation was Oracle, where its intern program rotated young graduates through different departments. I started with customer support, then went into pre-sales engineering, and finally technical education. Oracle provided hands-on, customer-centric training that I didn’t get in university and I’ve been in customer-centric roles ever since, mostly around pre-sales engineering.Redis: What is a solution architect?Jane Paek: Solution architects at Redis help customers design and deploy high-speed applications leveraging Redis in the most optimal way. We are technologists, but being customer-facing, we get to observe how technology impacts business and individuals, and how technology is leveraged to achieve business goals. At Redis, solution architects are pre-sales, but the role can mean other things in other industries or organizations.Redis: How did you make your way to Redis?Jane Paek: In 2008, I decided to take a break from work to focus on raising a family. Seven years and two children later, I re-entered the workforce through re-engaging with my network. It was emotionally challenging but by reaching out to a number of people I had previously worked with, I was able to land a job offer in a short period of time. After working remotely as an SA for two years, I wanted to find a company more local and that’s when I saw a Redis job posting on Glassdoor.Redis: Can you name three skills, technical or soft, that people need to make a career in technical sales?Jane Paek: First is the ability to listen. SAs connect the problems customers have to potential solutions we can provide. Our job is to make sure we fully comprehend the problem set to make sure the solution we propose provides the most benefit to the customer. If we don’t listen well, then we are going to waste a lot of everyone’s time.Second is a sense of urgency. The mantra that constantly runs in the back of my mind: “time kills deals.” Time is your most valuable resource. In a sales organization, you’re typically bounded by a window of time—we’re given a year and you need to achieve a certain revenue target. The ability to gauge time, manage time, and multitask is an absolutely critical skill to almost anything we do in sales.The third one is just a desire to learn and improve. Being curious and asking, “Why do you want to do this?” forces you to follow up and clarify things, but it also leads you to say, “You know what, maybe there’s a different path.”Redis: What are the key things about the tech industry they don’t teach you in school?Jane Paek: Oh, there’s so much that they don’t tell you, like networking for one. You do occasionally hear how important networking is, but you don’t appreciate it until you get into the workforce. For example, in recruiting the weight applied to someone who comes in through a referral versus a resume that came in out of the blue—it’s night and day. Make the effort to really get to know your coworkers and their strengths, likes, and what they are like to work with. Your paths could potentially converge again.Another lesson learned is salary anchoring. I didn’t realize it when I was younger, but women have a tendency of setting for, or anchoring, lower salaries than men when negotiating a job offer. Many women are uncomfortable asking for more, and by not asking, we settle for less. My advice to women and underrepresented minorities is to get over the imposter syndrome. Take a look at the market and anchor high. If the market can afford it, and there’s other people with your experience—or even less experience—getting that type of salary, ask for it. The worst you’ll get is no.Redis: You mentioned the imposter syndrome—how have you learned to get over that feeling of “I’m not good enough”?Jane Paek: I think both men and women experience imposter syndrome. We all have roles where we’re not sure whether we’re qualified. But you cannot be perfect and you can’t possibly know everything. Be comfortable acknowledging you can grow, and think, “It’s OK, I will learn.”Redis: When you were younger, were you comfortable with things like presenting and helping people solve their problems over the phone or video?Jane Paek: My first support call at Oracle? I thought, “Oh my… they want me to pick up the phone and answer a question where I just had three months of training. I don’t know enough to answer their questions!” Fortunately, my first caller just wanted to know where the documentation was, and I was like, “Oh, that’s an easy one!” It takes practice. Over time, you slowly build up your knowledge and confidence.I also hated presenting when I first started—and presentation skills weren’t taught in school. As you get into customer-centric roles, you start developing the skills, and you become more comfortable being uncomfortable. Every time you go up on stage you will be uncomfortable. Every time you get on a call with a customer, you will be uncomfortable. The reality is, if you’re comfortable, there’s a problem. It means you’re overly confident or too cocky. Being nervous can help you step up your game.In practical terms, I joined Toastmasters and took improv classes. They’re both risk-free ways of getting over the discomfort and improving your presentation skills.Redis: Let’s talk more about working in customer-facing tech roles. What are some other things you didn’t know?JP: I wish I had been exposed to alternative career paths in tech. How often have you heard people mention technical marketing as part of an engineering-career presentation? Or pre-sales engineering, technical account management, or sales engineers, solutions architects or professional services? These are all customer-centric roles that require technical expertise.Learning the pros and cons of different roles would have been great, especially as it pertained to travel. Some of these jobs have great travel opportunities. During my first year as a technical instructor at Oracle, I traveled almost 3 weeks out of the month. I visited 9 out of the 10 provinces in Canada and loved seeing the country. When you’re young, if you have the opportunity to travel and you have a job that’s willing to pay your expenses—take advantage of it.Especially for sales engineering, there are a lot of perks if you are successful. For example, club trips are all inclusive awards trips for sales teams who attain their annual quota. Through these trips, I’ve had a blast visiting Sydney, Australia; the Cayman Islands; Turks and Caicos; Cancun.Redis: My perception of technical roles was that you were stuck in the lab or the office all day, but that doesn’t seem to necessarily be the case.Jane Paek: You are right, that’s the general perception of engineers. The reality is, there are a lot more choices to what kind of environments we can work in. I had to learn about these career paths meandering through life as there was no menu out there. So I encourage people to be curious and ask people, “What do you do?”"
13,https://redis.com/blog/how-to-work-in-software-without-being-a-developer/,How to Work in Software Without Being a Developer,"October 30, 2020",Haley Kim,"In our Advice to Our Younger Selves series, Redis women tech staffers share insights they wish they knew when they were starting their careers.Liking too many subjects in school can have its downsides—for Mariana Aviv, it meant she felt no singular calling. As a self-described “very bossy” kid who loved numbers and management, she decided to study the humanities, eventually getting her degree in merchandising in her home country of Argentina.After moving to Israel to be with her husband, she joined a tech company as an office manager. Now she’s a project manager on the Redis R&D team based in Tel Aviv, working closely with her team of software developers in biweekly development sprints. In this Q&A, Mariana shares her day-to-day life as a project manager and offers tips for working on a software team without having a technical background.Redis: Let’s start at the beginning—how did you start working in tech?Mariana Aviv: My first year in Israel was a challenging period—while learning a new language and waiting for the validation of my university degree, I started working in a high-tech company as an office manager.I was excited to be accepted at the leading speech-recognition company in Israel at that time, and this is when I fell in love with high-tech organizational culture: talented people, respectful environment, diversity, employee care, and English-friendly. Coming from Argentina, these were totally new work conditions for me.With the help of a great professional growth program and my experience on merchandising campaigns management, I eventually advanced to an operational project management role.I never consider looking somewhere else. High-tech became my community and its people became my group of belonging. Moving from operation management to product management, release management, and program management came natural to me. My Agile journey started when I took part in an organizational Agile transformation. After getting Scrum Master and SAFe certifications, I found that my experience, skills, and personal characteristics are best represented in the Scrum Master role, while Agile project management became my approach to managing software projects. It took me a while, but I found my calling.Redis: Can you tell us about the Agile development process and your role as a Scrum Master?Mariana Aviv: We work in a Scrum framework, which is a subset of Agile, a project management methodology. A Scrum is a two-week period called a “sprint,” designed for efficient software development. As a Scrum Master for my team, my responsibility is to plan the Scrum process to ensure our projects run smoothly.Prior to starting a sprint, we have planning sessions to decide what we are going to work on within those two weeks. We prioritize tasks, which is done with the product owners and product management, which are different roles within the Scrum.We also have a series of “ceremonies” during every sprint, including daily standup meetings, where we give our updates on our work and what we accomplished, and retrospectives, where we reflect on what we learned and what we want to improve in the next sprint. The sprint review is where we show product management what we accomplished and gather feedback.We plan all the time, but often something comes up that breaks the sprint and we have to re-plan our content based on new priorities. We embrace changes and we accommodate accordingly. My goal is to keep things visualized to improve communication among all the stakeholders. I’m also part of the project management team. Each of us on the project management team has our own Scrums, but as a group we try to keep the trains rolling and moving forward.Redis: What do you like most about the Scrum Master role?Mariana Aviv: The interaction with the developers. I love working with them. They are a serious bunch of smart people, and I like making things fun and lightening the mood. I enjoy keeping things organized, providing a different point of view, being a focus point to gather information, tracking our progress, maintaining the focus of the team, and just making things easier for the developers. It’s very satisfying to be part of the process of creating new things and having all these smart people around you—feeling that you belong.Redis: What skills do Scrum Masters need?Mariana Aviv: You don’t need to be a developer to be a Scrum Master. But it’s very important to know yourself and what makes you different. Add your touch to everything. That sparkle or twist is what you need to bring, in addition to having the certification and degree and knowing the responsibilities.For a Scrum Master role, self-motivation is very important. Being a Scrum Master can get monotonous sometimes. So you need to keep moving, keep finding ways to be original.Teamwork is also important. That includes a lot of things, like the communication streams, the communication skills, the support, the problem solving, the tolerance, the attitude.Redis: If you could go back in time then to your high school self, what advice would you share with yourself?Mariana Aviv: I would tell myself to ask for help. It’s OK to ask for help in everything. I think that’s very important. Sometimes we just want to prove ourselves and move forward, especially as young women, who want to be tough and independent. But it’s OK to say I don’t understand, or I need help to improve and to develop the career I want.I also would tell myself to keep things in proportion. I changed careers at the beginning of my education in Argentina. I started something and I decided to change. That time, effort, and money invested can feel like a waste. But it’s not—you’re investing in things that make your path unique.Finally, keep investing in things that make you happy and balance your life—like sports or art. Not everything has to be centered on your career. Those things build up your individuality, and many tech companies value having unique people on their teams.Redis: Did you have any mentors in your career?Mariana Aviv: I didn’t have an official mentor. I choose my bosses as mentors. When I choose my positions and come to the interviews, I also try to figure out if my boss can bring more value to my career. I want to learn from them. I ask questions, learn the process, and bring my ideas. And I have colleagues who I learn from too. I didn’t feel like I needed to be part of a mentor program. It’s all about choosing the right boss.Redis: What should young people or women who want to work in tech know about the industry?Mariana Aviv: First of all, it’s really fun to work in high tech if you choose the right company. At Redis, we enjoy our time together, laughing and joking around while still performing our Scrum ceremonies and working on our development tasks. You probably already know this, but the Israeli high tech industry is mostly dominated by men. I am happy to be part of a company that supports and promotes women, and this is reflected in the increased number of women that recently joined the R&D team. I feel accepted and valued, and I am proud to say that at Redis, diversity and inclusion are being taken seriously.You should also think strategically. Tech companies tend to invest in their employees—they provide a great system to promote growth and development. And as a mom, I enjoy working at companies that respect the work-life balance.Redis: You mentioned that you’re comfortable being the only woman in the room—was that something you had to get used to?Mariana Aviv: I grew up with two brothers so I always felt comfortable around men. I think you need to be ready to be part of the team and not take things too seriously. Be confident that you will be respected, and then people will respect you. If you have something to say, be confident and speak up."
14,https://redis.com/blog/why-devops-teams-love-redis-enterprise/,Top 5 Reasons Why DevOps Teams Love Redis Enterprise,"December 10, 2020",Ajeet Raina,"In many enterprises, DevOps teams are leading the push toward digital transformation. This journey often begins with application and infrastructure modernization efforts designed to unlock the potential of the digital economy and confront competition that’s only a click away. Application performance lags of just a few seconds can have enormous downstream impact on the customer experience and ultimately the business’ success. For example, if the Gap app doesn’t load instantly or doesn’t give inventory updates within a few seconds, many shoppers won’t hesitate to buy their khakis somewhere else. Simply put, the application’s data processing must be fast enough to keep up with consumers’ demand for real-time performance.According to an Allied Market Research report, the global NoSQL database market is estimated to reach $22.08 billion by 2026. An increase in unstructured data, demand for real-time data analytics and a surge in application development activities across the globe are the driving factors. Traditional relational databases are often too slow and simply can’t match today’s web-scale demands. They were designed with the intention of scaling vertically and on a single node.  Modern distributed, non-relational NoSQL databases were designed from the start to be multi-node and scale horizontally, allowing enterprises to be more agile.NoSQL databases are perfectly suited for the flexible data storage and manipulation needs of developers and operations teams. DevOps embraces a vision of enterprise technology that integrates traditionally siloed  development, operations, and quality assurance departments.Emphasizing communication and cooperation among the various components, DevOps teams focus on ways to automate and integrate development, quality testing, and production of applications and services  to reduce their time-to-market.DevOps teams strive to deploy and manage their databases just like they do application code. Changes to databases are recognized as just another code deployment to be managed, tested, automated, and improved with the same kind of seamless, robust, reliable methodologies applied to application code. Databases are now part of the continuous integration/continuous deployment (CI/CD) pipeline. If the DevOps pipeline doesn’t include the database, it becomes a bottleneck slowing delivery of new features. In fact, DevOps teams integrate databases not only in the development pipeline but also in the overall release pipeline.Forward-thinking DevOps teams designing applications, including the data layer, seek to satisfy a number of critical requirements:Redis has become a popular database choice due to its ease of implementation and exceptionally high performance, among other benefits. Most real-time data eventually lands in Redis because of its impressively low latency (less than 1 millisecond). The highest-performing NoSQL database, Redis delivers up to 8 times the throughput and up to 80% lower latency than other NoSQL databases. Redis has also been benchmarked at 1.5 million operations/second at sub-millisecond latencies while running on a single, modest cloud instance. In Datadog’s 2020 Container Report, Redis was the most-popular container image in Kubernetes StatefulSets.Redis fits very well into the DevOps model due to its ease of deployment, rigorous unit and functionality testing of core and supplementary Redis technology, and ease of automation through tools such as Docker, Ansible, and Puppet. Redis Enterprise is an enterprise-grade, distributed, in-memory NoSQL database server, fully compatible with open source Redis. Redis Enterprise extends open source Redis and delivers stable high performance, zero-downtime linear scaling and high availability. It is uniquely positioned to help DevOps teams meet their goals with less management toil and lower overhead.So what, exactly, are DevOps teams looking for in Redis Enterprise? Here are the five most important capabilities:High availability is the holy grail for most DevOps teams, and they often spend immense amounts of time and money to keep their applications running. But failing to recover from a database failure in a timely manner may result in losing data and millions of operations. Redis Enterprise offers uninterrupted high availability, completely transparent to the DevOps team, with diskless replication, instant failure detection, and single-digit-seconds failover across racks, zones, and geographies. It delivers high throughput and low latency even during cluster-change operations such as adding new nodes to the cluster, upgrading software, rebalancing, and re-sharding data.This unique combination of high-availability technologies guarantees four-nines (99.99%) uptime and five-nines (99.999%) uptime in Active-Active deployments of globally distributed databases. Active-Active geo distribution enables simultaneous read and write operations on the same dataset across multiple geographic locations. Using academically proven conflict-free replicated data types (CRDTs) technology, Redis Enterprise automatically resolves conflicting writes, without changing the way your application uses Redis. It enables a disaster-proof architecture for geo-distributed applications, while also delivering local latency.In the current technology landscape, the amount of choice available when it comes to platforms is simply astonishing.It’s practically impossible to take the time to investigate every option, so enterprises often stick to platforms that they’re comfortable with, even if they aren’t necessarily the best tools for the task. Part of successfully implementing DevOps involves choosing the best platforms for the unique context of your organization’s environment and the nature of your processes. That’s exactly why Redis Enterprise takes a platform-agnostic stance towards DevOps.Redis Enterprise software is available on Amazon’s AWS Marketplace, Google Cloud Marketplace, and the Microsoft Azure Marketplace with the ease of single-click deployment. It can be deployed on any virtual machine/bare-metal configuration that supports Linux/RHEL/CentOS operating systems. Redis Enterprise software combined with purpose-built Redis Enterprise Operator is designed to provide enterprise-grade capabilities such as the declarative deployment of clusters and databases within minutes, leveraging Infrastructure-as-Code (IaC); automated cluster-lifecycle management, including upgrade and recovery; high availability with seamless failover; Active-Active deployment across Kubernetes clusters and data persistence. The Redis Enterprise Kubernetes Operator can be deployed across multiple Kubernetes platforms, including RedHat OpenShift, Google Kubernetes Engine (GKE), VMware Tanzu Kubernetes Grid (formerly Enterprise PKS) as well as upstream Kubernetes. (Learn more about the principles we use to deploy Redis Enterprise on Kubernetes on our Why Kubernetes page.)Redis Enterprise provides a tightly integrated solution with the VMware Tanzu application service. Application developers can natively use the Redis Enterprise Service Broker for VMware Tanzu for launching and managing the lifecycle of their databases/cache systems, and operators can employ a variety of automation tools for managing their Redis deployments with enhanced monitoring capabilities, failure recovery, seamless migration between plans, and seamless software upgrades. (Learn more about the benefits of Redis Enterprise in your Tanzu environment in Pivotal’s Redis Enterprise for VMware Tanzu documentation,)Redis Enterprise is also a great way to bring more power and flexibility to the CI/CD process. Redis can help distributed development teams release new features safely and roll them back with minimal impact when required.. Learn more about how feature toggles, feature context and error logs can enhance your CI/CD process in this blog post.)3. Virtually unlimited scalability and high performanceIn today’s fast-paced development environment, a well-thought-out preparation strategy for scalability is a must to make the process smooth and easy. Many DevOps failures occur because the underlying infrastructure is unable to scale to meet demand, causing the application to crash. That’s a real issue, because scaling database solutions requires massive additional infrastructure investments as they accrue non-linear overhead in scaled-out environments.Linear scaling, which means that to get 2x the performance you need roughly 2x the infrastructure, 4x performance demands approximately 4x the infrastructure, and so on, is critical to enable DevOps teams to affordably keep up with fast-growing requirements. Made for DevOps environments, Redis Enterprise fuels businesses that want to rapidly deploy dynamic apps to millions of users at a time. (Learn more about linear scalability in Redis Enterprise here.)4. Global distribution (with Active-Active geo distribution)DevOps teams deploy applications that are increasingly built using microservices. These apps leverage a multitude of different component parts, with different approaches to infrastructure, hosted in a variety of different locations, consumed by people everywhere, and distributed on many different platforms.To support the responsiveness and scalability required by distributed applications, DevOps teams are increasingly looking to innovative database technologies such as geo-distributed data processing to deliver highly interactive, scalable, and low-latency geo-distributed apps. Many are choosing Redis Enterprise as a modern database that can be deployed globally yet provides local latencies for writes and reads, while simplifying resolution of conflicts and enabling strong eventual consistency for datasets.Whether your environment includes applications running on-premises, in a hybrid cloud, or on multiple clouds—or on a mix of all three—Redis Enterprise’s Active-Active geo distribution promotes high availability and low latency. With built-in active-active database technology based on CRDTs, Redis Enterprise helps DevOps teams achieve high performance across distributed datasets. This significantly reduces the development effort involved in building modern applications that deliver local latencies even when they need to span racks, clouds, or regions.5. Multi-tenant architectureIn a multi-tenant software architecture, a single instance of a software application(including database) serves multiple tenants . Each tenant’s data is isolated from other tenants sharing the application instance. This ensures data security and privacy for all tenants. When choosing a database for multi-tenant applications, developers have to strike a balance between customers’ need or desire for data isolation and a solution that scales quickly and affordably in response to growth or spikes in application traffic. Hence, to ensure complete isolation, the developer can allocate a separate database instance for each tenant; at the other extreme, to ensure maximum scalability, the developer can have all tenants share the same database instance.Most developers opt to use Redis Enterprise because it offers software multi-tenancy support. A single deployment of Redis Enterprise Software (often deployed as a cluster of nodes) serves hundreds of tenants. Each tenant has its own Redis database endpoint which is completely isolated from the other Redis databases. As shown in the diagram on the left, there are multiple databases like DB1 for storing JSON data, DB2 for search and filtering, DB3 for storing and analyzing time series and so on.Rapid deployment is a key element of a successful DevOps approach. Redis Enterprise provides a fast database that helps DevOps teams more efficiently build and operate applications. Redis’ easy-to-learn data structures and modules are flexible enough to cover a variety of use cases—and Redis Enterprise features such as persistent-memory storage and shared-nothing cluster architecture help reduce operational burden. That’s why DevOps teams love Redis as much as developers do.What are you waiting for? Get started with Redis Enterprise today, it’s free in the cloud or download the software now."
15,https://redis.com/blog/why-technical-expertise-is-so-important-in-enterprise-marketing/,Why Technical Expertise is So Important in Enterprise Marketing,"December 21, 2020",Haley Kim,"In our Advice to Our Younger Selves series, Redis women tech staffers share insights they wish they knew when they were starting their careers.Maygol Kananizadeh received her first computer as a high schooler. Like most kids, she first used it to play games, but soon she found herself digging into the control panel settings and eventually signed up for programming classes. That’s when she found her calling. “I had a feeling at that time that if I didn’t get involved with computers, I was going to miss out on cool things,” she said.After majoring in computer science both in undergrad and graduate school—attending California State University, Sacramento, for the latter—Maygol began her career interning (three times!) at Intel, before accepting a full time position as a software engineer at the chip giant. Eventually, she found herself interested in not just the nitty-gritty details of new products, but the strategy around sharing key features and benefits to customers. In 2019, she joined Redis as a Technical Product Marketing Manager, focusing on competitive analysis. Read on to learn how Maygol uses her technical chops to turbocharge her marketing role, the importance of giving kudos to your colleagues, and more.Redis: Tell us about your first full-time role at Intel.Maygol Kananizadeh: I was hired as a software engineer for a team evaluating the end-user experience for Intel client solutions, like phones, tablets, laptops, and desktops. It was kind of a dream job—you graduate from school, and then you get to play with everything you have ever wanted to use in your life. I worked closely with marketing for product launches. For example, if we had a new laptop coming to market, we would want to figure out why somebody would buy this particular laptop versus the previous generation, what kind of problems it solved, what kind of tasks they could accomplish with it. My role was finding all those nuggets of information, quantifying them, and presenting the results to top decision makers.It was a fun job, and I was fortunate to work on a great team. We had folks from different backgrounds, from pure marketing to pure engineering. I enjoyed seeing how all of them addressed problems in different ways, instead of just looking through one specific lens.Redis: How did you make your way to Redis and a marketing role?Maygol Kananizadeh: You probably know that engineers love to look at raw numbers. But many people don’t really care about all the millisecond improvements you made, they just want to know the bottom-line impact. So at Intel, it was interesting for me to see all the results of many tabs distilled to a video or a slideshow. I was also interested in understanding why a consumer would care about the results of all the work that went into building this new product. I wanted to ask, “OK, what’s the point of all those things?” That’s what I was really interested in, and that’s why I pivoted.Redis: What do you do on a day-to-day basis at Redis?Maygol Kananizadeh: As a technical product marketer, I actively seek information to try to understand how our solutions help customers solve their business challenges and provide value compared to similar ones in the market. A lot of this process is also about understanding how we can make our own products better, distinguishing best practices and lessons learned.Redis: What is the best part of your job?Maygol Kananizadeh: I really enjoy understanding people’s different points of view. If you gather people from different backgrounds around the same table and then come up with a solution, it’s going to be better for the end user because all these different viewpoints were considered. What I really enjoy at Redis is how generous people are with sharing their knowledge and their time. There are no silos. If I ask, they happily provide the information I need, which doesn’t happen everywhere. And this goes both ways—if I spend a week learning something and I can just teach you that in a day and then you get to take that and move forward with another project, at the end of the day, we all win. The company wins and the customer wins at the end of the day because a solution came out faster.Redis: What skills do people need who aspire to work in technical marketing?Maygol Kananizadeh: I think listening is a very important skill. I come across so many people who think they are good at listening, but they’re actually not actively listening—they’re just waiting for their turn to speak, so they didn’t hear anything that was said. I’m not saying that I’m good at listening, but every day, I try to get better. Just follow the conversation. Don’t be afraid to ask questions, go back and forth until there’s a common understanding.For technical marketing, presenting is also an important skill. When I was in school and in my technical job at Intel, you tend to talk with people with similar backgrounds. Everybody comes from one knowledge base, so there’s no confusion around terminology. If you move from a technical background to a more end-user facing role, though, the talking points that you make are going to be different. You’re not going to take a magic pill and become a great presenter overnight. Just practice, and observe people you think are good presenters. See what they do and take notes. And if you’re bored in a meeting, also take note of what that person is doing that lost your interest.Third, for any role, it’s important to give credit where credit is due, especially early in your career. Many people are going to help you to solve particular problems, tasks, or projects. They’re going to invest in you. They’re going to spend their own time trying to help you with something. When you get recognized for the work that you’ve done, be sure to give credit to people who went out of their way to contribute. I’m not saying that every time you recognize a laundry list of people, but when it’s appropriate.You also want to learn to accept compliments gracefully. If somebody recognizes your work and gives you a shout out, just say “thank you” and don’t downplay it. Don’t have a huge ego or be entitled, but be confident that you’ve done a good job.Redis: What else should young people who want to work in tech know about the industry?Maygol Kananizadeh: Never stop learning. We are in an industry where things change overnight. What you know today can be outdated in six months. Don’t think just because you got a degree from a fancy college or have X number of years of experience that you’re done learning, because that’s not the case.It can be scary because you get comfortable, especially if you’ve been in the same job for a few years. And if you change jobs or pivot your career, the amount of stuff that you don’t know can seem overwhelming. But don’t worry, people do it all the time. As people say, get comfortable with being uncomfortable. And don’t be afraid to ask for help. It’s surprising how much people are willing to help when you ask.Redis: Any advice to share with your younger self?Maygol Kananizadeh: As I get more experience, every day I’m reminded how important it is to build bridges, especially right now when everybody is working from home. We spend many hours at work, possibly more than we spend with our families. So devote 10 minutes to chatting with a coworker or building bridges with somebody in a completely different department, even if you think that you will never work on a project together. Maybe you form a friendship. Maybe you find someone that you think, “Oh, this is a person I can go hiking with or talk about random things,” and just make your workday a little bit more fun.Without that human connection, work can become very transactional and get old fast. Invest in building human relationships. You might become co-workers at a future company. You might become friends. Work is not just punch in, do your job, then punch out. Definitely take advantage of the knowledge being shared, the relationships being built.Redis: It can be hard to put yourself out there to meet new people at work. Do you have any tips for starting that connection?Maygol Kananizadeh: You might not know the person that you want to connect with, but is there somebody that can introduce you to them? Before COVID-19, to socialize and build on these types of relationships, I used to attend performance meetups. I realized that I’m not good at socializing, so I just kind of forced myself to go to these meetups. Talking about measuring performance could be an icebreaker between me and another person, and provided practice talking and socializing. I hope that one day we will all go back to those events.In the meantime, if you want to talk with someone, just message them. Don’t be intimidated—I guarantee that after eight months of sheltering in place, just about anyone will appreciate a “knock on the door.” Don’t be afraid to take that first step."
16,https://redis.com/blog/redis-6-2-the-community-edition-is-now-available/,Redis 6.2—the “Community Edition”—Is Now Available,"March 9, 2021",Itamar Haber,"Not long ago—just four days before the project’s 12th birthday—we published the general availability release of Redis 6.2. Despite being “just” another minor open source Redis release, version 6.2 represents a major milestone for the Redis open source software project.The new version introduces dozens of new commands and extensions to existing ones. Along with performance and memory optimizations, it is also supported by older operating systems. And it demonstrates the new management principles of the Redis project’s development and also reverts several historical decisions. While all these improvements are great, they’re not enough to make Redis 6.2 a major event. Yet the new version of Redis is actually a really big deal.That’s true because of the intense involvement of the Redis community, which is why I like referring to Redis 6.2 as the “Community Edition,” albeit not in the term’s usual sense.Allow me to explain.Unless you’ve been living under a rock, you probably already know that last year antirez (Redis creator Salvatore Sanfilippo) stepped back from maintaining the project and that we’ve adopted a new governance model. The project is now led by the Redis core team, which consists of Madelyn Olson from Amazon Web Services, Zhao Zhao from Alibaba Cloud, and Redis’ Yossi Gottlieb, Oran Agra, and myself.Following a brief period of reorientation, the core team unanimously agreed that the project’s community is Redis’ most important asset. Our collective awesomeness as Redis’ maintainers will only be as useful as the value we provide our members. The team firmly believes that the project’s long-term value and sustainability depend on open, transparent, and full-duplex communication and cooperation between the developers and the users. After all, the users are also developers and technical folks from all walks of life who care deeply about Redis.So the team sat down together online (even pre-pandemic, because we are distributed all over  the world) and started to plan our next steps. There were a myriad of tasks that needed attention: issues to triage, bugs to reproduce and squash, improvements and optimizations to implement, pull requests to review and merge, features to design and develop, wishes to grant and fulfill, and more. The TODO list was and is long and ambitious.To have a real chance to move Redis forward, we needed help from the larger Redis community. I am happy to report that more than 35 developers (not counting the core team) contributed their brain-cycles in the making of Redis 6.2. While it seems like we’re doing something right, we would love to have even more community members contribute to Redis.To help make the most of Redis community participation, we also spent time setting principles to guide our activities. The updated definition of the Redis release cycle captures some of these principles and sets the guidelines for release schedules, backward compatibility, and support. To adhere to this “formal obligation” and effectuate it, we assembled the contents of Redis 6.2 based on several criteria.From a technical standpoint, a minor release like this allows us to add new functionality as well as modify existing behaviors. So Redis 6.2 was the perfect vehicle to make these changes before we set out to work in earnest on Redis 7. The 6.2 release also gave us the opportunity to address issues important to the community, regardless of their date of creation and open/closed status. The release notes detail all the changes, so I’ll focus on a few key examples:One place where the community’s voice was heard loud and clear is the relaxation of requirements for compiling Redis (see PR #7707). In this regard, Redis 6.0 made a big change by requiring compiler support for C11 and atomics. Because these are available only with newer versions of operating systems, the move left many community members frustrated and unable to use Redis. The backlash feedback from the community (see issue #7509 and its duplicates) was all the motivation we needed to resolve it. Even better, it was the community, and specifically Wang Yuan (@ShooterIT on Twitter) in this case, who contributed the fix. Those kinds of community contributions led to much of what’s new Redis 6.2.Besides the usual fixes and optimizations, Redis 6.2 brings more than 25 new commands. These are mostly small-ish features requested over the years by a substantial number of members in order to improve the project’s usability. For one reason or another, these extensions never made it into Redis despite offering real value. For example, the oldest issue addressed by this release is the 11-year-long absence of ZUNION and ZINTER from Redis. In that case, resolving the issue wasn’t merely about API-completeness but mainly to address the itch that only these commands can scratch, namely returning the results rather than storing them (which is what the new commands’ existing counterparts, ZUNIONSTORE and ZINTERSTORE, do).There’s also the GETEX command, first requested in 2015, which reads a value while setting its time-to-live; the how-come-they-were-never-there-from-the-beginning HRANDFIELD and ZRANDMEMBER commands, which return random values from the Hash and Sorted Set data structures; the SMISMEMBER and ZMSCORE commands are for variadic aficionados who want to query multiple values with one call; the generic LMOVE command is designed to succeed RPOPLPUSH and complement the functionality with all conceivable manners of moving elements between the ends of two List data structures; and bounding box GEOSEARCH queries; to name but a few.The Streams API, which was introduced in Redis 5.0, also received attention and improvements. The new additions make it simpler and easier to use streams and range from exclusive range queries, through filtering pending messages by idle time, to trimming by minimal ID (the MINID strategy) and the XAUTOCLAIM command, which should bring joy to any consumer group user’s heart by allowing users to reclaim messages lost in processing with a single command.There’s no need for this post to be a walkthrough to Redis 6.2’s release notes. More importantly, this release speaks volumes about the Redis community’s commitment to the project and about the momentum of Redis’ development. As an optimist-turned-pessimist, I was cautious about predicting how the Redis community would respond to all the changes to the project over the past year. So I was pleasantly surprised to witness the level of community participation and the number of useful improvements that we were able to put into Redis 6.2.Big thanks for all the hard work put in by the community’s developers and the entire global Redis community. Now it is time to move on to creating the next release!Interested in learning more about Redis 6.2? We’ll be discussing the release and all things Redis at RedisConf 2021, our annual real-time data conference taking place virtually on April 20-21."
17,https://redis.com/blog/build-your-redis-expertise-at-redisconf-2021-with-four-new-training-courses/,Build Your Redis Expertise at RedisConf 2021 with Four New Training Courses,"March 25, 2021",Kyle Banker,"RedisConf is back this year from April 20–21! Join thousands of Redis enthusiasts in more than 60 breakout sessions and keynotes, challenge yourself in the $100K hackathon, and build your Redis expertise in four new (and free!) training courses:You can read about each of these courses below. Just know that all of the training courses will open on April 12, the week before RedisConf. Each training course will include a video-based, on-demand component plus live sessions with the instructor. The live sessions are optional, but attendance is highly recommended. You’ll also get access to a Discord channel, where you can ask questions, provide feedback, and network with fellow students.See the live session schedule on the RedisConf site.We’re committed to bringing you a quality training experience, whether you’re brand new to Redis or looking to expand your expertise, at no cost to you. Here’s a closer look at our four training courses:Instructors: Kurt Moeller and Elena KolevskaRedis is every developer’s favorite database , but managing Redis in production can become challenging as the footprint grows. In addition to the many deployment options (from Redis Cluster to Redis Sentinel to Redis Enterprise), you need to be able to address legitimate questions around capacity planning, monitoring, and security. All of these concerns fall under the umbrella of running Redis at scale.This course, taught by our esteemed technical enablement architects, will teach you how to effectively run Redis for high throughput while ensuring high availability and the best security, observability, and operability.Through a series of hands-on activities and demos, you’ll learn about Redis’  latest security features in open source Redis (ACLs) and Redis Enterprise (RBAC and LDAP), while also exploring options for real-time data monitoring, alerting, and disaster recovery.Instructor: Brian Sam-BoddenMore and more, Redis is becoming a core component in the Spring application stack. This is obviously true when it comes to caching, but it’s also the case with custom-purpose microservices.This course provides Spring developers with a comprehensive introduction to building full-fledged RESTful APIs with Redis and Spring.With more than 20 years of experience crafting software systems, Redis Developer Advocate Brian Sam-Bodden will guide you through building the backend API for the Redi2Read Bookstore, a modern ecommerce platform. Learn to develop the functional aspects of the store—like modelling the store’s domain and indexing—before exploring non-functional aspects of API development like caching, session caching, and rate-limiting.Instructor: DaShaun CarterRedis Partner Solution Architect DaShaun Carter will teach .NET developers of all experience levels how to build complete RESTful APIs with ASP.NET Core and the .NET Redis client libraries. Similar to the Spring course, you’ll be building the API for the back end of a bookstore. Explore domain modeling, indexing, caching, session storage, and more!Instructor: Simon PrickettBuild a social check-in backend with Redis, Node.js, and Express! This course, taught by Curriculum Software Engineer Simon Prickett, will amplify your hands-on knowledge of application development and data modeling. Under Simon’s instruction, you will learn how to build a back-end application using core Redis data structures and Redis modules like RedisBloom and RedisJSON. In turn, the app you build will power a social check-in mobile app, allowing mobile users to give star ratings for their experiences at various types of locations.Register for training on the RedisConf site, and stay tuned for the release of the live sessions schedule. And while you’re signing up, consider also enrolling in the hackathon, where you’ll have an opportunity to win one of 52 prizes totaling $100,000. Put pen to practice and implement some of the course learnings into your hackathon application!Kurt, Elena, Brian, DaShaun, and Simon have worked really hard to reimagine what an online conference training can be and to provide you with the best possible training experience. We hope to see you in class! Do sign up below, and if you have questions, don’t hesitate to connect with us on Discord."
18,https://redis.com/blog/4-key-findings-from-dti-survey/,4 Key Findings From Our Inaugural Redis Digital Transformation Index Report,"April 12, 2021",Maygol Kananizadeh,"Digital transformation has been an ongoing journey for many organizations across industries for the past few years. However, this transformation was significantly accelerated this past year due to COVID-19.Since people’s day to day lives have become more digital, their expectations—not surprisingly—have grown much more real-time in nature. And having access to real-time data is a key component for all organizations to ensure they can deliver the best experience to their users.As the builders of a real-time data platform, we find ourselves at the center of many digital transformation projects. We believe that for organizations to be successful in their journey, they need to have both a digital mindset and the right toolset.Today, we are happy to announce the release of the first Redis Digital Transformation Index e-book. In this report, we are introducing a new composite index to highlight the maturity level of 550 AWS re:Invent survey respondents with respect to their use of technologies such as the cloud, microservices, containers, and NoSQL databases. We asked respondents how prevalent each of these technologies are in their stack and their plans for the future.Furthermore, the survey covers other topics such as caching strategies, types of databases, tolerance for downtime, factors for choosing a database and more. You can read the whole report, but read on for a quick rundown of the most interesting results:For the purposes of this survey, we created a composite metric called the Redis Digital Transformation Index (DTI). We combined participants’ descriptions of their strategies toward the cloud, microservices, containers, and NoSQL databases. The adoption of these technologies is a good indicator of an organization’s progress in its digital transformation journey.By looking at each component of the DTI, the data shows cloud strategies to be the most mature with an average of 3.64 out of 5. NoSQL database strategies came in last at 2.45, with the composite DTI for all participants at 3.24.In a world where we are more connected than ever, limited downtime can range from an inconvenience to hugely disruptive. So we asked our respondents to rank their tolerance regarding downtime in cache and database. Understandably, responses varied across the industries, but on average, 52% stated that they can’t afford any downtime either in database or cache. 38% responded that while they can tolerate limited downtime in cache, downtime would be unacceptable for their database. Finally, 11% said they can handle limited downtime since their application isn’t customer-facing.Caching is a widely used approach to improve an application’s performance and scalability. Our survey shows that 78% of respondents are either currently using caching or plan to do so soon. For the majority of respondents that use relational databases in their technology stack, having a highly performant and scalable caching layer is simply non-negotiable.When asked how many types of databases respondents use in their organization, the average number is two. Relational databases have been a dominant technology for decades, so it’s not a shock to see them take first place (55%). NoSQL databases are much younger in comparison. We were pleasantly surprised to see key-value databases coming in a close second at 52%. This shows that NoSQL databases are both widely adopted and an essential component of the majority of organizations’ data management systems.This survey demonstrates that organizations across industries are adopting a multitude of technologies and strategies to achieve their digital transformation goals. The full report contains details regarding DTI across industries, top factors when choosing a database per industry, popular languages for developing database applications, and whether users of tabs or spaces are the early adopters of new technologies.Please take a minute to fill out a 4-question survey about digital transformation. If you didn’t get a chance to participate, make sure to attend RedisConf 2021 (April 20-21) when we will be conducting the second edition of this survey."
19,https://redis.com/blog/rewards-at-redisconf-2021/,Rake in the Rewards at RedisConf 2021,"April 14, 2021",Britiana Andrade,"From April 20-21, join us at RedisConf 2021 for more than 60 on-demand breakout sessions, four brand-new, free training courses, and a $100K “Build on Redis” hackathon. You’ll also want to tune into our live keynotes and fireside chats, where we’ll discuss the latest Redis developments, our Redis Enterprise collaborations with our partners, and insights from customers who are making real-time data a part of their solutions.While working hard at learning, don’t forget to have some fun! We’re giving away more than $25,000 in prizes to more than 500 attendees—all you need to do is earn points by doing tasks like completing your profile, submitting surveys, and finding codes. Here’s a look at what, and how, you can win, along with tips on how to make the most of your RedisConf experience:We’re awarding gift cards to the top 500 participants. Each person will be ranked on a real-time leaderboard based on points scored. Once the clock strikes 11:59 p.m. PT on April 21, each participant’s ranking will be cemented and point totals will be locked. Here’s what you could win:Prize drawsThe rewards don’t stop with gift cards—you are eligible to win major prizes through attendance alone. On each day of the conference we are giving away an iPhone 12 (256 GB, because we know you love your data) and a DGI Mini 2 Quadcopter for a total of four prize draws. Win the iPhone just by attending the conference, and get entered in the drone draw by attending the live keynotes. Attending each conference day and each keynote will add an entry to the draw, giving you a chance to win a prize both days.Participation challengesThis isn’t basketball, but scoring points can make you a winner. One way to earn points is by participating in the following challenges:Participating can pay big dividends—if you plan on attending the keynotes, from 9–11 a.m. PT on both days, or specific breakout sessions, take the time to fill out a survey afterwards. For just a few minutes of your time, you can boost your rankings on the leaderboard!Scavenger huntThe other way you can earn points is through the scavenger hunt. The goal of this challenge is to find codes, and there are two primary ways to obtain them:There are multiple ways you can reach an expert, like in one-on-one text chats, live discussion room office hours, and the Discord community support platform. A code from a Redis expert will award you 50 points, while a code from a product flag will earn you 10. Here are the different flags you can find:The conference has been designed to optimize how attendees view content and interact with peers and instructors. The experience is mobile-friendly, so you can even take RedisConf on the go.Connecting with othersNetwork with like-minded peers in break rooms or create a dialogue in discussion rooms dedicated to popular topics. Connect directly with a fellow attendee through direct messages, or by requesting a phone or video chat.Engaging with contentMany of you are multitaskers—so we’ve found a way to enable you to watch sessions while roaming the platform! Click on the pop-out video tab on the bottom left of the session video to continue watching while you explore. You will also be able to take notes on each session within the platform, which you can then download and archive on your personal device. Also, keep in mind that engaging with the content can help you earn points and propel your ranking in the leaderboard.RedisConf was designed with fun and flexibility in mind for Redis enthusiasts all over the world.  There are many prizes to be won, not only from participating in the keynote presentations, breakout sessions, and training courses, but also from building an application during the “Build on Redis” hackathon (we’re giving away $100k total in prizes for winning projects!). Rediscover the power of real-time data at RedisConf 2021 and reward yourself by registering!"
20,https://redis.com/blog/developers-guide-redisconf-2021/,A Developer’s Guide to RedisConf 2021,"May 12, 2021",Kyle Banker,"We just wrapped a successful RedisConf 2021, where thousands of Redis enthusiasts gathered to rediscover the power of real-time data. This year’s agenda included 60+ breakout sessions, four comprehensive training courses, and several keynotes and interviews that explored Redis best practices, pro tips, and new features. Among the many great stories and presentations, we wanted to highlight a few that might be of interest to the developer community at large. Whether you’re brand new to Redis or already an expert, there’s something for everyone. Here’s our guide to RedisConf 2021:If you don’t know much about Redis and just want to know what it’s capable of, you should hear from Spleet Software Engineer Michael Owolabi in his session, Yes! Redis can do that. Michael provides an enthusiastic take on Redis, starting with caching before moving on to full-text search use cases and the ins and outs of persistence.With an in-depth guide to how he found Redis, Michael’s examination of Redis helped him realize its capabilities beyond the cache, and ultimately how it can be used as a general, high-performance in-memory data store.Redis remains committed to the development of open source Redis. And while we directly employ many of the core Redis contributors, we’re also encouraging the larger community to contribute more and more to core Redis.Itamar Haber, Technology Evangelist for Redis and a Redis core team member, presented Redis 6.2: For the community, by the community. This talk covers Redis’ new community-driven governance model and the formation of the core team. Now the emphasis is on community—since July 2020, we’ve seen an 86% increase in unique contributors!Itamar’s talk also covers the new features in Redis 6.2. These include ACLs for Pub/Sub, incremental eviction, and numerous improvements to the core Redis data structures (e.g., auto-claiming pending entries in streams and setting expiration on GET).If you want to know about the future of Redis, then you should check out Redis 7.0 and beyond. In this session, Meir Shpilraien, Software Architect at Redis, discusses the design of Redis Functions, which will provide a language-agnostic programmable interface (i.e., JavaScript support!) to Redis. Yossi Gottlieb, a Redis core team member and longtime Redis Chief Architect, also presents the latest on RedisRaft, a dual-licensed Redis module that provides strong consistency for Redis.Another core team member, AWS Senior Software Development Engineer Madelyn Olson, presented Better together: How AWS is helping build a stronger open source community. In this session, Madelyn reflects with her colleagues Carl Lerche and Matt Asay on their work with core Redis and on the strategic importance of open source Redis to AWS and its customers.RedisConf 2021 had Node.js developers covered. If you’re new to Redis, or just want to see how to build a complete and highly responsive application with it, see The Node.js Redis Crash Course. In this new course, built specifically for RedisConf, Senior Curriculum Software Engineer Simon Prickett teaches you how to build a social check-in app using Express, Redis, and the Redis modules. Not to be missed!For an enlightening take on enhancing the performance of Node.js apps on Redis, see Solving Head-Of-Line blocking with auto pipelining. In this session, NearForm Technical Director Matteo Collina describes the new auto-pipelining features of ioredis and how it improved some of his team’s  benchmarks by 35-50%.This year, one of our goals for RedisConf was to meaningfully support the Spring and Java community, and for that, we could not have found a better expert than Josh Long, Spring Developer Advocate at VMWare.In his talk, An application framework worthy of Redis, Josh quickly takes you through the process of building a Spring Boot application powered entirely by Redis. Brian Sam-Bodden, Developer Advocate for Java at Redis, joins Josh in this whirlwind talk to add support for RediSearch to the stack.If you want something more comprehensive, see Brian’s course, Redis and Spring: Building High Performance REST APIs. This training course covers everything you need to know to be successful with Redis and Spring Boot and Spring Data Redis. Brian teaches the basics of domain modeling and caching, but quickly progresses to implementing:This year, we put together a brand new training course called Running Redis at Scale. This course considers scaling broadly, including discussions of security and observability. But the course focuses primarily on classic scaling concerns such as high availability and sharding. Our technical enablement architects, Elena Kolevska and Kurt Moeller, present these topics for open source Redis and include numerous examples and labs using Redis Sentinel and Redis cluster.Elena and Kurt also briefly discuss Redis Enterprise Software and how it contrasts with the features of open source Redis.If you just want a Redis at scale war story, check out Martin Perez’s session Look ma, no database! Here, Martin explains how Cisco Webex uses Redis as a backend to a billion-request-per-week service.More and more, Redis is coming to the fore as a high-performance online feature store. In Redis as an online feature store, Redis’ Chief Business Development Officer Taimur Rashid and Software Engineer Dvir Dukhan show how you can use Redis to manage features and increase performance at the model building and inferencing stages.And for a real world example, see Redis as a scalable feature store. In this session, ML Platform Engineer Arbaz Khan and Technical Lead Manager Zohaib Hassan of DoorDash discuss how they use Redis to manage billions of features and serve them with ultra-low latency.RedisConf 2021 demonstrated the continued growth of Redis as a core component in every developer’s toolset. If you haven’t had a chance to check out this year’s 60+ sessions, we hope that the recommendations here provide a useful entry point. All content is available on-demand until May 20, so there’s still time to log in!As always, we love hearing your stories and helping where we can. If you have any thoughts, ideas, or questions about Redis today, consider stopping by our Redis Discord server and saying hello! And thanks again to the many speakers, community members, and Redis employees who made this year’s conference one of the best yet."
21,https://redis.com/blog/redisconf-2021-sessions-you-should-watch/,Here’s Which RedisConf 2021 Sessions You Should Watch,"May 27, 2021",Udi Gotlieb,"Every year the Redis community gathers for RedisConf, and this year was all about rediscovering the power of real-time data. More than 12,000 developers, architects, and business and technology leaders from 122 countries registered for live keynotes, fireside chats, and 60+ breakout sessions. As of May 20, all RedisConf 2021 sessions can be viewed on our YouTube channel!With many sessions covering unique use cases, we took a vote and compiled the top breakout sessions for 10 different superlatives. Here are the winners:The SitePro duo of CEO Aaron Phillips and Director of Technology Dustin Brown elicit great chemistry and infectious enthusiasm to detail how the company’s mission-critical IoT platform uses the RedisTimeSeries module in Azure to ensure congestion-free data flow. Aaron and Dustin begin the session discussing SitePro’s inception as an end-to-end metering system for oil and gas companies. They explain how the company started leveraging RedisTimeSeries after significant growth to give “historical context to real-time data.”The session never gets boring, as Aaron and Dustin play off each other’s points, add familial metaphors for context, and explain the tooling and features utilized with RedisTimeSeries. Give this session a look for some laughter and to learn how RedisTimeSeries can empower real-time and historical data at scale.Chris Oyler’s presentation slides go hand-in-hand with his humor and ability to visually showcase real-time event capture at scale. Less than 30 seconds into the presentation, Chris casually alludes to his startup, UserLeap, to a circus and himself to a clown. He keeps a calm, casual demeanor throughout, even while his slides transition from graphics of the UserLeap API to a picture of Sesame Street (which represents the anatomy of an event).But even while his session is hilarious, what stands out is his ability to maintain focus on the topic at hand. He details how UserLeap did not have a need for a relational database, yet needed high-write and high-read throughput—compelling the company to go with Redis Cluster and utilize HA mode with auto-failover. Later in the session, Chris shows a two-day graph comparison of UserLeap’s average latency, noting that there was an average positive change of about 100 milliseconds in response time after his company adopted Redis. Chris’ ability to relate the visuals in his slides to his presentation—and maintain a sense of humor—makes this session a winner.Masoud Koleini is not at all new to the IoT world, but he does take home the panel’s Best Newcomer award for his premiere RedisConf 2021 breakout session, High-performance edge inference with RedisAI. As data analysis on the edge continues to grow across industries, Masoud highlights the challenges of running applications on devices with limited computational power. He then details why Redis was chosen—because its low memory footprint enables machine learning applications to run on small devices.His breakout session is a case study on edge inferencing, with an explanation of how RedisAI’s deep inference capabilities help analyze tensor data, as well as best practices when running the RedisAI module on Arm machines. If you are curious to how RedisAI can run deep learning data inference on your machine learning application, give this video a view.Eric Brandes wins this award for his 20-minute walkthrough of addressing the common misconception that Redis cannot be a primary database. He gives a MythBusters-like presentation, explaining his firsthand experience in building a user-monitoring performance tool, Request Metrics, with Redis as the platform’s primary database. He then addresses the reasoning behind choosing Redis, like its low operational complexity, ability to use simplified architecture, low cost, and flexibility with durability requirements.In a small window of time, Eric does a tremendous job guiding you through the nuts and bolts of his developer operations, from explaining Request Metrics’ operational setup to its scaling strategy. If you are a newcomer to Redis and are deciding how to implement the NoSQL database into your tech stack, take the time to watch Crazy like a fox: Redis as your primary database.This is a session to watch if you have a strong background using Redis. In this breakout session, Jim Brunner explores interesting and recurring issues that come with using Amazon ElastiCache for Redis. As a developer on the ElastiCache team, Brunner has worked exclusively on Redis internal development for the Amazon platform. He dives into developer nightmares like maxmemory overuse, unparamaterized LUA, and the connection storm while breaking down methods to avoid them.Jim approaches each topic in a systematic way, addressing the subject of each use case, the scenario that occurs, the outcome, and lastly, the solution to mitigate the problem. While these use cases are in the “extreme,” this session is a useful asset for developers who want to mitigate potential issues with their cache.Digital transformation has become a critical focus area for banks to provide a successful user experience. Adi Shtatfeld of Redis introduces Bank Hapoalim’s Dani Sela and Yaniv Yair to deliver a firsthand look at how Redis Enterprise improved availability and customer satisfaction for the largest bank in Israel.With more than million customers while earning $14 billion in revenue each year, Bank Hapoalim was hit with a pandemic reality—it realized it needed to configure its banking infrastructure to cope with the digital demands of its customers. The massive number of requests on Bank Hapoalim’s website and mobile app began to create server overloads on their MF backend systems, causing slower latency times as well as timeouts for customers. To accommodate its infrastructure, Bank Hapoalim turned to Redis Enterprise caching to decrease requests to the bank’s mainframe and relieve the resource shortage—in turn saving mass expenses.SmartSim is a library dedicated to accelerating the convergence of AI, analytics, and data science with numerical simulation models. The data science libraries and simulations were not originally connected and did not traditionally communicate with one another in real-time.Simulations would take time, ranging from hours to weeks, to fully compute and be completed, while the data library could only be analyzed after the simulation’s completion. Using RedisAI, SmartSim can now interact with simulations at runtime with fast, in-memory storage at scale. The module also enabled online inference, training, and analysis of SmartSim’s simulations without using the filesystem and while allowing the team to view data online. View this session to see a highly granular presentation of leveraging RedisAI.With more than 40 machine learning models and an application that makes two million predictions per second, DoorDash takes home the prize for Best AI Story. ML Platform Engineer Arbaz Khan and Technical Lead Manager Zohaib Hassan explain how the company uses Redis as a scalable feature store to provide a batch of input variables for making real-time predictions. Compared to a disk-based database, Redis is able to provide a cheap, fast, and simple in-memory data store to limit storage overhead and provide data locality for DoorDash.This session takes home this superlative category because of how in-depth the presenters explain their company’s use case of Redis. They pinpoint their reasoning behind choosing Redis to scale ML operations, supplement their decisions with visuals and data results, and discuss the next steps of this ongoing project. Watch this breakout session to understand how Redis can fit in as a scalable feature store for machine learning platforms.When Doomsday is on the horizon—whenever that may be—you’d want knowledge from experts in multiple fields, right? With that in mind, this superlative goes to Ayaka Shinozaki of Techspert.io for her session Building a knowledge graph using RedisGraph. Techspert.io is a startup that uses AI-driven search technology to match world-leading experts with organizations in need of their specialist insight. Compiling billions of publicly available data points across the internet, Techspert.io needed a database solution to record relationships between experts, institutions, and semantic concepts—and then give a real-time overview of the knowledge landscape. That’s where RedisGraph came in.RedisGraph enables Techspert.io to connect facts such as location, field of expertise, and name with one another to create a defined relationship. Utilizing vertices and edges to represent facts, Techspert.io uses RedisGraph’s cost-effective full-text search querying to extract information about relationships through the knowledge graph. This breakout session is a must-see for developers wanting to create a knowledge graph database, as well as for developers seeking insight to prepare for a pending apocalypse.Redis has plenty of use cases, but our team crowned Dr. Alexander Mikhalev’s session, Using the Redis ecosystem to build NLP-based services, for its uniqueness and creativity. Dr. Alexander began building a natural language processing product during the RedisConf 2020 hackathon to help medical professionals navigate through medical literature. He envisioned creating a question answering system, or a chatbot, through machine learning, yet it had to be fast enough to exceed user expectations while also being highly available. Dr. Alexander realized he needed to build a knowledge graph that could pinpoint long, complex queries and a powerful engine to process fast API response times—so he developed a NLP pipeline with RedisGears, RedisAI, and RedisGraph all embedded into its core.This breakout session clearly outlines the product’s API framework and how each module is configured, giving the viewer a thorough presentation into the interconnectedness of Redis modules. Watch this RedisConf 2021 session for inspiration on how Redis can help you create an application with innovation at its heart.These RedisConf 2021 sessions caught our eye, but there are many more to choose from on the platform. If you watched a session that helped you rediscover the power of real-time data, feel free to share with us on Twitter with the hashtag #RedisConf2021 or join our Redis Discord server for further community support."
22,https://redis.com/blog/dbless-architecture-and-why-its-the-future/,What Databaseless (DBLess) Architecture Is—and Why It’s the Future,"July 8, 2021",Raja Rao,"You may be wondering: Why is a database company like Redis talking about Databaseless (DBLess) architecture? And what is it? That’s natural, but before we get into the details, let’s look at the new way of thinking behind this brand new architecture.To do that, I want to take a quick detour and talk about something called the First Principles thinking. It forces you to think for yourself and not just follow the tradition, but to question everything.The gist of First Principles is that unless you’re looking at a law of nature, such as the laws of gravity, every system or concept is man-made and can have inefficiencies. To add to that, the passage of time or technological innovation could have proven the concept obsolete. This means you should regularly question the traditional system or concepts and see if you can build something better.To find those inefficiencies, you must take a systematic and scientific approach that breaks them into smaller pieces to get to the fundamental truths. Then, see if the passage of time or invention of new technology has made any of these pieces obsolete. If so, it appears you have a chance to build a newer and better system.Fundamentally, whether people know it or not, the majority of the social, technical, and economic changes have happened because of people thinking with First Principles and challenging the tradition.In the above video, Elon Musk explains how he looked into the raw materials of a battery and was able to reduce the cost of it from $700 to $70.Let’s take another example of the first principle of thinking because it’s easier to explain: Gasoline cars vs. Electric cars.We all know that you can use an electric battery to run a car.But while a gasoline car, does have a battery, it’s not used for running the car. It uses batteries for starting the engine, A/C, audio systems, lights, sensors, locks, and so on, but not for running the car. Instead, it relies on an internal combustion engine (ICE) to run the car.It turns out, ICE cars are highly inefficient. Only 16% to 25% of the power that’s generated actually makes it into the wheels. On the other hand, electric vehicles provide about 90% power to the wheels! EVs also have major advantages when it comes to the environment, repair costs, and so on.If you are looking at this from a First Principles perspective, even though most cars that are built even today are gasoline cars, the fundamental truth is that they use an inefficient system.Now if you look at an electric car, it removes this inefficiency to build a new type of car. In this case, it simply gets rid of the complex and highly inefficient engine and replaces it with a large battery and a motor to directly spin the wheels.So you can see how First Principles thinking leads to identifying inefficiencies and creating a newer, better system.Now looking at these ideas, if you were to start a new car company, would you build gasoline cars or electric cars?Let’s shift gears and transition to the database world to see if we can apply the same First Principles thinking to that sector.Let’s first look at traditional architecture.In traditional architecture, you have a primary database (Postgres, MongoDB, etc.) and a secondary database or cache (e.g. Redis or Memcached). The primary DB is used to store all the data and support CRUD operations. The caching DB is used for caching, session storage, rate-limiting, IP-whitelisting, Pub/Sub, queuing, and many other things.If you think about it, even when there’s a cache-hit, we’re also using the secondary DB for part of the CRUD operations. And yet we’re still not fully utilizing it as a primary database.Does this remind you of the issue with gasoline cars? Just like they carry a battery to power numerous things except moving the car, the traditional architectures use things like Redis for everything except as the main DB.Do you see the similarities?What if we use the First Principles thinking to do what the electric car did? Similar to how EVs got rid of the engine, what if we get rid of the slow and inefficient primary database and simply use the cache DB as the main database?Say hello to Databaseless (DBLess) architecture.In this architecture, you get rid of the primary DB, hence the name DBLess. Instead, you use the formerly secondary/cache DB as the new primary database.Let’s imagine that we started using Redis or other similar caching databases as a primary database and got rid of the primary DB (such as Postgres, Mysql, MongoDB, etc) completely.Important Note: This is just an architectural discussion. The DBLess architecture is not a proprietary architecture that’s limited to Redis or Redis Enterprise, and it would work with any Redis-like system. Also, remember that Redis is an OSS project, so you can build this yourself or on any other Redis-hosted cloud provider.Yes, definitely. As you can imagine, we work with thousands of customers on a daily basis. And although Redis is still primarily used as a secondary database, we have started to see this new DBLess architecture emerge over the last couple of years. It started to get more momentum as Redis itself became more feature-rich, powerful, and as more people found success. Many who aren’t even our customers, like Request Metrics, have built their entire startup on this architecture and find it incredibly successful.Now that you know it’s real, let’s see what makes this possible.Let’s use Redis Enterprise as an example and compare it with the traditional primary DBs.As you can see, the short answer is that it fairs really well and, in fact, can even be better than some traditional Primary DBs.As a reminder, you can use this architecture by simply using Redis OSS or other Redis competitors. You just need to come up with a similar comparison table to see how well it works.You are correct. It started off as a cache store about a decade ago and is still great for that purpose.However, Redis and Redis Enterprise have expanded significantly over the years by incorporating virtually all the features of traditional DBs—and by having a module ecosystem that runs natively with core Redis.Take RedisJSON (10x faster than the market leader). You can use it and essentially have a real-time document-like database, or use the RediSearch module (4x to 100x faster) and implement real-time full-text searches like Elastic Search or Algolia.And you can use any of these modules as part of Redis OSS and host it yourself.We strongly believe this architecture is the future in the same sense that electric cars are the future. And even though electric cars make up less than 1% of all cars, they are the future. We think that this is just a natural evolution of technology. Seeing the success of many of our customers, we think that the more people learn about it, the more people will try and adopt this.It’s called DBLess because we are getting rid of the primary DB and we think it’s a fun and quirky name along the lines of “stateless,” “serverless,” “NoSQL,” and “No Software.”If you are one of the hundreds of thousands of Redis users, you’re in luck—you can do a quick proof-of-concept today!  We aren’t asking you to add something new, but instead get rid of something that’s inefficient.And here’s how to do it.If you are building a new system or a new feature, then it’s straightforward just start using this architecture—or at least do a Proof-of-Concept and see if it works for you.If you already have a primary DB, then do what many of our customers do and use a hybrid approach. They continue to use the traditional architecture, but migrate parts of the product into the newer architecture, such as places where they’re already relying heavily on Redis or newer features. And slowly but surely, they migrate all the features until they’re completely migrated over.We’re asking you to be a First Principles thinker. Just because something is traditionally used, it doesn’t mean it’s perfect and that you should blindly follow it. We’re asking you to question traditional thinking, critically examine it, and try alternatives. And when you do, you may just invent something useful for yourself and others.DBLess architecture provides an alternative to traditional thinking. Instead of wondering if it’ll work or not, try a Proof-of-Concept. It might just surprise you!"
23,https://redis.com/blog/3-real-life-apps-built-with-redis-data-source-for-grafana/,3 Real-Life Apps Built with Redis Data Source for Grafana,"September 28, 2020",Alexey Smolyanyy and Mikhail Volkov,"Update 3/29/21: https://coronavirusapi.com/ has recently closed free access to its data, making it only available for registered first-responders. While this unfortunately means the coronavirus case visualization project demo won’t work anymore, our hints will still be useful for many other implementations when you need to display RedisTimeSeries data on the Grafana Worldmap panel.In previous posts, we introduced and showed how to use the new Redis Data Source for Grafana:In this post, we want to explore some real-life examples designed to give you ideas about the many interesting datasets you can store in Redis and visualize in Grafana. We’ll cover three different applications:Do I really think that the world needs yet another weather dashboard? Yes, I do!Sure, most of the time I’m not much of a weather geek. I often don’t really care what the weather is like. And even when I do, the best weather dashboard is usually just a window looking out of my house. But sometimes, perhaps when I find some spare time to go out to take some pictures or fly my drone, my inner weather geek awakens.The big question, of course, is where should I go? There are a lot of interesting locations within a couple of hours drive from my house in New Jersey, and the weather can be very different from one to another. What I really want is a dashboard that shows the weather in a dozen locations on one screen, so I can compare conditions at a glance. Critically, I don’t just want to compare locations near each other.Just as important, I love working with numbers. I know that landscape photography works best when the amount of cloud cover is somewhere between 20% – 50%. For flying my drone, wind speeds below 10 mph are ideal, 10 – 20 mph is tough, and anything above 20 mph can be dangerous. So a cute weather icon like this doesn’t do me much good:With that in mind, here is how I built a weather dashboard using Redis.Fortunately, raw weather data is now widely available via a broad selection of weather APIs, many of them free (with some limitations). I chose to use OpenWeatherMap API, because its free plan gives me everything I need: a 48-hour hourly forecast, a 7-day daily forecast, and current conditions.I wrote a simple Python script to pull the data from the API and put hourly/daily forecasts into the Redis database using the RedisTimeSeries module and current conditions into another set of time series for history.My weather dashboard follows my two mandatory dashboard rules:The top section of the dashboard shows the current conditions for one location, which can be the primary or favorite or just selected from the list of available places (there is a Grafana template variable selection list in the top left corner of the dashboard). It also displays the periods of time good for the particular activity at that location. Grafana allows me to highlight low/high zones for temperature, dangerous zones for wind speed, display the degree of cloudiness, and mark periods of daytime and nighttime.The bottom section group locations by activity, making it easy to compare them. The bigger the circle, the more hours are good for a certain activity in a certain location.I love traveling, but 2020 made millions of people stay home for months on end (I hope not forever!). Now, as many states gradually start to reopen, it’s very important to travel smart and understand the COVID-19 situation in both your origin and destination locations. That means paying attention not only to the current picture but also the longer-term trends.This is what my Grafana U.S. States Coronavirus Dashboard shows. It is based on the Coronavirus API data, loaded into the RedisTimeSeries module.Besides standard graph representation, data is also drawn on the map. The Redis Data Source plug-in is not officially supported by the Grafana Worldmap panel, but you can make it work in two simple steps:Step 1: Apply the transformation “Labels to fields” to the time-series output result:Step 2: Change the map parameter “Location Data” to table mode and map the relevant fields:These two steps will allow you to stick the time-series value to the relevant point on the map, which is set by the “geohash” label of RedisTimeSeries. The label “state” is used to display the name of the location on the map.I am a big fan of Redis Streams, a new data type introduced in Redis 5.0, and I was looking for a fast and simple solution to monitor queues for data processing. While working on the Redis Data Source, our team started to explore RedisGears—a dynamic framework that lets developers write and execute functions that implement data flows in Redis while abstracting away the data’s distribution and deployment—for another project and we decided to use them together for this data-pipeline demo for a pop-up store.RedisGears supports several types of readers that operate on different types of input data. To watch streams messages, we used StreamReader in the event mode. In this mode, the reader is executed in response to events generated by new messages added to the stream.StreamReader accepts multiple arguments that control how it is triggered. In our case, we want the reader to be triggered every 5 seconds or after it receives 10,000 messages. The last option, trimStream, specifies not to trim the stream after execution.We used RedisTimeSeries to store the number of incoming messages and the queue-size samples. As explained in the introductory blog post on the Redis Data Source Plug-in for Grafana, time series can be easily visualized in Grafana.To demonstrate how Redis Streams, RedisTimeSeries, RedisGears, and Redis Data Source can work together, we created the Pop-up store demo with a dynamic dashboard:This Grafana dashboard displays:Please take a look at the GitHub repository for this project to see how we generated the load, the RedisGears scripts we used, and how to configure Redis Data Source to query RedisTimeSeries data using the TS.RANGE command.These three real-life scenarios demonstrate ways to use Redis Data Source for Grafana with various Redis modules. We hope they inspire you to use this technology and build your own applications.You’ll be in good company. Since the Redis Data Source plug-in was published in the Grafana repository it has already been downloaded more than 10,000 times, included in popular community plug-ins that can improve your Grafana dashboard, added support for Redis cluster, Sentinel, Unix socket, and access control lists (ACLs):But that’s not all. Stay tuned to the Redis Tech blog to learn about an exciting project combining Grafana streaming capabilities with interactivity to take Grafana beyond observability with a redis-cli panel."
24,https://redis.com/blog/datanami-names-redis-streams-best-streaming-analytics-solution-two-years-running/,Datanami Names Redis Streams Best Streaming Analytics Solution Two Years Running,"October 23, 2020",Steve Naventi,"At Redis, we’re constantly working to create the best solutions to modernize your data layer to drive business success. So it’s always nice when folks recognize our efforts with awards. That’s why we want to send out a huge “thank you” to the editors and readers of Datanami for honoring Redis and Redis in their 2020 Datanami Readers’ and Editors’ Choice Awards!For the second year in a row, Redis Streams has won the Datanami Editor’s Choice Award for Best Big Data Product or Technology: Streaming Analytics! And that’s not all! Open source Redis earned a Reader’s Choice Award as one of the Top 5 Open Source Projects to Watch, while Redis won a Reader’s Choice Award as one of the Top 5 Vendors to Watch.Redis Streams is a data structure built into open source Redis that helps users manage data streams by creating a channel that connects producers and a variety of consumers with different data needs. The fastest streams engine on the market, Redis Streams helps manage the collection of large volumes of unstructured data at high velocity—supporting faster business decision-making and accelerating the arrival of zero-latency apps and services.Learn more about Redis Streams here!Datanami is a news portal dedicated to providing insight, analysis, and up-to-the-minute information about emerging trends and solutions in big data. The Datanami Readers’ Choice and Editors’ Choice Awards recognize companies, products, and projects that have shaped the data community over the past year. Datanami readers nominate and vote on the awards, so we’re super excited to have been recognized for multiple awards this year.The Datanami awards represent yet another accolade for Redis. In June, Redis was named the Most Loved Database for the fourth consecutive year in Stack Overflow’s Annual Developer Survey. And just a few weeks ago ago, Redis was recognized by InfoWorld’s editors in their BOSSIE (best of open source) awards based on its “powerful blend of speed, resilience, scalability, and flexibility,” in addition to improvements introduced in Redis 6, “most importantly threaded I/O, which accounts for a 2x improvement in speed.”We also closed our Series F funding round in August, boosting our total funding raised to $246 million at a company valuation of more than $1 billion. And we signed an agreement with Microsoft to offer Redis Enterprise as an integrated managed service on Microsoft Azure Cache—the public preview is launching this fall. (That follows on the heels of Redis Enterprise becoming available through the Google Cloud Marketplace late last year.) And don’t forget, Redis has achieved Advanced Partner Status on the Amazon Web Services Partner Network.Ultimately, of course, it’s all about the software, and we’ve also announced a number of product advancements, including releasing the RedisAI and RedisGears modules, introducing RedisRaft, and updating our existing modules—most recently RediSearch 2.0.The Datanami awards help demonstrate the momentum of Redis for data layer applications from caching to primary-database use cases and beyond. There’s never been a better time to rediscover the power of Redis!"
25,https://redis.com/blog/digital-transformation-is-accelerating-is-your-data-layer-ready/,Digital Transformation Is Accelerating: Is Your Data Layer Ready?,"September 15, 2021",Mike Anand,"Much ink has been spilled on digital transformation in the past decade. At Redis, we see a direct connection between a set of technology layers that enable transformations (cloud, microservices, containers, and NoSQL databases) and how quickly and successfully companies are able to push through their transformation journeys.But how do we quantify that connection into actionable insights? That’s where the Digital Transformation Index (DTI) comes in. First, we combine the adoption of those four technologies to create a single composite index for how far a company is on their journey. Then we compare the index across companies to see the impact of technology adoption on maturity. And since this is the second DTI report, we can also compare results to the first report for additional insights.The results give us an accurate view into the digital transformation journey and what struggles companies run into along the way—and how they overcome them.Download the report, or read on for how we got the numbers and key findings!Organizations moving forward with NoSQL had a DTI nearly 1.4 higher than those who are moving away from NoSQL. That’s the biggest gap for any question in the survey. It shows how the greater speed, flexibility, and scalability of NoSQL databases are essential for an upgraded data stack.By a wide margin, cultural changes were named the most significant issue when mobilizing an organization to transform. This probably isn’t a surprise to anyone who’s implemented technical initiatives across an organization, but we were struck by the magnitude of the problem. Cultural changes were a bigger challenge than technical and talent hurdles combined. It shows why it’s important to remember that it’s a “digital transformation” and not “technology upgrades.” You’re changing how your company operates, which means sometimes it’s just as important to focus on change management as onboarding new technology.The good news here is that executive support for digital transformation is the least common problem. Finding the right talent and overcoming technical challenges, such as breaking down monoliths, were far less difficult than making cultural changes.Caching has moved from being an optimization for improving performance to a critical application component with a wider variety of use cases. No matter what your architecture, your application or microservice won’t scale without a cache. In a recent interview, thought leader Lee Atchison put it bluntly:While we’re on the subject of caching, we can’t help but recommend Caching at Scale With Redis. Packed with examples, advice, and architectural diagrams, it’s a great resource for anyone interested in building modern applications that scale.Overall, the survey shows how our customers are moving forward with their digital transformations. The pace of innovation is picking up, making cultural changes is the most difficult part of a transformation, and caching is playing an evermore important role in modern architectures. And, of course, none of this matters without a modern, performant, scalable data layer.To understand where our customers are on their journey, we asked thousands of RedisConf 2021 attendees about the way they’re using these modern technologies. Or starting to use them. Or maybe not using them at all.We wanted to find out about the technologies our customers use, and changes in their broader IT environments. So we asked about cultural changes, application development trends, and how people in different roles perceived their organization’s progress. What emerged told us about important changes in how database applications are built, how caching is evolving, and how often high performers dream about coding. (Like we said, we tried to be comprehensive.).As we work with customers around the world, one of our key principles is: Your organization only moves as fast as your data. This report bears that out, with the types of databases in use and how those databases are used being key indicators of an organization’s digital success.Download the full report today to see all the insights and takeaways from the survey. Want to join the conversation and influence future research? Tell us your transformation story (and challenges!) at DTI@redis.com."
26,https://redis.com/blog/the-one-big-thing-missing-from-modern-digital-banking/,The One Big Thing Missing from Modern Digital Banking,"October 26, 2020",Redis,"This blog post was adapted from our white paper The Power of Personalization: Driving Digital Banking Success. Download it for free now!It may not always seem that way, but many banks were early adopters of technologies that enabled the development of digital banking services. By 2000, a majority of banks in advanced economies offered some form of online banking, although by modern standards these services were slow and very limited. Today, however, many digital banking services are much faster, more secure, and increasingly sophisticated, with payment and other banking functions instantly accessible via a huge range of connected devices. These capabilities are now complemented by a powerful array of other online financial services, including savings, investments, mortgages, insurance, and account aggregation.The missing puzzle piece: personalizationThat sounds great, right? But one critical element of the ideal digital banking service is still too often missing: genuine personalization. Banks may offer increasingly well-crafted products and useful services, but in many cases the customer experience fails to reflect an individual’s circumstances, choices, or previous interactions with the bank. Banks still have a long way to go in order to tailor products, services, and customer interactions to match their users’ specific needs and preferences.That is a real problem in a world where software companies are disrupting industry after industry, in large part because they offer more personalized services than their legacy counterparts. Growing numbers of consumers are now accustomed to interacting with a wide variety of vendors offering fast and reliable online services that incorporate some degree of personalization, from retailers to on-demand streaming services. In that context, consumers may find it difficult to understand why their bank cannot provide the same level of customization. While other businesses treat consumers as individuals and not account numbers, too many banks still make their customers feel more like the latter.Rethink your data layer strategy to enable personalizationObviously, banks and financial service institutions have access to a rich trove of customer data, and it appears that banks and fintech companies that use this data to successfully personalize their digital services enjoy significant commercial benefits. In 2019, research by KPMG based on the views of 84,000 consumers in 20 countries revealed a clear correlation between personalization and brand loyalty in 18 of those markets, including the U.S., the UK, Australia, Brazil, France, Germany, and Italy. Consumers consistently awarded higher ratings to banks with the best personalization capabilities.But banks can build desirable personalized services only if they rethink their data layer strategies. They need data infrastructures able to provide the power, speed, flexibility, and scalability to draw useful information out of enormous volumes of customer data—and turn that information into personalized offers and interactions. Achieving this for a massive customer base requires the ability to respond quickly to very large numbers of processing requests.So, what must financial institutions do in order to rethink their data layer strategies to enable personalization and reap its benefits? Find out in our new white paper, The Power of Personalization: Driving Digital Banking Success, which explains what institutions need to consider in their data architecture in order to successfully deliver the personalization that today’s customers expect. You’ll also learn how Redis Enterprise delivers the performance, scalability, and security required to enable the future of digital banking.Download it today!And if you’re looking to learn more about the importance of the data layer in the financial-services industry, check out:"
27,https://redis.com/blog/intro-to-caching-at-scale-primer/,Intro to Caching from the Caching at Scale Primer,"July 22, 2021",Redis,"With over 34 years of experience architecting and building SaaS applications, Lee Atchison is a recognized thought leader and expert on application modernization, cloud migration, and DevOps transformations. You can read or watch his expertise on display through any of his three books, 70+ published articles, or 100s of presentations, classes, and seminars.You and your team have developed an application worthy of people’s attention. Word of mouth spreads and it’s popularity soars, but then operation costs begin to skyrocket, the app continuously crashes, and a customer does the unthinkable—they tweet about super slow speeds.As your customer base grows, how do you keep costs from rising? You welcome the challenge of scaling, but you also can’t sacrifice growth for higher failover rates and increased maintenance costs.One thing is for certain: You need an application cache.When there’s a request to fetch data, a cache provides a copy of that data in real-time. And as more and more customers begin using your app, you need an advanced application architecture that can handle those increased data requests without going back-and-forth between your primary data store.Lee sums it up pretty simply: “Our modern world demands modern applications.”It’s no secret that customers demand better and better performance. If your application isn’t performing up to customer expectations, those customers will leave and flock to your competitors.And behind every fast, easy-to-use application is a ton of moving parts. As Lee says, “Today’s applications must be able to handle large quantities of data, perform complex operations, maintain numerous relationships among data elements, and operate on distinct and disparate states between transactions.”Headaches happen within complex applications, and a cache is there to minimize them.In Caching at Scale With Redis, Lee introduces what caching is, why and when you need it, and caching practices that will help your application achieve the highest performance possible.When a user requests data from a service, an operation is performed to acquire that data from a store, then relay that information back to the consumer. Yet, Lee notes that these operations can be very resource and time intensive—especially if the same data is being retrieved over and over again.Caching isn’t a one-size-fits-all data store. Application architectures vary across the board, especially due to size of the app or even the industry use case. However, Lee says there are four primary features that caching will help improve:This doesn’t mean implementing a cache will automatically provide these for you.“In many cases, caching may not add value, and in a few cases, caching can actually degrade performance,” Lee says. He raises three potential problems:Lee notes that certain variables must be true in order for a cache to be useful (see page 8 of the e-book for the list of rules). “For a cache to be effective, you need a really good understanding of the statistical distribution of data access from your application or data source,” Lee says.Now that you know you need a cache, this free e-book is the only primer you need to build and scale your cache with Redis.From Lee himself: “This book describes what caching is, why it is a cornerstone of effective large-scale modern applications, and how Redis can help you meet these demanding caching needs.”As you dive deeper into the e-book, Caching at Scale With Redis discusses the different types of caching strategies and offers practical explanations on how these strategies may fit for you—especially if you’re in the cloud.“While there are many methods, processes, and techniques for making an application retain high availability at scale, caching is a central technique in almost all of them,” Lee says.For more information on the e-book, visit our page."
28,https://redis.com/blog/redisraft-license-update/,RedisRaft License Update,"October 20, 2021",Yiftach Shoolman,"As of October 20, 2021, we’re updating the RedisRaft license from dual AGPLv3 + RSAL to RSAL only.This change applies to the RedisRaft project only. The license change does NOT affect core Redis, which is, and will always be, the 3-clause BSD license.RSAL lets users freely download, modify, and redistribute source code without worrying about copyleft, just like Apache 2.0 and BSD. RSAL also permits anyone to use the source code in commercial software and in cloud SaaS offerings, as long as the offering does not provide the software itself. For example, the RSAL license prohibits you from using the software to create a database-as-a-service based on RedisRaft.This flexibility provides the Redis ecosystem with freely available clients and modules under permissive licenses, enables the core Redis license to always be under the three clause BSD license, and provides a foundation for the enterprise software and services we provide commercially.We’re continuing our commitment to the success of the Redis ecosystem. Our efforts include the open source governance of the core project, supporting and managing contributions, empowering and mentoring contributors, developing new features, fixing bugs and patching security issues, educating the market about Redis, and sponsoring Redis events across the globe.We’re also continuing to invest heavily in building, extending, and supporting many clients and modules, all freely available under open-source or source-available licenses. We believe that RedisRaft is going to be an important addition for Redis users, and we’re committed to the significant effort involved in building and maintaining it."
29,https://redis.com/blog/say-hi-to-redis-new-old-friends/,Say Hi to Redis’ New-Old Friends,"November 4, 2021",Guy Korland,"One of the most amazing things a first-time Redis user might notice is a 42-page long list of clients, all developed by the great open source community of Redis. Currently, it consists of 218 different clients!That list not only covers all the major programming languages, but also languages that I, at least, didn’t hear about until I set out to write this blog, like Racket, Rebol, and Lasso. To be technically correct, the list covers 36 different programming languages! (I guess you already got the point. Perhaps I can drop the “!” from now on…)At a first glance, writing a Redis client sounds like a simple task. All you have to do is read the Redis Protocol (RESP), which is pretty simple and text based, then just follow the Redis Commands documentation and implement these commands one by one. To be accurate, the list contains 264 different commands! (Sorry, I couldn’t help myself—had to add another “!”)Well, these 264 commands are just the tip of the iceberg. If you really want to harness the full power of Redis, covering only these commands won’t do Redis justice. On top of these “simple” commands Redis also supports what is expected from any NoSQL database these days: High-Availability, Sharding, Transactions, Access-Control list (ACL), Notifications, and Extensibility.In order to keep Redis simple and stable, chunks of  processing are offloaded to the client. For example, in order to support Redis Transactions, the client has to promise all the operations are done on the same exact socket and can’t multiplex this socket as long as the transaction wasn’t “executed.” Such nuances should be taken into consideration when one starts to develop a new Redis client, and it gets even trickier when it comes to supporting High-Availability and Clustering.About two years ago, a few months before the move of Redis code to the Redis organization, we started to think: How can we make sure Redis users not only get the best Redis server, but also get the best development experience while being able to use all the latest and greatest Redis can offer?We started to approach the different clients used by the vast Redis community, setting our target at helping client maintainers be up to speed with the vast Redis feature set. First, we formed a forum with many of the maintainers, providing them a platform to share their experience and dilemmas, but also to get the Redis core team their direct feedback.Then, we mapped the gaps and missing features in the more adopted clients in the 10 leading programming languages. We realized that while some of the clients have millions of downloads a week, they’re still missing valuable features (like Redis Cluster or Pipeline) which many users can’t utilize, so we immediately started to contribute code to these clients.Last, after interviewing many Redis users, we realized the amazing variety of clients users can pick from suggests bootstrapping a Redis client is amazingly simple, but maintaining an up-to-date client is full-time work, and users find it hard to decide which client to pick and which client they can rely on for long runs.In the last year, we started to suggest some of the more adopted clients to join the Redis organization, so we are happy to welcome three new members Jedis, node-redis, and redis-py. These three clients join the old timer members Hiredis and redis-rb. We hope that it will reduce confusion among users, increase their assurance about the clients road map, and help us make sure these clients provide the complete Redis experience.The last couple of months were pretty hectic, and the three clients got a complete face lift. A long list of missing features were added to make sure the clients support all the goodies Redis has to offer, while also being refactored to provide a more “modern” experience (these clients are 10+ years old). The result of this hard work is a new 4th version for each one of them. You’re more than welcome to download the new client release candidates Jedis@v4, node-redis @v4, and redis-py@v4. Please share your feedback.The work has just started and the road map holds a couple of interesting features, so expect more to come in the near future. Some of them are adding native support for the most adopted Redis Modules like RediSearch and RedisJSON, which should simplify the developer experience. Also, some complete other long-awaited features, such as client side caching support, new Redis protocol (RESP3) added in Redis 6, new Stream APIs, and Redis Functions upcoming in Redis 7."
30,https://redis.com/blog/aws-marketplace-flexible-plan-redis-enterprise-cloud-launch/,New Plan Brings Increased Simplicity and Flexibility to Redis Enterprise Cloud in AWS,"December 1, 2021",Nir Schachter,"AWS customers can now procure Redis Enterprise Cloud in AWS with a simplified consolidated bill, spend that counts toward EDP consumption, and flexible on-demand pricing.To make this possible, we’re delighted to share the next step in Redis’ partnership with AWS: general availability of Redis Enterprise Cloud – Flexible plan in AWS Marketplace.This demonstrates the level of commitment between Redis and AWS to provide joint customers with a modernized purchasing experience and flexible pricing options when building and running high-performance applications. No commitment, no upfront cost—simply subscribe/unsubscribe as needed.Redis Enterprise Cloud is available across AWS, Azure, and Google Cloud and provides industry-leading functionality like unparalleled cost-effectiveness with Redis on Flash, high-performance modern data models with Redis modules, and built-in high availability and low latency with Active-Active Geo-Distribution—all fully managed in your cloud of choice.Redis Enterprise Cloud’s Flexible plan can now be purchased through the AWS Marketplace. This empowers you to provision according to changing business needs, with no budget commitment, and pay monthly through your AWS account. You also have the freedom and flexibility to change your provisioning at any time.With this new procurement method you will benefit from unified billing—which means you’ll get a single bill from AWS, including Redis Enterprise Cloud, along with your existing AWS usage. In addition, you can use your financial commitments in the form of an Enterprise Discount Program (EDP) with AWS towards consuming Redis Enterprise Cloud resources.Beyond the ease of purchasing, joint customers can leverage the many benefits of Redis Enterprise without the operational hassle of installing and managing it.Redis has seen tremendous adoption throughout the years. But in order to really get the most out of Redis, you need the benefit of a fully managed Redis Enterprise service in the cloud. This frees you from operational complexities with a suite of fully managed Redis services so you don’t need to hire your own team.Designed for modern distributed applications, Redis Enterprise Cloud already has a large customer footprint on AWS, has been recognized as an AWS Advanced Technology Partner, and achieved the AWS Outposts Ready designation. The expansion of our partnership with AWS enables current and future customers to quickly, easily, and cost-effectively procure Redis Enterprise Cloud.First, go to the AWS Marketplace and search for Redis Enterprise Cloud – Flexible plan as shown in Figure 1.Select Flexible plan, which will allow you to provision according to your business needs, without any budget commitment.Click Subscribe to confirm your subscription through AWS Marketplace.Click Set Up Your Account to redirect to Redis Enterprise Cloud and register a new account or log in to an existing account.Register a new Redis account or log in to an existing one.After registering/logging in, there’s one final step before creating your databases: mapping between your AWS Marketplace account and your Redis account. If you have multiple accounts, choose one from the list.Create a new subscription in your account. The AWS Marketplace logo in the top left corner will confirm the mapping has been successfully created.Choose Flexible Subscription and follow the subscription creation wizard. You specify your database memory limit (and if your database requires high availability, make sure your memory limit takes into account the size of your replica), the expected throughput (in ops/sec), and the number of databases of this type.At the final stage, AWS Marketplace will be automatically chosen as the payment method for the subscription.It’s that simple. Want to deploy the best version of Redis in AWS with a unified bill, flexible on-demand pricing, and consumption toward your AWS commit? Click here to start now!We look forward to the continued growth of our partnership with AWS, and the value this service will provide to our customers. Feel free to reach out to us at aws@redislabs.com to learn more!"
31,https://redis.com/blog/deploy-redis-enterprise-google-anthos-bare-metal-qwiklabs/,Learn How to Deploy Redis Enterprise on Google Anthos Bare Metal with new Qwiklabs,"December 6, 2021",Gilbert Lau,"If you’d like to deploy Redis Enterprise in a Kubernetes environment on Google Anthos Bare Metal, then our latest Qwiklabs module is for you. This module, produced in collaboration with Google Cloud, will teach you how to run Redis Enterprise and a Knative serverless application on an Anthos Bare Metal cluster deployment. Anthos unifies the management of infrastructure and applications across on-premises, edge, and in multiple public clouds, with a Google Cloud-backed control plane for consistent operation at scale. This is part of Redis’ effort to run Redis anywhere and on any cloud, whether it’s on-premises or in the cloud.The first joint Redis/GCP Qwiklabs covered the foundational knowledge of Redis and RediSearch. Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker. It’s designed for applications requiring the fastest possible response times. Redis uses key/pair values at its core, and those keys point to any number of supported data structures such as strings, hashes, sets, and lists to empower many use cases in retail, gaming, and banking industries. When it comes to hashes, what if you want to retrieve hashes based on their contents, not just based on their key? What if, for example, you want to get a list of all movies directed by Steven Spielberg? RediSearch is the tool that makes this possible and lightning fast.Deploying Redis Enterprise in a Kubernetes environment has been gaining a lot of traction, as Redis Enterprise serves very effectively as the real-time data platform for building microservices due to its speed and performance. In this latest Qwiklabs, you’ll learn how to install the Redis Enterprise Operator. The operator is responsible for creating two custom resources: Redis Enterprise cluster and Redis Enterprise database. It provides a cloud-native means of managing the lifecycle of Redis Enterprise instances in a Kubernetes environment. This lab will also guide you through how to create and deploy a Knative serverless application on an Anthos Bare Metal cluster. The app you create from this lab will connect to a Redis Enterprise database that you will create and will scale out based on the traffic from a performance testing workload.Learning and earning credentials from Qwiklabs is one of the easiest and most convenient ways to gain hands-on knowledge about Google Cloud and other third-party technologies, such as Redis Enterprise. If you want to learn how to build real-time apps, check out the Redis Developer Hub at developer.redis.com. There you’ll find huge repositories of working code examples to learn from, and realize the power of our primitive data structures and enterprise modules, such as RediSearch, RedisJSON, RedisTimeSeries, RedisGraph, and RedisBloom.Ready to give it a try? Get into the driver seat to take our “Deploying Redis Enterprise for GKE and Serverless App on Anthos Bare Metal” self-paced lab at https://www.cloudskillsboost.google/focuses/21603?parent=catalog. Once you complete the lab, please provide your feedback and rating. Your input means a lot to us!"
32,https://redis.com/blog/redis-labs-2020-magic-quadrant-cloud-database-management-systems-gartner/,Redis Labs Recognized in Inaugural 2020 Magic Quadrant for Cloud Database Management Systems by Gartner,"November 30, 2020",Mike Anand,"This year has been like no other in memory. Among other challenges, COVID-19 has dramatically changed the way the world lives and works. For the database market, 2020 has accelerated shifts that were already transforming the industry: not merely moving to the cloud, but toward fully managed services from providers who can satisfy a wide array of advanced customer needs and use cases.Recognizing this shift, Gartner has issued the first Magic Quadrant dedicated to cloud database management systems. Today, I’m happy to share that Redis Labs was positioned furthest to the right on the Completeness of Vision axis in the Challenger quadrant in the November 2020 Magic Quadrant for Cloud Database Management Systems by Gartner. A complimentary copy of the report is available here.According to Gartner, “For the past few years cloud-managed DBMS service revenue has been outgrowing on-premises DBMS revenue. Most of the DBMS vendors with on-premises offerings have shifted their innovation and development efforts to managed cloud services.” The report further states, “We forecast that, by 2023, cloud DBMS revenue will account for 50% of the total DBMS market revenue.”For this new Magic Quadrant, “Gartner defines the cloud database management system (DBMS) market as being that for products from vendors that supply fully provider-managed public or private cloud software systems that manage data in cloud storage. Data is stored in a cloud storage tier (such as a cloud object store, a distributed data store or other proprietary cloud storage infrastructure), and may use multiple data models—relational, nonrelational (document, key-value, wide-column, graph), geospatial, time series and others.”November 2020 Magic Quadrant for Cloud Database Management SystemsRedis Labs has seen tremendous growth and momentum over the past 12 months. Redis Labs closed our $100 million Series F financing round and achieved unicorn status as our valuation now exceeds $1 billion. This success has been driven, in part, by our growing partnerships with the three leading cloud providers. Our collaboration with Google Cloud is seeing massive adoption as we hit the first year anniversary of its launch. We also just launched Redis Enterprise tiers in public preview on Microsoft Azure Cache for Redis. And we have announced important new achievements in the Amazon Web Services (AWS) Partner Network––with more to come soon!Ultimately, though, there’s a simple reason why companies of every size, industry, and geography are turning to Redis Enterprise Cloud: it simplifies and automates database provisioning so they can focus on building new innovations, rather than the required infrastructure. We’ve also built a fully managed data platform designed for the needs of modern applications; unified data across any deployment model (on-premises, hybrid, and multi-cloud) and around the globe, seamless migration of datasets, and elasticity with five-nines (99.999%) availability.Redis Enterprise Cloud is now increasingly being used by customers for their primary database needs. Native Redis data structures and data models are enabling a wide range of high-performance use cases like transaction scoring, fraud mitigation, personalization, and much more. These uses are critical to many industries, including financial services, retailers, and gaming, as they seek to win in the online era.Redis Enterprise Cloud solves the complex problem of provisioning and maintaining different databases for each data model by allowing multi-model operations across and between modules and the core Redis data structures. This is executed in a fully programmable and distributed manner, while maintaining instant, sub-millisecond latency developers expect from Redis. Meanwhile, we continue to build for the future to understand how applications’ data layer is changing with artificial intelligence and how Redis can help companies maximize the value of the real-time data they generate.Even as Redis Enterprise was earning its place in the Magic Quadrant from Gartner’s analysts, Redis Enterprise users have also been showing unprecedented appreciation. In fact, our rating on Gartner’s Peer Insights for Operational Database Management Systems has climbed all the way to 4.9 out of 5 stars as of November 23, 2020, based on some 30 reviews in the last 12 months! That includes 87% five-star reviews and 13% four-star reviews, with no lower rated ones. In addition, 100% of the reviewers “would recommend” Redis Enterprise.You can see an overview of the results on the Gartner Peer Insights page for Redis Labs.While Redis Enterprise garnered high marks across the board, it’s highest ratings came for “High-speed, high volume processing” and “Ease of deployment” and “Timely and complete response to product questions,” “followed by “Ease of integration using standard APIs and tools,” “Timeliness of vendor’s response,” “Quality of technical support,” “Multiple data types/structures,” and “Administration and management.”To illustrate, let’s look more closely at a few of the Gartner Peer Insights reviews of Redis Enterprise. In one review, an Associate Technical Manager in R&D at a $1 billion+ enterprise described Redis Enterprise as a “One-stop solution for enterprise caching needs.” The manager went on to explain that, “deployed as a centralized cache in a high-volume and highly available system, Redis [Enterprise] does what it does best for our deployment at scale.” In fact, the manager wrote, if they could do it all over again, they would “Start with Redis Enterprise first up, which would have definitely been a paradigm shift in terms of the technical approach but worthwhile nevertheless, instead of ending up with Redis Enterprise as the last option after trying out other options.” (Read the full review here.)In another review, a Director of Information Systems Operations at a $500 million – $1 billion financial services company says that “Redis has been a great investment for our organization as we needed a solution for high-speed data caching. We needed a solution to provide instant data for our customers while also being extremely flexible and easy to work with for our engineers. The learning curve for Redis is small and within hours our database team had the solution up and running in a clustered POC.” The director also noted their team’s satisfaction with support: “Since we’ve purchased the solution, Redis support has been with us every step of the way to answer any questions about function, architecture, scalability, etc. We didn’t just buy a leader in caching, we invested in a partner relationship and couldn’t be happier.” (Read the full review here.)Redis Enterprise also continues to win fans in the cloud. One sales and marketing executive of a $50 million – $250 million enterprise put it as clearly as possible, calling Redis Enterprise an “excellent solution across cloud, on-premises, and hybrid deployments with resiliency, massive scalability, ease of management, and simple operation.” While the reviewer highlighted Redis Enterprise’s “performance, scalability, and innovation,” that scalability was what they liked best. (Read the full review here.)Notably, another Solutions Engineering Executive at a $30B+ healthcare organization said that Redis’ “Multi-model capabilities allows expertise to be built on one database. Our industry is moving fast and Redis Enterprise allows our company to move even faster.” This executive notes that vendor relations are “A+” and their team is able to “get answers regarding hardware, licensing, and architecture/design without being passed to different areas and/or teams,” making it “very convenient and [enabling] quick decision making for our company.” (Read the full review here.)A site reliability engineer at a $30B+ services company loves Redis Enterprise’s “bulletproof reliability” the most. “Redis Labs has been an excellent partner. We leverage their Enterprise Cluster technology in a core segment of our architecture,” they write. “We have found their technologies to be extremely reliable. Furthermore their support has been responsive and effective.” (Read the full review here.)We love hearing from our customers, so now that you’ve read the November 2020 Gartner Magic Quadrant for Cloud Database Management Systems please share your thoughts on Gartner Peer Insights after experiencing the benefits of Redis Enterprise Cloud. We appreciate your support and feedback, so please keep them coming!Finally, it’s once again time for AWS re:Invent, and we invite you to connect with Redis Labs on our special re:Invent page during the event, running from Monday, November 30 through Friday, December 18, 2020. This year’s event will be virtual, of course, but that means you can attend for free without flying to Vegas. And we’ve got a great lineup of exclusive content, sessions, and much more to help you learn about Redis Enterprise in the cloud.Gartner disclaimerGartner does not endorse any vendor, product or service depicted in our research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of the Gartner research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.Gartner Peer Insights reviews constitute the subjective opinions of individual end users based on their own experiences and do not represent the views of Gartner or its affiliates.Gartner, Magic Quadrant for Cloud Database Management Systems, Donald Feinberg, Merv Adrian, Rick Greenwald, Adam Ronthal, Henry Cook, 23 November 2020"
33,https://redis.com/blog/active-active-geo-distribution-in-azure-cache/,Active-Active Geo-Distribution Now Generally Available in Azure Cache for Redis Enterprise,"February 2, 2022",Shreya Verma,"We’re excited to announce the general availability of Active-Active Geo-Distribution from Redis in Azure Cache for Redis’ Enterprise and Enterprise Flash tiers. Azure Cache for Redis customers can now have up to five Enterprise tier cache database instances in different Azure regions to form an active geo-replicated cache using conflict-free replicated data types. An Enterprise tier active-active deployment allows businesses to create global applications that provide local sub-millisecond read/write latencies with considerably better resilience to failure, covered by 99.999% Azure availability SLA. In addition, Active-Active Geo-Distribution provides strong eventual consistency, supporting writes to multiple Redis instances across the globe, and takes care of merging local changes while predictably resolving conflicts.Active-Active Geo-Distribution known as active geo-replication in Azure enables several key scenarios for enterprise applications:Note that Active-Active Geo-Distribution, known as active geo-replication in Azure, is only available on Azure Cache for Redis Enterprise and Enterprise Flash tiers.Active geo-replication must be initially set up when creating the Redis instance. You can setup active geo-replication for your instance in the Azure Portal as follows:1. Create a new Redis Enterprise instance.2. Click on the ‘Advanced’ tab in the create experience.3. Then click ‘Configure’ in the active geo-replication section to set up active geo-replication.4. Select an existing replication group, to add a new cache instance to the existing group. Or create a new replication group, by providing a new replication group name.Detailed steps to create and set up active geo-replication on enterprise and enterprise flash tiers can be found in Azure Cache for Redis documentation.When an Azure region experiences a large-scale event or an outage, other cache instances participating in the replication group continue to take the read and write requests. Even if the majority of instances in the replication group are down (3 out of 5) the remaining instances are uninterrupted. The application needs to monitor the cache instances and switch to another region when an instance becomes unavailable.A globally distributed application can send all their read and write requests to the closest or local instance and enjoy single digit (ms) Redis read/write latencies.Active-Active on Azure Cache for Redis Enterprise unlocks critical capabilities for enterprise applications, which need to be highly available anywhere at any time. Whether deploying a nationwide multi-region application or a globally distributed one, Active-Active is essential to use-cases like global session management, world-wide fraud detection, geo-distributed search, and real-time inventory management.And we’re excited to announce that this functionality is now generally available in the Azure Cache for Redis Enterprise Tiers! Find links to documentation and demos below. Please give it a try and leave feedback. We look forward to hearing from you.Microsoft’s documentation on Azure Cache for Redis geo-replicationCreate your own Azure Cache for Redis Enterprise: Azure Cache for Redis – Microsoft AzureOverview of the Enterprise Tiers and the MIcrosoft/Redis partnershipSee a demo of active geo-replication in action"
34,https://redis.com/blog/introducing-redis-7/,Redis 7.0: An Evolution Across Multiple Fronts,"March 2, 2022",Redis,"The release of Redis 7.0 is on track, and we’ve just published its second release candidate. This release candidate is a planned milestone intended to complete the version’s features, but it’s also an opportunity for us to present additional content in the new version. For example, Redis Functions has evolved from the existing support for scriptings that Redis had since version 2.6. Similarly, more of Redis’ features have evolved.In nature, evolution happens apparently at random and natural selection takes care of sorting out its results. Generally, in software in general, and specifically with Redis, this process occurs backward: we select the desired path and evolve the project to follow it accordingly. Rather than letting randomness dictate the future, our roadmap’s planning and execution are primarily guided by the users’ feedback and new use cases where Redis is a good fit.In Redis 7.0, the Access Control List (ACL) had also stepped up the evolutionary ladder. Introduced in Redis 6.0, ACL had reversed the long-time view about security as being out of the project’s scope by adding the mechanisms to manage users and their permissions. However, our community quickly educated us that while it was a step taken in the right direction, the feature still lacked the necessary capabilities.One of the gaps in ACL was already addressed by Redis 6.2, namely controlling the allowed patterns of Pub/Sub channel names. But that was only a partial stop-gap and it took us longer to come up with a simple and effective approach to address the remaining ones.ACL’s original design had only provisioned for basic permission-control use cases. It enabled granting or denying access only to a single set of commands, keys’, and channels’ names patterns per user. It was impossible, for example, to restrict the `SET` command to one subset of keys, and at the same time allow `GET` to another subset of keys for a given user. Effectively, ACL wasn’t an effective mechanism for implementing security policies.Redis 7.0’s Access Control List, or ACLv2 for short, is compatible with the original but adds two important improvements. Firstly, ACLv2 is all about selectors. Secondly, ACLv2 allows setting access type permissions to specific keys. This capability makes it possible to exclusively limit the user to read-only, write-only, or read-write operations to a subset of the keys.The original ACL design provided only one selector per user – the default selector. The selector describes the keys and channels that the user can access, categories and commands. ACLv2 allows adding any number of selectors on top of the default that are applied in order. This approach satisfies the requirements of more demanding security policies and brings ACL nearer to completeness.The server’s introspective abilities are another aspect of Redis that have significantly advanced in version 7.0. Redis exposes an API, its commands’ dictionary, through which it interacts. As the project evolved, the number of different commands (and their sub-commands) grew, reaching an excess of 380 in version 7.0. Because every Redis command is specialized for given tasks, documenting each command’s invocation arguments and behavior is a core principle of the project. This documentation is the only contract between the server and its clients.Historically, commands were documented externally to the project itself in a separate code repository. We kept all documentation in a (mostly) human-readable format because it was intended to be read by people. This presented a challenge for the machines (or rather, the people who program machines) because translating prose to code is messy and brittle. Specifically, authors of clients for Redis could only hope to keep their project up-to-date by monitoring changes to the documentation and reading the release notes.So, by version 2.8, we realized that we also needed a programmatic way that allows the server to report its commands. The aptly-named (but also tongue-twister) `COMMAND` command lists in runtime the commands that the server supports. Furthermore, the `COMMAND GETKEYS` subcommand lets clients send a verbatim command and its arguments to have the server extract any names of keys from it. Extracting key names from a command is required so the client can direct the operation correctly in a clustered deployment.Partly driven by ACLv2-related efforts but also to make the runtime commands list more useful for clients, version 7.0 overhauls much of the internal mechanisms of the server’s command management. In addition, we’ve enriched the metadata that the server keeps about each command, making it possible to build sophisticated clients that have (almost) no a prior knowledge about the server’s capabilities. Lastly, the revamped commands table is built in such a way that Redis modules can extend it with their respective commands, to provide the same level of introspection as core commands.The new command key specifications allow clients to locally extract keys from verbatim commands without involving the cluster’s servers, thus improving latency and reducing network bandwidth. The metadata about command arguments lets clients discover and adapt to changes in commands’ syntax between server versions. Clients can obtain even more information about executing commands under special circumstances and different deployment types from the command tips.This effort also included the promotion of subcommands to first-class citizens of the server’s commands table. Originally, subcommands were introduced to Redis to combat the ever-growing cardinality of the API. The reasoning was that instead of adding a new command for every task, related tasks are invoked by calling just one “parent” command. The “parent” command accepts as its first argument the name of the subcommand, which in turn dictates the action performed. From a technical standpoint, subcommands inherited all of their parent command’s traits (e.g., ACL category, read/write flags, key specification, and so forth), making it impossible to obtain a fine-grained distinction between their different behaviors.For example, the `CLIENT` command is a catch-all basket for connection management tasks that boasts no less than 15 different subcommands. Some of its subcommands, like `CLIENT SETNAME`, are routinely called by normal client connections, whereas others such as `CLIENT KILL` have the potential for misuse and should therefore be restricted to admin use alone. Earlier versions of Redis lacked the internal mechanisms that support making such distinctions, thereby directing developers to the documentation and creating confusion. In Redis 7.0, however, every subcommand has its own set of traits, regardless of its parents or siblings, allowing for its accurate description.This post had turned out to be quite the wall of text that goes perhaps into too much technical detail towards its end (if there’s even such a thing as too much technical detail). We hope it wasn’t all boring though, and that we’ve shed some light on the new version and its fit in the project. We’re working on the final release candidate(s) towards the version’s general availability, but stay tuned for our next posts in the series that will continue the new version’s tour."
35,https://redis.com/blog/redis-days-london-2022/,Register for RedisDays London 2022,"March 3, 2022",Redis,"How are today’s tech leaders staying up-to-date on the latest evolutions of real-time data? What developments, products, and best practices should modern digital businesses be harnessing to stay ahead of the pack in 2022 and beyond? Find your answers at this year’s RedisDays, the three-part virtual roadshow featuring informative sessions with members of the Redis community, including customers and data experts, highlighting the products and use cases that power the businesses modernizing today’s digital landscape.RedisDays kicks off its three-part worldwide tour in London on Tuesday, March 15th, followed by a stop in San Francisco on Wednesday, March 23rd, ending in New York City on Thursday, March 31st. So what can attendees expect to see at RedisDays’ first stop in the United Kingdom? Take a look below at RedisDays London’s current run of show.What’s the trick to leaving noticeable latency issues behind and getting your customers, users, and clients to experience your applications with sub-millisecond responses? It’s the new reality thanks to products like RedisJSON document store. Hear from Redis’ product team on how our products are helping teams turn digital pipe-dreams into reality.Is it possible to build a truly modern data architecture on the back of legacy technologies in the financial services industry? Is your data layer keeping you at the forefront of technology, or hindering your full potential? What happens when you overhaul your architecture with a multi-model real-time database like Redis Enterprise? Hear from President of Knowledge Integrity Inc., David Loshin, a well-known thought leader in data management and business intelligence, and Henry Tam, Sr. Solutions Marketing Manager at Redis, on the best practices and use cases for keeping up with the breakneck speed, scale, and demands of customers in modern digital financial services.What does it take to design and build real-time Natural Language Processing (NLP) data pipelines? Take part in this fireside chat and listen in as our speakers dive deep into artificial intelligence and machine learning.REGISTER for RedisDays London"
36,https://redis.com/blog/an-iconic-conversation-an-irreverent-look-at-what-makes-a-great-icon-infographic/,An Iconic Conversation—An Irreverent Look at What Makes a Great Icon [Infographic],"November 12, 2020",Haley Kim,"This summer we redesigned our website and company branding, introducing a fresh new set of colors, fonts, and iconography. We hope you’re loving our new look, and wanted to spotlight a few of these icons—along with some we’d like to see and a few that didn’t make the cut.Take a peek into our design process with this infographic, adapted from Rediscover Magazine, which is available free (online and in print) at Redis.com/rediscover-magazine. The premiere issue features more than a dozen stories on the power of data, real-time financial services, database trends, serving artificial intelligence, plus exclusive interviews with Redis creator Salvatore Sanfilippo and other tech leaders. We hope you enjoy this light-hearted sample, and please feel free to share!"
37,https://redis.com/blog/redis-py-410-release/,"Hello redis-py, It’s Been a Minute","January 18, 2022",Chayim Kirshen,"Python support for Redis has been growing for over a decade, reaching over 1 million downloads per week, and we’re proud to announce the latest release, redis-py 4.1.0!This release almost brings us within spitting distance of complete Redis command support in Redis 6.2. We’ve added support for missing options in existing commands, SSL support for Sentinel connections, and generally improved the developer experience. It also brings large structural changes such as a focus on Python 3.6, and even Python 3.10 support. We’ve revamped our client documentation, making it easier to find Redis commands, and encouraging everyone to get involved! More importantly, we’ve done it with minimal disruption, so you can upgrade without compatibility issues.Redis 6.2 brings with it a huge swath of new Redis commands, and it’s now easier than ever to use them with Python. From making it easier to fetch data and update its expiration in a single command with GETEX, to managing your Redis instances by disconnecting clients with CLIENT INFO and CLIENT KILL redis-py 4.1.0 has you covered. Let’s look at two examples below of the over 30 commands added in redis-py 4.1.0!redis-py 4.0 was the first version of the Python library to bring first-class support for Redis modules. With the release of redis-py 4.1.0, we now have support for RedisJSON, RediSearch, RedisTimeSeries, RedisGraph, and RedisBloom. That’s right, you can easily store JSON data in Redis! You can operate on it – you can even search it! Just remember,  when you store JSON data in Redis, you’re storing a new data type, so you need to use the JSON specific commands to manipulate it. Here are some examples:What about documents like myDoc above? Did you know that you can fetch multiple keys by querying a portion of the document?You can even combine RedisJSON and RediSearch for document capabilities or add your custom Redis modules support. It’s all there in redis-py 4.0.Did you know that redis-py 4.1.0 also integrates redis-cluster support, with two easy ways to connect? The same great experience for interacting with standalone Redis nodes is now part of redis-py.Maybe you’d like to connect to your cluster via SSL or prefer the class init based approach, and want to run commands, targeting specific nodes. redis-py 4.1.0 supports those scenarios!The work is never done and we’re moving full steam ahead! We will add support for more modules, such as RedisAI, with the same first class experience. We’re going to add support for RESP3 functions that will be released in Redis 7, and speed up the code base. We’re also working on developer tooling, to make contributing to and working with redis-py even better!"
38,https://redis.com/blog/redis-on-flash-now-3-7x-faster/,Redis on Flash: Now 3.7x Faster with New Data Engine and Amazon EC2 I4i Instances,"May 3, 2022",Alon Magrafta and Filipe Oliveira,"Redis on Flash (RoF) has been one of our most popular enterprise functionalities, making in-memory computing cost-effective by storing up to 80% of the datasets in SSD rather than expensive DRAM, and while still keeping the sub-millisecond latency and high-throughput of Redis. In typical deployments, RoF provides up to 70% TCO discount.Now with two new, exciting collaborations, we’re proud to announce RoF can now deliver up to 3.7x the performance while maintaining the same attractive total cost of ownership for running large datasets on Redis. First, AWS has announced the general availability of a new generation of instances, Amazon EC2 I4i, powered by 3rd generation Intel Xeon Scalable processors (code-named Ice Lake) and AWS Nitro SSD NVMe-based storage. This new generation promises to deliver a remarkable increase in performance for data-intensive Redis customers. Additionally, we’re pleased to share our plans to open the data engine for RoF to any RocksDB compatible database, with the newly announced technology from Speedb, as the first option.We believe the combination of the new I4i instances and providing customers a choice for their data engine will make RoF even more attractive for massive datasets, as the need for real-time experiences increases for companies building modern applications or low latency microservice architectures.Let’s dive into what’s new on RoF. AWS is offering the new Amazon EC2 I4i (the ‘i’ is for Intel) instances across 4 regions. The I4i instances use the latest technology, Intel Ice Lake processors and AWS Nitro SSD, which improves IOPS and reduces latencies compared to the previous generation of I3 instances.Separately from the hardware, we’ve been looking for ways that RoF can deliver even greater performance and identified an opportunity to open the data engine to innovation by customers and entrepreneurs.  We’re pleased to announce that RoF is now open to any RocksDB-compatible data engine and Speedb is the first alternative to be offered by Redis. Working with us, the team at Speedb has redesigned the RocksDB internal data structure which impressively increases both performance and scale while reducing CPU resources.In this blog post, we share how Speedb provides a significant performance boost of almost 50% at our sub-millisecond tests compared to RocksDB, regardless of which AWS EC2 instance we were using, the I4i or the I3.RoF with Speedb is currently available in private preview. Redis customers can contact their account team for additional information or to trial the new service.Our performance engineers couldn’t wait to get their hands on the AWS’ I4i instances and test them with RoF’s new Speedb data engine. We are pleased to be the first AWS partner to thoroughly test the Amazon EC2 I4i instances. As a reminder, RoF intelligently tiers large datasets and was designed to take advantage of lower-priced per GiB of NVMe SSDs compared to DRAM. This enables us to deliver Redis-grade performance, now even faster with the I4i instances, at 30% total cost-of-ownership of DRAM-based instances.Before getting to benchmark results and numbers, let’s review when a customer should consider RoF. We developed RoF for use cases where the working dataset is smaller than the total dataset and moving to additional DRAM is cost-prohibitive. Another common RoF use case is batch data processing, where huge amounts of data need to be processed for business-critical applications, that require steady low latency and high throughput over time.Now to the fun part, the benchmark results.i3.8xlarge – 244GB RAM, 4xNVMe SSD drives, for a total 7.6TBI4i.4xlarge – 128GB RAM, 1xNVMe SSD drives, for a total 3.75TBI4i.8xlarge – 256GB RAM, 2xNVMe SSD drives, for a total 7.5TBI4i.16xlarge – 512GB RAM, 4xNVMe SSD drives for a total 15TBWe used 1KiB value sizes, covering most standard Redis use casesWe tested 50% and 85% RAM hit ratios (i.e. many requests served directly from RAM)We tested 20:80 RAM:Flash ratioWe benchmarked various read:write ratios: 1:1, 4:1 and 1:4All tests were done using two serversThe following are the database sizes that we run according to the instances types:In each configuration, we tested how many ops/sec could be achieved while keeping sub-millisecond client latency (not including the network)The graph below shows the i3 vs the I4i and RocksDB vs Speedb:We can see the following improvements:The graph below shows scaling with I4i on Speedb and different read:write ratios:We can see the following results and improvements:Scaling wise, we can see that RoF on I4i with Speedb scales almost linearly.4xlarge → 8xlarge has a factor of ~1.55x-1.8x8xlarge → 16xlarge has a scaling factor of ~1.85x-1.95xA second notable result is that RoF on I4i with Speedb is very agnostic to the application access pattern (the read:write ratio). That means performance stays steady and is predictable. That can be useful when working with multiple different applications or when the access pattern varies over time.The graph below shows the overall 3.7x performance improvement:Appendix:Benchmark setupRedis Enterprise version: v. 6.2.8-39 on Ubuntu 18.04Redis on Flash database: see details per instance aboveClient machine: EC2 m5.8xlarge (32 VCPUs, 128 GB RAM)Load Generation tool: memtier benchmarkMemtier sample command for 85% ram hit ratio:memtier_benchmark -s <database host IP> -p <database port>–hide-histogram –pipeline=1 -c 4 -t 256  -d 1000–key-maximum=771141855 –key-pattern=G:G –key-stddev=50358491–ratio=1:1 –distinct-client-seed –randomize –test-time=1200Setup illustration:"
39,https://redis.com/blog/redis-timeseries-1-6-generally-available/,RedisTimeSeries 1.6 Is Out!,"April 28, 2022",Lior Kogan,"Today, we’re happy to announce the General Availability of RedisTimeSeries 1.6. This blog post details the major new features that are now available.RedisTimeSeries is a high-performance, memory-first time-series data structure for Redis. RedisTimeSeries supports time series multi-tenancy (it can hold many time series simultaneously) and can serve multiple clients accessing these time series simultaneously. It is now available also as part of Redis Stack.Before version 1.6, only one type of aggregation was possible:Since version 1.6, two new aggregation types are possible:Let’s demonstrate the first new aggregation type. First, let’s create two stocks, and add their prices at 3 different timestamps:TS.CREATE stock:A LABELS type stock name ATS.CREATE stock:B LABELS type stock name BTS.MADD stock:A 1000 100 stock:A 1010 110 stock:A 1020 120TS.MADD stock:B 1000 120 stock:B 1010 110 stock:B 1020 100Now, we can retrieve the maximal stock price per timestamp:redis:6379> TS.MRANGE - + WITHLABELS FILTER type=stock GROUPBY type REDUCE maxThe FILTER type=stock clause leaves us only with a single time series representing stock prices. The GROUPBY type REDUCE max clause splits the time series into groups with identical type values, and then, for each timestamp, aggregates all series that share the same type value using the max aggregator.FILTER label=value is supported by TS.MRANGE and TS.MREVRANGE. Additional filtering clauses are supported as well (see documentation).Next, we will demonstrate the second new aggregation type:First, let’s create two stocks, and add their prices at 9 different timestamps.TS.CREATE stock:A LABELS type stock name ATS.CREATE stock:B LABELS type stock name BTS.MADD stock:A 1000 100 stock:A 1010 110 stock:A 1020 120TS.MADD stock:B 1000 120 stock:B 1010 110 stock:B 1020 100TS.MADD stock:A 2000 200 stock:A 2010 210 stock:A 2020 220TS.MADD stock:B 2000 220 stock:B 2010 210 stock:B 2020 200TS.MADD stock:A 3000 300 stock:A 3010 310 stock:A 3020 320TS.MADD stock:B 3000 320 stock:B 3010 310 stock:B 3020 300Now, for each stock, we will calculate the average stock price per 1000 millisecond time frame, and then retrieve the stock with themaximal average for that timeframe:redis:6379> TS.MRANGE - + WITHLABELS AGGREGATION avg 1000 FILTER type=stock GROUPBY type REDUCE maxGROUPBY label REDUCE reducer is supported by TS.MRANGE and TS.MREVRANGE.Reducer can be sum, min, or max.When using TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE, you don’t always want to retrieve or aggregate all the samples.Using [FILTER_BY_TS ts...] you can filter the sample by a list of specific timestamps.Using [FILTER_BY_VALUE min max] you can filter the sample by minimal and maximal values.Consider, for example, a sampled metric where the normal values are between -100 and 100, but the value 9999 is used as an indication of bad measurement.TS.CREATE temp:TLV LABELS type temp location TLVTS.MADD temp:TLV 1000 30 temp:TLV 1010 35 temp:TLV 1020 9999 temp:TLV 1030 40Now, let’s retrieve all values, ignore out-of-range values:TS.RANGE temp:TLV - + FILTER_BY_VALUE -100 100Let’s also retrieve the average value, ignore out-of-range values:TS.RANGE temp:TLV - + FILTER_BY_VALUE -100 100 AGGREGATION avg 1000When using TS.MRANGE, TS.MREVRANGE, and TS.MGET, we don’t always want the values of all the labels associated with the matching time series, but only the values of selected labels.SELECTED_LABELS allows selecting which labels to retrieve. Given the following time series and data:TS.CREATE temp:TLV LABELS type temp location TLVTS.MADD temp:TLV 1000 30 temp:TLV 1010 35 temp:TLV 1020 9999 temp:TLV 1030 40To get all the labels associated with matched time series we’ll use WITHLABELS:redis:6379> TS.MGET WITHLABELS FILTER type=tempBut suppose we want only the location, we can use SELECTED_LABELS:redis:6379> TS.MGET SELECTED_LABELS location FILTER type=tempSuppose we want to receive the average daily temperature, but our ‘day’ starts at 06:00. In this case, we would like to ALIGN the intervals to 06:00 to 05:59, instead of to 00:00 to 23:59.When using TS.RANGE, TS.REVRANGE, TS.MRANGE, and TS.MREVRANGE, it is now possible to align the aggregation buckets with the requested start, end, or specific timestamp – using ALIGN.To demonstrate alignment, let’s add the following data:TS.CREATE stock:A LABELS type stock name ATS.MADD stock:A 1000 100 stock:A 1010 110 stock:A 1020 120TS.MADD stock:A 1030 200 stock:A 1040 210 stock:A 1050 220TS.MADD stock:A 1060 300 stock:A 1070 310 stock:A 1080 320Next, we’ll aggregate without using ALIGN (which means default alignment: 0)redis:6379> TS.RANGE stock:A - + AGGREGATION min 20And now with ALIGN:redis:6379> TS.RANGE stock:A – + ALIGN 10 AGGREGATION min 20Setting ALIGN to 10 means that a bucket should start at time 10, and all the buckets (each with a 20 milliseconds duration) are aligned accordingly.When the start timestamp for the range query is explicitly stated (not ‘-‘), it is also possible to set ALIGN to that time by setting align to ‘-‘ or to ‘start‘.redis:6379> TS.RANGE stock:A 5 + ALIGN – AGGREGATION min 20Similarly, when the end timestamp for the range query is explicitly stated (not ‘+’), it is also possible to set ALIGN to that time by setting align to ‘+’ or to ‘end’.TS.DEL allows deleting samples in a given time series within two timestamps.For example, TS.DEL stock:A 1020 1050 will delete all samples with timestamps between 1020 and 1050 (inclusive). The returned value is the number of samples deleted.Many optimizations were implemented, and most queries would now execute much faster compared to RedisTimeSeries 1.4.The following table details the number of queries per second achievable on a single node, for TSBS queries (which we described here). The table lists only the subset of the TSBS queries that were supported in version 1.4.We can observe an improvement of 8% to 65% in the number of queries per second, compared to RedisTimeSeries 1.4.Redis Keyspace notifications allow Redis clients to subscribe to Pub/Sub channels in order to receive events affecting the Redis data set in some way. You can, for example, use RedisGears to trigger a function with these notifications.As an example, it is possible to implement a time series predictor or anomaly detector that listens to the stream of samples and generates real-time predictions and warnings.Check, for example, this test which subscribes to various RedisTimeSeries commands and generates events.Since version 1.6, RedisTimeSeries can run in a Redis on Flash, but it’s important to note that RoF is implemented on the key level. Namely, the value of the whole time series lives either on FLASH or RAM.RedisTimeSeries is now part of Redis Stack. You can download the latest Redis Stack Server binaries for macOS, Ubuntu, or Redhat, or install them with Docker, Homebrew, or Linux.RedisInsight is a visual tool for developers that provides an excellent way to explore the data from RedisTimeSeries during development using Redis or Redis Stack.You can execute time series queries and observe the results directly from the graphical user interface. RedisInsight can now visualize RedisTimeSeries query results.In addition, RedisInsight contains quick guides and tutorials for learning RedisTimeSeries interactively.Learn more on RedisTimeSeries on redis.io and developer.redis.com."
40,https://redis.com/blog/redis-tls-internode-encryption-in-redis-enterprise-6-2-4/,Redis TLS – Internode Encryption in Redis Enterprise 6.2.4,"May 19, 2022",Brandon Felker,"Redis Enterprise 6.2.4 introduced internode encryption. The scope of internode encryption in Redis Enterprise is to achieve TLS encryption for all internal Redis cluster connections between nodes, including:Redis Enterprise uses several techniques to optimize performance and availability. The Redis cluster uses a shared-nothing architecture, which increases reliability and availability and makes it easy to add and remove nodes. When approaching the requirement to encrypt all internode connections, the team focused on availability and operational simplicity.A distributed, always-on system such as Redis Enterprise powers many mission-critical applications. It is required to meet the highest standard of operational availability. Redis Cloud, powered by Redis Enterprise, provides 99.999% availability, meaning it can only be down for a few minutes a year. Internode cluster communications are critical to maintaining quorum (control path operations) and enabling Redis replication (data path operation). Therefore, it is imperative to have high availability to maintain the reliability of internode communications at all times.A Private Certificate Authority (CA) is a cluster-specific certificate authority that functions like a publicly-trusted CA. The Redis cluster creates its own private root certificate which can issue other private certificates for each node. Private certificates issued by a Private CA are not publicly trusted. The Private CA is not exposed outside of the cluster. The Private CA is not shared between Redis clusters, or with any external client or service. The certificates signed by the Private CA (end-entity certificates) are exclusive to the node they are issued to.The Private CA utilized in Redis Enterprise provides a seamless internal self-rotation mechanism. Certificate rotation will be accomplished automatically prior to expiration. Certificate rotation can also be accomplished upon request through the REST API. Alerting will also be provided for monitoring by application, database, and security teams. The Private CA removes the dependency on an external CA availability for certificate rotation, removing a potential point of failure and improving the overall availability of the cluster.Redis Enterprise achieves this through the use of a Private Certificate Authority (CA) to manage the certificates required for internode encryption. Customers typically have the following primary concerns associated with the use of a Private CA.The first concern really isn’t a security concern. This is a compliance requirement. Many organizations have written blanket policies to disallow self-signed certificates entirely without considering valid use cases where a self-signed certificate is the best option. This requirement makes sense for many instances, but not all, and has caused those same organizations to require in-depth processes to grant exclusions for instances where the blanket policy does not fit. One example is Redis Enterprise internode encryption. In this specific example, a Private CA serves a specific purpose.First, let’s define a self-signed certificate. A self-signed certificate is one that is not signed by a publicly trusted third-party CA. This type of certificate is created, issued, and signed by the organization responsible for the website or application the certificate is issued for. These certificates are free to issue and use the same ciphers as certificates issued by a trusted third-party CA.You may ask yourself: “If an organization does not allow the use of self-signed certificates, what is the alternative?” The typical alternative is to use certificates that have been issued by a trusted third-party CA. An organization can purchase these certificates from many different third parties trusted CA’s and then issue the certificates to their website or application. A certificate issued by a trusted third-party CA is the same as a self-signed certificate as far as security is concerned.The next question we should ask is – What, difference, if any, is there between a self-signed certificate and a certificate issued by a trusted third-party CA? Each certificate supports the same ciphers. Each consists of root, intermediate, and leaf certificates. Each can also be expired or revoked as needed. The only difference is a functional difference between a self-signed certificate and a certificate issued by a trusted third-party CA. That function is trust. A trusted third-party CA can issue certificates that can be used to establish trust between two unrelated entities.When is trust required? Trust is a required part of the solution when two unrelated entities are communicating. A good example of this is the communication between a web browser and a web application. Using a third-party trusted CA-issued certificate allows for encrypted communication but also lets the web browser know that the web application is indeed who they present itself to be. As a result, the browser will prompt the user if they trust the web application and want to continue the communication.When is trust not required? Trust is not a required part of the solution when two related entities are communicating. Redis Enterprise’s internode encryption is a good example of this type of communication. Redis Enterprise consists of one cluster, and a single cluster can contain multiple nodes. There can be single or multiple databases on a node, as well. Because each node belongs to the same cluster, no third party is required to establish trust. Each node already trusts every other node because they belong to the same cluster.Redis Enterprise mitigates these compliance and security concerns because the Private CA-generated certificates are only used within the cluster. Because each of the nodes are known and trusted by all other nodes within the same cluster, a third-party trusted CA adds nothing and is not required to establish trust within the Redis cluster."
41,https://redis.com/blog/the-data-economy-podcast/,Introducing The Data Economy Podcast,"March 9, 2022",Britiana Andrade,"How are today’s technology leaders using data in innovative ways to increase business value within their fields? What blueprints did they follow as they work to create better user experiences, ease infrastructure constraints, and accelerate business growth in their organization? The answers to all this and more are now available in our new The Data Economy podcast.Presented by Redis and hosted by Michael Krigsman, Industry Analyst and Publisher of CXOTalk, The Data Economy Podcast is a thought leadership destination where today’s Chief Information Officers (CIO), Chief Technology Officers (CTO), and IT Executives come to unpack their data-driven stories and share their knowledge, strategies, and use-cases with an audience of contemporaries looking for insights on how to leverage their data to work smarter, not harder.The first six episodes of The Data Economy Podcast are now available to listen to in their entirety in our Podcast Library. Episodes are also available to listen to on Spotify, Apple Podcasts, or wherever you get your podcasts. If you’re interested in similar peer-led executive opportunities like events, reports, and ebooks, check out redis.com/business for the latest in data.Looking for the full vlog experience? Watch the first six episodes on our YouTube channel, and make sure to hit the subscribe button to be notified as soon as upcoming videos arrive."
42,https://redis.com/blog/fast-machine-learning-with-tecton-and-redis-enterprise-cloud/,Delivering Fast Machine Learning with Tecton and Redis Enterprise Cloud,"March 10, 2022",Eddie Esquivel and Ed Sandoval,"Real-time machine learning (ML) applications are everywhere — from approving credit card transactions as they happen, to immediately generating personalized recommendations for your favorite streaming service. These applications can’t afford any delay; they require live access to fresh data in order to provide ultra-low-latency inference (100 milliseconds or less). To give developers and organizations cost-effective real-time capabilities for high-scale ML applications, we’re excited to jointly announce a first-class integration of Tecton and Redis Enterprise Cloud.Redis Enterprise Cloud is the best version of Redis, delivering best-in-class performance, scalability and cost-effectiveness across cloud vendors. Redis has been voted the most loved database by developers for five consecutive years in Stack Overflow’s annual developer survey. Popular with financial services, e-commerce and gaming industries, Redis has a track record of meeting the most demanding latency (sub-millisecond) and high-availability requirements, making it ideally suited to serve the needs of real-time ML applications.Tecton is the leading feature store for enterprises looking to accelerate their time to production for ML projects. Its foundations came from the experience building Uber Michelangelo, the platform powering every ML application at Uber. Tecton is a system to operate and manage data pipelines and features for production ML applications. Typical use cases include fraud detection, real-time recommendations, dynamic pricing, and personalization.Now that Tecton’s Feature Store integrates with Redis Enterprise Cloud for online serving, it’s easier and more cost-effective than ever to productionize low-latency, high-throughput ML use cases. For Tecton users running at high scale, a benchmark analysis shows that Redis Enterprise achieves 3x faster latency while at the same time being 14x less expensive than Amazon DynamoDB (read more on Tecton’s blog here). In this article, we’ll take a deeper look at how Redis Enterprise and Tecton work together.To understand where Tecton and Redis fit together to enable real-time ML, let’s take a look at steps for productionizing an ML use case like fraud detection:Tecton’s Feature Store is designed to handle these steps, abstracting away all the work of orchestrating feature transformations and data pipelines so your data science and data engineering teams can focus on building models. However, Tecton is not a compute engine or a database. Instead, it sits on top of the infrastructure that customers already use, so you’re free to build the ML stack that’s right for you.Redis Enterprise Cloud is one of these infrastructure components, bringing customers a new high-performance option for the online store used by Tecton’s Feature Store.Tecton’s Feature Store supports the two main access patterns for ML: retrieving millions of rows of historical data for model training, and retrieving a single row, in a matter of milliseconds, to serve features to models running in production. Since these use cases are so different in terms of performance and cost tradeoffs, we support different types of databases for offline vs. online feature retrieval.For the offline feature store, Tecton supports S3, as it provides cost-effective storage that’s able to scale to meet your offline feature serving needs for model training. For the online feature store, Tecton now offers customers a flexible choice between DynamoDB (on-demand capacity mode) and Redis Enterprise Cloud.Without this dual database approach, many organizations implement separate data pipelines for offline training and online serving. Minor differences in how the pipelines are implemented can completely derail model performance, because the data the model sees in training does not match the data it encounters in production. This mismatch is called training-serving skew, and it is incredibly time-consuming to debug.Tecton’s Feature Store automatically resolves training-serving skew by coordinating data across offline and online environments, so it’s always synchronized. Users can start off using an offline store only with batch inference, and once they’re ready for online inference, update a single line of code to start materializing data into the online store.For Tecton users operating at high scale, one of the key advantages of using Redis Enterprise Cloud is performance and cost savings. Based on a benchmark analysis of serving online features at high-throughput, Redis was 3x faster and 14x cheaper than DynamoDB onTecton.Redis Enterprise Cloud also offers excellent operational capabilities ready to meet  current and future low-latency storage needs. It offers high availability with an SLA of 99.999% uptime, multiple database persistence, backup, and recovery options. Customers with large feature datasets can achieve additional cost savings by tiering the online feature store over DRAM and SSD.If you aren’t already using Redis Enterprise Cloud, you can sign up for an account here. We recommend that Tecton users deploy Redis Enterprise Cloud in AWS to minimize latency, as Tecton runs natively in AWS and is able to establish peering connectivity into Redis Enterprise Cloud. In the future, Tecton plans to add native support for other cloud vendor platforms.If you’re not a Tecton user and are interested in learning more, you can register for a free trial of Tecton here."
43,https://redis.com/blog/redis-enterprise-monitoring-options/,Redis Enterprise Monitoring Options,"June 8, 2020",Alexey Smolyanyy,"What would you say if you started a new role and arrived at your very first meeting with your very first customer, equipped with two days worth of plans for capacity planning, DNS matters, geo-distribution, and development in .NET … and the first question you got was: “How do we efficiently monitor the cluster and database?”Well, with 16 years years of operations experience, I thought I was ready. In reality, of course, “The more I learn, the more I realize how much I don’t know.” Now, when I see a new piece of software, one of the first questions comes to my mind is how to ensure it runs properly in production? In other words, how do I monitor it?With that in mind, I wrote this summary of Redis Enterprise monitoring options, from the system’s built-in monitoring capabilities to Prometheus Metrics Exporter—the best choice for many organizations—and the REST API for the most flexible way to integrate with third-party monitoring systems. I dedicate this blog post to my fellow operations people everywhere—the ones who don’t just want to run Redis Enterprise, but fully enjoy its flawless performance.The Redis Enterprise cluster management UI monitoring console is often the best place to start with monitoring Redis. It’s visually appealing and doesn’t require any additional installation or configuration. Using the “minute” interval, it works almost real-time; and you also can switch between 5-minute, hour, day, week, month, and year intervals. The UI provides metrics for the cluster, each cluster node, and each database, all in separate screens:The Redis Enterprise cluster management UI monitoring console provides separate screens for cluster, node, and database metrics.Redis Enterprise also has an alert mechanism. You can set separate alerts for a cluster and each database. Alerts are displayed in the management UI on relevant pages (alerts for the cluster show up on the Cluster page, and so on) and you can configure alerts to be sent by email (SMTP), as shown here:The set of the displayed metrics and alarms includes all major indicators, so you can start to monitor your Redis Enterprise just minutes after completing the installation. But what if you need more? Specifically, what if you want to integrate Redis Enterprise into your company’s existing monitoring infrastructure? That’s where Prometheus and Grafana come in.Prometheus and Grafana are a world-famous couple, together creating one of the most reliable modern monitoring tools. Redis Enterprise Cluster software includes an exporter for Prometheus metrics, so the most challenging task of any monitoring integration—collecting the proper metrics—is taken care of automatically.That helps make setup and configuration of Prometheus and Grafana quick and easy. The Redis documentation includes instructions for integration with Prometheus, a comprehensive list of exported metrics, and basic Grafana dashboards. Metrics are exposed at the node, database, shard, and proxy levels. Alerts can be set up in Prometheus AlertManager and conveniently delivered via the dozen different channels including email, Slack, PagerDuty, and others.For many companies, Prometheus and Grafana represent the best way to monitor Redis, not to mention a wide variety of other modern hardware and software.Just as important, using the Prometheus Metrics Exporter, a wide variety of monitoring platforms with the capability to scrape the metrics from the Prometheus exporter can be connected to Redis Enterprise. For example, New Relic recently published a blog post about integrating New Relic with Prometheus exporters.REST APIFor companies that need even more flexibility, the Redis Enterprise REST API provides a great deal of functionality, including getting both statistical metrics and alerts. The REST API is the most universal and flexible way to achieve a third-party monitoring integration, both internal or industry standard. The REST API documentation can be found in the usr/share/doc/redis/rlec_rest_api.tar.gz directory on each node where Redis Enterprise software is installed.A great example of using the REST API is the Redis Enterprise Add-On for Splunk, available in the Splunk marketplace. Another good example is the AppDynamics plugin, available at AppDynamics Exchange.Put it all together, and it’s clear that Redis Enterprise is well equipped with monitoring capabilities, giving customers a choice of which monitoring solution to use in a particular situation.In my opinion, if Redis Enterprise’s built-in monitoring is not enough for you, the best choice is usually Prometheus and Grafana. Especially since it can work with many popular enterprise monitoring systems. If that solution is not applicable for your organization, you can use the REST API to integrate with virtually any third-party monitoring system. If none of those options are acceptable, well, it’s time to get creative!"
44,https://redis.com/blog/announcing-public-preview-of-redis-enterprise-on-azure-cache/,Announcing Public Preview of Redis Enterprise on Azure Cache,"November 9, 2020",Cassie Zimmerman and Amiram Mizne,"At RedisConf 2020 Takeaway in May, we announced our expanding partnership with Microsoft Azure and introduced Redis Enterprise as two new tiers on Azure Cache for Redis. We are humbled by the tremendous interest and positive feedback we have received from the Redis community and are excited to announce that the service is now available in public preview.Today’s era presents new requirements for enterprise applications. Developers need the ability to iterate quickly with confidence in their underlying infrastructure and services. Consumers expect responsive and easy-to-use interfaces. Businesses require their applications to deliver sub-millisecond response time across millions of requests per second with the highest availability.It’s for these reasons that we have partnered with Microsoft to expand upon the already popular Azure Cache for Redis service. Together we have integrated Redis Enterprise Software with Azure Cache for Redis, providing customers access to an enhanced feature set best suited for business-critical workloads. The service provides two new Enterprise tiers, which are powered by Redis’ technologies and aimed specifically at the needs of enterprise customers. These tiers are engineered jointly by Redis and Microsoft. Moreover, as part of a native Azure service, they are operated and supported by Microsoft. Now available in public preview, these new tiers enhance the open source based Azure product suite with new support for:The public preview supports availability of up to 99.99% and is backed 24 x 7 x 365 by the Microsoft and Redis support teams. Public preview can be used for more than just testing and proof of concept purposes, and customers may consider it an option for staging, pre-production, or production workloads if the service meets your requirements.Redis Enterprise was built with enterprise developers in mind. Deciding whether the Enterprise tier is the right option for your organization is dependent on your business goals and application requirements. Here are a few key considerations:Redis is the home of open source Redis and Azure is committed to the contribution and support of open source projects. With the Enterprise tiers, developers will have access to the most up to date version of Redis released by the community. Users will receive direct access to support from the professionals who build and maintain Redis through the Azure portal.Customers and partners can now begin using Redis Enterprise on Azure Cache for Redis, in the same way they are used to with the existing Azure Cache tiers. We encourage you to explore the content available in the Azure Cache for Redis documentation and on the Azure Cache for Redis product page. Get started now with Azure Cache for Redis Enterprise."
45,https://redis.com/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/,Count-Min Sketch: The Art and Science of Estimating Stuff,"March 30, 2022",Itamar Haber,"This post is about what are, in my opinion, two of the most exciting things in the world: probabilistic data structures and Redis modules. If you’ve heard about one or the other then you can surely relate to my enthusiasm, but in case you want to catch up on the coolest stuff on earth just continue reading.I’ll start with this statement: large-scale low-latency data processing is challenging. The volumes and velocities of data involved can make real-time analysis extremely demanding. Due to its high performance and versatility, Redis is commonly used to solve such challenges. Its ability to store, manipulate and serve multiple forms of data in sub-millisecond latency makes it the ideal data container in many cases where online computation is needed.But everything is relative, and there are scales so extreme that they make accurate real-time analysis a practical impossibility. Complex problems only become more difficult as they get bigger, but we tend to forget that simple problems follow the same rule. Even something as basic as summing numbers can become a monumental task once data is too big, too fast or when we don’t have enough resources to process it. And while resources are always finite and expensive, data is constantly growing like nobody’s business.Count-min sketch (also called CM sketch) is a probabilistic data structure that’s extremely useful once you grasp how it works and, more importantly, how to use it.Fortunately, CM sketch’s simple characteristics make it relatively easy for novices to understand (turns out many of my friends were unable to follow along with this Top-K blog).CM sketch has been a Redis module for several years and was recently rewritten as part of the RedisBloom module v2.0. But before we dive into CM sketch, it is important to understand why you’d use any probabilistic data structure. In the triangle of speed, space, and accuracy, probabilistic data structures sacrifice some accuracy to gain space—potentially a lot of space! The effect on speed varies based on algorithms and set sizes.The right probabilistic data structure allows you to keep only part of the information in your dataset in exchange for reduced accuracy. Of course, in many cases (bank accounts, for example), reduced accuracy is unacceptable. But for recommending a movie or showing ads to website users, the cost of a relatively rare mistake is low and the space savings could be substantial.Basically, CM sketch works by aggregating the count of all items in your dataset into several counter arrays. Upon a query, it returns the item’s minimum count of all arrays, which promises to minimize the count inflation caused by collisions. Items with a low appearance rate or score (“mouse flows”) have a count below the error rate, so you lose any data about their real count and they are treated as noise. For items with a high appearance rate or score (“elephant flows”), simply use the count received. Considering CM sketch’s size is constant and it can be used for an infinite number of items, you can see the potential for huge savings in storage space.For background, sketches are a class of data structures and their accompanying algorithms. They capture the nature of your data in order to answer questions about it while using constant or sublinear space. CM sketch was described by Graham Cormode and S. Muthu Muthukrishnan in a 2005 paper called “An Improved Data Stream Summary: The Count-Min Sketch and its Applications.”CM sketch uses several arrays of counters to support its primary use cases. Let’s call the number of arrays “depth” and the number of counters in each array “width.”Whenever we receive an item, we use a hash function (which turns an element—a word, sentence, number, or binary—into a number that can be used as a location in the set/array or as a fingerprint)  to calculate the location of the item and increase that counter for each array. Each of the associated counters has a value equal to or higher than the real value of the item. When we make an inquiry, we go through all the arrays with the same hash functions and fetch the counter associated with our item. We then return the minimum value we encountered since we know our values are inflated (or equal).Even though we know that many items contribute to most counters, because of natural collisions (when different items receive the same location), we accept the ‘noise’ as long as it remains within our desired error rate.The math dictates that with a depth of 10 and a width of 2,000, the probability of not having an error is 99.9% and the error rate is 0.1%. (This is the percentage of total increments, not of unique items).In real numbers, that means if you add 1 million unique items, on average, each item will receive a value of 500 (1M/2K). Though that seems disproportionate, this falls well within our error rate of 0.1%, which is 1,000 on 1 million items.Similarly, if 10 elephants appear 10,000 times each, their value on all sets would be 10,000 or more. Whenever we count them in the future, we’d see an elephant in front of us. For all other numbers (i.e., all mice whose real count is 1), they are unlikely to collide with an elephant on all sets (since CM sketch considers only the minimum count) since the probability of this happening is slim and diminishes further if you increase the depth as you initialized CM sketch.Now that we understand the behavior of CM sketch, what can we do with this little beast? Here are some common use cases:In RedisBloom, the API for CM sketch is simple and easy:The following commands were used to create the animated example at the top of this post:As you can see, the value of ‘Redis’ is 4 instead of 3. This behavior is expected since, in CM sketch, the count of an item is likely to be inflated.Software engineering is all about making trade-offs, and a popular approach to addressing such challenges in a cost-effective way is to forgo accuracy in favor of efficiency. This approach is exemplified by Redis’ implementation of the HyperLogLog, a data structure that’s designed to efficiently provide answers to queries about set cardinality. The HyperLogLog is a member of a family of data structures called “sketches” that, just like their real-world artistic counterparts, convey information via an approximation of their subjects.In broad strokes, sketches are data structures (and their accompanying algorithms) that capture the nature of your data—the answers to your questions about the data, without actually storing the data itself. Formally stated, sketches are useful because they have sublinear asymptotic computational complexity, thus requiring less computing power and/or storage. But there are no free lunches and the gain in efficiency is offset by the accuracy of the answers. In many cases, however, errors are acceptable as long as they can be kept under a threshold. A good data sketch is one that readily admits its errors, and in fact, many sketches parameterize their errors (or the probability of said errors) so that they can be defined by the user.Good sketches are efficient and have bound probability of error, but excellent sketches are those that can be computed in parallel. A parallelizable sketch is one that can be prepared independently on parts of the data and that allows combining its parts into a meaningful and consistent aggregate. Because each piece of an excellent sketch can be computed at a different location and/or time, parallelism makes it possible to apply a straightforward divide-and-conquer approach to solving scaling challenges.The aforementioned HyperLogLog is an excellent sketch but it is only suited for answering a specific type of question. Another invaluable tool is the Count Min Sketch (CMS), as described in the paper “An Improved Data Stream Summary: The Count-Min Sketch and its Applications” by G. Cormode and S. Muthukrishnan. This ingenious contraption was contrived to provide answers about the frequency of samples, a common building block in a large percentage of analytical processes.Given enough time and resources, calculating samples’ frequency is a simple process – just keep a count of observations (times seen) for each sample (thing seen) and then divide that by the total number of observations to obtain that sample’s frequency. However, given the context of high-scale low-latency data processing, there’s never enough time or resources. Answers are to be supplied instantly as the data streams in, regardless of its scale, and the sheer size of the sampling space makes it unfeasible to keep a counter for each sample.So instead of accurately keeping track of each sample, we can try estimating the frequency. One way of going about that is to randomly sample the observations and hope that the sample generally reflects the properties of the whole. The problem with this approach is that ensuring true randomness is a difficult task, so the success of random sampling may be limited by our selection process and/or the properties of the data itself. That is where CMS comes in with an approach so radically different, that at first, it may seem like the opposite of an excellent sketch: not only does CMS record each and every observation, each one is recorded in multiple counters!Of course, there is a twist, and it is as clever as it is simple. The original paper (and its lighter version called “Approximating Data with the Count-Min Data Structure”) does a great job of explaining it, but I’ll try to summarize it anyway. The cleverness of CMS is in the way that it stores samples: instead of tracking each unique sample independently, it uses its hash value. The hash value of a sample is used as the index to a constant-sized (parameterized as d in the paper) array of counters. By employing several (the parameter w) different hash functions and their respective arrays, the sketch handles hash collisions found while querying the structure by picking the minimum value out of all relevant counters for the sample.An example is called for, so let’s make a simple sketch to illustrate the data structure’s inner workings. To keep the sketch simple, we’ll use small parameter values. We’ll set w to 3, meaning we’ll use three hash functions – denoted h1, h2, and h3 respectively, and d to 4. To store the sketch’s counters we’ll use a 3×4 array with a total of 12 elements initialized to 0.Now we can examine what happens when samples are added to the sketch. Let’s assume samples arrive one by one and that the hashes for the first sample, denoted as s1, are: h1(s1) = 1, h2(s1) = 2 and h3(s1) = 3. To record s1 in the sketch we’ll increment each hash function’s counter at the relevant index by 1. The following table shows the array’s initial and current states:Although there’s only one sample in the sketch, we can already query it effectively. Remember that the number of observations for a sample is the minimum of all its counters, so for s1 it is obtained by:min(array[1][h1(s1)], array[2][h2(s1)], array[3][h3(s1)]) =
min(array[1][1], array[2][2], array[3][3]) =
min(1,1,1) = 1The sketch also answers queries about the samples not yet added. Assuming that h1(s2) = 4, h2(s2) = 4, h3(s2) = 4, note that querying for s2 will return the result 0. Let’s continue to add s2 and s3 (h1(s3) = 1, h2(s3) = 1, h3(s3) = 1) to the sketch, yielding the following:In our contrived example, almost all of the samples’ hashes map to unique counters, with the one exception being the collision of h1(s1) and h1(s3). Because both hashes are the same, h1‘s 1st counter now holds the value 2. Since the sketch picks the minimum of all counters, the queries for s1 and s3 still return the correct result of 1. Eventually, however, once enough collisions have occurred, the queries’ results will become less accurate.CMS’ two parameters – w and d – determine its space/time requirements as well as the probability and maximal value of its error. A more intuitive way to initialize the sketch is to provide the error’s probability and cap, allowing it to then derive the required values for w and d. Parallelization is possible because any number of sub-sketches can be trivially merged as the sum of arrays, as long as the same parameters and hash functions are used in constructing them.Count Min Sketch’s efficiency can be demonstrated by reviewing its requirements. The space complexity of CMS is the product of w, d, and the width of the counters that it uses. For example, a sketch that has a 0.01% error rate at a probability of 0.01% is created using 10 hash functions and 2000-counter arrays. Assuming the use of 16-bit counters, the overall memory requirement of the resulting sketch’s data structure clocks in at 40KB (a couple of additional bytes are needed for storing the total number of observations and a few pointers). The sketch’s other computational aspect is similarly pleasing—because hash functions are cheap to produce and compute, accessing the data structure, whether for reading or writing, is also performed in constant time.There’s more to CMS; the sketch’s authors have also shown how it can be used for answering other similar questions. These include estimating percentiles and identifying heavy hitters (frequent items) but are out of scope for this post.CMS is certainly an excellent sketch, but there are at least two things that prevent it from achieving perfection. My first reservation about CMS is that it can be biased, and thus overestimate the frequencies of samples with a small number of observations. CMS’ bias is well known and several improvements have been suggested. The most recent is the Count Min Log Sketch (“Approximately counting with approximate counters” from G. Pitel and G. Fouquier), which essentially replaces CMS’ linear registers with logarithmic ones to reduce the relative error and allow higher counts without increasing the width of counter registers.While the above reservation is shared by everyone (albeit only those who grok data structures), my second reservation is exclusive to the Redis community. In order to explain, I’ll have to introduce Redis Modules.Redis Modules were unveiled earlier this year at RedisConf by antirez and have literally turned our world upside-down. Nothing more or less than server-loadable dynamic libraries, Modules allow Redis users to move faster than Redis itself and go places never dreamt of before. And while this post isn’t an introduction to what Modules are or how to make them, this one is (as well as this post, and this webinar).There are several reasons I wanted to write the Count Min Sketch Redis module, aside from its extreme usefulness. Part of it was a learning experience and part of it was an evaluation of the modules API, but mostly it was just a whole lot of fun to model a new data structure into Redis. The module provides a Redis interface for adding observations to the sketch, querying it, and merging multiple sketches into one.The module stores the sketch’s data in a Redis String and uses direct memory access (DMA) for mapping the contents of the key to its internal data structure. I’ve yet to conduct exhaustive performance benchmarks on it, but my initial impression from testing it locally is that it is as performant as any of the core Redis commands. Like our other modules, countminsketch is open source and I encourage you to try it out and hack on it.Before signing off, I’d like to keep my promise and share my Redis-specific reservation about CMS. The issue, which also applies to other sketches and data structures, is that CMS requires you to set it up/initialize it/create it before using it. Requiring a mandatory initialization stage, such as CMS parameters setup, breaks one of Redis’ fundamental patterns—data structures do not need to be explicitly declared before use as they can be created on-demand. To make the module seem more Redis-ish and work around that anti-pattern, the module uses default parameter values when a new sketch is implicitly implied (i.e. using the CMS.ADD command on a non-existing key) but also allows creating new empty sketches with given parameters.Probabilistic data structures, or sketches, are amazing tools that let us keep up with the growth of big data and the shrinkage of latency budgets in an efficient and sufficiently accurate way. The two sketches mentioned in this post, and others such as the Bloom Filter and the T-digest, are quickly becoming indispensable tools in the modern data monger’s arsenal. Modules allow you to extend Redis with custom data types and commands that operate at native speed and have local access to the data. The possibilities are endless and nothing is impossible.Want to learn more about Redis modules and how to develop them? Is there a data structure, whether probabilistic or not, that you want to discuss or add to Redis? Feel free to reach out to me with anything at my Twitter account or via email – I’m highly available 🙂"
46,https://redis.com/blog/redis-iso-27001-cybersecurity-certification/,Redis Achieves ISO 27001 Cybersecurity Certification,"July 19, 2022",Quincy Castro,"We are excited to announce that Redis has achieved the ISO/IEC 27001:2013 cybersecurity certification!ISO/IEC 27001:2013 is an internationally recognized standard that defines best practices for a company’s information security management system (ISMS). The standard is designed to ensure security management best practices are followed and ensures comprehensive security controls are in place. ISO 27001 certification, as audited and attested to by an independent third party, further demonstrates the ongoing security investments Redis is making to deliver the most robust, scalable version of Redis available anywhere.The ISMS defines how Redis continuously manages security by demonstrating that:With ISO/IEC 27001:2013 certification, Redis’ ISO 27001 certification brings you closer to meeting the compliance requirements that are relevant to your business.  In addition to ISO 27001, Redis Enterprise Cloud maintains SOC 2 Type II compliance."
47,https://redis.com/blog/introducing-our-first-redis-insiders/,Introducing Our First Redis Insiders,"August 10, 2022",Suze Shardlow,"Back in May, we launched our first ambassador program: Redis Insiders. We received a great response from the community. Now, we can introduce our first four Redis Insiders to the world.Jyotsna is based in Bangalore, India, and is a Senior Software Engineer at Gojek (GoPay). She is an open source enthusiast and has been using Redis for the past three years, with Golang and Java. She applied to become a Redis Insider because she is passionate about communities and loves to share and learn together. When she’s not working or volunteering, she energizes herself by playing badminton, table tennis, basketball, Carrom, and chess.Moiz is a Paris, France-based Solution Architect at Capgemini who builds apps in the enterprise landscape of a major European car manufacturer using Java / J2EE. Moiz was drawn to the Redis Insiders program because, since he started using Redis 1.5 years ago, the simplicity and power of Redis have fascinated him. His hobbies include cycling along the River Seine, camping, reading classic literature, and philately.Michael is a Senior Software Engineer at Spleet from Lagos, Nigeria. He is a JavaScript programmer and has been using Redis for over two years. Michael enjoys traveling, adventure, writing, and volunteering in his spare time. Michael applied to become a Redis Insider because, as a lover of Redis himself, he wanted an opportunity to meet with and learn from other Redis professionals around the globe and also share his knowledge of Redis through speaking and writing.Stevan is a Senior Software Engineer at Vela with five years of Redis experience. He builds web, mobile, and desktop apps for various industries, including shipping/logistics, finance, retail, and health, using JavaScript, Swift, Java, C#, and Python. Stevan lives in the Port of Spain, Trinidad and Tobago, and, in his spare time, enjoys hiking, fitness / CrossFit, watching movies, and learning new technologies. He is excited to join the Redis Insiders program because he wants to be a positive influence in the adoption of Redis as a primary database for developers.The Redis Insiders will voluntarily work with me and the rest of the Redis Developer Relations team to educate and enthuse our large and growing user base. What’s obvious to you and me isn’t necessarily obvious to everyone. Our overarching goal is to make it frictionless for developers to get started using Redis to solve problems they’re working on.They’ll spread the word about Redis via our channels, including our blog, live streams, Discord server, and third-party channels like conferences, meetups, and tech publications. They’ll act as the developer community’s voice and give us feedback on the community’s pain points and what they’d love to see more of.Read more on our Redis Insiders page."
48,https://redis.com/blog/redis-7-generally-available/,Redis 7.0 Is Out!,"April 27, 2022",Itamar Haber,"Today we’re happy to tell the world about the general availability of Redis version 7.0, as announced at the Redis Days SF keynote earlier this year. The release has been under development for almost a year, and three release candidates preceded it, so we feel it is stable enough for use in production.Upgrading from earlier versions is a relatively straightforward process, as backward compatibility has always been a design principle of the Redis project. However, before upgrading to Redis 7.0, please take a few minutes and get acquainted with the new version by reading the release notes.In a nutshell, Redis 7.0 includes incremental improvements to almost every one of its aspects. Most notable are Redis Functions, ACLv2, command introspection, and Sharded Pub/Sub, which represent a significant evolution of existing features based on users’ feedback and lessons learned in production.Version 7.0 adds almost 50 new commands and options to support this evolution and extend Redis’ existing capabilities. For example, the bitmap, list, set, sorted set, and stream data types have all been added with functionality that supports their use cases for data management. In addition, cache semantics have been extended to support existential and comparative modifiers.While user-facing features are easy to boast of, the real “unsung heroes” in this version are efforts to make Redis more performant, stable, and lean. A large share of our developers’ brain cycles was invested in making the operation of Redis more effective by focusing on its performance vis-à-vis the resources that it uses. Redis 7.0 brings a number of improvements to almost every subsystem it manages, including memory, computing, network, and storage. Whereas some optimizations are enabled by default, others may require configuration. Please refer to the inline documentation in the redis.conf file for details.While the release of this new version is something for us to celebrate here at Redis, we are already busy working on making Redis 7.2 a reality. If you encounter any issues or have thoughts to share, we’d love to hear from you at the Redis repository."
49,https://redis.com/blog/streaming-analytics-with-probabilistic-data-structures/,Probabilistic Data Structures in Redis,"August 11, 2022",Savannah Norem,"Hello World! My name is Savannah, and I’m a new-ish Developer Advocate at Redis. I’ve hopped on livestream to talk about RedisJSON with our Senior Developer Advocates Justin and Guy to discuss probabilistic data structures. That exploration with Guy is ephemeral on Twitch, but I recently did some coding using a few more probabilistic data structures that you can watch on YouTube. There can be some confusion about what a probabilistic data structure is, so I figured I’d take some time to write out the highlights of these data structures that take large datasets.Google Chrome, Akami, and your ISP will likely use some types of probabilistic data structures to do things that improve your life as an end user. So if you’ve got large datasets, think data structures are cool, or want to see where those big names would use these things, read on!Download our white paper “Redis HyperLogLog: A Deep Dive.”Redis offers a few different probabilistic data structures – some in the RedisBloom module (that’s included in redis-py, my language of choice) and one in core Redis, but RedisBloom is now included with all the client libraries, so there is no need to import anything else if you’re using Redis Stack. In this post, I’ll tell you about Bloom filters, cuckoo filters, Top-K, Count-Min Sketch, and Hyperloglog. We’ll walk through what the probabilistic nature of each is, a high-level overview of how they work, and some key use cases. We’ll start with the following Redis connection and definition for these code examples.Named after the person that first wrote down the idea, Bloom filters can tell us probabilistic membership, that is, whether or not something has been added to the filter.Bloom filters work as a bit array, where items are added, and certain bits are set. This can have overlap; however, multiple things could map to some of the same bits. These items are added with no issue to the filter unless all the bits they correspond to have already been set, in which case the response is that the item may have already been added to the filter. Since a not set bit means that an item has not been added, there are no false negatives.However, because of the bit overlaps of different items and the fact that Bloom filters don’t store any information about what has been added, there is no deletion. We’ll show how to create our Bloom filter, a multi-add, and a multi-exists check. The output here is probabilistic and could be either a false positive or a true negative. In this case, with only adding three things to a filter set to store 1,000, it’s almost guaranteed to be a true negative.Other commands are available to get information about the Bloom filter, save and load it, and a few others. But creating it, adding things to it, and checking if those things exist is all you need to accomplish the main purpose of the Bloom filter.These filters can be used in place of testing set membership for large datasets quickly. For example, the Google Chrome browser maintains a Bloom filter of malicious URLs. When someone tries to go to a website in Chrome, the browser first checks the Bloom filter. Since there are no false negatives, if the filter returns that it’s not there, Chrome allows the user to go through to the URL quickly and without having to check a large dataset. Because Bloom filters are just bit arrays, they can often be stored in a browser cache, saving time against large and far away database lookups.On top of the standard Bloom filter I’ve explained above, the Redis implementation has an option to become a Scaling Bloom filter. There are a couple of different things that people have tacked on to Bloom filters over the years, adding different functionality while mostly maintaining the core ideas. For instance, a Stable Bloom filter can deal better with unbounded data without the need to scale but can introduce false negatives. Other types of Bloom filters include Counting Bloom filters, Distributed Bloom filters, Layered Bloom filters, and more.Named after a bird that famously pushes other birds’ eggs out of their nests and takes them over, cuckoo filters also tell us probabilistic membership, but in a very different way than Bloom filters.Cuckoo filters keep hold of a fingerprint of each item that’s been added and store multiple fingerprints in each bucket. The length, in bits, of the fingerprints determines how quickly there are collisions, which helps determine the number of buckets you’ll need and what error rate you can get. When a collision occurs, one of the fingerprints in the bucket will get kicked out and put into a different bucket, hence the name.Because a fingerprint is stored, cuckoo filters do support deletion. However, the collisions still exist, and you could end up deleting something that you didn’t mean to delete, which can lead to some false negatives. Like the Bloom filter, the main things we want to do are create one, add things to it, and check if they exist. However, we must use the `insert` command to add multiple things to the cuckoo filter. The cuckoo filter also has a command for adding an item if it doesn’t already exist: `addnx`.However, because the `exists` command is probabilistic, attempting to delete an item added this way can cause false negatives. Here we’ll see the same probabilistic output of `mexists` as the Bloom filter.The main draw of a cuckoo filter is the ability to delete entries, and aside from cases where that’s an essential requirement, Bloom filters are generally used.Like a sketch is a rough drawing, a Count-Min Sketch (aka CM Sketch) will return a rough guess of the minimum count of how many times an item was added to the sketch. This means, that this data structure answers item frequency questions. In some ways, CM Sketch is similar to HyperLogLog as they both count elements, with the crucial difference that CM Sketch doesn’t count unique elements but rather how many copies of each element you added to it.Like a piece of the Top-K structure, the Count-Min Sketch is a table where each row represents a different hash function being used, and each item added gets a spot in each row. However, unlike the Top-K, this data structure is just the counts, with no regard for what the counts are attached to. So in the case of a collision, the count is simply incremented with no concern for the fact that the count is now the count of two different things combined. This is why when you query for the count of an item, the counts from each row are considered, and you get back the minimum.For a Count-Min Sketch, we can either initialize it by the probabilities we want to maintain or the dimensions we want. This will be somewhat important because to merge two Count-Min Sketches, they must have the same dimensions.Some important things to note (about the commands in the Python code below), you don’t actually add things to the Count-Min Sketch use `incrby` and increase the count by a specified amount. Even if you only increment one thing, it has to be passed in as a list. They can then be merged with specified weights, where the weights essentially multiply the counts in the respective Sketches. The merged Sketch is then queried.In this code, I am no longer able to see what used to be in `cms1`, but I can still use the original `cms2`. I could have also created a new Count-Min Sketch `cms3` (with the same dimensions) and then merged `cms1` & `cms2` into `cms3`, preserving the original data of both `cms1` & `cms2`.The output of querying for baz is 6 because we used `incrby` with 2 and then a weight of 3. However, once a significant number of things are added, this count will become less accurate.This data structure can be used for large amounts of data when storing what each item becomes impractical because instead of storing whatever the item is, you’re only storing a table of numbers. You can then query how many times any given item has been added and get the minimum count of the spaces it maps to, which may still be slightly inflated depending on the number of collisions that have happened.Named after its functionality, a Top-K data structure will give you back an approximation of the Top-K counts stored in it.There are two data structures at play here; a table that keeps track of fingerprints and their counts and a min heap structure that keeps track of what the Top-K things in the table are. Each row in the table represents a different hash function being used, so each item added gets hashed to a spot in each row. Redis’ implementation, in particular, uses an algorithm called HeavyKeeper. With this algorithm, the basic idea is that whenever there’s a collision between fingerprints, there’s a probability that the count of what was already there will be decremented. This means that items that get added a lot have high counts and a low probability of being decayed, while items that don’t will be overwritten over time.To learn more about available Top-K commands, we’ll reserve the space for one, specifying first the number of top things we want to save and then the width, depth, and decay that we want (required by redis-py, but not in the Redis CLI, as you can watch me find out in one of our Redis Live recordings.  . We add things to Top-K we can add multiple things or just one with the same command. We can then get the count of any of the things we’ve added.But when we use `query`, we only find out whether or not the thing we query for is in our specified Top-K items. So, for instance, we initialize ‘topk’ to only keep the top 1 item in the min heap. So querying for bar will return 0 since we add ‘foo’ multiple times. Querying for ‘foo’ will return 1. We can also get the list of the Top-K items. When a significant number of things are added and go through the probabilistic decay, this Top-K can lose some of its precision.The more hash functions used, the less likely collisions are. In the Top-K structure, the maximum count for a single item is what will be checked against the min heap. So for an item to really have its count decreased, it has to have a hash collision on every row. This still means there’s a chance that the Top-K min heap will end up with, for example, the sixth most common thing added instead of the fifth. This data structure is often used to monitor network traffic and track what IP addresses are flooding a network and potentially trying to cause DDoS.After starting with linear counting, then moving to probabilistic counting, this name started with a paper from 2003 proposing the LogLog algorithm and an improved version called Super-LogLog, and now we have HyperLogLog.One of the first data structures available in Redis, HyperLogLog keeps a probabilistic count of how many unique items have been added. Like the Bloom filter, this data structure runs items through hash functions and then sets bits in a type of bitfield. Also, like a Bloom filter, this structure doesn’t retain information about the individual items being added, so there is no deletion here. This data structure doesn’t have many commands available, making it easier to use than others. With only add, count, merge, and two testing commands, we can easily see what you can do. You can add things to it, get a count of how many unique things have been added, and merge them together. Merging them together will result in a HyperLogLog that only contains the unique items from each – that is – items that were in both will not be duplicated, and they’ll only be counted as one unique item in the merged HyperLogLog.Because HyperLogLog is in core Redis – not the RedisBloom module – in the commands below, we don’t see the `r.hll().add`, nor do we have to actually create or reserve one; we just start adding things to it. We can merge multiple HyperLogLogs together and get a probabilistic count of how many unique things are in them. Again, as more things are added, the more likely this count is to lose some precision.This data structure is useful for counting unique items in large datasets, for instance, unique IP addresses that hit websites, unique video viewers, etc.Well, if you have a very large dataset or a big data application, you likely want some quick things from it that are okay with certain inaccuracies because those inaccuracies come with major space-saving. Knowing quickly that something is definitely not present might be worth the rate of false positives. Knowing that out of your top 10, you might have a few things that are really in the top 15 and missing a few of the 10, but depending on what you’re hoping to do with that information, that’s just fine. If you’re using Chrome, Akami, or really any part of the internet, you’re probably benefitting from some probabilistic data structures.I hope you learned something about why and when probabilistic data structures can help you get information about large datasets, all while not taking up as much space – granted, with a bit less precision in the information. I hope you check out the probabilistic data structures Redis has to offer for you. Should you have any questions, come find us on discord, or check out the Redis YouTube channel and see what you can do with Redis."
50,https://redis.com/blog/register-for-redisdays-san-francisco-2022/,Register for RedisDays San Francisco 2022,"March 11, 2022",Redis,"RedisDays, the three-part virtual roadshow that spotlights the latest real-time data innovations at Redis, makes its first stateside stop in San Francisco, CA on Wednesday, March 23rd.Our San Francisco day will be 100% dedicated to developers. During the day, you’ll hear firsthand the latest developer Redis announcements, you’ll be able to tune into fireside chats, live demonstrations, and watch practical workshops on building Redis-based apps with newly introduced Redis tools.Plus, attendees are automatically entered to win some incredible prizes. We are raffling off Oculus Quest 2™ VR systems and Nintendo Switches™ at the conclusion of each RedisDays, plus win gift cards (with mystery amounts ranging from $5 to $500) when you complete an event survey after watching the keynote or a customer session. Learn more about the scheduled sessions below and be sure to register for RedisDays New York City, taking place on Thursday, March 31st.Join Redis Co-Founder and CTO, Yiftach Shoolman, as he and other Redis guests share the latest developments in the evolution of Redis with product updates primed for the Redis developer community.How does open source help drive innovation in creating unique shopping experiences in brick-and-mortar and digital stores within the beauty industry? Sit in on this fireside chat between Redis’ Udi Gotlieb and Omar Koncobo, IT Director of E-commerce and Digital Systems at Ulta Beauty.How has Redis upped its developer offering? It’s all about providing the tools necessary to build faster by being more flexible with your data types. Hear from VMware’s Josh Long and Redis’ Brian Sam-Boden on how to get search, JSON documents, key-value data, streams, graph data, and more at your fingertips, to help you build better schemas to synthesize your data.REGISTER for RedisDays San Francisco"
51,https://redis.com/blog/redis-io-refresh/,Refreshing and Extending Redis.io,"March 23, 2022",Kyle Banker and Itamar Haber,"Today we’re pleased to announce the relaunch of Redis.io. Redis.io has always been the home of Redis and the entry point for new Redis users. With this launch, we’ve revised the core Redis documentation while modernizing the site’s design and updating its infrastructure.In this post, we’d like to explain our motivations for the site relaunch, share the principles we’ve adopted in revising the Redis.io site, and preview what’s next.Redis has come a long way in the thirteen years since its first commit. Now established as a foundational database technology, deployed widely across data centers and clouds the world over, and reliably processing untold billions of requests every second, the Redis open source project continues to advance.All the while, redis.io has served as the home and core documentation site for Redis. But this site has remained largely unchanged since its unveiling back in 2012. When we started asking ourselves how we could improve upon redis.io to reflect the importance of the Redis open source project and to better serve the Redis community, we discovered several glaring issues worth addressing.The first of these issues concerned the Redis documentation itself. Modern software documentation should be well organized, navigable, and up to date. For this first major update, we’ve reorganized the Redis docs, added site navigation, removed a lot of legacy content, and completed a light, first-pass copyedit. In doing so, we’ve significantly improved the user experience of the Redis docs while also setting the stage for the many longer-term improvements we and the larger Redis community need to make.Our next concern was with the site’s design. We wanted to create a fresh, navigable, and modern design for the site while maintaining certain elements from the original design. We hope that the final result speaks for itself.Finally, we wanted to update the site’s infrastructure. The site had been running on a bespoke Ruby application, written eleven years ago, that was getting hard to maintain. Now, redis.io runs on Hugo, a well-known and widely deployed static site generator.As the sponsors of Redis, we strongly believe in the importance of open source for the health and continued growth of the Redis project. We’re committed to keeping Redis open source, as it’s been since day one. We are the driving force behind the project’s governance, contributing the majority of its code, and mentoring and educating hundreds of developers around the world on how to become Redis contributors. We have never asked anyone to sign a Contributor License Agreement, and we don’t plan to do so in the future. The bottom line is that we take our stewardship of Redis seriously, and we’re explicitly committed to Redis remaining open source.At the same time, redis.io has always been focused on helping developers build with Redis. We’re now taking this further by also introducing Redis Stack on redis.io. Licensed under the Redis Source Available License (or RSAL), Redis Stack unites the capabilities of the leading Redis modules (RediSearch, RedisJSON, RedisGraph, RedisTimeSeries, and RedisBloom) under a single product. We believe that Redis Stack represents an important progression for any developer wanting to take the speed and stability of Redis into new domains and problem spaces.And, while we are now hosting both OSS and RSAL-licensed projects on redis.io, we’re also taking great pains to ensure that the line between these projects is clear. For example, the documentation for open source Redis will always be maintained with its current license and in its own repository. We’re also explicitly calling out the portions of the site that apply to RSAL-licensed projects.At the end of the day, we believe that we can effectively sponsor and promote open source Redis while also providing an onramp to the RSAL-licensed Redis Stack. This new redis.io represents a significant and long-needed improvement to the core Redis docs; to the look, feel, and usability of the site; and to the future development of redis.io for the benefit of Redis developers everywhere. We welcome your comments and contributions, and we look forward to continuing to improve the home of Redis."
52,https://redis.com/blog/redis-insiders-program/,Announcing the Redis Insiders Program,"May 2, 2022",Suze Shardlow,"We’re excited to launch our first ambassador program: Redis Insiders.Redis is open source and has millions of users around the world. That puts our developer community at the heart of our products. We’re eager to work together with our most dedicated and passionate community members to raise their profiles by educating the world about Redis.If you’re a developer who uses Redis and loves to write and talk about your code, you could become a Redis Insider. We particularly encourage applications from underrepresented groups in tech.The Redis Insiders will work closely, on a voluntary basis, with the Redis Developer Relations team to educate and enthuse our large and growing user base. What’s obvious to you and me isn’t necessarily obvious to everyone, and our overarching goal is to make it frictionless for developers to get started using Redis to solve problems they’re working on. We’re looking for folks who are and will continue to be good Redis community members: kind, respectful and inclusive.You’ll be spreading the word about Redis, via our channels and third-party channels (e.g. conferences, meetups, tech publications). We’re asking you to commit to a minimum tenure of six months in the first instance, with the opportunity to renew your role up to three times (for a maximum possible total of two years). Possible activities for Redis Insiders include:Another important component of the Redis Insiders role is to act as the voice of the developer community and bring us feedback around your Redis pain points and what you’d love to see more of. We want you to tell us what you need from our products to help you reach your goals, and we’ll meet with you on a regular basis to talk this through. We’d also love it if you could make code contributions to our repos, for example, the Redis code base, Redis University courses, or our client libraries and demo applications.We are immensely grateful to anyone who gives up their time to champion our technology, and we will support you in your role as a Redis Insider. While we’ll be expecting you to have some experience in technical blogging and public speaking, we know that some people are stronger at, say, writing, than they are at presenting in public, or vice versa. Our Developer Relations team is comprised of professional speakers and writers. We can coach you in the areas where you feel you need it, as well as proofread your blog posts and help you ideate and practice your talks.Some of the other things we can offer include:As a Redis Insider, you’ll be part of a select group of developers who love to write and talk about code.  Bearing in mind the types of activities you’ll be working on, you’ll need to be a confident public speaker with demonstrable experience in coding, using Redis, and creating different types of content. You’ll also need a good standard of written and spoken English. In terms of Redis knowledge, at a minimum, you’ll need to have passed our Redis University RU101 course, Introduction To Data Structures by the time you become an Insider. (I took this course when I joined the company – see how I got on!)Demonstrable experience in any of the following will be a plus:If this all sounds interesting, you can apply by completing the Redis Insiders application form. If we think you might be a good fit, we’ll invite you to join us for a call so we can get to know you a bit better. We want to bring together a diverse group of developers. We don’t have a closing date for applications, but we want to onboard the first cohort of Insiders by the end of July. If you are someone from an underrepresented group in tech who is unsure about applying and would like to chat with me beforehand, send me an e-mail. I would love to hear from you."
53,https://redis.com/blog/redis-developer-hub-expansion/,Redis Developer Hub Expands to Support the Needs of DevOps Teams,"April 6, 2022",Talon Miller and Ajeet Raina,"The Redis Developer Hub has always been a great resource for developers looking to build with Redis. With more than 200+ free tutorials, which were accessed by millions of developers over the last year, it offers invaluable training and instruction for developers looking to Create, Develop, and Explore.However, Redis recognizes that DevOps teams must focus on different things than developers do—issues such as stability, maintainability, and keeping a consistent flow of applications improved and deployed. Your job as a DevOps professional requires you to think holistically, deploying and maintaining many apps on your infrastructure frequently at a worldwide scale. Recognizing the specific challenges of DevOps, we have now expanded the Redis Developer Hub to specifically address the needs of DevOps professionals.Today, with the addition of Operate, we are announcing an exciting milestone for developers, DevOps engineers, and SREs seeking to learn how to easily integrate Redis into their DevOps cycle to accelerate application deployment.Redis has become a popular database choice, not only for developers but among DevOps teams, due to its unmatched simplicity and exceptionally high performance. In Datadog’s 2021 Container Report Redis was the most popular container image running in Kubernetes StatefulSets. Redis fits very well into the DevOps model due to its ease of deployment, reduced management toil, and low overhead. Redis Enterprise offers uninterrupted high availability, low latency, and automated linear scalability—all crucial features for DevOps teams.Here at Redis, we know that rapid deployment is key to a successful DevOps approach. Therefore, in this new section of our Developer Hub, we have introduced a collection of rich technical content to help DevOps and development teams operate Redis at a faster pace.Databases are now part of the continuous integration and continuous deployment (CI/CD) pipeline. If a DevOps pipeline doesn’t include the database, it becomes a bottleneck that slows down the delivery of new features. In fact, DevOps teams integrate databases not only in the development pipeline but also in the overall release pipeline. To address this reality, we have included tutorials to help you embed Redis easily into your CI/CD pipeline.One method to embed Redis into your CI/CD pipeline is through Argo CD. We have a tutorial that goes through what Argo CD is, how it works, and ultimately how to deploy an application with Redis Enterprise inside of an Argo CD pipeline.Observability goes significantly beyond basic monitoring and is a key capability for high-performing DevOps teams. Therefore, we’ve introduced tutorials around the tools and techniques that enable DevOps teams to observe key indicators to operate Redis at scale, such as throughput, latency, and capacity.An example of how you can stay on top of key service level objectives (SLOs) on your Redis Enterprise database and/or cluster is through our integration with Datadog. We have a tutorial that goes through crucial Redis service level agreements (SLAs), key performance indicators to watch, and how to get started with this integration to improve observability.DevOps teams strive to provision and manage their databases just like they do with application code. Changes to databases are recognized as just another code deployment to be managed, tested, updated, automated, and improved with the same kind of seamless, robust, reliable methodologies applied to application code.An example of how you can efficiently provision Redis Enterprise is through Azure Cache using Terraform. We now have a tutorial that dives into the key features of Azure Resource Manager to manage your Redis Enterprise cluster and instructs you on how to get started with the Azure Cache for Redis Enterprise using Terraform with Private Link.Orchestrating databases is a unique challenge for DevOps teams. Rapid release of new application features and reduction in deployment time are the two primary concerns for most DevOps teams today. That’s why we address orchestration in our new DevOps resources to help you quickly and efficiently provision Redis and accelerate app deployment.An example of orchestrating a Redis Enterprise database and/or cluster is learning about how to run a Node.js application using Nginx, Docker, and Redis as the database with a detailed step-by-step tutorial.Does our new DevOps journey sound exciting? We invite you to check it out today and we welcome your feedback and comments. Feel free to contribute by raising a Pull Request."
54,https://redis.com/blog/real-time-trading-platform-with-redis-enterprise/,Building a Real-Time Trading Platform with Redis,"June 10, 2021",Toby Ferguson and Prasanna Rajagopal,"Portfolios form the foundation of the wealth and asset management industry. Ever since Harry Makowitz pioneered modern portfolio theory, asset and wealth management professionals have obsessed over maximizing the returns on their portfolio for a given level of risk. Today, the professionals in the industry are joined by millions of retail investors, whom have forever changed the landscape of investing. These new entrants are creating huge ramifications for the technology underpinning the trading infrastructure of retail brokerages, exchanges, and clearing houses.Take, for instance, the GameStop stock mania of January 2021. Retail investors started trading GameStop stock at record levels. These investors also piled into other meme stocks like AMC Entertainment, causing the overall market volatility to rise more than 76% in a matter of a few trading days as measured by the VIX. This volatility led to price pressure on thousands of securities. Millions of investors were frantically trying to access their portfolios at the same time, but were faced with apps that couldn’t keep up with demand. Investors are not kind to companies whose apps do not perform well when they need it the most.During these frantic times, most investors are looking for two data points about their portfolio that they need access to at all times:The answers to these questions can lead an investor to buy, sell, or hold specific securities. In today’s fast-moving markets, any delays could mean lost opportunity and profits. You need real-time access to prices to answer these questions—yet there are two big challenges:The prices of securities can quickly change based on the trading volume, volatility of a specific security, and the volatility in the market. On the other hand, a brokerage could have millions of customers, each of them with a couple dozen securities in their portfolio. As soon as the customer logs in, their portfolio needs to be updated with the latest prices—and keep them updated as the brokerage receives prices from the exchanges.Essentially, we are creating a real-time stock chart. Many brokerage apps don’t try to do this at scale. Instead, these apps pull the latest prices rather than push the prices to millions of clients.  For example, there might be a refresh button on their portfolio page.These next-generation challenges are not trivial and cannot be easily solved with disk-based databases, which weren’t designed to handle millions of operations per second. The requirements of the financial industry need a database that can scale easily and handle hundreds of millions of operations each second. In comes Redis Enterprise, an in-memory database platform, has the potential to tackle these myriad challenges.This is the first of a series of blogs covering various real-time use cases in the financial world. We will cover the details and business challenges of each use case and the role Redis Enterprise can play in solving those challenges. As part of the blog, we offer sample designs, data models, and code samples. We will also discuss the pros and cons of each approach.In this blog post we will cover the following:Once the client app has retrieved the portfolio and is receiving the latest prices, it can:Let’s begin by modeling a holding in a portfolio. In the illustration below, CVS Health Corp. (NYSE: CVS) is one of our example holdings. There were two separate lots of CVS—the first was acquired on January 4, 2021 and the second on March 1, 2021. The same number of shares were purchased during the buy trade for each lot. Both trades were for 10 shares, however at different prices per share—$68.3378 for the first lot and $68.82 for the second. The total quantity of the CVS holding in the portfolio is 20 with an average cost calculated as follows: (($68.3378 * 10) + ($68.82 * 10))/20 = $68.5789 per share.Redis’ data representation is flat—one cannot embed Sets, for example, within another Set. Therefore the data model as described by an ER diagram cannot necessarily be implemented directly. Implementing the Entity Model directly might not have the desired performance characteristics, so you’ll have to think a little differently as you get down to implementation. In this section we cover some of the basic design principles required when designing a high-performance and scaling implementation using Redis.The data model here mentions the following entities:An ER diagram provides a visual representation which can help one see what’s going on.What’s missing from the diagram above is the set of incoming prices although they are recorded in the security’s price history—and the calculation of the instantaneous value and gains as prices change. So, the ER diagram represents the comparatively static data that is the context in which portfolio valuation is performed.Key points that need to be considered about this system include:Given these points, some general approaches are:Here are the major computational components and the data flow:Note that Redis Enterprise consists of one or more nodes across many machines—deployed on-premises, Kubernetes, hybrid cloud deployment, managed service, or native first party cloud service—and there will be hundreds of thousands of investors online with their client(s) of choice.Security price updates will be absorbed by Redis Streams. Updates for securities will be mixed together in this stream and will need to be disaggregated to make the data useful. A consumer group will be used to perform that disaggregation, and to process data into two structures, per security:The following diagram details this part of the architecture:The most important factor in our model is the account-specific data representing lots and the related security. We’ll compare two implementations as an example of how to think about modeling data in Redis, with a focus on performance. Other implementations are possible—our goal here is to introduce the overall design principles and thought processes when implementing data in Redis.We’ll use the following information as a concrete example:We are pricing in the lowest possible currency denomination to avoid floats and keep everything in integers. We can allow the client to handle transformation to dollars and cents. In this example, we are using the prices with precision of two decimal places.Our first implementation records the id’s of all the lots in the account using a SET, identified by account id, and then uses one Redis HASH per LOT, identified by LOT ID, with the ticker, quantity, and purchase price as the fields. In other words, we’re using the HASH to model the LOT entity structure, with each attribute of the LOT entity being a field in the Redis HASH.With this data model we have a key for each account and a value containing all the LOT IDs for that account stored as a Redis SET:In addition, for each lotid, we’d have a HASH whose fields are the ticker, quantity, and purchase price:Concretely, we’d create the keys like this:The RedisTimeSeries module allows the storage and retrieval of related pairs of times and values, along with high-volume inserts and low-latency reads. We are going to get the latest price for the tickers of interest the client would access when using the corresponding time series keys:and subscribe to the pricing channel for updates:To get all the data, the client would perform the following operations:The overall time complexity is O(N +T).Concretely, operations one and two would be:We can minimize network latency by using pipelining (a form of batching on the client side) and/or repeated use of LUA scripts (using SCRIPT LOAD & EVALSHA). Side note: Transactions can be implemented using pipelines and can provide reduced network latency, but this is client specific and their goal is atomicity on the server, so they don’t really solve the network latency problem. Pipelines comprise commands whose inputs and outputs must be independent of one another. LUA scripts require all keys to be provided in advance and that all of the keys are hashed to the same slot (see the Redis Enterprise docs on this topic for more details).Given these constraints we can see that the assignment of operations to pipelines is:and that using LUA scripts isn’t possible because each operation uses different keys and those keys have no common part that can be hashed to the same slot.In utilizing this model we have a time complexity of O(N+T) and three network hops.An alternative model is to flatten the LOT entity structure and to represent each entity attribute using a key identified by account id—one such key for each attribute (quantity, ticker, price) of a lot. The fields in each HASH will be the LOT ID and a corresponding value to either quantity, ticker, or price. Thus we’d have keys:tickers_by_lot: <ACCOUNT_ID> HASH <LOTID TICKER>quantities_by_lot:<ACCOUNT_ID> HASH <LOTID INTEGER>prices_by_lot:<ACCOUNT_ID> HASH <LOTID INTEGER>These hashes would replace the LOTID and LOT keys from Data Model A, while the price_history and <TICKER> keys would remain the same.Creating the keys:Retrieving the values:The operations required by the client would now be:This has an overall time complexity of O(N+T)—same as before.From a pipeline perspective this becomes:So we’ve reduced the number of network hops by one—not a lot in absolute terms, but 33% in relative terms.In addition, we can easily use LUA since we know the keys, and we can map all the keys for any specific account to the same slot. Given the simplicity of operations we’ll not dig into LUA further, but note that this design makes it at least possible!In a simple benchmark, Data Model B ran 4.13 ms faster (benchmarked over thousands of runs). Given that this is only run once each time a client is initialized for an account, this likely has no impact on overall performance.In this blog, we’ve shown two possible implementations of the Entity model using Redis data types. We’ve also introduced the time complexity analysis that should be performed whenever choosing a Redis data type, along with a consideration of network performance improvements—a critical step when large scale and high performance are required. In subsequent blogs, we’ll expand further on these ideas as the data model is expanded.We have introduced some of the business challenges in managing securities portfolios at scale and have shown the following:With these two critical features in place, a brokerage app client can provide real-time portfolio updates that perform and scale to handle millions of accounts. This design can present the total value of the portfolio and the gain or loss across each holding in real-time. This data model and architecture can also be applied to use cases beyond securities to cover crypto, ad exchanges, etc."
55,https://redis.com/blog/redisjson-public-preview-performance-benchmarking/,RedisJSON: Public Preview & Performance Benchmarking,"November 16, 2021",Pieter Cailliau and Filipe Oliveira,"In July, we introduced the private preview of a real-time document store with native indexing, querying, and full-text search capabilities powered by the combination of RedisJSON and RediSearch. During the preview, we’ve greatly improved the performance and stability of the offering, simplified packaging for easier onboarding, and enhanced our driver support. Today, we’re happy to announce that RedisJSON* (powered by RediSearch) is now available as a public preview and available in our cloud. Public preview means generally available (GA) quality code that’s available in limited deployment environments.RedisJSON* is a source-available real-time document store that allows you to build modern applications by using a dynamic, hierarchical JSON document model. Our early customers have started using it in various architectural scenarios across their data stack: as a cache, as a query accelerator to speed-up hotspots, and as a primary database. We’re seeing growing adoption in our developer community as well. In the last 3 months alone, there’ve been a million Docker pulls of RedisJSON and RediSearch combined.Redis is the most loved database for the 5th consecutive year (Stack Overflow survey) and we wanted RedisJSON* to have the same simplicity, scalability, and, most importantly, speed of Redis that our community is familiar with, so we purpose-built it from the ground up. Given that performance is Redis’ key strength, we decided to check how we fared against the previous version and comparable players in the market. To provide a fair comparison of RedisJSON, MongoDB, and ElasticSearch, we relied on the industry standard Yahoo! Cloud Serving Benchmark (YCSB).Each product comes with a different architecture and a different feature set. For a given use case, one product might be a better fit than another. In full transparency, we’re aware that MongoDB and ElasticSearch have been around longer. It’s up to you, the developer, to choose the right tool for the problem at hand. We’re adding a lot of context to our benchmark results in this blog because there are many variables at play that might differ from your use case. We found that:Additionally, RedisJSON*’s latencies for reads, writes, and searches under load are far more stable in the higher percentiles than ElasticSearch and MongoDB. RedisJSON* also handles an increasingly higher overall throughput when increasing the write ratio while ElasticSearch decreases the overall throughput it can handle when the write ratio increases.As mentioned before, RediSearch and RedisJSON are developed with a great emphasis on performance. With every release, we want to make sure you’ll experience stable and fast products. Whether we’re finding room for improving the efficiency of our modules or pursuing a performance regression investigation, we rely upon a fully automated framework for running end-to-end performance tests on every commit to the repos (details here), and we follow up if required with performance investigations by attaching telemetry and profiling tools/probers.This allows us, in a concise methodical way, to keep bumping up performance with every release. Specifically for RediSearch, 2.2 is up to 1.7x faster than 2.0 both on ingestion and query performance, when combining the throughput and latency improvements.The next two diagrams show the results of running the NYC taxi benchmark (details here). This benchmark measures both the throughput and the latency observed while ingesting about 12 million documents (the rides that have been performed in yellow taxis in New York in 2015).As can be seen on these diagrams, each new version of RediSearch comes with a substantial performance improvement.To evaluate search performance, we indexed 5.9 million Wikipedia abstracts. Then we ran a panel of full-text search queries (details here).As seen above, you’ll benefit from faster writes/reads/searches (latency charts) by moving from v2.0 to v2.2 and consequently increase the achievable throughput of the same hardware on which Search and JSON are running.To assess RedisJSON’s performance, we’ve decided to benchmark it in comparison to MongoDB and ElasticSearch. When talking about document stores, these solutions often come into the picture. They’re both available on-premises, available in the cloud, provide professional support, and are committed to providing scalability and performance. Of course, everything depends on the use case and, in the future, we plan to extend this benchmark to other vendors providing a comparable scope of capabilities to Redis. Let’s have a look at our approach.We’ve used the well-established YCSB, capable of evaluating different products based on common workloads measuring the resulting latency/throughput curve until saturation. Apart from the CRUD YCSB operations, we’ve added a two word search operation specifically to help developers, system architects, and DevOps practitioners find the best search engine for their use cases. The generated documents have an approximate size of 500 bytes, and each solution creates one secondary index indexing one text field and one numeric field.For all tested solutions, the latest available stable OSS version was used: MongoDB v5.0.3, ElasticSearch 7.15, and RedisJSON* (RediSearch 2.2+RedisJSON 2.0).We ran the benchmarks on Amazon Web Services instances provisioned through our benchmark testing infrastructure. All three solutions are distributed databases and are most commonly used in production in a distributed fashion. That’s why all products used the same general purpose m5d.8xlarge VMs with local SSDs and with each setup being composed of four VMs: one client + three database Servers. Both the benchmarking client and database servers were running on separate m5d.8xlarge instances placed under optimal networking conditions, with the instances packed close together inside an Availability Zone, achieving the low latency and stable network performance necessary for steady-state analysis.The tests were executed on three-node clusters with the following deployment details:In addition to this primary benchmark/performance analysis scenario, we also ran baseline benchmarks on network, memory, CPU, and I/O in order to understand the underlying network and virtual machine characteristics. During the entire set of benchmarks, the network performance was kept below the measured limits, both on bandwidth and PPS, to produce steady stable ultra-low latency network transfers (p99 per packet < 100micros ).We’ll start by providing each individual operation performance [100% writes] and [100% reads] and finish with a set of mixed workloads to mimic a real-life application scenario.As you can see on the charts below, this benchmark shows that RedisJSON* allows for 8.8x faster ingestion vs. ElasticSearch, and 1.8x vs. MongoDB, while keeping a sub-millisecond latency per operation. It’s worth noting that 99% of the requests to Redis completed in less than 1.5ms.In addition, RedisJSON* is the only solution we tested that atomically updates its indices on every write. This means that any subsequent search query will find the updated document. ElasticSearch doesn’t have this granular capacity; it puts the ingested documents in an internal queue, and this queue is flushed by the server (not controlled by the client) every N documents, or every M seconds. They call this approach Near Real Time (NRT). The Apache Lucene library (which implements the full-text capabilities for ElasticSearch) has been designed to be fast in searching, but the indexing process is complex and heavy. As shown in these WRITE benchmark charts, ElasticSearch pays a big price due to this “by design” limitation.When combining the latency and throughput improvements, RedisJSON* is 5.4x times faster than Mongodb and >200x faster than ElasticSearch for isolated writes.Similarly to writes, we can observe that Redis is the top performer for reads, allowing for 15.8x more reads than ElasticSearch and 2.8x more than MongoDB, while retaining sub-millisecond latency across the complete latency spectrum, as visible on the table below.When combining the latency and throughput improvements, RedisJSON* is 12.7x times faster than MongoDB and >500x faster than ElasticSearch for isolated reads.Real-world application workloads are almost always a mix of reads, writes, and search queries. Therefore, it’s even more important to understand the resulting mixed-workload throughput curve as it approaches saturation.As a starting point, we considered the scenario with 65% searches and 35% reads, which represents a common real-world scenario, in which we do more searches/queries than direct reads. The initial combination of 65% searches, 35% reads, and 0% updates also results in an equal throughput for ElasticSearch and RedisJSON*. Nonetheless, the YCSB workload allows you to specify the ratio between searches/reads/updates to match your requirements.“Search Performance” can refer to different kinds of searches, e.g. “match-query search,” “faceted search,” “fuzzy search,” and more. The initial addition of the search workload to YCSB that we’ve done focuses solely on the “match-query searches” mimicking a paginated two-word query match, sorted by a numeric field. The “match-query search” is the starting point for the search analysis for any vendor that enables search capabilities, and, consequently, every YCSB-supported DB/driver should be able to easily enable this on their benchmark drivers.On each test variation, we’ve added 10% writes to mix and reduce the search and read percentage in the same proportion. The goal of these test variations was to understand how well each product handles real-time updates of the data, which is something we believe is the de facto architecture goal, i.e. writes are immediately committed to the index and reads are always up to date.As you can see on the charts, continuously updating the data and increasing the write proportion on RedisJSON* doesn’t impact the read or search performance and increases the overall throughput. The more updates produced on the data, the more affected ElasticSearch performance is, ultimately making the reads and searches slower.Looking at the achievable ops/sec evolution from 0 to 50% updates for ElasticSearch, we notice that it started at 10k Ops/sec on 0% update benchmark and is deeply affected up to 5x fewer ops/sec reaching only 2.1K ops/sec on the 50% update rate benchmark.Similar to what we observed in the individual operations benchmarks above, MongoDB search performance is two orders of magnitude slower than RedisJSON* and ElasticSearch, with MongoDB reaching a max overall throughput of 424 ops/sec vs. the 16K max ops/sec with RedisJSON*.In the end, for mixed workloads, , RedisJSON* enables up to 50.8x more ops/sec than MongoDB and up to 7x times more ops/sec than ElasticSearch. If we center the analysis on each operation type latency during the mixed workloads, RedisJSON* enables up to 91x lower latencies when compared to MongoDB and 23.7x lower latencies when compared to ElasticSearch.Similarly to measuring the resulting throughput curve until saturation of each of the solutions, it’s important to also do a full latency analysis at a sustainable load common across all solutions. This will allow you to be able to understand what is the most stable solution for all issued operations in terms of latency, and the one less susceptible to latency spikes provoked by application logic (for example, the Elastic query cache miss). If you want to dive deeper into why we should do it, Gil Tene provides an in-depth overview of latency measurement Do’s and Don’ts.Looking at the throughput chart from the previous section, and focusing on the 10% update benchmark to include all three operations, we do two different sustainable load variations:In the first image below, showcasing the percentiles from p0 to p9999, it’s clear that MongoDB is deeply outperformed by Elastic and RedisJSON* on each individual time to search. Furthermore, focusing on ElasticSearch vs. RedisJSON*, it’s clear that ElasticSearch is susceptible to higher latencies which are most likely caused by Garbage Collection (GC) triggers or search query cache misses. The p99 of RedisJSON* was below 2.61ms, in contrast to the ElasticSearch p99 search that reached 10.28ms.In the read and update charts below, we can see that RedisJSON* is the top performer across all latency spectrums, followed by MongoDB and ElasticSearch.RedisJSON* was the only solution to maintain sub-millisecond latency across all analyzed latency percentiles. At p99, RedisJSON* had a latency of 0.23ms, followed by MongoDB at 5.01ms, and ElasticSearch at 10.49ms.On writes, MongoDB and RedisJSON* maintained sub-millisecond latencies even at p99. ElasticSearch, on the other hand, showed high tail latencies (>10ms), for most likely the same reasons (GC) that cause the Search spikes for ElasticSearch.Focusing solely on ElasticSearch and RedisJSON*, while retaining a sustainable load of 6K ops/sec, we can observe that the read and update patterns of Elastic and RedisJSON* remained equal to the analysis done at 250 ops/sec. RedisJSON* is the more stable solution, presenting p99 reads of 3ms vs. the p99 reads of 162ms for Elastic.On updates, RedisJSON* retained a p99 of 3ms vs. a p99 of 167ms for ElasticSearch.Focusing on the Search operations, ElasticSearch and RedisJSON* start with single-digit p50 latencies (p50 RedisJSON* of 1.13ms vs. p50 of ElasticSearch of 2.79ms) with ElasticSearch paying the price of GC triggering and query cache misses on the higher percentiles, as is clearly visible on the >= p90 percentiles.RedisJSON* retained a p99 below 33ms vs. the 5x higher p99 percentile at 163ms on ElasticSearch.We take performance very seriously and would like to invite other partners and community members to collaborate in our efforts to create a standard benchmark definition for search and document workloads. RedisJSON and RediSearch performance data and tooling will be opened to the community over the coming months.Similar to the self-managed (on-premises) benchmark numbers above, we’ll also be benchmarking the DBaaS performance of Redis Cloud against other comparable cloud document databases in the coming weeks. In addition to the benchmarks, we’ve published a technical blog on how to build a fast, flexible, and searchable product catalog using RedisJSON.Lastly, we’ve also extended RediSearch with vector similarity search, which is now in private preview.To get started with RedisJSON* you can create a free database on Redis Cloud in all regions. Alternatively, you can use the RedisJSON docker container. We updated the documentation on redisjson.io to easily get started with query and search capabilities. Additionally, as mentioned in  our recent client libraries announcement, here are the client drivers for several popular languages to help you get started.Here’s some links to get you started"
56,https://redis.com/blog/5-reasons-redis-enterprise-on-azure-is-the-right-move-for-app-developers/,5 Reasons Redis Enterprise on Azure is the Right Move For App Developers,"March 31, 2021",DaShaun Carter,"The Azure Cache for Redis Enterprise tiers are now released for general availability, and that’s great news for app developers. It brings together the advanced performance, high availability, and extended data structure functionality of Redis Enterprise with Azure’s global presence, flexibility, security, and compliance in an incredible tool for developers.The service consists of two new Enterprise tiers:Azure Cache for Redis Enterprise has been in preview since October 2020 and has already been adopted by multiple organizations. App developers who want to take familiar Redis caching and data to the next level will also want to get their hands on this fully managed Azure native service. Here are five reasons why.App developers aim to deliver a great user experience and keep making it better—and even a few milliseconds of response time can make a big difference. Redis Enterprise delivers database latency under one millisecond, so applications can respond instantly without being dragged down by slow data functions.Azure Cache for Redis Enterprise offers measurable performance advantages:Developers need to know that data is available for their applications at virtually any traffic level. Redis Enterprise is highly scalable. It has been benchmarked to demonstrate true linear scaling—offered on Azure with:And the service fully uses infrastructure by splitting loads across multiple cores on every compute node.App downtime—whether due to an outage or a pause to refresh an index—costs money; an hour of downtime can equate to millions lost. Developers need uninterrupted high availability to deliver exceptional user experience and to continue to innovate and evolve that experience.Azure Cache for Redis Enterprise offers the highest levels of availability—up to 99.999%, using Redis’ active geo-replication technology and with the combination of Azure’s multi-region and multiple availability zone deployment capability. The service was built to safeguard applications with full resilience to any kind of failure, including process failure, node failure, complete data center outage, or a network split event.Redis Enterprise offers developers new opportunities for advanced use cases with add-on modules including RediSearch, RedisTimeSeries, and RedisBloom. And the service’s NoSQL database makes it easier and more intuitive for developers to build modern applications and incorporate innovation. For example, they can access portions of the database without having to query the entire set for faster development.You can literally launch Redis Enterprise on Azure with a click. The service is fully managed by Microsoft, and users access setup and configuration through the familiar Azure Portal, with seamless integration into Azure security and monitoring tools. Customers with a MACC can simply consume Redis Enterprise from their existing Azure commitment with no extra billing.And because Redis is so well loved and widely used within the developer community, users can tap into their collective wisdom to quickly realize a wider variety of capabilities for their applications.Azure Cache for Redis Enterprise tiers is the most resilient, highly available, and scalable Redis option on Azure. Discover how developers can use it to make the most of what Redis can do, right within Azure. Learn more at Azure Cache for Redis Enterprise or get started today on the Azure Marketplace."
57,https://redis.com/blog/redisinsight-1-6-brings-redisgears-support-and-redis-6-acl-compatibility/,RedisInsight 1.6 Brings RedisGears Support and Redis 6 ACL Compatibility,"July 2, 2020",Stévan Le Meur,"RedisInsight is an easy and intuitive GUI for Redis, allowing you to oversee all your databases and manage your data, with built-in support for the most popular Redis modules. It provides tools to analyze your database’s memory usage and profile its performance.With the latest release, RedisInsight 1.6, RedisInsight hits another important milestone with new capabilities and enhancements designed to make your developer experience even more enjoyable. In this blog post, you’ll learn all the details about the latest developments in RedisInsight.Since the beginning of the year, we have released several new versions of RedisInsight. Each one included a mix of new capabilities, enhancements, and bug fixes. Key highlights include:The latest RedisInsight 1.6.0 is available for both local installation and as a Docker container. Here’s how:Local installation:Download RedisInsight for Windows, Mac, and Linux from the Redis website.Docker installation:docker run -v redisinsight:/db -p 8001:8001 redis/redisinsight:latestYou can find more information on installing RedisInsight in the documentation.Upgrades:Since Version 1.2.2, RedisInsight notifies you when a new update is available. Alternatively, you can upgrade RedisInsight by simply uninstalling and reinstalling the application from your operating system. Note that updating persists all your preferences, and especially all the database connection details.RedisGears beta support in RedisInsightRedisGears is a dynamic framework that enables developers to write and execute functions that implement data flows in Redis, while abstracting away the data’s distribution and deployment. You can use RedisGears to improve application performance and process data in real time. RedisGears lets you program in Redis, deploy functions, and run your serverless engine where your data lives. (You can learn more about RedisGears in this blog post: Announcing RedisGears 1.0: A Serverless Engine for Redis.)RedisInsight 1.6 includes a new tool—accessible from the application’s main menu—that lets you interact with the RedisGears serverless engine. With this new tool, you can explore the history of the latest executed functions and analyze the results (and eventually the errors) of those functions. You’ll get a summary of the execution, as well as the result data, depending on what your function is actually doing.With RedisGears, you can also register functions to be triggered by specific events on your data. Within RedisInsight, you can manage and explore the registered functions—the UI displays all the functions running in Redis at a quick glance.Finally, we also added a simple code editor. Obviously, we’re not trying to replace your favorite IDE or development tool, but an integrated editor lets you quickly write a script to process data in real time or capture when a particular event is happening with your data.This new capability is currently in beta, so we’re excited to hear your feedback and thoughts on the Redis Community Forum.Redis 6 and access control lists (ACLs) supportRedisInsight is now fully compatible with Redis 6—they work seamlessly and transparently together.One of the key new capabilities introduced with Redis 6 is access control lists. ACLs bring the concept of “users” to Redis, which lets you control what level of Redis access each user has. You can configure which commands specific users can execute and which keys they can access. This allows for much better security practices: you can now restrict any given user’s access to the least level of privilege needed. This is particularly helpful if you are building different services in your application: You can create dedicated users to grant only a specific set of commands on the database. ACLs allow users access to only particular commands, keys, or even patterns of keys based on user-based specified permissions.Each defined user can have its own password. Now, when you connect to Redis from RedisInsight, you can specify the user you want to connect with, as well as the password, as shown here:Note: In order to use RedisInsight, a user must have at least the permissions to run the following commands: INFO and PING. Those commands are used for properly configuring RedisInsight with Redis.Unleashed CLIRedisInsight provides most capabilities with an easy UI, but in certain cases you might still need to run some advanced commands. For those purposes, we have integrated a web CLI into RedisInsight, so you have it handy and always ready to interact with your database. In the latest version of RedisInsight, the CLI has been improved by removing most command restrictions and providing better support for output formatting.We initially limited the list of commands that a RedisInsight user could execute on the database. But we heard from our users that those limitations were not always helpful and that most of you would just like to run all commands as you do with redis-cli. That’s why RedisInsight’s integrated CLI can now run all non-blocking commands supported in the redis-cli.If you are already familiar with how the redis-cli lets you interact with data and renders the data structures, we are now rendering them exactly the same way in RedisInsight 1.6. You’ll feel at home when switching between them.Last but not least, the escape-string inputs and outputs are also rendered the same way as in redis-cli.Note: The RedisInsight CLI does not currently support a few blocking commands, as well as some commands that do not return standard streaming responses: MONITOR, SUBSCRIBE, PSUBSCRIBE, SYNC, PSYNC, SCRIPT DEBUGMulti-line query editingWe’ve made it easier to build and edit queries for RediSearch, RedisGraph, or RedisTimeSeries as you can now better structure them by using the multi-line editor. Often, however, your queries will need multiple lines, either because it’s a long query, or because you would like to structure it for easier understanding. So we improved the query editor to allow using multiple lines:In order to use the multi-line query editor, just:You’ll notice that the formatting of your queries is protected in the history of your queries (when navigating using the Down Arrow or Up Arrow keys to see your previously executed queries).Secure connection: TLS supportTo prevent unauthorized access to your data and to encrypt the communication between the database and the clients, you can enable the Transport Layer Security (TLS) protocol on your Redis Enterprise databases. (To learn more about configuring TLS on your Redis Enterprise databases, refer to the Configuring TLS Authentication and Encryption in the Redis Enterprise documentation.)When TLS is enabled, Redis Enterprise sends its client certificate to the database for authentication. To configure your database with TLS enabled, use the choices shown in the screenshot below:If your database requires client authentication for mutual authentication, just select “Require TLS Client Authentication” in the “Add Redis Database” form—you’ll have the ability to provide the certificate-key pair as shown here:Auto-discovery of Redis databasesThe latest version of RedisInsight makes it possible to configure connection details of your Redis Enterprise Software or Redis Enterprise Cloud databases with a single-click. Click on the “ADD REDIS DATABASE” button to search for databases from different locations, as shown here:In Redis Enterprise, you can explore databases on a particular cluster. Just provide the connection details and your databases will be listed, which lets you select and configure them in RedisInsight:Configuration is even easier in Redis Cloud Enterprise (Pro Subscriptions only), as you have to provide only your account key and secret key to connect your Redis Enterprise Cloud account and get the list of all your databases, as shown here:If you have multiple subscriptions, you can choose from which ones you want to connect the databases:Note: You can learn more in the Automatically Discovering Databases section of the RedisInsight documentation.Full-screen mode in RedisGraph, RedisTimeSeries, and RediSearchIntegrated in the latest version of RedisInsight is the ability to maximize the space used for interacting with graphs or with the data from RediSearch or RedisTimeSeries. This is convenient when you need to explore a large set of data. You can also use this capability when you are screensharing or showcasing your models.You can find the other notable enhancements and all bug fixes in the Release Notes section of the RedisInsight documentation."
58,https://redis.com/blog/redis-enterprise-can-save-you-money/,Can Redis Enterprise Save You Money?,"March 21, 2022",Udi Gotlieb,"Redis is an in-memory database, and Redis Enterprise adds a whole layer of management, automation, resiliency, and security capabilities on top of it, making it enterprise-ready.  To many, in-memory equates to “expensive”—a premium you choose to pay for top performance. And the functionality provided by enterprise software can also bring a hefty price tag. So how can such a technology turn out to be a cost-saving vehicle?Find out how much Redis Enterprise can save your business.To help answer that question in a deep and methodological way, we commissioned the best experts in that field, Forrester Consulting, and their Total Economic Impact™ framework, to conduct research, interview Redis customers, and share their findings. There’s a lot of great insight in the study, but when it comes to how Redis Enterprise saves you money, these four really stood out to me:Redis Enterprise helps legacy applications and databases better scale and deliver more value from their existing footprints by front facing them as an enterprise cache. It also helps in modernizing legacy NoSQL and SQL databases and simplifying the tech stack when used as a primary database, so the overall cost of building and maintaining new apps goes down.Redis Enterprise helps get applications to market faster as a direct outcome of its speed and stability, and as a result has a positive impact on digital income generation. Think about it. Your ability to grab market share in any space is really dependent on two key factors: time to market, and the quality of service once your product is in the market. Redis Enterprise has a material impact on both.In regulated industries, and highly governed businesses, SLA breaches often carry financial penalties. When demand rises and tech stacks fail to deliver against those SLAs, penalties kick in and can have a material impact on the cost of running the business and its profitability. Redis Enterprise helps in avoiding penalties and in recouping income from improved performance. That means reducing customer churn and gaining more returning business from existing customers.We’re all very familiar with the notion that open source solutions are free. But this notion starts to really change as your operations grow and you need to deploy more and more DevOps resources to build, maintain, and operate those “free” stacks. Redis Enterprise, with its embedded management capabilities, alleviates DevOps teams from much of the workload associated with deploying, configuring, scaling, and maintaining their Redis environment. This significantly improves DevOps and IT teams’ efficiency, freeing them to focus on pushing new code rather than maintaining the database infrastructure.How much money can Redis Enterprise save exactly? The study found that a composite organization realized  $4.12M in net financial benefit over a three-year period, with 350% ROI over three years and a payback period of less than 6 months. Want to find out more about how everything connects and how those numbers are calculated? The Total Economic Impact™ of Redis Enterprise study has all those answers for you.In the next chapter of the Redis Enterprise value blog series, I’ll share the story of one of our long-time customers, Fiserv, whom I met with in early February. We’ll provide a perspective on Redis Enterprise’s impact on their tech stack and their business over the past few years. Stay tuned!"
59,https://redis.com/blog/introducing-redisearch-2-0/,Introducing RediSearch 2.0,"September 17, 2020",Pieter Cailliau,"RediSearch, a real-time secondary index with full-text search capabilities for Redis, is one of the most mature and feature-rich Redis modules. It is also becoming even more popular every day—in the past few months RediSearch Docker pulls have jumped 500%! That soaring popularity has led customers to come up with a wide variety of interesting use cases ranging from real-time inventory management to ephemeral search.To extend that momentum, we’re now introducing the public preview of RediSearch 2.0, designed to improve the developer experience and be the most scalable version of Redisearch. RediSearch 2.0 supports Redis’ Active-Active geo-distribution technology, is scalable without downtime, and includes Redis on Flash support (currently in private preview). To meet those goals without negatively impacting performance, we created a brand new architecture for RediSearch 2.0—and it worked: RediSearch 2.0 is 2.4x faster than RediSearch 1.6.Having a rich query-and-aggregation engine in your Redis database enables a wide variety of new use cases that extend well beyond caching. RediSearch lets you use Redis as your primary database in situations where you need to access data using complex queries. Even better, it preserves Redis’ world-class speed, reliability, and scalability, and doesn’t require you to add complexity to the code to let you update and index data.For RediSearch 2.0 we re-architected the way indices are kept in sync with the data. Instead of having to write data through the index (using the FT.ADD command), RediSearch now follows the data written in hashes and synchronously indexes it. This re-architecture comes with several changes in the API, which we discussed in a previous post when RediSearch 2.0 Hit Its First Milestone.This new architecture brings two main benefits. First, it’s now easier than ever to create a secondary index on top of your existing data. You can just add RediSearch to your existing Redis database, create an index, and start querying it, without having to migrate your data or use new commands for adding data to the index. This drastically lowers the learning curve for new RediSearch users and lets you create indexes on your existing Redis databases—without even having to restart them.In addition to implementing a new way to index data, we also took the index out of the keyspace. This enables Redis Enterprise’s Active-Active technology, which is based on conflict-free replicated data types (CRDTs).  Merging two inverted indices conflict-free is difficult, but Redis already has a proven CRDTs implementation of Hashes. So the second big benefit of this new architecture is making RediSearch 2.0 even more scalable. Because RediSearch now follows Hashes and the index was moved out of the keyspace, you can now run RediSearch in an Active-Active geo-distributed database.A document will be replicated to all databases in the replication set in a strongly eventual consistent manner. In each replica, RediSearch will simply follow all the updates on the Hashes, which means all indices are strongly eventual consistent as well.We didn’t want to limit increasing the scalability capabilities to only Redis Enterprise users, so we added support for scaling a single index over multiple shards with the open source Redis cluster API. Previously, a single RediSearch index, and its documents, had to reside on a single shard. This meant that dataset size and throughput for OSS Redis was bound to what a single Redis process could handle. Redis Enterprise offered the ability to distribute documents in a clustered database and aggregate the results at query time. This fan-out and aggregation is handled by a component called the “coordinator” that is now also publicly available under the Redis Source Available License so it will work with open source Redis clusters as well as Redis Enterprise. The result is the most scalable version of RediSearch yet.To assess RediSearch 2.0’s ingestion performance, we extended our full-text search benchmark (FTSB) suite with the publicly available NYC Taxi dataset. This dataset is used across the industry due to its rich set of data types (text, tag, geographic, and numeric), and a large number of documents.This benchmark focuses on write performance, using trip-record data of rides in yellow cabs in New York City. Specifically for this benchmark we used the January 2015 dataset, which loads more than 12 million documents with an average size of 500 bytes per document. For the full benchmark specification please refer to the FTSB on GitHub.All benchmark variations were run on Amazon Web Services instances, provisioned through our benchmark-testing infrastructure. The tests were executed on a 3-node cluster with 15 shards, with RediSearch Enterprise versions 1.6 and 2.0. Both the benchmarking client and the 3 nodes comprising the database with RediSearch enabled were running on separate c5.9xlarge instances.Given that RediSearch 2.0 comes with the ability to follow changes in Hashes in Redis and automatically index them, we’ve added variants for the FT.ADD and HSET commands. To make upgrades easier, we remapped the now deprecated FT.ADD command to the HSET commands in RediSearch 2.0. The two charts below display overall ingestion rate and latency for both RediSearch 1.6 and RediSearch 2.0, while retaining sub-millisecond latencies.RediSearch has always been fast, but with this architectural change we’ve moved from indexing 96K documents per second to 132K docs/sec at an overall p50 ingestion latency of  0.4ms, drastically improving write scaling.Not only will you benefit from the boost in the throughput, but each ingestion also becomes faster. Apart from the overall ingestion improvement due to the changes in architecture, you can now also rely on the OSS Redis Cluster API capabilities to linearly scale the ingestion of your search database.Combining throughput and latency improvements, RediSearch 2.0 delivers up to a 2.4X speedup compared to the RediSearch 1.6.To sum up, RediSearch 2.0 is the fastest and most scalable version for all Redis users that we have ever released. In addition, RediSearch 2.0’s new architecture improves the developer experience of creating indices for existing data within Redis in a seamless manner and removes the need to migrate your Redis data to another RediSearch-enabled database. This new architecture allows RediSearch to follow and auto-index other data structures, such as Streams or Strings. In upcoming releases, it will let you work with additional data structures such as the nested data structure in RedisJSON.We plan to keep on adding more features to further enhance the developer experience. Coming next, look for a new command that allows you to profile your search queries to better understand where performance bottlenecks occur during query execution.Ready to get started? Check out Tug Grall’s blog on … Getting Started with RediSearch 2.0! Then follow the steps in this tutorial on GitHub or create a free database in Redis Enterprise Cloud Essentials. (Note that the public preview of RediSearch 2.0 is available in two Redis Enterprise Cloud Essentials regions: Mumbai and Oregon.)"
60,https://redis.com/blog/redis-7-first-release-candidate/,Redis 7.0: The First Release Candidate is Here!,"January 31, 2022",Redis,"Click here to get started with Redis Enterprise. Redis Enterprise lets you work with any real-time data, at any scale, anywhere.The new version is an evolutionary step to some of Redis’ features, adds new ones, and delivers more raw power at better resource utilization.Today, we proudly announce the availability of Redis v7’s first release candidate (RC1). Version 7.0 is a significant milestone for the project that’s been under development for the better part of last year. This major release is an opportunity to make more radical changes in terms of functionality and internals. We expect to follow our usual release cycle, so two more candidates, each about three weeks apart, are planned before the general availability of Redis 7.0.Redis 7.0 adds many new features, such as Functions, ACL v2, sharded Pub/Sub, and more. We’ll cover these in this and a series of following posts. In addition, a lot of the work put into this version focused on improving and optimizing parts of Redis that are less visible to most users. This effort spanned practically every Redis subsystem, touching persistence, replication, network, memory, and compute usage. The result is a faster, more stable, and more economical Redis than ever before.The new Redis version doesn’t herald new data structures or push its operational aspects to new realms. Instead, it is more about taking an introspective look at the project, challenging some of the existing design assumptions, taking bolder steps towards improving the infrastructure, and internalizing the community’s needs and use cases.Redis Functions are an example of the above. Functions belong to the broader topic of Redis Programmability, which means the ability to program Redis. To “program Redis” means to have the server execute user logic, primarily for data locality. Programmability has been a part of Redis ever since the introduction of Lua scripts in version 2.6 (please refer to Introduction to Eval Scripts for more details).The adoption of Lua scripts by Redis users has been rising ever since. Scripts provide an effective and simple way to compose server-side workflows from core Redis operations and control structures. They can serve as an abstraction of the underlying data structures from the application’s native code. An application’s set of Lua scripts implements its logical operations (for example, an “add user” or “place order” script).Because scripting is a popular Redis feature that has been around for a long while, we’ve collected a lot of feedback about it. We found that most of the requirements fall into three buckets:That brings us back to Redis Functions. In Redis 7.0, functions are entirely independent of the application. As a result, functions are executable software artifacts that are first-class database citizens. The Redis server manages functions just like user data, so they are persisted and replicated for availability. Loading functions to the database is no longer the application’s responsibility during runtime and becomes an administrative maintenance task that can be scheduled and managed.Having functions reside in the server’s context frees the application from the functions’ implementation. Instead, the application relies on function signatures as its API to the embedded logic and is entirely oblivious of the Redis command and types that are in play. It allows different applications to share functions. It also enables developing, testing, and maintaining functions without a mandatory dependency on the application.The design of Redis Functions is engine-agnostic. Although Redis 7.0 only supports functions written in Lua 5.1, the implementation of internals is nearly ready to hook with other execution engines. We plan to add support for more types of execution engines in the future.Another paradigm shift that functions make is the use of libraries. Libraries can consist of one or more registered functions and any additional internal ones. Registered functions are the library’s entry points and the application’s contractual API. On the other hand, internal functions can be natively called inside the library for code reusability.We expect that the introduction of functions in Redis 7.0 is a big step in the direction of taking Redis’ programmability to the next stage. If you’re already using Lua scripts in your application, migrating to using functions is optional and simple (see Redis Functions for more). What’s even better is that, as we go forward, functions offer a better foundation to improve and innovate with Redis programmability."
61,https://redis.com/blog/redis-om-spring/,What’s New in Redis OM Spring?,"August 12, 2022",Brian Sam-Bodden,"The Redis OM projects are progressing nicely. After six months of hard work, the team has created a usable and stable set of APIs for Redis Stack. We expect these 10 new features in Redis OM Spring to simplify and optimize Redis-powered Spring applications.The release of Redis Stack in March 2022 marked a watershed moment in Redis distribution. Redis Stack is the first publicly available Redis distribution that integrates several popular and battle-tested Redis modules. Redis Stack packages a JSON document database, a full-blown search engine, a time-series database, a graph database, and probabilistic data structures.The Redis object mapping (OM) family of libraries started life as an effort to provide high-level APIs to several of Redis’ supported modules. With the release of Redis Stack, our efforts have been refocused on the modules and functionality provided by Stack. The first wave of functionality is centered around the JSON document database (RedisJSON) and its integration with the search engine (RediSearch).As you probably know, Redis OM Spring is a client library that helps you model your domain and persist data to Redis in Spring applications. The latest release of ROMS (version 0.6.0) reflects our focus on Documents and Search (although we sneaked in a few other features in preview form).So what’s new since we announced the project in late 2021 and published a Redis OM Spring tutorial? Here are the highlights.Redis OM Spring, as an extension to Spring Data Redis, is centered around working with “persistent entities:” Java objects annotated so that they can be serialized and stored in a data store. Redis OM Spring is one of the first Spring data projects to offer multi-model persistence of Java entities, such as Redis Hashes and Redis JSON Documents. In the GA release, you can now mark nested objects with @Indexed and search based on the nested object properties:With such mapping, you could, for example, declare a method in a repository to search for a Person given the City in their Address:Or, if you wanted to search by City and State, you could declare a method like:If you have any full-text search methods in an entity, you can search over all the fields at once, simply by declaring a method called “search“:The Indexed and Searchable annotations provide full control on the generation of the underlying search index:The ability to pass fine-tuning parameters to the underlying RediSearch engine opens the possibility for more powerful queries. In the example, the definition of the buildingType field includes the sortable, nostem, and weight parameters. The sortable parameter speeds up sorting by the specific field at the cost of some extra memory; the nostem parameter can speed up indexing by disabling the stemming of the field; and finally, the weight parameter is a multiplier that affects the importance of the field when calculating result accuracy.Several utility classes were added to control how objects are serialized. For example, say you wanted to store a Set of Strings as a comma-delimited string in your JSON document. You would use the @JsonAdapter GSON annotation with the Redis OM SetToStringAdapter class:On top of the CrudRepository, a PagingAndSortingRepository abstraction adds additional methods to ease paginated access to entities. The RedisDocumentRepository implements this interface and allows the passing of a Pageable object to any declared method to return “pages” of results. The findAllByTitleStartingWith method retrieves all instances of the MyDoc entity with a title starting with a given prefix:Such a query could return millions of results. In a typical web application, we might prefer to show a page with a few hundred results at most. Now, using a page request, we can ask only to return the first page of results containing 100 records:Applications often need to get a list of IDs of one sort or another: user IDs, customer numbers, and so on. Redis OM Spring RedisDocumentRepository provides a built-in getIds method that takes a page object to retrieve IDs for an entity efficiently:In real applications, JSON documents can become very large. Retrieving a whole document can become an expensive and time-consuming operation.Imagine that you’ve saved an object using the RedisDocumentRepository. You can now update a single field efficiently using the updateField method and Entity Metamodel class:Redis OM Spring provides sophisticated support to transparently keep track of when a change happens to an entity. You must equip your entity classes with auditing metadata to benefit from that functionality. You can define them using the annotations @CreatedAt; to store when the entity was created and @LastModifiedDate; to store when the entity was last changed:If you have an indexed collection in your entity, such asYou can declare a method in your repository likeThat efficiently returns all distinct values in that collection across all documents.RediSearch provides a Suggestions/Autocomplete API. This API is surfaced in Redis OM Spring via the @AutoComplete and @AutoCompletePayload annotations.For example, imagine you have an entity representing Airports with the airport name, code, and state:You can declare methods that start with ""autoComplete*”, as shown below for the “name” property:And get a list of suggestions/autocompletions given a query string:Also, if you pass an “AutoCompleteOptions” you can control how the search is performed (fuzzy or not) and also include in the payload any of the fields marked with the @AutoCompletePayload annotation:One powerful feature of Redis Stack is the RedisBloom module which provides a collection of probabilistic data structures. A Bloom filter is a probabilistic data structure that provides an efficient way to verify that an entry is certainly not in a set. This feature is ideal when you search for items on expensive-to-access resources and is particularly useful if you have extremely large data collections.Imagine that you had a list of users for your SaS application, and you reached FAANG levels of users, say a little more than 100 million. (Congratulations!) In your early days, it was easy to check if an email address was already used in the system since you only had to query a dataset of thousands and not several hundred million. But that’s changed.The RedisBloom feature makes this process easier. Use the @Bloom annotation to create and maintain a Bloom filter for the email property:With the filter in place, add an “existence” query to the repository:The query runs in linear time, ensuring a consistent and responsive UI for your customers.This is the beginning of a journey as we work to make Redis Stack useful and enjoyable for developers across multiple languages and platforms. This release will remain fairly stable on the JSON/Search front; we expect minor changes to the current APIs. We have begun working on the remaining modules and APIs and are excited about what comes next."
62,https://redis.com/blog/kipp-eyes-e-commerce-opportunity-through-real-time-payment-approval/,Kipp Eyes E-commerce Opportunity Through Real-Time Payment Approval,"June 2, 2022",Henry Tam,"According to eMarketer, “worldwide e-commerce sales will exceed $5 trillion for the first time, accounting for more than a fifth of overall retail sales” in 2022. With independent, Main Street shop owners to global retailers competing fiercely for the same shoppers, these businesses need to be able to fight for every legitimate sale to be completed. However, global payments consultancy CMPSI estimated in 2020, “only 85% of online transactions being approved, compared to 97% in-store.”Because time is of the essence in authorizing a sale before a shopper may choose to take their business elsewhere, Kipp turned to Redis Enterprise Cloud on AWS. Beyond the performance Redis is known for, as a startup, Kipp required a proven, fully managed service that offers guaranteed backup and redundancy to enable their team to focus on the challenge of optimizing the payment flow between e-commerce merchants and credit card-issuing banks.A significant portion of e-commerce business is being lost, in part, because technology limits collaboration between credit lenders and merchants. Everybody is losing in this system. The shopper doesn’t get what they want, sellers don’t sell, and bankers don’t get their investment opportunity.Typically, a purchase is declined for two basic rationales; first, the customer doesn’t have enough funds, and second, the transaction falls outside their policy or is suspected to be fraudulent.Legacy code and infrastructure don’t allow merchants to be more proactive in helping issuers of credit understand the credit risk of customers. It also doesn’t allow merchants to share the cost of risk if they wish to save the transaction. Because loan decisions are traditionally based on entrenched credit and risk policies, real-time environmental data that could affect a new understanding is absent from the process.The challenge is to leverage the insight and business interests of the merchant to optimize for a decision to meet the needs of each stakeholder: The customer, the merchant, and the bank.With e-commerce transactions becoming more challenging because today’s shoppers are demanding very fast system responses, any credit approval process must therefore take place in milliseconds, allowing for internet traffic.Kipp’s aim is to design a system that automates real-time collaboration between merchants and banks. Their challenge was to find the technology to ensure it was viable.After evaluating various databases, including open-source Redis, the combination of blazing speed and high availability from Redis Enterprise Cloud met Kipp’s system design imperatives.Because the user experience is the ultimate measure of success, system redundancy or availability is of equal importance as Redis’ speed of operation. Redis Enterprise Cloud ensures more than just system resilience. As a fully managed service it includes automated configuration and proactive maintenance to ensure the environment is always operating at peak performance. Optimization routines make scalability as simple as a point-and-click for a startup like Kipp.Kipp’s founders believe their ultimate innovation will be to enable merchants to participate in the risk with banks to help close the gap in declined e-commerce transactions. Because of the expectations of the modern digital consumer, performance and availability is essential. Redis Enterprise Cloud is ensuring Kipp can meet the requirements of merchants, credit issuers, and ultimately their shared customers.Learn how Redis is empowering retailers and financial services companies to win in the real-time economy. Request a meeting with a Redis expert today or start for free."
63,https://redis.com/blog/19-redis-geeks-to-follow/,Redis Geeks to Follow,"February 10, 2020",Redis,"Keeping up with the latest Redis news and information isn’t always easy. Redis community events like RedisConf, RedisDays, and various meet-ups are great, immersive experiences, but they come around only so often and not everyone can attend an in-person event.Thanks to the magic of the internet and social media, though, you can stay on top of all things Redis right from your computer or phone. Follow the folks on this list of some of our favorite “Online Redis Geeks” and you’ll always be in the know. (The list starts with Redis folks, but also includes Redis experts from a wide variety of backgrounds and regions.)"
64,https://redis.com/blog/introducing-redisinsight-2/,Introducing RedisInsight 2.0: A Whole New Redis Developer Experience,"November 23, 2021",Olga Lopaci,"RedisInsight 2.0 which provides an updated UI, browser tool, advanced CLI, custom data visualization, and built-in guides to help with using Redis data models like JSON and time series. Click here to try RedisInsight 2.0.The Redis manifesto states, “We’re against complexity,” followed by, “One of the main Redis goals is to remain understandable.” Although mainly referring to the Redis underlying code, we took these beliefs to heart when we worked on the brand new version of RedisInsight.RedisInsight is the free tool that lets you do both GUI- and CLI-based interactions with your Redis database, and so much more when developing your Redis based application.In the last year, we have seen great adoption of the tool—the number of active users has more than doubled over the year. This growth in user adoption has come with a lot of extensive feedback provided by our users via the in-app survey. So a makeover was much needed and due!Our aim has always been to improve the developer experience and provide an environment that would encourage Redis-based ideation and prototyping. RedisInsight 2.0 is a complete product rewrite based on a new tech stack. This version contains a number of must-have and most-used capabilities from previous releases, plus a number of differentiators and delighters. We focused on ease of use and functionality so the app could deliver increased developer productivity and shorter time to market when developing with Redis!RedisInsight now incorporates a completely new tech stack based on the popular Electron framework. You can run the application locally along with your favorite IDE, and it remains cross-platform, supported on Linux, Windows, and MacOS.We’ve revamped a number of existing features, starting with the most popular ones:A new workspace where you can build and run commands in a multiple line format and visualize their output in a way that’s easily digestible. Workbench highlights include:Redis can hold a range of different data types. Visualizing these in a format that’s convenient to you for validation and debugging is paramount. You can now easily extend the core functionality independently by building your own custom visualization plugin. Reference example and documentation is provided for you to get quickly started.We’ve introduced the much-requested Dark mode and adjusted the app color palette to adhere to the highest level of Web Content Accessibility Guidelines.RedisInsight 2.0 code is now publicly available on GitHub! The reason for this is that we want to stay open and transparent, but also create a channel to easily engage with the Redis community looking to contribute towards the best-in-class Redis GUI.This is just the start of the journey. We’d love your help to make RedisInsight even better, so we welcome feedback, suggestions, and contributions!We’re already working on the next iteration of the app that will include, among other features:RedisInsight 2.0 is currently available in public preview. You can download it from here and learn more from the release notes.Feel free to provide feedback and check out the GitHub project!You also might be interested to read about the public release of RedisJSON and its performance benchmarking (blog). RedisInsight 2.0 provides a built-in guided tutorial of the document capabilities that come with RedisJSON."
65,https://redis.com/blog/redis-on-kubernetes-redis-on-flash/,Redis Enterprise for Kubernetes Now Supports Flash Memory,"November 3, 2022",Brad Ascar,"Redis on Flash works with Redis Enterprise for Kubernetes. That’ll speed up your software – and save your company money.One exciting feature in Redis Enterprise is called Redis on Flash (RoF). RoF enables databases to extend DRAM capacity using Flash memory or solid-state drives (SSDs).Ordinarily, Redis Enterprise keeps an entire dataset in DRAM. That is good for most purposes, but it becomes an issue when a dataset is prohibitively large. However, when you use RoF, far less information is stored in DRAM: the keys, the Redis dictionary (the data structure behind the keys), and the dataset’s frequently accessed data (also called “hot data” or the working set). DRAM is still the faster tier, which is why it holds the critical data.However, with RoF the inactive data (also called “the warm values”) are moved to the lower tier, the local Flash storage tier.RoF is based on a multi-threaded asynchronous architecture that guarantees no blocking between a heavy caching request made to Flash and a light caching request made to DRAM. That avoids head-of-the-line blocking scenarios resulting from the single-threaded nature of Redis Enterprise.Redis on Flash is not designed as an alternative mechanism for data persistence. The same Append Only File and snapshot data-persistence mechanisms, persisting data to disk, are used with RoF as with Redis Enterprise.So far, so good. We’ve had RoF for several years. Beyond its technical merit, RoF has saved companies a lot of money.However, RoF wasn’t part of our Kubernetes offering – until now.RoF is now available for Redis Enterprise for Kubernetes, starting with version 6.2.12 (option to enable) and with 6.2.18 due in mid November  – a boon for customers with large datasets. Even when a RAM-only solution works technically, it often is cost-prohibitive, and customers would prefer to use Flash.Just as with RoF on non-Kubernetes clusters, there are, of course, storage prerequisites. The underlying hardware needs to be performant and directly attached to the Kubernetes cluster node. Kubernetes also has a particular way to use storage, so be sure to follow the setup guidelines to ensure a smooth installation.After the prerequisites, the rest of what you do in Redis Enterprise for Kubernetes feels like any other operation. We extended the usual way you create and use Redis Enterprise Cluster (REC) and Redis Enterprise Database (REDB) by adding YAML to express the configuration.Getting started is remarkably easy. All you need is a simple configuration inside Kubernetes to use this powerful capability. Simple, fast, and efficient. What else could you ask for?To turn on this feature, add these lines to a REC:This code example turns on RoF for the cluster, then sets the storage engine, the Kubernetes storage class, and the amount on disk for this cluster node.Similarly, you turn on RoF support for your database by adding these lines to the REDB spec:With these instructions, you tell REDB to use RoF by turning it on, identifying how much memory to allocate for the database size, and how much you want in RAM.Naturally, there are options, and some of them are mighty important. For instance, the setting here for how much data is in RAM directly impacts performance. Don’t merely copy the example above; there are configuration guidelines, so take the time to read the documentation for Redis on Flash configuration and the Kubernetes-specific details.Happy RoF-ing, and have a great day!"
66,https://redis.com/blog/redisedge-iot-database-for-edge-computing/,RedisEdge: A Dedicated IoT Database for Edge Computing,"April 29, 2020",André Srinivasan,"RedisEdge from Redis is a purpose-built, multi-model database for the demanding conditions at the Internet of Things (IoT) edge. It can ingest millions of writes per second with <1ms latency and a very small footprint (<5MB), so it easily resides in constrained compute environments. It can run on a variety of edge devices and sensors ranging from ARM32 to x64-based hardware. RedisEdge bundles open source Redis (version 5 with Redis Streams) with the RedisAI and RedisTimeSeries modules, along with RedisGears for inter-module communication.To participate in the RedisEdge Preview Program, send an email to RedisEdge@redis.com.Redis has partnered with the emerging leaders in the IoT edge platform space, EdgeX Foundry and Microsoft Azure IoT Edge, to bring RedisEdge to their users. EdgeX Foundry is a Linux Foundation project with more than 70 member companies. It provides an open source IoT edge platform designed to make it easy for anyone to develop IoT edge applications.RedisEdge is also available as a module for Azure IoT Edge, making it easy for IoT application developers using Azure IoT services to leverage the power of Redis. Azure IoT Edge with RedisEdge helps businesses focus on insights instead of data management. Developers can configure and deploy their solutions via standard containers and monitor them from the cloud.In the wild of the IoT edge environment, diverse conditions and requirements can tax any data services platform. Applications inevitably require multiple data models (e.g. time-series, graph) to support video streaming analytics, image recognition, and other complex computing requirements. RedisEdge is a multi-model database that handles various data models gracefully, removing the complexities of polyglot persistence architectures.RedisEdge supports all 10 native Redis data structures, including the new Redis Streams data structure, providing ultimate flexibility and simplicity for application developers.IoT edge application developers should not have to deal with the complexities of the IoT edge stack, such as messaging and networking protocols. With RedisEdge embedded in EdgeX Core Services and Azure IoT Edge platforms, developers can instead focus on their applications and business needs, and leave the data services and platform to us, partnered with the leading IoT edge platform providers."
67,https://redis.com/blog/redis-interview-questions/,7 Redis Interview Questions Job-Seekers Should Be Ready To Answer,"October 24, 2022",Carol Pinchefsky,"The job you’re applying for says, “Redis experience required.” Are you ready for the questions a hiring manager is likely to ask? Here’s how to prepare for the job interview.Oh, job interviews. They’re the necessary evil that stands between you and a steady paycheck – kind of how Gandalf stood between the Balrog and the Fellowship of the Ring. You need to come across as resourceful as a hobbit. Determined as a dwarf. Quick as an elf. And you absolutely can’t answer the question, “Where do you see yourself in ten years?” with “Chillaxing with the elves in the Undying Lands.”You should prepare for the job interview to come across as the person best suited to join the fellowship of [insert company name]. We came up with seven questions–and their answers–to help you convince a company’s Gandalf to let you speak Redis…and enter.For most technology jobs, it’s not important to show years of experience with a specific tool or product feature, such as data caching. What does matter is that you can show versatility.Most technology hiring managers know that tools and languages change; they want to see that you learned something from every experience. If you have worked with multiple databases, that shows a breadth of experience, and it’s something interviewers look for.So even if you don’t know Redis when you apply for a job that asks for that expertise, you may qualify if you demonstrate how often you adapted to new databases or development environments.Potential answer: One senior software engineer has an example of how she’d respond to an interviewer’s question about database experience: “I started off using MongoDB, but I pivoted to MySQL,” says Louise R. Howard. “MongoDB has weak data types, which is great for future-proofing your database, but it has a slow processing speed. I now prefer MySQL because it can shave milliseconds off my record processing…and in cases with millions of transactions, such as with Facebook, those milliseconds add up.”A ringbearer’s answer: “What have I got in my pocketses? String!…or object.”If you’re fluent in Redis or conversational, you would directly leverage the API to Redis and execute commands. Never worked with Redis, or even caching, before? That’s okay. Even Frodo’s journey to Mordor started with a single step beyond the Shire. It’s how you get to where you’re going that’s important.Potential answer: “There is no right or wrong answer to this question,” says Brad Barnes, Redis solution architecture manager.Many interviewees don’t know Redis, Barnes says. “They may or may not have executed specific commands or figured out the commands they need to run. But they do know a language or frameworks like Spring, Node.js, Python, or .Net.”This is useful, Barnes says. “When you adopt the framework, sometimes there’s the shortcuts, like a configuration-only approach, that allows you to use common patterns and get to an end result without knowing the system you’re using at too much depth.”So, what do you say when the interviewer looks at you expectantly? Point out the related background and expertise that you do have – such as Spring or another NoSQL database. Explain that you’d start with that knowledge and build your Redis expertise from there. Perhaps with the help of Coursera or Redis University.A ringbearer’s answer: “Although I don’t know Redis, Node.js took me there and back again.”Rajesh Namase, co-founder of tech blog TechRT, asks job candidates this question above. “It is a way for me to identify what specific tasks the applicant mostly works on using Redis,” he explains. Your answer lets an interviewer know if you have what it takes to solve a cache avalanche or climb the Misty Mountains.Potential answer: Although info commands and string commands are mighty useful, Howard would tell an interviewer she frequently reaches for “Time to Live,” which allows a Redis client to check how many seconds a given key will continue to be part of the dataset.Howard gives her Redis objects different times to live, depending on the use case. For example, she sets some objects live for 48 hours, so the information will be there if a user quickly returns to the page but expires otherwise. In other cases, she doesn’t set a time to live at all: “If I want to save configuration information that the server is going to use, I save it in the Redis cache, and I only update if I want to update the configuration.”Thanks to Time to Live, Howard says, “You don’t need to clean up your disk space. Redis automatically purges these objects for you.”The point is not to explain the virtues of any specific command, such as Time to Live. It’s to demonstrate to the interviewer that you know Redis features and when to use them. Plus, the commands you choose suggest the type of work you’re most comfortable doing.A ringbearer’s answer: “I am the King of Gondor, and I command you, the Army of the Dead, to fight for me. The Time to Live is now.”The interviewer might ask this Redis interview question to find out whether your knowledge is narrow and deep or shallow and wide. Perhaps you have a lot of experience with Redis in one knowledge domain or a vertical industry but are utterly ignorant in another. A hiring manager wants to know the scope of your knowledge.Potential answer: Software developer Morshed Alam says, “Redis can be used in place of a database to improve performance for caching or storing transient data. Additionally, it can be used for session management or to keep track of active users.” Session management, which tracks a user’s activities on a website site, is important for user engagement; just ask anyone who has had a site make a genuinely good recommendation.“Redis can also be used for real-time analytics or to power a chat application,” Alam says. Real-time analytics may be mission-critical for financial data, but it’s important for any kind of transaction. After all, you don’t want Amazon Prime to forget you recently watched The Rings of Power.A ringbearer’s answer: “I track session states with Redis. They don’t call me a Ranger for nothing.”Redis is a real-time database famously known for its caching capabilities, and like the rings of Lord of the Rings, there’s more than one available. (If we were going to talk about systems where there can be only one, we’d have mentioned The Highlander.)The interviewer may ask you, for example, about the caching system Memcached. It’s reassuring for them to learn that you know exactly how Redis stacks up against competitors and why you would choose it over the others (other than “because you use it at this company, and I want this job”). You can show that you earned that “Knows Redis” bullet point on your resume.Potential answer: Treat this question like the “compare and contrast” essays you used to answer on your college exams.For example, you might respond: One distinction is the rate of updates. For example, Memcached has remained relatively unchanged over the past few years. In comparison, Redis has gained new features, such as Streams.Howard has another answer for specific companies that use Redis: “I would choose Redis over Memcached because Redis has more documentation.”A ringbearer’s answer: “Redis is one cache to rule them all.”Programmers know their work like Merry and Pippin know it’s time for second breakfast. But even you don’t know absolutely everything about Redis. So how do you work your way around a question that leaves you mystified? It’s important that you know where to find good information.How you answer this question shows the interviewer your resourcefulness, as well as your ability to solve problems.Potential answer: In addition to Redis.com, Redis.io, Dev Hub, Launchpad, and Dev.to, says Barnes, it’s a good idea to cite StackOverflow or HackerNews. He also cautions, “You might need to go to four different articles because they don’t have the exact problem you do. You might need to interpolate between them.”Consider getting yourself into the public record. That gives you gravitas and credibility during a job interview – and it also demonstrates a willingness to help other people. Barnes says, “It helps to have answered questions on StackOverflow, and it certainly helps to have a GitHub repository that elaborates on an example.” Anyone can search StackOverflow, but only the chosen few have their answers voted to the top.A ringbearer’s answer: “Keep it secret. Keep it safe. Hey, wait…don’t keep it secret.”Redis is a fast, stable caching system, but no technology is a perfect match for all situations, kind of like how a ring that gave Frodo the power of invisibility ultimately exposed him to Sauron.Hiring managers want tech experts who have their eyes open. When you know a product well, you know its imperfections – and how to circumvent them. Better yet, experienced developers can identify potential technology conflicts and mismatches, point out “worst practices” among practitioners, and suggest workable options.Experts aren’t the only people who know what to do. They’re the people who know what not to do.Potential answer: You have to pay attention to whether the cache is up to date, says Alam – and then he’d openly explain his reasoning during the job interview.An engineer needs to be aware of how data is being used by the user. Take storing medical records. A patient’s blood sugar level changes daily, even hourly, Alam says. “The system is using a cache to get the data faster, but if we are not telling Redis to keep that caching up to date, then it can give the patient out-of-date data. If the software engineer [who is using Redis] is not aware of how the data is being used, then it could give the wrong information to consumers,” says Alam.Another issue you could raise in your response: Managing Redis at scale can be a complex task. It might be a good idea to consider a hosting solution.A ringbearer’s answer: “Pippin, everyone knows I’m the tall one. It’s in the database, which is the single source of truth.”If you’re job-hunting for a job that uses Redis, perhaps it’s a good idea to catch up on your reading.For interview questions that delve into more technical Redis questions, consider the following resources:"
68,https://redis.com/blog/signs-to-upgrade-to-redis-enterprise/,6 Signs It’s Time To Upgrade to Redis Enterprise,"October 13, 2022",Redis,"If you’re starting to notice these pain points in your business, it might be an indicator that your Redis open source (Redis OSS) instance is no longer sufficient to support your growth.When a business uses Redis to develop a new product or service, its technical teams commonly ask, “Should we use Redis open source software or the enterprise-supported version?”The open source version may suit your needs forever! Installing and deploying Redis OSS is simple, and it takes very little time. It’s great for small projects that stay small to medium in size. It’s also grand for Redis proofs-of-concept, as it allows developers, architects, and DevOps engineers to “test the waters.”However, businesses that have happily coasted along with the open source version of Redis sometimes encounter issues. Rather than a technical limitation, the hiccup may be a sign that organizational needs have changed, and the open source version is no longer sufficient to meet those needs.Here are six signs that it may be time to make the switch to Redis Enterprise:If your business is expanding fast—and that’s a good thing!—you may find yourself struggling to scale properly using the basic Redis OSS instance. For example, say you’re a video gaming platform faced with a booming user base. What started out as an application accessed by hundreds of users has increased to thousands and now tens of thousands of users—and it shows no sign of letting up. Even with a small army of developers deploying Redis instances, you can no longer keep up with accelerated growth.The signal: You spend more time maintaining your infrastructure than you spend enhancing your application, a good indicator that you need help in order to scale your business. It’s much easier to expand to more nodes or more instances with Redis Enterprise than trying to do it manually, which is necessary with the open source solution.Uptime matters, but sometimes it really matters. Perhaps you have a new customer that considers your application critical to its business. Failing to provide the customer with a high level of availability could be disastrous—for the customer, the customer’s users, and you. Especially if your customer, dismayed by an outage, decides to start looking for another service provider.The signal: You recognize that “going dark” is not an option.Instead of trying to configure multiple instances by hand to provide high availability, you could deploy on Redis Enterprise Cloud or easily configure Redis Enterprise with Active-Active clusters, ensuring those important customers get up to five-nines availability.Your business has grown so much that your legacy systems are no longer sufficient. Or perhaps the organization is expanding to serve new markets. Maybe you need to support a more forward-thinking IT strategy. It’s time to think about a real data center, or maybe you are ready to move to the cloud.The signal: Buzzwords like “digital transformation” get bandied about frequently, but you realize that today, it’s the accurate term.You need more control over your data. That probably means you need a flexible deployment model (for example, Kubernetes) that allows you to define and design your architecture to fit your needs. Perhaps you want certain data in your data center, other data in AWS, and other data in GCP. Instead of worrying about going to each individual machine to deploy a Redis OSS instance and then connecting everything, you could use a single tool—Redis Enterprise Operator for Kubernetes—to deploy wherever you like while maintaining total control.If you’ve done a financial audit of your business recently, you may have been shocked to discover how much money you’re spending on infrastructure. Redis OSS natively runs on memory, and systems with a lot of memory can be more expensive.The signal: You blanched when you saw the bill.It’s useful to store fresh data (three months old or less) in memory storage. However, if you store older (and likely stale) data in memory, you’re paying a lot to store data that probably isn’t that useful.Redis on Flash, a feature only available for Redis Enterprise customers, allows you to tier your memory storage. You can store your old data on Flash. You still have the same response times and high availability—but without paying the cost to store it in memory.There’s a lot of data to sort through. There always is. But sometimes the database gets so big or complex that finding information takes too long.For instance, imagine your company stocks a wide variety of inventory, such as pet supplies or car parts. You need the most up-to-date information about the stock on hand, whether you sell to resellers or directly to the public. However, you have tens of thousands of parts in your warehouse and hundreds of providers, and searching through that massive amount of data takes time. Too much time.The signal: You’re dismayed by application performance statistics.Redis Enterprise features additional processing engines that can be hugely helpful for anyone who needs powerful search capabilities, with secondary indexes that allow you to search for data inside your data. That lets you do real-time searches as well as real-time serving to whoever buys your products.Data security is always an issue, but some circumstances move it up your technology priority list. Perhaps your customer base is growing, or you move into new (and potentially regulated) industries, markets, or regions, and your data security needs might be changing.More and more companies now use Redis as their primary database, confident that its five-nines availability and data persistence is a worthy alternative to more costly databases.The signal: You realize how much heavy lifting is involved in hardening Redis OSS to be secure.That doesn’t mean it cannot be done. But it takes effort. You need to incorporate other open source projects to validate them and maintain them.Redis Enterprise offers robust security to keep data safe. For example, if you deploy using Kubernetes, Redis guarantees your data is encrypted in transit. Redis Enterprise takes worries about data security off your shoulders so you can focus on the business and sleep better at night.Need real-world examples? These Redis OSS users decided to upgrade:“For us, it wasn’t the dollars and cents business case. It was the operational availability of support and the fact that Redis Enterprise offered high availability without manual intervention.” — Steve Allen, manager, technology strategy, TELUS“It really wasn’t cost effective to maintain [open source] Redis internally; it was a lot better to get involved with others who were experts in the technology.” — Spenser Aden, senior director of product architecture, HealthStreamConsult the Redis Open Source vs. Redis Enterprise chart for a technical comparison of product features, and use our caching assessment tool to learn where your company stands."
69,https://redis.com/blog/rsalv2-sspl-announcement/,Redis Stack and Our Redis Modules Are Now Standardized Under a Dual License: RSALv2 and SSPL,"November 15, 2022",Yiftach Shoolman,"It’s been almost four years since we introduced the Redis Source Available License 1.0 (RSALv1) for our Redis modules. During this time, we’ve held an open dialogue with the Redis community about our approach to licensing. Most users like our license’s permissive, non-copyleft spirit. But we’ve also seen a couple of challenges: first, it’s hard for many users to understand the practical implications of the text of the RSAL license; second, we have not standardized on any widely-used source-available license.Today, we’re pleased to offer the Redis community more freedom and clarity by releasing Redis Stack and our Redis modules under a dual license: a new version of our Redis Source Available License (RSALv2) and the Server Side Public License (SSPLv1).The new RSALv2 license is simple to read and makes its permissions and limitations clear. And, for those users who require a more standardized license, we hope that the added option to use our software under the SSPL opens Redis Stack and our Redis modules to an even wider audience. Created by MongoDB and adopted by Elastic and many others, SSPL is becoming the de facto standard for source available licenses and is being used by millions of developers worldwide.We want to highlight that this change doesn’t affect Redis open source core in any way, which remains licensed under the 3-clause BSD license.This change also doesn’t affect our customers who use Redis Enterprise Software or Redis Enterprise Cloud.RSALv2 is a permissive non-copyleft license, allowing the right to “use, copy, distribute, make available, and prepare derivative works of the software” and has only two primary limitations. Under RSALv2, you may not:For example:We worked closely on the updated RSALv2 with Heather Meeker, who is well known for helping to draft many OSS licenses, including the Mozilla Public License 2.0 and source-available licenses like the Confluent Community License, SSPL, Elastic License 2.0, and others. We hope this change clarifies our intent and addresses the questions we’ve received about the RSALv1 license over the past few years.We believe that the permissive approach of RSALv2 and the standard wording we use to define its limitations solve many of the challenges raised by our community, but we are also aware that, like any newly created license, it will take time for some users (and their legal teams) to digest it. For this reason, we’ve also added an option to use the SSPL.This dual-license approach will allow users to choose between a permissive but less well-known license, RSALv2, or a more standardized but copyleft license, such as SSPL.Starting today, November 15th, 2022, our default binary distributions of Redis Stack and our Redis modules will be licensed under RSALv2, and when using the source code, users can apply either RSALv2 or SSPLv1. More details are in the table below:Please note that any fixes for previously released versions of any of Redis Stack or our modules will be made under RSALv1.See our FAQs for more information about RSALv2 and our dual licensing approach.To be clear, neither RSALv2 nor SSPL is an OSI-approved license, and each has its restrictions. Simply put, RSALv2 places some limits on commercializing the software. SSPL requires that if you provide the product as a service, you must publicly release any modifications and the source code of your management layers under SSPL.The necessity of source-available licenses in the cloud era has been discussed many times, and we are proud to contribute to this effort by adopting standards developers already know and use. We believe the dual license provides clarity and flexibility for Redis Stack developers in how they can leverage our latest technologies."
70,https://redis.com/blog/improving-redis-performance/,"Making the Fast, Faster! Methodically Improving Redis Performance","June 15, 2022",Filipe Oliveira,"Redis is developed with a great emphasis on performance. We do our best with every release to ensure you’ll experience a very stable and fast product.Nevertheless, if you’re finding room to improve the efficiency of Redis or are pursuing a performance regression investigation, you will need a concise methodical way of monitoring and analyzing Redis’ performance. This is the story of one of those optimizations.In the end, we’ve improved stream’s ingest performance by around 20%, an improvement you can already take advantage of on the Redis v7.0.Before jumping into the optimization, we want to give you a high-level idea of how we got to it.As stated before, we want to identify Redis performance regressions and/or potential on-CPU performance improvements. To do so, we felt the need to foster a set of cross-company and cross-community standards on all matters related to performance and observability requirements and expectations.In a nutshell, we constantly run the SPEC’s benchmarks by breaking them down by branch/tag and interpret the resulting performance data that includes profiling tools/probers outputs and client outputs in a “zero-touch” fully automated mode.The used instruments are all open source and rely on tools/popular frameworks like memtier_benchmark, redis-benchmark, Linux perf_events, bcc/BPF tracing tools, and Brendan Greg’s FlameGraph repo.If you’re interested in further details on how we use profilers with Redis, we recommend taking a look at our extremely detailed “Performance engineering guide for on-CPU profiling and tracing.”As soon as this first step was given, we started interpreting the profiling tools/probers’ outputs. One of the benchmarks that presented an interesting pattern was the Streams’ ingested benchmark which simply ingests data into a stream with a command similar to the one below:`XADD key * field value`.We’ve observed that when adding to a stream without an ID, it creates duplicate work on SDS creation/freeing/sdslen that costs about 10% of the CPU cycles, as showcased in detail in the next two perf report prints.For the same inputs, sdscatfmt and _sdsnewlen were being called twice:This allowed us to optimize Streams ingestion in around 9-10% as confirmed following benchmark results:Baseline on unstable branch (6b403f5) :First commit of this PR (avoid dup work):The initial focus of this use-case improvement lead to further analysis from Oran (one of the core-team members) that noticed yet another waste of CPU cycles. This time, it was due to non-optimal memory management within the same code block. We were allocating an empty SDS, and then re-allocating it. Reducing the number of calls will give us yet another speed improvement, as shown below.Second commit (avoid reallocs):As expected, by simply reusing intermediate computation and consequently reducing the redundant computation and allocations within the internally called functions, we’ve measured a reduction in the overall CPU time of ~= 20% of Redis Streams.We believe this is an example of how methodical simple improvements can lead to significant bumps in performance, even for already deeply optimized code like Redis.Our goal is to expand the performance visibility we have of Redis, and members from both industry and academia, including organizations and individuals, are encouraged to contribute. If we don’t measure it, we can’t improve it."
71,https://redis.com/blog/processing-time-series-data-with-redis-and-apache-kafka/,Processing Time-Series Data with Redis and Apache Kafka,"June 22, 2021",Abhishek Gupta,"RedisTimeSeries is a Redis module that brings native time-series data structure to Redis. Time-series solutions, which were earlier built on top of Sorted Sets (or Redis Streams), can benefit from RedisTimeSeries features such as high-volume inserts, low-latency reads, flexible query language, down-sampling, and much more!Generally speaking, time-series data is (relatively) simple. Having said that, we need to factor in other characteristics as well:Thus, databases such as RedisTimeSeries are just a part of the overall solution. You also need to think about how to collect (ingest), process, and send all your data to RedisTimeSeries. What you really need is a scalable data pipeline that can act as a buffer to decouple producers and consumers.That’s where Apache Kafka comes in! In addition to the core broker, it has a rich ecosystem of components, including Kafka Connect (which is a part of the solution architecture presented in this blog post), client libraries in multiple languages, Kafka Streams, Mirror Maker, etc.This blog post provides a practical example of how to use RedisTimeSeries with Apache Kafka for analyzing time-series data.The code is available in this GitHub repo https://github.com/abhirockzz/redis-timeseries-kafkaLet’s start off by exploring the use case first. Please note that it has been kept simple for the purposes of the blog post and then further explained in the subsequent sections.Imagine there are many locations, each of them has multiple devices, and you’re tasked with the responsibility to monitor device metrics—for now we will consider temperature and pressure. These metrics will be stored in RedisTimeSeries (of course!) and use the following naming convention for keys—<metric name>:<location>:<device>. For example, temperature for device 1 in location 5 will be represented as temp:5:1. Each time-series data point will also have the following Labels (key-value pairs)—metric, location, device. This is to allow for flexible querying as you will see in the upcoming sections.Here are a couple of examples to give you an idea of how you would add data points using the TS.ADD command:# temperature for device 2 in location 3 along with labels:TS.ADD temp:3:2 * 20 LABELS metric temp location 3 device 2# pressure for device 2 in location 3:TS.ADD pressure:3:2 * 60 LABELS metric pressure location 3 device 2Here is what the solution looks like at a high level:Let’s break it down:Source (local) componentsAzure servicesPlease note that some of the services were hosted locally just to keep things simple. In production grade deployments you would want to run them in Azure as well. For example you could operate the Kafka Connect cluster along with the MQTT connector in Azure Kubernetes Service.To summarize, here is the end-to-end flow:It’s time to start off with the practical stuff! Before that, make sure you have the following.Follow the documentation to provision Azure Cache for Redis (Enterprise Tier) which comes with the RedisTimeSeries module.Provision Confluent Cloud cluster on Azure Marketplace. Also create a Kafka topic (use the name mqtt.device-stats) and create credentials (API key and secret) that you will use later on to connect to your cluster securely.You can provision an instance of Azure Spring Cloud using the Azure portal or use the Azure CLI:Before moving on, make sure to clone the GitHub repo:The components include:I installed and started the mosquitto broker locally on Mac.You can follow steps corresponding to your OS or feel free to use this Docker image.I installed and started Grafana locally on Mac.You can do the same for your OS or feel free to use this Docker image.You should be able to find the connect-distributed.properties file in the repo that you just cloned. Replace the values for properties such as bootstrap.servers, sasl.jaas.config etc.First, download and unzip Apache Kafka locally.Start a local Kafka Connect cluster:To install MQTT source connector manually:If you’re using Confluent Platform locally, simply use the Confluent Hub CLI: confluent-hub install confluentinc/kafka-connect-mqtt:latestCreate MQTT source connector instanceMake sure to check the mqtt-source-config.json file. Make sure you enter the right topic name for kafka.topic and leave the mqtt.topics unchanged.In the GitHub repo you just cloned, look for the application.yaml file in the consumer/src/resources folder and replace the values for:Build the application JAR file:Create an Azure Spring Cloud application and deploy the JAR file to it:You can use the script in the GitHub repo you just cloned:Note—all it does is use the mosquitto_pub CLI command to send data.Data is sent to the device-stats MQTT topic (this is not the Kafka topic). You can double check by using the CLI subscriber:Check the Kafka topic in the Confluent Cloud portal. You should also check the logs for the device data processor app in Azure Spring Cloud:Browse to the Grafana UI at localhost:3000.The Redis Data Source plugin for Grafana works with any Redis database, including Azure Cache for Redis. Follow the instructions in this blog post to configure a data source.Import the dashboards in the grafana_dashboards folder in the GitHub repo you had cloned (refer to the Grafana documentation if you need assistance on how to import dashboards).For instance, here is a dashboard that shows the average pressure (over 30 seconds) for device 5 in location 1 (uses TS.MRANGE).Here is another dashboard that shows the maximum temperature (over 15 seconds) for multiple devices in location 3 (again, thanks to TS.MRANGE).Crank up the redis-cli and connect to the Azure Cache for Redis instance:Start with simple queries:Filter by location and get temperature and pressure for all devices:Extract temperature and pressure for all devices in one or more locations within a specific time range:– + refers to everything from beginning up until the latest timestamp, but you could be more specific.MRANGE is what we needed! We can also filter by a specific device in a location and further drill down by either temperature or pressure:All these can be combined with aggregations.It’s also possible to create a rule to do this aggregation and store it in a different time series.Once you’re done, don’t forget to delete resources to avoid unwanted costs.On your local machine:We explored a data pipeline to ingest, process, and query time-series data using Redis and Kafka. When you think about next steps and move towards a production grade solution, you should consider a few more things.Optimizing RedisTimeSeriesThis is not an exhaustive list. For other configuration options, please refer to the RedisTimeSeries documentationData is precious, including time series! You may want to process it further (e.g. run machine learning to extract insights, predictive maintenance ,etc.). For this to be possible, you will need to retain this data for a longer time frame, and for this to be cost-effective and efficient, you would want to use a scalable object storage service such Azure Data Lake Storage Gen2 (ADLS Gen2).There is a connector for that! You could enhance the existing data pipeline by using the fully-managed Azure Data Lake Storage Gen2 Sink Connector for Confluent Cloud to process and store the data in ADLS and then run machine learning using Azure Synapse Analytics or Azure Databricks.ScalabilityYour time-series data volumes can only move one way—up! It’s critical for your solution to be scalable:Integration: It’s not just Grafana! RedisTimeSeries also integrates with Prometheus and Telegraf. However, there is no Kafka connector at the time this blog post was written—this would be a great add-on!Sure, you can use Redis for (almost) everything, including time-series workloads! Be sure to think about the end-to-end architecture for data pipeline and integration from time-series data sources, all the way to Redis and beyond."
72,https://redis.com/blog/redisearch-2-0-hits-its-first-milestone/,RediSearch 2.0 Hits Its First Milestone,"July 29, 2020",Pieter Cailliau,"We are happy to announce the release of the first milestone in the development of RediSearch 2.0. RediSearch is a real-time search engine that lets you query your Redis data to answer a wide variety of complex questions.This milestone, dubbed 2.0-M01, marks the re-architecture of the way indices are kept in sync with the data. Instead of having to write data through the index (using the FT.ADD command), RediSearch will now follow the data written in hashes and automatically index it.The big advantage here is that you can now add RediSearch to your existing Redis instance and create a secondary index without having to update your application code. This lets you immediately start using RediSearch on your existing data, simply by loading the RediSearch module and defining the schema. General availability of RediSearch 2.0 is expected this Fall.(Note: This new feature introduces some changes to the API (listed below). We try to maintain backward compatibility as much as we can, but in this case it was just not possible. We plan to make adjustments and fixes going forward as we gather customer feedback.)As noted above, this RediSearch 2.0 milestone includes several changes to the API:The biggest update to the API is how indices are created. In RediSearch 2.0 the command FT.CREATE is used to create indices. The additions to the API are highlighted in yellow here:Let’s dig into some of the details:The RediSearch 2.0-M01 milestone also brings a few other updates:FT.ADD idx doc1 1.0 LANGUAGE eng PAYLOAD payload FIELDS f1 v1 f2 v2Is mapped toHSET doc1 __score 1.0 __language eng __payload payload f1 v1 f2 v2This means that the score, language, and payload fields on your index must be called __score, __language, __payload, accordingly, in order for the mapping to work as expected.We are really excited about these changes because you can now load RediSearch into your existing Redis database and index your existing data that resides in hashes, without having to update your application logic when manipulating these documents.  You can try out this milestone release by taking the source code from GitHub or by using the 1:99:1 RedisSarch Docker image. This version is not yet production ready, but we wanted to share it with you now to gather your feedback. Please share any comments or issues on our GitHub repository or in the Redis Community forum."
73,https://redis.com/blog/redisearch-2-build-modern-applications-interactive-search/,RediSearch 2.0 Lets You Build Modern Apps with Interactive Search Experiences,"February 18, 2021",Ashish Sahu and Emmanuel Keller,"Today we are excited to announce the general availability of RediSearch 2.0, bringing its powerful querying, indexing, and full-text search engine to all Redis users. In public preview since September 2020, RediSearch 2.0 has already garnered a growing list of customers who rely on it for a myriad of use cases from creating modern applications to full-text search to real-time analytics. RediSearch 2.0 introduces a brand new architecture that makes it more than twice as fast as RediSearch 1.6, and RediSearch now supports Redis’ Active-Active geo-distribution and Redis on Flash.Modern organizations are capturing large amounts of structured and unstructured data. Too often, however, this data is locked in slow, disk-based databases that don’t support real-time experiences for modern applications. RediSearch eliminates these performance bottlenecks by allowing users to easily index their Redis datasets and then query and aggregate the data in a fully distributed manner in real-time, at the speed of Redis.As shown in the diagram above, RediSearch brings more sophisticated data modeling to Redis by providing several indexing strategies for the value part of the key, including full-text, geo-location, numbers, and tags. Without indexes, Redis must perform a SCAN operation for every query, which can be extremely slow and inefficient. And creating and maintaining these indexes manually is complex and error-prone. RediSearch maintains these indexes for the user and allows you to query across data structures in a clustered database.The addition of RediSearch to your technology stack simplifies the data infrastructure, extends applications with rich search experiences, and unlocks the power of analytics in Redis. Developers no longer need to flip back and forth between multiple technologies, query languages, data models, and bolt-on search engines to create modern applications.Written in C, RediSearch is built with performance in mind using in-memory data structures such as Trie and leveraging modern distributed indexing and query algorithms. This makes it 5x – 10x faster than existing search engines (for more on RediSearch’s speed, see Search Benchmarking: RediSearch vs. Elasticsearch). RediSearch’s low-latency indexing and data querying makes it suitable for frequently updated datasets. And the new RediSearch 2.0 is 2.4x faster than the previous version.RediSearch allows you to quickly create indexes on datasets in multiple data types in Redis. (Hashes are currently supported and we plan to release support for JSON soon, followed by Streams.) RediSearch uses an incremental indexing approach for lightweight index creation and deletion. Its rich query language lets you query your data at lightning speed, perform complex aggregations, and filter by properties, numeric ranges, and geographical distance.RediSearch supports full-text indexing and stemming-based query expansion in multiple languages, including Chinese, Spanish, Russian, French, German, and many more. Furthermore, you can enrich users’ search experiences by implementing auto-complete suggestions using ‘fuzzy’ search technology.This latest release also makes RediSearch easier to scale than ever before. With RediSearch 2.0, customers can now quickly grow to query and index billions of documents on hundreds of servers. And with support for Redis on Flash, that can be done in a more cost effective way than ever before. RediSearch can also be deployed in a globally distributed manner by leveraging Redis Enterprise’s Active-Active technology to deliver five-nines (99.999%) availability across multiple geo-distributed replicas, which enables read operations (like querying and aggregation) and write operations (e.g. indexing) to be executed at the speed of local RediSearch deployments without worrying about conflict resolution..Enabling indexing, querying, and full-text search across different data types and data structures is essential to helping users unlock the power of their data. RediSearch’s ability to run these queries in a fully distributed manner without scaling limitations and at sub-millisecond latencies is truly a game changer.Our  customers are using RediSearch to not just accelerate their legacy applications but also to create their next-generation real-time applications. GoMechanic, for example, uses RediSearch to search across a database of 10 million spare parts (for more, see the RediSearch 2.0 press release). Many e-commerce apps are using RediSearch to provide interactive search across millions of products in their catalogs and using fuzzy search to give users auto-complete suggestions.With RediSearch’s ephemeral search capability, creating indices is lightweight, enabling thousands of indices in the same database so developers can rapidly create and expire indices based on a customer’s purchase history, for example. A health insurance company, meanwhile, is using RediSearch to let users run geospatial queries on their websites and apps to find appropriate healthcare providers in their neighborhood. All of these uses are already deployed at scale in production environments.Learn how you can use RediSearch 2.0 to accelerate your application modernization journey. Or to get started right away, visit the RediSearch Quick Start page."
74,https://redis.com/blog/connecting-spark-and-redis-a-detailed-look/,Connecting Spark and Redis: A Detailed Look,"February 2, 2016",Itamar Haber,"The spark-redis package on github is our1 first step in the Spark-Redis journey. Spark has captured the public imagination around the real-time possibilities of big data and we1 hope to contribute in making this possibility a reality.The spark-redis package is a Redis connector for Apache Spark that provides read and write access to all of Redis’ core data structures (RcDS) as RDDs (Resilient Distributed Datasets, not to be confused with RDBs).I thought it would be fun to take the new connector for a test run and show off some of its capabilities. The following is my journey’s log. First things first…There are a few prerequisites you need before you can actually use spark-redis, namely: Apache Spark, Scala, Jedis and Redis. While the package specifically states version requirements for each piece, I actually used later versions with no discernible ill effects (v1.5.2, v2.11.7, v2.8 and unstable respectively).I fumbled quite a lot trying to get it all working. Just after I finished putting everything in place, friend and fellow Redis-eur Tim Spann @PaaSDev published a step-by-step on “Setting up a Standalone Apache Spark Cluster” over at @DZone. That should get you through the hairiest parts if you’re on Ubuntu like me.Once you’ve fulfilled all the requirements, you can just git clone https://github.com/RedisLabs/spark-redis, build it by running sbt (oh, yeah, install that one too) or just use the package from spark-packages.org () and you should be all ready to go… but go where exactly?With this being an educational exercise, I needed a problem that could be solved with an advanced Directed Acyclic Graph engine and the fastest NoSQL Data Structure Store. After extensive research, I managed to identify perhaps the biggest challenge of contemporary data science – the counting of words. Since the word count challenge is the de-facto “Hello, World!” equivalent in Spark core, I elected to use it as my basis for tentative exploration and see how it could be adapted for use with Redis.The first thing you need when counting words is, of course, words. Being the single-minded individual I am, I decided on counting the words in Redis’ source code files (this commit specifically), also hoping to reveal some interesting data-sciency facts in the process. With everything ready, I started by jumping into spark-shell:The blinking cursor meant that the world was ready for my first line of Scala, and so I typed in:That’s awesome! I barely got started and already data science proved to be useful: there are exactly 100 Redis source files! Of course, doingEncouraged by my success, I rushed forward intent on getting the contents of the files transformed to words (that could later be counted). Unlike the usual examples that use the TextFileRDD, the WholeTextFilesRDD consists of file URLs and their contents, so it turned out that the following snippet did the work needed for splitting and cleaning the data (the call to the cache() method is strictly optional, but I try to follow best practices and expected to use that RDD later on again).A note about variable names: I like them meaningful and short, so naturally wtf means WholeTextFiles, fwds is FileWords and so forth.Once the fwds RDD had clean filenames and all the words were neatly split, I was off for some serious counting. First, I recreated the ubiquitous word counting example:Pasting the above into the spark-shell and following with take confirmed success:A note about the results: take isn’t supposed to be deterministic, but given that “requirepass” keeps surfacing these days, it may well be fatalistic. Also, 12657 must have some meaning but I’ve yet to find it.Now comes the really fun stuff, a.k.a Redis. I wanted to make sure that the results were stored somewhere safe (like my non-persisted, unbound, password-less Redis server ;)) so I could use them in later computations. Redis’ Sorted Sets are a perfect match for the word-count pairs and would also allow me to query the data like I’m used to. It took only one line of Scala code to do that (actually three lines, but the first two don’t count):With the data where I was comfortable examining it, I fired up the cli and did a few quick reads:Nice! What else can I keep in Redis? Why, everything of course. The filenames themselves are perfect candidates, so I made another RDD and stored it in a regular Set:Despite being very useful for science purposes, the contents fnames Set is pretty mundane and I wanted something more… so how about storing the word count for each file in its very own Sorted Set? With a few transformations/actions/RDDs I was able to do just that:Back to redis-cli:I could have danced (stored word-count data) all night, but if you only write and never read you’d be better off using a spark-/dev/null connector. So to make practical use of the data in Redis, I ran the following code that took the per-file word counts and reduced them to basically the same output of the classic WC challenge:Then back to spark-shell to test this code and get a grand total of all words:To wrap things up, I couldn’t just take Spark’s result for granted, so I double checked using a Lua script:Back in the days when data was small, you could get away with counting words using a simple wc -w. As data grows, we find new ways to abstract solutions and in return gain flexibility and scalability. While I’m no data scientist (yet), Spark is an exciting tool to have – I’ve always liked DAGs – and its core is extremely useful. And that’s even without going into its integration with the Hadoop ecosystem and extensions for SQL, streaming, graphs processing and machine learning… yum.Redis quenches Spark’s thirst for data. spark-redis lets you marry RDDs and RcDSs with just a line of Scala code. The first release already provides straightforward RDD-parallelized read/write access to all core data structures and a polite (i.e.SCAN-based) way to fetch key names. Furthermore, the connector carries considerable hidden punch as it is actually (Redis) cluster-aware and maps RDD partitions to hash slots to reduce inter-engine shuffling. This is just the first release for the connector, and there’s another release coming Soon(tm) which may break and change a few things, so take that into consideration. Of course, since the package is open source you’re more than welcome to use/extend/fix/complain about it 🙂1. ‘we’ being Redis and the talented Sun He @sunheehnus of the Redis community"
75,https://redis.com/blog/db-performance-in-memory-databases-solve/,Three Database Performance Problems That In-memory Databases Resolve,"October 4, 2022",Eric Silva,"Performance woes make cranky users – if they even stick around to use the software. Because if your software is too slow, it doesn’t matter how cool it is. Fortunately, in-memory databases solve a host of these problems.Everyone expects real-time information and personalized online experiences, especially your customers and end users. The first way someone loses faith? The site is too slow. Or it takes too long to load data. Or any number of other annoyances that urge a user to click away, often to a competitor’s site.If organizations deliver anything less than optimum performance, they lose customer loyalty. After only two to three bad customer experiences, 86% of consumers will leave a brand to which they were once loyal, reports customer experience vendor Emplifi, based on a global survey of 2,000 consumers.That makes it an urgent requirement for developers to make online applications perform splendidly at scale. That can be a significant challenge; real-time data may be difficult to deliver when data requirements scale beyond expectations. And tuning the database isn’t always the answer.There are lots of ways that database performance can suffer. These three top the list – and all of the problems are addressed with an in-memory database.It’s hard to find an application where “meh, it’s okay” database access speed is acceptable. Nearly every application needs to respond instantly to customer requests– whatever “instantly” feels like to that user.However, customer databases grow steadily in volume and table size, and traditional data management practices cannot keep up. “Big data” used to represent outliers in computer science, the behemoths of data collections, but yesterday’s “huge” database is today’s “ho-hum” collection. And as developers discover, extensive datasets—measured by volume, velocity, and/or variability—require a scalable architecture for efficient storage, manipulation, and analysis.As customer databases grow, it becomes more difficult to query the database for a single unique identifier. The slow query performance hinders customer service. It also makes it harder to implement customer 360° initiatives that aim to provide an accurate, single customer view to everyone in the company and customers themselves. Slow database queries do not provide enough time for data manipulation in real-time to create insightful aggregate data views.The Fix: Move customer lookup tables and other customer-related data tables to an in-memory database.An in-memory database works like other databases, except all data is stored in DRAM rather than on a traditional disk. Data is periodically stored on disk for persistence and data restoration (if and when the need arises).Performance is dramatically enhanced with in-memory databases because no time is expended writing to disk or retrieving from disk. Memory operations perform many times faster than disk-based drives, even much faster than newer NVMe or SATA solid-state disk drives.In short: In-memory data queries have low latency, which means the code can scale. That means applications can search through tens of millions of customer records to find information related to a single customer and get the results in real-time.The performance of search and query can change dramatically depending on the data operations performed. To learn how data operations affect the query performance of Redis vs. others, see this performance comparison blog post.Database performance extends beyond data retrieval to serve up customer records or to store transactions. Organizations use real-time database queries to support the analysis behind business metrics, such as project dashboards and troubleshooting alerts – any data presentation that enables better choices.For analytics, the age of the data is directly related to its quality. Older, stale data is less valuable for real-time analysis and decision-making.Bottlenecks causing query performance issues can occur anywhere along the data operations process flow. Database search and query operations are expensive in computational terms. It takes intensive resources to index and deliver search results.Simultaneously ingesting data and querying disparate data structures (like Hash and JSON documents) is demanding. This is especially true for disk-based SQL databases such as Oracle and SQL Server. The typical answer is database performance tuning, but that goes just so far due to underlying database architecture and the type of database workloads.The Fix: Employ a real-time search engine to deliver fast data results for analysis.A real-time search engine queries and aggregates massive data sets with immediate results, which generates timely data for accurate analytics. Data scientists can then consume and analyze fresh data in dashboards, graphs, charts, or other applications.I go into more detail about data search algorithms and the cool math and science behind it in my last post on Fuzzy Matching.A real-time search engine provides:This graphic depicts the consolidation of data from multiple sources of record into one real-time search engine to provide timely data for analytics and new business insights:Another common database issue is performing repetitive lookups on massive master data tables. Master data tables help define important database entities in the database, usually representing its foundation: product, partners, vendors, and orders. As with any other data management element, these grow as the company grows.Performance issues arise when databases repetitively perform large master data table lookups when the master data tables reach millions of primary or secondary key values. The common symptom is noticeably slow user search or delayed application page rendering, particularly when searching massive product databases for e-commerce sites.The Fix: Distribute the data ingest, index, and query loads across database partitions or shards and use secondary indexes.Geo-distributed database topologies can scale master data tables to tens of millions of primary and secondary keys. This enables powerful search auto-suggestion and flexible category-based (faceted) search capabilities providing instant search results for online customers and business users. Distributing both reads and writes across multiple database partitions or shards enables a massive scale of master data tables and high-performance search results.Secondary indexes are non-primary key indexes created to provide a rapid lookup of data. Their search results may contain duplicate values, such as finding all the products with a manufacturer listed as “Apple.” Secondary indexing of databases allows for flexible and rapid search of master data tables in any database field. You can create thousands of indexes for a single record or hundreds of thousands across an entire database. And the database provides automatic index management once the index is created.We provide more depth elsewhere on secondary indexing and sorted sets.Redis Enterprise provides a powerful indexing, querying, and full-text search engine for real-time data, available on-premises and as a managed service in the cloud. The Redis search engine can be used for real-time customer aggregations, as a secondary index for data hosted in Redis, to consolidate data from other data stores for analytics, and act as a fast full-text search or auto-complete engine.And – this won’t surprise you, given the points above – the Redis Enterprise real-time search engine overcomes common database performance problems:We hope this piques your interest in learning more and discovering whether Redis Enterprise is a good fit for your needs. Certainly, we have plenty of information available to help you learn more, such as a webinar on data ingestion, a webpage about real-time search, and a video that teaches how to create secondary indexes.Now is the time to discover the power of real-time search. Speak to a Redis expert or download Redis Enterprise for free. Get started today and deliver valuable fresh data to your customers and business partners with Redis Enterprise’s real-time search capabilities."
76,https://redis.com/blog/redis-at-reinvent-2022/,Visit Redis at re:Invent 2022,"November 14, 2022",John Noonan,"Visit our booth in Las Vegas on December 1st, learn about Redis, get a Redis Geek T-shirt, and enter our raffle for an Oculus Quest 2.It’s that time of the year. Days are getting shorter, the holidays are approaching, and tens of thousands of people are eagerly awaiting the tech Industry’s favorite fall tradition: AWS re:Invent.This year, there is some added excitement: a relative return to normalcy. For the first time since 2019, re:Invent is once again expected to be a largely in-person conference and to deliver all the thrill and grandeur that Las Vegas and Amazon Web Services have to offer.Redis will attend, too. And while we’re excited to join in the revelry, we’re even more excited to help businesses answer their most critical questions. Questions like, “What does a real-time data layer mean to my business?”It means a lot. Your data layer is the collection of technologies and services that you use to process, deliver, and store data for your applications.And having a data layer that supports real-time applications is more important than ever. Performance matters. Your employees expect instant responses from internal applications. If a customer application lags, our competition is one click away.To implement a real-time data layer, most organizations cobble together a Frankenstein solution by combining a primary database (such as Amazon Aurora, PostgreSQL, or MongoDB) with a secondary caching database to add a real-time acceleration, such as Redis.But to meet the requirements of today’s real-time user expectations, you need to reimagine the real-time data layer. With Redis Enterprise,  employing a primary database and enterprise cache within the same solution, with sub-millisecond responsiveness for a simplified and cost-effective real-time data layer.Want to learn how? Visit us at booth #845 at re:Invent, or check out our re:Invent 2022 page.Have questions about your real-time data layer? A team of experts is eager to answer questions about Redis. We’ll also showcase Redis Enterprise in action, with regular product demonstrations.The solution architects at the booth will be happy to help you dive deep into the details of your application stack, too. Bring your technical questions (the hard ones!), so you can discover how best to use Redis Enterprise to meet your goals.Want more individual attention? Trying to organize your time at the conference? Schedule a meeting to speak with a Redis expert onsite at re:Invent. We look forward to learning more about your business and answering your questions about Redis.The hottest show in Vegas will take place on Thursday, December 1, at 11:20 am in Lightning Theater Two. Come check out my session covering the top five ways to reimagine your data layer with Redis.Among the topics I plan to discuss:While you probably won’t win it big on the casino floor (the statistics are against you), we’re sure you’ll look like a million bucks in one of our Redis Geeks T-shirts. Stop by our booth to pick one up. Additionally, one lucky visitor to Redis’ booth will also walk away with an Oculus Quest 2!We can’t wait for re:Invent – and to talk with you!"
77,https://redis.com/blog/redis-intel-performance-testing/,"Introducing the Redis/Intel Benchmarks Specification for Performance Testing, Profiling, and Analysis","November 17, 2022",Filipe Oliveira and Martin Dimitrov,"Redis and Intel are collaborating on a “zero-touch” performance and profiling automation to scale Redis’s ability to pursue performance regressions and improve database code efficiency. The Redis benchmarks specification describes cross-language and tools requirements and expectations to foster performance and observability standards around Redis-related technologies.A primary reason for Redis’s popularity as a key-value database is its performance, as measured by sub-millisecond response time for queries. To continue performance improvement across Redis components, Redis and Intel worked together to develop a framework for automatically triggering performance tests, telemetry gathering, profiling, and data visualization upon code commit. The goal is simple: to identify shifts in performance as early as possible.The automation provides hardware partners such as Intel with insights about how software uses the platform and identifies opportunities to further optimize Redis on Intel CPUs. Most importantly, the deeper understanding of software helps Intel design better products.In this blog post, we describe how Redis and Intel are collaborating on this type of automation. The “zero-touch” profiling can scale the pursuit of performance regressions and find opportunities to improve database code efficiency.Both Redis and Intel want to identify software and hardware optimization opportunities. To accomplish that, we decided to foster a set of cross-company and cross-community standards on all matters related to performance and observability requirements and expectations.From a software perspective, we aim to automatically identify performance regressions and gain a deeper understanding of hotspots to find improvement opportunities. We want the framework to be easily installable, comprehensive in terms of test-case coverage, and easily expandable. The goal is to accommodate customized benchmarks, benchmark tools, and tracing/probing mechanisms.From a hardware perspective, we want to compare different generations of platforms to assess the impact of new hardware features. In addition, we want to collect telemetry and perform “what-if” tests such as frequency scaling, core scaling, and cache-prefetchers ON vs. OFF tests. That helps us isolate the impact of each of those optimizations on Redis performance and inform different optimizations and future CPU and platform architecture decisions.Based on the premise described above, we created the Redis Benchmarks Specification framework. It is easily installable via PyPi and offers simple ways to assess Redis performance and underlying systems on which Redis runs. The Redis Benchmark Specification currently contains nearly 60 distinct benchmarks that address several commands and features. It can be easily extended with your own customized benchmarks, benchmark tools, and tracing or probing mechanisms.Redis and Intel continuously run the framework benchmarks. We break down each benchmark result by branch and tag and interpret the resulting performance data over time and by version. Furthermore, we use the tool to approve performance-related pull requests to the Redis project. The decision-making process includes the benchmark results and an explanation of why we got those results, using the output of profiling tools and probers outputs in a “zero-touch” fully automated mode.The result: We can generate platform-level insights and perform “what-if” analysis. That’s thanks to tracing and probing open source tools such as memtier_benchmark, redis-benchmark, Linux perf_events, bcc/BPF tracing tools, Brendan Greg’s FlameGraph repo, and Intel Performance Counter Monitor for collecting hardware-related telemetry data.If you’re interested in further details on how we use profilers with Redis, see our extremely detailed Performance engineering guide for on-CPU profiling and tracing.So, how does it work? Glad you asked.A primary goal of the Redis Benchmarks Specification is to identify shifts in performance as early as possible. This means we can (or should) assess the performance effect of the pushed change, as measured across multiple benchmarks, as soon as we have a set of changes pushed to Git.One positive effect is that the core Redis maintainers have an easier job. Triggering CI/CD benchmarks happens by simply tagging a specific pull request (PR) with ‘action run:benchmarks‘. That trigger is then converted into an event (tracked within Redis) that initiates multiple build variants requests based upon the distinct platforms described in the Redis benchmarks spec platforms reference.When a new build variant request is received, the build agent (redis-benchmarks-spec-builder) prepares the artifact(s). It adds an artifact benchmark event so that all the benchmark platforms (including the ones on the Intel Lab) can listen for benchmark run events. This also starts the process of deploying and managing the required infrastructure and database topologies, running the benchmarks, and exporting the performance results. All of the data is stored in Redis (using Redis Stack features). It is later used for variance-based analysis between baseline and comparison builds (such as the example of the image below) and for variance over time analysis on the same branch/tag.New commits to the same work branch produce a set of new benchmark events and repeat the process above.The framework can be deployed both on-prem and on the cloud. In our collaboration, Intel is hosting an on-prem cluster of servers dedicated to the always-on automatic performance testing framework (see Figure 2).The cluster contains six current generation (IceLake) servers and six prior generation (CascadeLake) servers connected to a high-speed 40Gb switch (see Figure 3). The older servers are used for performance testing across hardware generations, as well as for load generation clients in client-server benchmarks.We plan to expand the lab to include multiple generations of servers, including BETA (pre-release) platforms for early evaluation and “what-if” analysis of proposed platform features.One of the observed benefits of the dedicated on-prem setup is that we can obtain more stable results with less run-to-run variation. In addition, we have the flexibility to modify the servers to add or remove components as needed.Today, the Redis Benchmarks Specification is the de-facto performance testing toolset in Redis used by the performance team. It runs almost 60 benchmarks in daily continuous integration (CI), and we also use it for manual performance investigations.We see benefits already. In the Redis 7.0 and 7.2 development cycle, the new spec has already allowed us to prepare net new improvements like the ones in these pull requests:In summary, the above work allowed for up to 68% performance boost on the covered commands.Our present performance engineering system enables us to detect performance changes during the development cycle and to enable our developers to understand the impact of their code changes. While we have made significant progress, there is still much that we would liketo improve.We are working to improve the ability to aggregate performance data across a group of benchmarks. That will let us answer questions like, “What are the top CPU-consuming stacks across all benchmarks?” and “What is the lowest hanging fruit to optimize and produce the largest impact across all commands?”Furthermore, our baseline versus comparison analysis deeply depends upon simple variance-based calculation. We intend to approach better statistical analysis methods that permit trend-based analysis on more than a group of data points and for finer-grained analysis to avoid the “boiling-frog issue” of the cloud’s noisy environments.Redis API has more than 400 commands. We need to keep pushing for greater visibility and better performance across the entire API performance. And we need to do that while also focusing on the most-used commands, as determined by community and customer feedback.We expect to expand the deployment options, including cluster-level benchmarking, replication, and more. We plan to enrich the visualization and analysis capabilities, and we plan to expand testing to more hardware platforms, including early (pre-release) platforms from Intel.Our goal is to grow to a larger usage of our performance platform across the community and the Redis Developer group. The more data and the more different perspectives we get into this project, the more likely we are to deliver a faster Redis."
78,https://redis.com/blog/get-redis-cli-without-installing-redis-server/,How to Install Redis CLI Without Installing Redis Server (Even on Windows),"July 2, 2022",Redis,"Redis CLI is invaluable for writing software for getting to know a new module. If you didn’t have a Redis command line interface, understanding Redis’ data structures and testing connections would be far more complicated.Redis CLI has two modes; the first is the interactive mode – REPL (Read Eval Print Loop) – where Redis commands deliver replies from the Redis server. The other option is to set command mode. With this mode, redis-cli needs additional arguments to get a reply on the standard output. However, getting this jewel of a tool is not straightforward for many. The source code for the Redis command line interface is included in the Redis GitHub repository and is automatically compiled when you build Redis from source. But what happens if you can’t (or don’t want to) create a Redis configuration from source? It means you also don’t have a Redis command line interface and building an entire database, even a Redis database, from source just to get access to the command line interface utility is overkill and sometimes not even an option.In this post, we’ll share how to get Redis CLI without having to install Redis or make a full Redis server, but first, let’s look at a couple of scenarios.For those of us on Linux or macOS, building Redis from source involves having the relevant compilers and tools on your system, which produces both the command line interface (CLI) and the Redis server. For most developers on these platforms, that’s not a huge burden. For those interested in best practices for deploying and operating NoSQL, read our industry report on “Best Practices for Deploying and Operating Your NoSQL Initiative.”However, if you’re not on a UNIX-like system, things get complicated quickly. For various reasons, you can’t just compile Redis on Windows. Microsoft once supported a fork of Redis that ran directly on Windows-based machines, but it’s no longer maintained. That means that, on Windows, you can’t get a current version of Redis CLI. While it’s possible to use the Windows Subsystem for Linux that can run Redis, this has its own challenges, such as file system limitations and just generally not feeling native or appropriate for the system. In addition, there are many developers who have development machines locked down in a fun and creative ways to explicitly block this type of operation.For example, you might be in a situation where you’re on a low-spec server and you just need to do some quick checks in Redis – getting the dependencies and building the software may not be possible in these constrained environments.There are many situations where you may be building software that uses Redis, but you’ll never personally manage or administer a localhost process of Redis. Imagine if you’re using Redis Enterprise Cloud – you can have a Redis instance in seconds, but if you want to do anything with it you need to have CLI, which requires building the whole package from source code. Or perhaps you’re at a large organization that is running a self-managed Redis Enterprise Software cluster. Here too, you may not have an actual need to build the Redis server on your development machine, since you just want to connect remotely.Finally, you might want to get up and running quickly. Pulling down the entire Redis C project (and all the tools needed to build that) might not be efficient for your workflow.If you fall in one of the above scenarios, read on.In 2007, Jeff Atwood wrote, rather disparagingly, “any application that can be written in JavaScript will eventually be written in JavaScript.”Bringing this to Redis, Lu Jiajing started a small project (less than 250 lines of JavaScript!) in 2015 to reimplement the overall operation of Redis CLI in Node.js. Since then, it’s gotten closer to mimicking the Antirez-provided Redis CLI. While not perfect (yet), it provides the bulk of the functionalities that you’d need on a day-to-day basis.You may ask, why bother with this if you still have to install Node.js first? Well, first off, Node.js provides a much wider range of installation options than Redis. You can get it as a GUI MSI for Windows or a pkg for macOS, as well as plain old compressed binaries for Windows, macOS, or Linux. You can also install Node.js via a package manager on many platforms.Once you’ve installed Node.js and npm, it’s a simple one-liner to get and install the Node.js version of redis-cli:npm install -g redis-cliThen you can run it with the command:rdcli -h your.redis.host -a yourredispassword -p 11111(using your relevant connection information).Alternately, if you don’t like global installs, you can clone the repository and install the dependencies:git clone https://github.com/lujiajing1126/redis-clicd redis-clinpm installThen you can run it from this directory by invoking the index.js file directly with the command line arguments:node index.js -h your.redis.host -a yourredispassword -p 11111(using your relevant connection information).There you have it. You can get redis-cli up and running on your development machine quickly and easily with Node.js redis-cli written by Lu Jiajing. Instead of building the whole Redis project with C, you can just grab Node.js (even better if you already have it installed, and you probably do, let’s be honest), install this small module, and start hacking away in Redis.We established that Redis CLI has two main modes, the interactive mode, and the command mode. Here are a few of the supplemental operations Redis CLI performs:One cool little use of this module is to make it a part of devDependencies in your package.json on a Node.js project. This way, you can effectively “pack in” redis-cli with your project, making sure everyone on your team has the tool. To do this, install it as a development dependency:npm install -save-dev redis-cliThen in your package.json, add the following line to the beginning of the scripts object:""rediscli"": ""node ./node_modules/redis-cli/index.js"",Now, anyone who has your project can start Redis CLI by running:npm start rediscli -h your.redis.host -a yourredispassword -p 11111(using your relevant connection information).You can even hard code in arguments if need be, but never include your Redis password in your package.json file!"
79,https://redis.com/blog/respapp-joining-redis/,Delivering a Better Developer Experience: Redis and RESP.app Are Joining Forces,"November 2, 2022",Pieter Cailliau,"I am excited to share that Redis is joining forces with the creator of RESP.app, Igor Malinovskiy. In addition to bringing RESP.app’s popular features into RedisInsight, we share a vision to offer the best possible developer experience that empowers anyone––from individuals to global companies––to unlock the full potential of Redis.RESP.app provides a graphical user interface that offers popular features among developers and IT operators of Redis Open Source, Redis Enterprise, and managed cloud services.Igor created RESP.app as a response to his own frustrations when he began building with Redis, so he created features he personally was challenged with or limited by using just the command line interface. Since its humble beginnings in Ukraine, the RESP.app project has created an admirable community that has contributed to the overall Redis ecosystem.At Redis, we know the importance of the developer experience, a core value that Salvatore Sanfilippo (a.k.a antirez) wrote in the original Redis Manifesto: “We optimize for developer joy.” A bad experience frustrates people, which results in developer attrition. In contrast, a positive experience promotes efficiency, translating into highly effective code and applications (and fewer grumpy coworkers).Redis has been developing a similar tool in parallel. RedisInsight has the same purpose – an enhanced user experience for Redis developers and administrators – although RedisInsight has a somewhat different feature set, including support for Redis real-time data platform capabilities such as the popular search and JSON, but also graph and time series capabilities. The features of these two tools intersect nicely. So it felt natural to join forces to benefit all developers.I’m thrilled that Igor is joining the Redis team to empower developers to create the most productive code for business-critical applications powered by the fastest database on the planet.We intend to make RedisInsight the go-to companion tool for software development with any flavor of Redis. Until we reach that goal, current RESP.app users can continue using it. Ultimately, we believe a happy developer can accomplish more in less time and is more likely to leverage the Redis database for more use cases.The result? We will help developers to create the most productive code for business-critical applications.Can’t wait? Try it for yourself! Download RedisInsight. We welcome your feedback on GitHub and take a look at the FAQs for answers to our most common questions."
80,https://redis.com/blog/5-key-takeaways-for-developing-with-redis/,Redis Namespace and Other Keys to Developing with Redis,"May 18, 2022",Itamar Haber,"Download Nine Essential Database Capabilities and make sure your database has what it takes to meet all your demandsDeveloping an application with Redis is a lot of fun, but as with any technology, there are a few points you should keep in mind while designing a Redis-based or Redis namespace application. You’re probably already familiar with relational database development, but while many of the same practices apply, keep in mind that Redis is an in-memory database and it is (mostly) single-threaded. Read on to explore Redis keys best practices.Therefore, there are several peculiarities you should pay attention to when using Redis:Databases store data, but any developer can lose track of some of the data you’re putting into Redis. This is only natural, due to your application’s requirements changing or you changing the way you store the data. Perhaps you’ve neglected to EXPIRE some of the keys, or maybe a module of the application has been retired.Whatever the case, chances are that some of the data in your Redis database are no longer used and taking up space for no purpose. Redis’ schema-less nature makes it extremely difficult to make sense of your dataset’s contents unless you use a solid nomenclature for your keys. Using a proper naming methodology with Redis namespace for your keys can make the housekeeping of your database much easier. When you namespace your keys by application or service – the convention is to use the colon (‘:’) character to delimit parts of the key’s name -a Redis namespace best practice. This way, you’ll be able to identify them easily during a data migration, conversion, deletion, or move. Redis namespace & Redis namespace keys help with this identification.Beyond Redis namespace, Another common use case for Redis is as a secondary data store for “hot” data items, while most of the data is kept in another database (e.g. PostgreSQL, MongoDB). In such cases, developers quite often forget to remove the data from Redis when it is moved from the primary data store. This sort of cross-datastore dependency requires a cascading delete, which can be implemented by keeping all the identifiers for a given data item in a Redis set. This ensures that a cleanup procedure invoked after deletion from the primary data store only needs to iterate through that set’s contents in order to remove all the relevant copies and related tidbits (including the set itself upon completion).This might seem contradictory to the above regarding Redis namespace, but since key names take up memory as well, you should strive to keep them short. Obviously, this becomes an issue with datasets consisting of millions or billions of keys, but the fact is that long keys have a price with any hashtable.For example: consider that storing 1,000,000 keys with Redis namespace, each set with a 32-character value, will consume about 96MB when using 6-character key names, and 111MB with 12-character names (on a 32-bit Redis server). This overhead of more than 15% becomes quite significant as your number of keys grows. With Redis deleting keys with prefixes is also a possibility.Either because of memory usage or performance, sometimes one data structure is a better fit for your data set than another. Here are a few best practices to bear in mind:The SCAN command is available starting with Redis v2.8 and enables you to retrieve keys in the keyspace using a cursor. This behavior is unlike that of the (hiss) KEYS command, which returns all matching elements at once, but is considered risky in production because it may block your Redis server and even exhaust its RAM resources. SCAN, on the other hand, makes it possible to inspect the data without the risk of blocking your server or having to rely on a slave.Note that SCAN requires you to read a cursor value that’s passed to the subsequent call to SCAN. SCAN also accepts a keyname pattern and an optional count argument. Another difference between SCAN and KEYS is that it is possible to get the same key name more than once with SCAN.SCAN is accompanied by SSCAN, HSCAN, and ZSCAN, which allow you to iterate the contents of sets, hashes, and sorted sets (respectively).As a developer, you’ll be navigating familiar ground once you embrace Redis’ ability to run Lua scripts. One of the easiest languages to pick up, Lua offers you the ability to express your creativity with code that runs inside the Redis server itself. When applied correctly, Lua scripts can make a world of difference in terms of performance and resource consumption. Instead of bringing data to the (application’s) CPU, scripts allow you to execute logic near the data, which reduces network latency and redundant transmission of data.A classic example of Lua’s dramatic impact happens when you’re fetching a lot of data from Redis only to filter or aggregate it in your application. By encapsulating the processing workflow in a script, you only need to invoke it in order to get the significantly smaller answer at a fraction of the time and the resources.Pro Tip: Lua is great, but once you move workflows to it you may find that error reporting and handling are harder (you are running, after all, inside the Redis server). One clever way around that is using Redis’ Pub/Sub and having your scripts publish their “log” messages to a dedicated channel. Then, set up a subscriber process to get these messages and handle them accordingly.There are probably many other important tips you’ll pick up during your Redis escapades, but this list should help to get you started with some of the most important ones. If you have other suggestions that you want to share, feedback or questions – please feel free to shout at me, I’m highly available 🙂"
81,https://redis.com/blog/data-architecture-modernization-in-financial-services-webinar/,Best Practices For Data Architecture Modernization in Financial Services,"May 10, 2022",Henry Tam,"Best Practices for a Modern Data Layer in Financial Services is an essential white paper on the best steps to modernize from a rigid and slow IT legacy system to a modern data layer. Meet customer demands in real-time.Redis recently took part in a webinar with data giants TDWI to discuss best practices for data architecture modernization in financial services. Leading the webinar is the world-renowned expert in information management, David Loshin, who speaks about data layer modernization frequently at different TDWI events and conferences.Below we’ve provided insights into what was covered in the webinar on the most powerful ways a financial services organization can modernize its data layer.Technology has transformed the financial services industry from top to bottom. Brick and mortar are being phased out and replaced by mobile banking. Transaction processing cadence has evolved from periodic batch to real-time processing, posing a significant challenge to banks; IT legacy systems are outdated and incapable of providing a real-time digital banking experience.This is problematic because customer expectations aren’t just sky-high, they’re in the stratosphere. The demand for a consistent real-time digital banking experience has made the financial services industry even more cutthroat and banks are struggling to meet the demands of the modern consumer with rigid legacy IT systems.Moreover, banks are feeling additional pressure from government legislation and the market to modernize their data layers. Fintechs are agile and are reaping the rewards of having a flexible architecture that allows them to provide an omnichannel experience as well as bring new products to the market.Banks need to modernize now to level the playing field and provide a real-time digital banking service that’s fast, easy, and accessible.Modernizing your financial data layer doesn’t need to be complicated, nor disruptive. Data layer modernization follows a chronological process that begins with identifying data accessibility gaps and ends with operationalizing the data layer with microservices.In his PowerPoint slide, David reveals the six most optimal ways a financial services organization can modernize its data layer without disruption. All of the technical components behind each method underwent a comprehensive review to crystalize the point of direction financial services should follow when modernizing the data layer.These include:Many banks need to modernize their data layer to overcome myriad challenges in today’s fast-paced digital environment. Some of the world’s leading banks have recognized IT legacy systems as performance inhibitors and have modernized their data layers with Redis Enterprise to overcome the biggest obstacles standing in their way.Redis’ Senior Solutions Marketing Manager Henry Tam shares a number of use cases to reveal what results banks can expect when using Redis Enterprise to modernize their data layer. Henry explores how Redis Enterprise is used to supercharge performance levels in each use case:Thanks to Redis Enterprise, customers enjoy an omnichannel experience that allows them to seamlessly access a range of different digital products and services in real-time, regardless of their location, bank accounts, mortgages, loans, investments, and much more.Data sharing with third parties is mandatory for providing customers access to innovative digital solutions that augment the user experience. But doing so exposes financial institutions to the possibility of cybercriminals infiltrating their data.In this use case, Henry highlights how Redis Enterprise is used to shield customer information from cyber-criminals while sharing data with third-party organizations. This involves improving the developer experience with granular access to APIs, data, and resources.Henry explores how RediSearch is used to speed up manual investigations by sifting through reams of data with hyper-efficiency, allowing it to identify any suspicious patterns that would indicate fraudulent activity.It only takes a few seconds for fraudsters to commit online theft. Therefore, fraud detection is entirely dependent on the speed at which a bank can identify and react to suspicious data patterns. Henry unpacks how Redis’ ability to guarantee real-time data enables a large bank to swiftly dispel fraudulent activity through real-time digital identities, AI models, and more.Take the path toward data architecture modernization."
82,https://redis.com/blog/redis-enterprise-in-google-cloud-available-in-delhi/,Redis Enterprise’s Fully Managed Service in Google Cloud Platform Now Available in Delhi,"April 6, 2022",Gilbert Lau and Abhishek Srivastava,"We are very excited to share that our fully managed Redis Enterprise service in the Google Cloud Platform is now available in the Delhi region (Asia-South2) in addition to the Mumbai (Asia-South1) region of India. Today, there are over 20 Google Cloud Platform regions where customers can deploy Redis Enterprise through GCP Marketplace to support their real-time data use cases; those include enterprise cache, session management, gaming leaderboard, fraud detection, high-speed transactions, asynchronous communication, and more. Many of our customers have been using Redis Enterprise as their primary database in retail, gaming, the financial services industry, and more to improve customer experience through sub-millisecond response time.Active-Active Geo-Distributed topology has always been one of the primary differentiators of Redis Enterprise. Active-Active Redis databases provide a few core benefits over other geo-distributed solutions. For instance:With both Delhi and Mumbai regions in service, this enables our customers to bring real-time applications closer to their users in the Delhi region as well as provides them with a 5-9s (99.999%) service availability from Redis Enterprise’s Active-Active Geo Distribution to power their mission-critical workloads. More importantly, this allows our customers to execute their data governance strategy to comply with their data sovereignty or regulatory requirements and align with Indian data privacy laws.Redis Enterprise Active-Active Geo-Distribution delivers local latency for globally distributed applications, unifies the data layer across regions and clouds, and ensures business continuity even when disaster strikes. To learn more about how to develop applications using Active-Active Geo-Distribution from Redis Enterprise, download this whitepaper at no cost.The following diagram is a reference architecture that leverages Delhi and Mumbai as deployment regions for Redis Enterprise through GCP Marketplace. Both of these Redis clusters are part of the same Active-Active replication group. Each is capable of reading or writing independently of one another.We have separate GKE clusters in each of these regions. The applications in each region connect to their respective Redis Enterprise cluster. This architecture utilizes the Multi-Cluster Ingress controller for GKE. It’s a Google-hosted service that supports deploying shared load balancing resources across clusters and across regions.More Google Cloud Platform regions are expected to come online on a regular basis to make their managed cloud services more accessible to their customers. Redis is the same. We will run in lockstep with the Google Cloud Platform region roadmap to make Redis Enterprise available and truly accessible to an ever-growing Google Cloud Platform customer base around the globe. Digital natives who exist primarily or entirely online are no longer coming exclusively from advanced economies. Entrepreneurs of many developing countries are leveraging Google Cloud Platform’s global footprint to jumpstart their digital businesses and serve their valuable customers at a regional or global scale in no time and without any limitation.Together, Google Cloud and Redis not only provide their customers with a ubiquitous platform to run their business in an always-on fashion. They also allow them to significantly shorten their time-to-market delivery of new products and services, as well as accelerate their product lifecycle. Through GCP Marketplace, Google Cloud Platform and Redis Enterprise provide a consumption-based OpEx model, allowing startups or small businesses to scale up or down, based on their respective seasonal or industry-specific business cycles. This makes any business become more agile and immune to volatile economic cycles. Coupled with the speed and high availability of Redis Enterprise, customers can rely on a real-time data platform to power their business-critical applications.Google Cloud Platform’s distributed regional footprints are strategic to our partnership, as this will enable us to expand our reach to existing and new customers, support use cases requiring a global reach, and a highly available country-wide presence.  We are currently offering a coupon credit to get you started with Redis Enterprise on Google Cloud!"
83,https://redis.com/blog/redisdays-new-york-2022-overview/,RedisDays New York 2022 Overview,"April 22, 2022",Henry Tam,"After kicking things off in London with a day centered on all-things real-time data, followed by a developer-focused day in San Francisco, RedisDays 2022 came to a close in New York with a day of sessions all about the latest Redis developments in artificial intelligence / machine learning (AI/ML).The RedisDays New York sessions made it clear that any modern data stack has to be built with a performance-first approach. That means it must “support modern data models, and processing engines, enable AI/ML, and deliver composable microservices,” as described by Redis CMO Mike Anand in the keynote. The dev experience should be fast, smart, and simple. How are microservice-based apps and analytics powered by AI/ML algorithms helping to deliver an end-to-end real-time user experience? Let’s take a look at the day’s sessions, what they covered, and even watch the full recordings on-demand.Mike Anand Chief Marketing Officer, Redis“The digital world is so deeply ingrained in our daily routines,” says Mike Anand. “Making an online purchase, playing online games, transferring money, paying by credit card – this is all performed with real-time data.” So many of our everyday processes are distinctly connected to real-time data.Today’s customers demand instantaneous results and as simple a process as possible. That’s why companies relying on legacy systems need to catch up and start creating modern applications. As Mike notes in the keynote, companies need to “rethink [their] applications using new design patterns like microservices or event-streaming architectures.”That’s where Redis Stack comes in. “Redis Stack allows you to power your legacy systems that may still be in place without a risky rip and replace strategy,” he continues. With Redis Stack, companies can begin their application modernization journey by delivering advanced AI/ML-powered services like fraud detection, algorithmic trading, loan approval, underwriting, middle office, back-office processes, and more.Listen to the keynote to hear more from Mike Anand on how businesses can integrate AI/Ml into their standard operations and how they can sidestep the challenge of operationalizing AI/ML.Taimur RashidChief Business Development Officer, RedisThe keynote progressed to an overview of trends in the financial services industry and specific use cases that can be addressed when applications are strengthened by AI capabilities and become more real-time. Taimur Rashid, Chief Business Development Officer at Redis notes that the key differentiator is the customer experience.“When you look at digital applications’ capabilities becoming more real-time, there’s a variety of imperatives that most modern applications have,” he notes. Chief among them are “speed, scalability, staying always-on, global distribution, security, and AI-enhanced.” Speed is the most important variable here. “In the financial services industry, speed really matters. Milliseconds can make the difference between millions, sometimes billions of dollars.”In the keynote, Taimur highlights some of the trends in financial services, like digital capabilities, the customer experience, new business growth, as well as financial crime, and cybersecurity. “As more businesses move their operations online,” he says, “there’s a need to make sure that money that’s being moved globally is being moved in the proper way and according to the right regulations. Organizations need to protect themselves from financial crime. With more activity being shifted online, cybersecurity is very important. How do you protect yourself against bad actors and threats that are outside your organization?”After a demo from Redis Sr. Product Manager AI/ML Ed Sandoval on vector similarity search in action (more on this in the following session), Taimur sat down for a discussion with Microsoft’s Managing Director for Customer Success Data and AI Group, Pascal Belaud.During this chat, Pascal breaks down how Microsoft works with different customers, like Allstate Insurance, for instance, to leverage AI/ML. Some of the uses he specifies include capturing voice calls with customers, which are then transcribed using AI techniques. These transcripts are immediately enriched with all the details Allstate needs to handle any claims without having to contact the customer a second time. It’s all part of the customer experience AI/ML-infused digital applications are working toward.Watch the full keynote.Midway through the keynote, Ed Sandoval presented a demo featuring the vector similarity search available in Redis. In this session, Ed is joined by Charles Morris, the Head of Enterprise Data Science and Financial Services at Microsoft, to present a behind-the-scenes look at how that demo was built. While Ed focuses on Redis’ vector similarity technology, Charles focuses on Microsoft Azure’s ML infrastructure and the services used to build the demo.In this demo, Ed and Charles demonstrate how AI is used to surface copious amounts of buried valuable information out of U.S. public company corporate filings submitted to the SEC. Watch and see as they both share insights from their collaboration and some of the key lessons learned along the way.Watch the behind-the-scenes demonstration.What are the tools and processes that empower data scientists? What needs to be done to push everything into production when integrating AI/ML into your operations?In this panel discussion, Taimur Rashid is joined by Mike Gualtieri, VP, Principal Analyst at Forrester Research, and Mike Del Balso, Co-Founder and CEO of Tecton, to talk about the transformational qualities of machine learning. Companies are trying to mainstream AI/ML tech, but how does it get scaled reliably? Many organizations run on legacy systems, but how do they modernize with growing needs in the digital space?“AI has become foundational,” says Mike Gualtieri. “It’s become strategic for enterprises. It’s not a science fair experiment. It’s not an innovation experiment. Most companies believe that they’re going to need AI. AI is software and those software development processes are very important to getting that model, to operationalizing it, and getting the business value on those applications.”Mike Del Balso of Tecton shared some of his experiences with ML, in particular with regard to his previous role at Uber. “Our goal with the ML platform at Uber (Michaelangelo) was to democratize ML,” he says. “To make ML possible for the 100+ use cases we identified from the beginning, where ML intelligence could really influence the product and the customer experience.”Get the full scope when you watch “Operationalizing AI/ML in the Enterprise.”What are the best ways to implement best practices design patterns to build and scale microservice apps reliably? That’s the topic of discussion in this session between Redis’ Field CTO Allen Terleto and Viren Baraiya, Co-Founder and CTO of Orkes.“The overhead and inflexibility of a monolithic application is unacceptable,” says Mike Anand before introducing the speakers. “With the shift to microservices, one of the main benefits is that each service can have its own fully decentralized data store and each component can be changed and updated quickly without impacting the entire service while reducing the blast radius if a microservice goes down.”In this session, Allen and Viren look into microservice issues, such as orchestration, state management, error handling, and observability. But as noted by Allen, growing microservices at scale can be complicated. He notes that when moving to componentized microservices, “[you have] new architectural patterns, architectural concerns, new technologies, a laundry list of pitfalls to worry about, especially as you scale to hundreds of microservices.”Uncover more insights into microservice design patterns when you watch “Microservice Patterns Made Easy.”"
84,https://redis.com/blog/redis-enterprise-software-6-2-18/,"Shipping Now: Redis Enterprise Software 6.2.18, the Most Secure Redis Software Ever","November 29, 2022",Brandon Felker and Adi Shtatfeld and Yoav Peled,"Redis Enterprise Software 6.2.18 is out! We’re delighted with this new release, which has a renewed focus on security, and we think you will be, too.Here’s an overview of the reasons we think that you’ll be as excited as we are.Security concerns naturally are everyone’s top priority. We at Redis understand those challenges, particularly in enterprise organizations, and are working diligently to enhance our products’ security to comply with regulations and industry requirements.At the top of the list for Redis Enterprise Software 6.2.18 are two new top enterprise-grade security features.In computer systems, an audit trail means the process of tracking events and activities that take place while the system is in use. According to the U.S. National Institute of Standards and Technology, “Audit trails can provide a means to help accomplish several security-related objectives, including individual accountability, reconstruction of events (actions that happen on a computer system), intrusion detection, and problem analysis.” For database systems, one key activity to audit is database connections.Auditing connections helps you track and troubleshoot connection activity: who connected to a database, when the connection was established, for how long it was active (until a disconnection event), and what authentication events were sent during that time. For instance, an e-commerce site might track a payment process’ origin and record that it began at 1:00, lasted 20 seconds, and then disconnected. If you later encounter a problem with the payment – the transaction wasn’t completed due to a network failure or a refused credit card – everyone concerned can see what happened and when.Starting with version 6.2.18, Redis Enterprise Software lets you audit database connections and authentication events.For example, one of our customers, a large European financial institution, wants to track failed login attempts to its databases. While occasional login failures are common and acceptable, repeating login failures from the same origin might indicate an attempt to hack the system. Once auditing is enabled, Redis Enterprise Software generates and publishes connection audit records, which the customer can then consume and process for detecting such cases.Once your listener consumes the data, you can store, process, and analyze it according to your needs.For more details and examples, see the audit documentation.Also, starting with version 6.2.18, Redis Enterprise Software lets you integrate private key encryption with a third-party password manager.When enabled, private key encryption encrypts private keys stored in the cluster configuration store (CCS). The encryption mechanism is managed entirely by Redis Enterprise and does not require any level of human interaction.A third-party password manager is an encrypted digital storage. The password manager stores passwords that are used to access applications or user accounts. A good password manager can also be used to generate secure passwords for applications and user accounts, which makes it an important tool in the CISO toolbag. Using a password manager removes the responsibility of remembering a password which makes it easier to use better passwords, such as longer passwords that contain random characters.Enterprise customers have internal compliance requirements to manage encryption keys using third-party password managers. To help our customers meet those internal compliance needs, we developed a method by which private key encryption can be managed using a third-party password manager.Many of our global financial customers are integrating this feature with external password managers (such as Vault and Cyberark) to externally manage the encryption key. This allows them to follow their internal guidance requiring passwords to be managed using an external secure password management system controlled by their certificate teams.Adding more security features was a top priority. But we want to draw your attention, too, to a few new enhancements that can help with daily tasks such as troubleshooting and upgrades.Memory usage is a Redis command that reports the total memory footprint of a key: MEMORY USAGE key [SAMPLES count]. It includes the memory allocation of the data and any additional memory allocations required for managing the key and its value.Starting with 6.2.18, you can use this command for keys in Active-Active databases; We mostly use it to determine which key consumes a lot of memory, and one way to do it is by running redis-cli with the --memkeys option.When investigating Active-Active databases issues, you can use the crdb-cli task list command to display the running and recent tasks for all the active-active databases in the cluster. Run crdb-cli task --task-id <task-id> to get more information about a given task. For more information, read about the crdb-cli tool.Use the API /v1/nodes/status or /v1/nodes/<uid>/status/ to get the node’s role: primary (master) or secondary (slave). This API is extremely useful when applying node maintenance or when upgrading a cluster where you start with the primary (master) node.An expired certificate can cause outages and client disconnections in the cluster. Today the cluster alerts about soon-to-be-expired certificates via the UI and email. Now you can monitor your certificates’ expiration using Redis integration with Prometheus as well.The primary goal for Redis Enterprise Software 6.2.18 was adding security features to help create a safe journey for the user and to make life easier for administrators working with our product.We invite you to download the new version for a 30-day free trial. Try it out today!"
85,https://redis.com/blog/redisdays-san-francisco-2022-overview/,RedisDays San Francisco 2022 Overview,"April 14, 2022",Udi Gotlieb,"RedisDays San Francisco was a day fully dedicated to the Redis developer community. During these sessions, our guest speakers showcased how Redis’ real-time data innovations are helping the community to build apps faster by simplifying the developer experience. With new product announcements, product updates, and step-by-step walkthroughs, RedisDays San Francisco was all about delivering the developer community the tools they need to make their app development simpler and faster. Let’s dive right in.“Real-time is what consumers and businesses now expect,” says Yiftach Shoolman, Redis Co-Founder and CTO, during his ‘Building Real-Time Data Apps Just Got (Much) Easier‘ keynote. In this opening session, he unveils our latest product – Redis Stack. “Redis Stack consolidates the capabilities of the leading Redis modules into a single product,” he says. “This makes it easier for developers to build more real-time applications with the speed and stability of Redis.”Redis Stack offers a new and complete developer experience. With Redis Stack, building Redis apps has become easier and faster than ever before. Redis Stack includes all the key Redis data models (RedisSearch, RedisJSON, RedisTimeSeries, RedisGraph, and RedisBloom) for its initial release and will enable teams to build using the RedisInsight visual tool while offering the capability to build on any language with the supported client libraries.Watch the full keynote.Later sessions in the day explored Redis Stack even further. In the ‘Buckle Up: The Wild Ride Into the New World of Redis Stack,’ Redis Technical Enablement Manager Elena Kolevska walks viewers through the server-side of Redis Stack, the breadth of the different data models it supports, and how they help in solving many real-world use cases.In her presentation, Elena demonstrates how easy it is to develop services from very different domains using the new RedisInisght visual guides for Redis Stack. She also merges her personal interests (mountain biking) with how Redis Stack functions, demonstrating how it’s possible to build a bike shop app that sells bikes, works as a community forum, catalogs personal stats, and has the capability to analyze sales data. She also explains how to rapidly build apps like full-featured recommendations engines, analytics engines, and fraud detection engines.Sign in and watch ‘Buckle Up’ now.The final session to focus on the capabilities of Redis Stack comes courtesy of Josh Long, a world expert in Java spring framework and Spring Dev Advocate at VMware, and Brian Sam-Bodden, Dev Advocate at Redis. Together, they hone in on the benefits of the client side of Redis Stack in their ‘Bootiful Spring Deep Dive Into Redis Stack’ session.Josh and Brian provide an app-building walkthrough, leveraging Redis’ object mapping library for Spring to quickly start building a fully functional app with many built-in constructs to expedite the build process.Register to follow the ‘Bootiful Spring Deep Dive Into Redis Stack’ walkthrough.During his keynote, Yiftach Shoolman, Redis Co-Founder and CTO celebrates the Redis project’s 13 year anniversary. To honor the moment, he also previews the Redis 7.0 release.Redis 7.0 reemphasizes our commitment to the open source project. Redis has nearly doubled the number of active contributors in the last four years. With Redis 7.0, we are bringing many new features that developers and DevOps teams can utilize to easily build real-time data apps.“The most exciting news of Redis 7.0 is Redis Functions,” Yiftach notes. “Redis Functions makes it easier and simpler to write code within Redis, as every function is accessible through API. Redis treats functions like it treats data and ensures your functions will sustain failure or restart events by supporting them with existing replication in data persistence mechanisms.”Redis 7.0 also introduces the concept of Shared PubSub, “for better scaling PubSub command in a clustered environment. Also included is improved security with ACLv2, better usability with Introspective ability, and a lot of internal Optimizations that act on Redis in a robust, safer manner,” he adds.Itamar Haber, Redis Technology Evangelist, Oran Agra, Redis Core Project Lead, and Madelyn Olson, Redis Core Team Member, AWS go deeper into the new capabilities offered in their ‘Redis 7.0 Unpacked’ session.Redis 7.0 showcases Redis’ commitment to the open source community with a slew of new capabilities, the highlight being Redis Functions, which opens the door to a new level of programmability options. Here, Itamar, Oran, and Madelyn’s agenda cover Redis Functions, Sharded Pub/Sub, ACL V2, Command Introspection, and Internals and Optimizations.Watch the ‘Redis 7.0 Unpacked‘ on-demand.How has the tech industry evolved in the last five years? The last two alone have accelerated the space to unprecedented levels. As Redis CMO Mike Anand states at the beginning of this session, “You have so many choices today when it comes to tools, databases, and languages to build apps.” So how does this abundance affect the developer experience?That’s the topic of discussion in ‘The Database and the Developer Experience,’ presented by Steve O’Grady, Co-Founder of RedMonk, a developer-focused industry analyst firm. RedMonk’s expertise, as Steve explains it, is to “try to understand what practitioners are using, why you’re using it, and how those trends impact the industry around us.”Watch this session to understand what changes have occurred in the database space, why developers are demanding a fully integrated experience, and what it means for database providers.Start viewing ‘The Database and the Developer Experience.’Udi Gotlieb, VP of Enterprise Product Marketing at Redis is joined by Omar Koncobo, IT Director of E-commerce and Digital Systems at Ulta Beauty for ‘Building a Digital Muscle in the World of Beauty.’ This fireside chat covers the role that open source plays in creating innovative shopping experiences at both brick and mortar and digital shopping spaces in accordance with changing customer needs and behaviors.So what role do developers play in steering a retail business towards success? In this session, Omar Koncobo discusses how he approaches the many challenges Ulta Beauty has faced in the last couple of years, the value he sees in open source software, and what the move to cloud and microservices architecture did to Ulta Beauty’s technology stack.View ‘Building a Digital Muscle in the World of Beauty‘ in its entirety."
86,https://redis.com/blog/feature-stores-for-real-time-artificial-intelligence-and-machine-learning/,"Feature Stores for Real-time AI/ML: Benchmarks, Architectures, and Case Studies","April 7, 2022",Nava Levy,"Real-time artificial intelligence / machine learning (AI/ML) use cases, such as fraud prevention and recommendation, are on the rise, and feature stores play a key role in deploying them successfully to production. According to popular open source feature store Feast, one of the most common questions users ask in their community Slack is: how scalable/performant is Feast?  This is because the most important characteristic of a feature store for real-time AI/ML is the feature serving speed from the online store to the ML model for online predictions or scoring. Successful feature stores can meet stringent latency requirements (measured in milliseconds), consistently (think p99) and at scale (up to millions of queries per second, with gigabytes to terabytes-sized datasets) while at the same time maintaining a low total cost of ownership and high accuracy.As we will see in this post, the choice of an online feature store, as well as the architecture of the feature store, play important roles in determining how performant and cost-effective it is. It’s no wonder that oftentimes companies, before choosing their online feature store, perform thorough benchmarking to see which choice of architecture or online feature store is the most performant and cost-effective. In this post, we will review architectures and benchmarks from both DIY feature stores built by companies successfully deploying real-time AI/ML use cases and open source and commercial feature stores.  Let’s begin.Let’s first have a look at benchmarking data and then the data architecture of the Feast open source feature store. Feast recently did a benchmark to compare its feature serving latency when using different online stores (Redis vs. Google Cloud DataStore vs. AWS DynamoDB). It also compared the speed of using different mechanisms for extracting the features (e.g. Java gRPC server, Python HTTP server, lambda function, etc.). You can find the full benchmark setup and its results in this blog post. The bottom line: Feast found it was by far the most performant using the Java gRPC server and with Redis as the online store.In the diagram above you can see an example of how the online mortgage company Better.com implemented its lead scoring ranking system using the open-source Feast feature store. As presented by Vitaly Sergey, Senior Software Engineer at Better.com, the features are materialized from the offline stores (S3, Snowflake, and Redshift) to the online store (Redis). In addition to that, features are also ingested into the online store from streaming sources (Kafka topics). Feast recently added support for streaming data sources (in addition to batch data sources) which is currently supported only for Redis. Supporting streaming data sources is very important for real-time AI/ML use cases as these use cases rely on fresh live data.As an example, in a lead scoring use case for Better.com, new leads are being ingested continuously throughout the day. The features come from many different sources, and both the entities (the leads) and the features used to score them get updated all the time, thus, the leads get ranked and re-ranked. As soon as there is a new lead it is ingested and scored by the model. At the same time that it’s ingested into the online store, we may want to re-rank it soon after. Better.com leads expire after 48 hours, and this is implemented in the Redis online store by simply setting time to live (TTL) to 48 hours, to expire the entity (lead) and associate feature vectors after 48 hours. So the feature store cleans itself automatically and there are no stale entities or features taking up valuable online storage.Another interesting implementation of Feast is the Microsoft Azure Feature Store. You can have a look at its architecture here. It runs on the Azure cloud optimized for low latency real-time AI/ML use cases, supporting both batch and streaming sources, as well as integration into the Azure Data & AI ecosystem. The features are ingested in the online store from both batch sources (Azure Synapse Serverless SQL, Azure Storage / ADLS) and from streaming sources (Azure Event Hub). If you are already deployed on Azure or familiar with the Azure ecosystem, then this feature store may be the right one for you. For the online store, it uses the Azure Cache for Redis, and with Enterprise Tiers of Azure Redis, it includes Active-Active Geo-Duplication to create globally distributed caches with up to 99.999% availability. In addition, further cost reductions can be achieved by using the Enterprise Flash tier to run Redis on a tiered memory architecture that uses both in-memory (DRAM) and Flash memory (NVMe or SSD) to store data.Below is a different architecture for implementing real-time AI/ML use cases. It is the feature store architecture of the popular website building platform Wix. It is used for real-time use cases such as recommendations, churn and premium predictions, ranking, and spam classifiers. Wix serves over 200M registered users of which only a small fraction are ‘active users’ at any given time. This had a big influence on the way the feature store was implemented. Let’s take a look at it.The information below is based on a TechTalk presentation that was delivered by Ran Romano when he headed ML Engineering at Wix. Over 90% of the data stored in the Wix feature store are clickstreams and the ML models are triggered per website or per user. Ran explains that for real-time use cases, latency is a big issue. Also, for some of their production use cases, they need to extract the feature vectors within milliseconds.The raw data is stored in Parquet files on AWS in an S3 bucket, and is partitioned by business units (e.g. ‘editors’, ‘restaurants’, ‘bookings,’ etc.)  and then by date. It is part of the Wix data platform used by its data analysts which predated the Wix ML Platform by years. In a daily build batch process using Spark, SQL (which takes minutes to hours) all the users’ history features are extracted from S3, pivoted and aggregated by the user, and ingested into the offline store (Apache Hbase). This provides a much faster, by ‘user’, lookup of its users’ history. Once the system detects that a user is currently active, a ‘warmup’ process is triggered and the features of that user are loaded into the online store (Redis) which is much smaller than the offline store since it holds only the user history of the active users. This ‘warmup’ process can take several seconds. And finally, features in the online feature store are continuously updated using fresh live real-time data directly from the streaming sources per each event coming from the user (using Apache Storm).This type of architecture has a lower ratio of writes to reads compared to the Feast architecture,  which is very efficient in terms of materialization and online storage since it only stores features for active users in the online store rather than those for all users. Because active users make up a small fraction of all registered users within Wix, this represents a huge saving. However, this comes at a price. While retrieving features from the online store is very fast, within milliseconds, it’s only if the features already exist within the online store. Due to race conditions, because the warmup process takes several seconds, it won’t be fast enough to load the relevant features as the user becomes active. So, the scoring or prediction for that user will simply fail. This is OK as long as the use case is not part of the critical flow or for mission-critical use cases such as approving transactions or preventing fraud. This type of architecture is also very specific to Wix, in which only a small fraction of users are active users at any given time.Let’s look now at the architecture of the commercial enterprise feature store Tecton. As we can see in the diagram below, in addition to batch data sources and streaming data sources, Tecton also supports ‘out-of-the-box’ real-time data sources. These are also called ‘real-time features’ or ‘real-time’ transformations. Real-time features can be calculated only at the inference request. For example the difference between the size of the suspected transaction and the last transaction size. So, in the case of Better.com with open source Feast above, Better.com developed on its own the support for real-time features. With the Tecton feature store, this is easier to implement as it’s already natively supported by the feature store. Like with Feast and the Wix feature stores, Tecton also defines the features in the registry so that the logical definition is defined once for both offline and online stores. This significantly reduces training-serving skew to ensure high accuracy of the ML model also in production.With respect to the choice of offline store, online store, and benchmarking, for the offline feature store Tecton supports S3, for the online store Tecton now offers its customers a choice between DynamoDB and Redis Enterprise Cloud. In a recent presentation, Tecton CTO Kevin Stumpf provided tips on how to choose your online feature store, based on benchmarks the company recently performed. In addition to benchmarking latency and throughput, Tecton also benchmarked the cost of the online store. Why is this important? For high throughput or low latency use cases, the cost of the online store can be a large and a significant portion of the total cost of ownership of the whole MLOps platform, so any cost savings can be substantial.The bottom line of Tecton’s benchmarking is that for high throughput use cases typical for Tecton’s users, Redis Enterprise was 3x faster and at the same time 14x less expensive compared to DynamoDB.So what’s the catch you may ask? If you have only one use case, and it doesn’t have high or consistent throughput, and it doesn’t have any strict latency requirements, then you could go with DynamoDB. You can find the full details and the results of Tecton’s benchmarks here.Below is another example of a feature store architecture. This one is used by Lightricks, based on the commercial feature store Qwak. Lightricks is a unicorn company that develops video and image editing mobile apps, known particularly for its selfie-editing app, Facetune. It uses the feature store for its recommendation system.As shown in the diagram above, like Tecton, the Qwak feature store supports three types of features sources – batch, streaming, and real-time features.It is important to note that with the Qwak feature store, the materialization of features into the feature store is done directly from the raw data sources for both the offline store (using Parquet files on S3) and the online store (using Redis). This is different compared to the feature stores examples from Wix, Feast, or Tecton in which the materialization of the online store is done from the offline store to the online store for the batch sources. This has the advantage that not only the transformation logic of a feature is unified across training and serving flows (as with the feature stores of Feast, Wix, and Tecton above), but also the actual transformation or feature computation is done uniformly, further decreasing training-serving skew. Having a unified data pipeline for offline and online directly from raw data has the potential to ensure even higher accuracy during production. You can find more information on Qwak’s feature store architecture and components in this presentation.In this post, we reviewed key highlights of benchmarks and architectures of several feature stores for real-time AI/ML. The first one is Open Source Feast, the second DIY Wix feature store, the third from Tecton, and the fourth by Qwak. We also reviewed the highlights of some of the benchmarks these companies performed to see which online store is the most performant and the most cost-effective. We also explored which mechanism or feature server to use for extracting the features from the online store. We saw that there are significant differences in the performance and cost of feature stores, depending on the architecture, supported type of features, and components selected.Originally published in KDnuggets."
87,https://redis.com/blog/how-optimizing-the-data-layer-can-help-retailers-stay-competitive/,How Optimizing the Data Layer Can Help Retailers Stay Competitive,"December 4, 2020",Redis,"This blog post was adapted from our white paper Driving Retail Transformation with Redis Enterprise. Download it for free now!Around 10,000 U.S. retail stores closed in 2019, victims of a shift to online shopping that has accelerated in recent years. And due in part to the COVID-19 crisis, another 25,000 are predicted to shutter in 2020. Many retailers and chains of all sizes are still unprepared for these sea changes, and could face bankruptcy. Those that try to compete online face a challenging marketplace with online-first retailers establishing big leads in critical areas like speed of selection, a digitally enabled supply chain, broad product catalogs, website availability, and customer experience and personalization.Online retailers are simplifying the shopping experience for their customers and building loyalty. As more customers shop online, foot traffic decreases and even more stores close, which drives frustrated customers online, and so on. But this vicious cycle can be broken. Brick-and-mortar retailers can compete by leveraging the right technology both online and in-store (where many customers still prefer to shop). For many retailers, the best way to implement this kind of omnichannel strategy is by embracing the cloud.Ironically, many large physical retailers have been offering online shopping for so long that they are now being held back by their legacy in-house technology. Many of their online stores were built 20 years ago with now-aging technology and infrastructure that is more costly to manage and can throttle innovation compared to more modern digital-first competitors.Increasingly, the answer is to build real-time applications in the cloud. Cloud-native applications are faster and more responsive than ever, which is essential for today’s consumers, who expect everything from catalog searches to customer-service interactions to happen quickly and efficiently. Just as important, the cloud is more scalable, and its pay-as-you-go pricing models can also cut costs. Why pay year-round for infrastructure capacity that you need only on Black Friday?Beyond cloud adoption, there are many other steps retailers can take to modernize their IT infrastructures and stand out from the crowd. Download our Driving Retail Transformation with Redis Enterprise whitepaper to learn more about why you need to speed transactions to deliver a great customer experience, optimize the digital supply chain, and perfect an omnichannel approach to retailing. And you’ll also learn and how Redis Enterprise is a key component of this approach—its fast performance, low latency, high availability, linear scalability, and multiple data-persistence options help combine the power of the cloud with in-store shopping and fulfillment to flip the script on online-only retailers and turn physical locations into assets instead of liabilities."
88,https://redis.com/blog/leaderboards-ranked/,Leaderboards Aren’t Just for Games: A Pep-talk To Inspire Developers,"November 16, 2022",Carol Pinchefsky,"You undoubtedly are used to computer game leaderboards, where a visual display tracks player scores. But applications can use leaderboards for non-gaming purposes, too. They measure progress, status, and other competitive metrics, and they often encourage participation by using gamification. This short guide helps you brainstorm how you might incorporate these common leaderboard types in your own applications.If you like games of any kind (board, video, sports, Squid), you understand how often competition can challenge you to be your best and to crush your enemies. Gamification takes the competitive urge up a notch by incorporating gaming aspects into everyday tasks. You’re not just telling the world that you know the murderer was Professor Plum in the library with the candlestick. You’re telling Brad from Sales that you intend to claim his Employee of the Month title.Gamification is attractive to user experience designers because games are effective drivers of behavior at the early stages of habit forming. “By nature, games are whimsical and playful,” explained Stephanie Cheng, who designed a mobile app for a political campaign a few years ago. Making an experience more fun encourages people to engage with the activity. “Game mechanics provide clear incremental guidance and rewards for nudging behavior.”In gamification projects, leaderboards usually are represented by a chart that ranks participants in a comparative endeavor. The leaderboard displays relative achievement or other comparisons, with up-to-the-moment timeliness. The visual component makes information easy to understand, allowing people to compete against their peers (and sometimes themselves). The winner is displayed at the top of the chart.Leaderboards are more than names and numbers on a page, according to Austin Rolling, founder, and CEO of gaming CRM Outfield, which specializes in sales gamification. “There’s an aspect of game theory and psychology,” he says. Leaderboards help you make decisions in response to external stimuli, that is, your competition. In other words, a leaderboard “makes you want to become more competitive.”Our emphasis in this blog post is on leaderboards for business applications, but it’s probably easiest to grasp the concept from current entertainment. There are leaderboards in the Disney+ Star Wars series Andor. When the anti-hero Cassian is imprisoned, he soon learns that manual labor is competitive. His shift manager keeps a watchful eye on the leaderboard’s highs and lows. The slowest team in the room receives painful shocks to their bare feet. The winners receive flavoring with their food. This is possibly a better outcome than your own sales meetings.Some leaderboards are more useful than others. If you are contemplating incorporating one into the software you’re creating, you might as well design one that blows all others out of the water. To help you achieve that goal, here are a few examples of leaderboards that may inspire your creativity – beyond game leaderboards.  And because they’re leaderboards, we rank them in terms of usefulness too. Now, all we need is to write more of these articles, so we can rank them as well. Rank-ception!Educational systems like Duolingo, a popular app for teaching human languages, let you learn at your own pace. To help move you along, Duolingo ranks users based on the number of lessons they completed and students’ success at learning language skills. Leading the leaderboards gives you gems and lingots–currency used to purchase digital items from Duolingo’s shop–as well as a je ne sais quoi that you can define now that you know French.Leaderboard design takeaway: Consider the purpose and the user persona when you design a leaderboard. To make learning less stressful, educational leaderboards tend to be colorful and friendly. At first glance, they say, “I’m at the top, and I’m happy to be here.” Unlike sales leaderboards, they don’t say, “I’m at the top, and I’m happy to be here because I’ve beat my competitors.”Billboard’s famous leaderboard sorts the best-selling music singles and albums of the week from 200 territories worldwide by sales figures. The music that’s tracked includes country, dance/electronica, soundtrack, and more. Each place on the leaderboard represents thousands of sales or more. You can dance to those songs all the way to the bank.Leaderboard design takeaway: Comparing one genre of music to another is akin to comparing apples to diamonds. They’re both made of the same building blocks (carbon/musical notes), but that’s about it. But Billboard’s leaderboards sort music within the same genre. This means a pop song’s sales are never measured against a genre like Deathgrass, for instance. When designing a leaderboard, keep in mind you may have to design more than one.Another thing to keep in mind: leaderboards are meant to be broken.You may not think of it this way, but you see a leaderboard every time you visit a train station or airport. Your ticket to ride is ranked by departure times. If your conveyance is late, it remains atop the board with a “Delayed” notice next to it–or worse, pushed down to the bottom of the list.You may think being stuck at a terminal is like Hell, but you would be wrong. It’s more like being in Purgatory because you’re continually watching other people leave for a better place.Leaderboard design takeaway: You don’t need a lot of database fields to make a useful leaderboard. Here, the only metrics used are location, departure time, flight number, gate number, and gate status (boarding, departing on time, etc.). It’s a rather non-competitive leaderboard in that everyone who leaves on time wins.Financial systems have leaderboards that track stock market fluctuations and subsequent wealth.For most of us, these financial leaderboards exist as a “what if” exercise, where we can imagine a life where money is no object, and television shows may feature Lifestyles of the Rich and [Insert Name Here]. But for the several dozen billionaires who occupy the Fortune 500 leaderboard, this comes with bragging rights over other billionaires, where the winners get to look down on Warren Buffet.Leaderboard design takeaway:  Consider interactivity. Forbes’s leaderboard lets you watch, in real-time, as some of the world’s wealthiest people passively earn yet another superyacht filled to the brim with Bugattis. “In real-time” is a key element, because the data has to be up-to-date or it’s useless; that adds performance requirements to your development tools shopping list.In order to get this info, leaderboards rely on APIs to pull information from multiple, disparate sources. Consider sharpening your API skills. Those skills may not land you on the Forbes list, but they may get you on the Company Promotions leaderboard.A good leaderboard provides statistics, such as how many points you make when you hit a stick with a ball and people wearing the same clothes as you run around a diamond circle. In some cases, teams or individual athletes are stacked against people in other countries, so when you’re not at the top, this humiliation is shared with the entire world. Isn’t sports fun?Leaderboard design takeaway: Sabermetrics quantifies every aspect of a sport (most popularly baseball, but with inroads made in U.S. football and UK football), from the amount of time spent touching the puck in hockey to a pitcher’s winning percentage. Any action a player takes during gameplay is measured and calculated. Thanks to sabermetrics, you always have a wealth of stats to populate a leaderboard. In other words, when it comes to statistics, think inside the box. Or, in this case, the whole stadium.Review sites such as Consumer Reports allow you to sort items – say, washing machines – and rank them by Consumer Reports exacting standards. Other sites, such as the movie-centric IMDB, allow users to chime in. After people rate a film or TV show, IMDB ranks the movie by average score. It isn’t a static list; IMDB lets users toggle metrics, including release date and the number of reviews.Leaderboard design takeaway: IMDB isn’t a good leaderboard: It’s two good leaderboards. One focuses on reviews, and the other specializes in box office take. The leaderboard display of box office take is further broken down by location, year, genre, brands, franchises, and the fabulous showdown.If you have a great deal of information to present and can’t find a way to integrate it onto one page, consider other ways to present the information. That might be more than one leaderboard or perhaps a drop-down menu that lets users sort by more than one metric.Salespeople often have a base salary that is beefed up by a bonus with each sale and additional benefits accruing from a top spot in the quarterly sales leaderboard. Salespeople are competitive by nature, so their tools often include points to measure achievements and give rewards. A sales leaderboard can showcase metrics in a rank, card, multi-metric, or most-improved fashion to provide visibility into individual and team performance.Leaderboard design takeaway: Sales leaderboards track many metrics, such as the region with the most sales, which campaign is getting the most leads, and how much revenue they rake in. Because of this, some leaderboards keep track of these many factors. In one column, Alice may top a revenue column, while Bob has the most deals made, and Charlie made the most calls. That way, everyone wins. Except David.Salespeople also need motivation, which brings up another element of a gamification design: how users are rewarded. In Cheng’s aforementioned example of a mobile app for a political campaign, the leaderboard captures the measurement of progress that results in a useful reward. Users who completed in-app challenges earned stars; enough of them and you could get a signed message from the political candidate.Some leaderboards are less about competing with other companies or individuals and more about one’s own goals. Someone with a weight loss goal is encouraged by a leaderboard that shows the user their progress. It’s gamification at its finest, motivating someone to engage in a worthwhile activity. And nobody else needs to see the results.If you’ve ever been uncomfortable with working out in public or even working out at all, exercise leaderboards are like being 12 years old in gym class all over again. Crossfit gyms, Peleton, iFit, and many exercise apps rank their participants. Who pedals or runs the fastest? Who burns the most calories? Who spends the most time running a particular segment? Exercise leaderboards don’t pit you against the creme de la creme of athletes. They can sometimes pit you against the sweaty guy right next to you. You, your fellow athletes, and in some cases, the entire internet can see your score.Leaderboard design takeaway:  Contemplate how you encourage competition: With gamified exercise, you, your fellow athletes, and in some cases, the entire internet can see your score. That’s the point of Crossfit, which has some of the most competitive amateur athletes this side of the Olympics.However, not every leaderboard needs to be public. Privacy and permissions are an important part of leaderboard design.Contrast that to video games – the leaderboards that most people associate with the term. While privacy is a consideration for some business and personal applications, that isn’t a default state. Gamers want bragging rights, even if it’s under their nom de guerres. Gamers want the world to know just who beat them, and they want to tell their spouses, “Hey, I’m not goofing around. I’m practicing!”This may not be an exhaustive list of every type of leaderboard application. But they should give you enough fodder to brainstorm one that meets your application needs.Interested in incorporating a leaderboard into your next application? Consider how Redis can help."
89,https://redis.com/blog/introducing-redis-stack-6-2-6-and-7-0-6/,New Redis Stack Is Stuffed With Dozens of New Features,"December 7, 2022",Pieter Cailliau,"Whether you are a long-time Redis developer or you are just getting started, Redis Stack represents the latest innovations we have to offer.Today Redis is happy to announce the availability of the latest version of Redis Stack.Redis Stack extends Redis Open Source (OSS) with data processing engines (such as query and search) and modern data models (such as document JSON, time series, and graph). The combination empowers developers to build real-time applications that are joyful to develop as well as easy to maintain and support.This latest edition of Redis Stack provides a number of compelling features that developers have asked for. Here are the highlights:All of these Redis Stack capabilities are also available on Redis Enterprise Cloud and Redis Enterprise Software.Need more information? Here’s some additional details about the enhancements.Redis Stack allows you to index, query, and perform full-text searches on data residing in Redis hashes or in nested JSON documents. This latest release adds several frequently requested improvements that let developers use infix and suffix wildcards in search queries, such as  `*vatore` and `ant?rez`.Developers can create multi-value indexing and query attributes irrespective of type (TEXT, TAG, NUMERIC, GEO, and VECTOR), defined by a JSONPath leading to an array or to multiple scalar values. Previously, you could only index scalar attributes.For  example, imagine the following document representing a bicycle:Now, with a single JSONPath `$..color`, you can index all color fields in the document, irrespective of their location in the document. (See the RedisSearch release notes.)We also updated the JSONPath parser with improved performance and bug fixes.Redis Stack contains a set of useful probabilistic data structures. Probabilistic data structures allow developers to control the accuracy of returned results whilst gaining performance and reducing memory. These data structures are ideal for analyzing streaming data and large datasets.Within this latest release, Redis Stack now includes a new probabilistic data structure, t-digest, for estimating quantiles based on a data stream or a large dataset of floating-point values. You can use it to answer questions such as:Redis Stack also has dedicated data structures for time series data and highly connected data (graph), each with its own powerful query engine.Many users are using our graph capabilities for resource, identity, and access management use cases. Modeling your data as a graph can help you ensure that the right entities have the right access to the right resources at the right time, where both entities and resources can be members of hierarchical groups.This latest Redis Stack release enhances that experience further. We added new path algorithms for finding minimal-weight, optionally bounded by cost or length paths between a given pair of nodes or starting at a given node. We also added language constructs for adding and removing node labels, changing node properties and edge properties, and deleting paths. We have 29 new functions covering type conversion, trigonometric, and logarithmic functions. Lastly, all write queries are now executed atomically. (Atomicity is the guarantee that each query either succeeds or fails with no side effects. Whenever a failure occurs, the query effect is rolled back from an undo log.) (See the graph release notes for more details.)In terms of the advancement of our time series capabilities, we added the ability to retrieve aggregation results for the latest (still open) bucket of compactions. We introduced a new aggregator: time-weighted average, for estimating more accurately average-over-time when samples of a continuous signal are available at non-constant intervals. A new gap-filling capability allows interpolating or repeating the last value for empty buckets. In addition, you can now control how bucket timestamps are reported (the bucket’s start time, mid-time, or end-time) as well as controls for alignment for compaction rules. A set of new reducers is introduced as well. (See the time series release notes.)RedisInsight is the ideal developer companion for Redis and Redis Stack. It allows you to visualize and optimize Redis data. RedisInsight also helps you learn about our latest innovations as they are introduced, using guided tutorials.The latest RedisInsight enhances diagnostic capabilities. A database analysis feature helps you optimize performance and memory usage. It shows data type distribution, memory allocation, and top keys and namespaces, sorted by consumed memory or key length and count of keys, respectively. The new slow log tool makes it easier to troubleshoot performance issues. You can also validate your data with the help of newly added formatters (JSON, HEX, MessagePack, ASCII, and so on).The latest release of Redis Stack is available in two versions: Redis Stack 6.2.6, built on top of Redis 6.2; and Redis Stack 7.0.6, a release candidate, built on top of Redis 7.0.You can choose one of the following ways to install and get started with Redis Stack:The new Redis Stack features are also available today on Redis Enterprise Cloud and Redis Enterprise Software.Once you have Redis Stack Server up and running, you can instantly leverage RedisInsight to visualize, analyze, and optimize your Redis data.Choose the client library in the language of your choice, and consult our tutorials for .NET, Node.js, Python, and Spring to get started coding. For a deep dive into the query and search capabilities of Redis Stack, enroll in our free course, RU204: Storing, Indexing, and Querying JSON at Speed.In the next Redis Stack release, we plan to introduce additional mechanisms for triggered functions. Specifically, we plan to include an embedded JavaScript engine. Functions written in JavaScript can be triggered by an event arriving in a stream, synchronously with a Create/Update/Delete operation, or can be scheduled at a given time. This will allow you to execute business logic within the database in combination with Redis Stack capabilities such as query and search. By manipulating data server-side, triggered functions can significantly lower the latency of updates and reduce stale data.The next Redis Stack release is planned for Spring 2023, but you can preview its programmability capabilities now. In our Expanding the Database Trigger Features in Redis blog post, we show how to write functions that react to data changes in Redis.Note: To offer the Redis community more freedom and clarity, Redis Stack is now provided under a new dual license: a new version of our Redis Source Available License (RSALv2) and the Server Side Public License (SSPLv1)."
90,https://redis.com/blog/top-blog-posts-2022/,The 5 Most Popular Redis Stories of 2022,"December 20, 2022",Esther Schindler,"As 2022 draws to a close, we take a look back at the Redis blog posts that resonated with our readers this year – judged by how often they were read, shared, and discussed. The results reflect the company’s progress. Look how much we accomplished!It’s far more fun to present this as a countdown, isn’t it? So let’s go.In March 2022, we announced Redis Stack, which consolidates the most-used Redis modules into a single product. Naturally, the announcement blog post got a lot of attention among Redis users, and it attracted quite a few new ones.In late December, we announced the latest version of Redis Stack, which is chock full of even more features.In this retrospective, Redis top technologists Yiftach Shoolman, Yossi Gottlieb, and Filipe Oliveira contemplate Redis’s architectural underpinnings, assess its performance (with attention to competitors), and discuss its design philosophy. “For the foreseeable future, we will not abandon the basic principle of a shared-nothing, multi-process architecture that Redis provides,” they conclude. “This design provides the best performance, scaling, and resiliency while supporting the variety of deployment architectures required by an in-memory, real-time data platform.”In November, Redis joined forces with the creator of RESP.app, Igor Malinovskiy, and announced that we’d bring RESP.app’s features into RedisInsight. That attracted a lot of attention from the open-source community, which is interested in the tool’s future. (And that future is rosy, in case you couldn’t guess.)Not everything is a product release or a company announcement. Most Redis users merely want to get their work done… and this super-popular blog post demonstrates it.Developers love using the command line interface (CLI) because it’s the fastest way to get things done. As this post explains, Redis CLI is an invaluable tool. “If you didn’t have a Redis command line interface, understanding Redis’ data structures and testing connections would be far more complicated,” it explains, before offering plenty of “follow the bouncing ball” instructions and invoking Atwood’s Law.Our most popular blog post is supremely practical: a guide to help you install Redis on Windows, including instructions for ideal configuration. That includes Python installation, the Redis client library, and the challenges to expect from running Redis in a Windows environment.This list suggests that we post only about Redis news and products. Oh, goodness, no! Here are a few of the Editor’s Choices that may not have generated the most pageviews but certainly made us smile."
91,https://redis.com/blog/design-pattern-mean-mern-stack-performance/,Speed Up MEAN and MERN Stack Applications With This Effective Design Pattern,"December 21, 2022",Prasan Rajpurohit,"If you don’t design and build software with attention to performance, your applications can encounter significant bottlenecks when they go into production.Over time, the development community has learned common techniques that work as reliable design patterns to solve well-understood problems, including application performance.So what are design patterns? They are recommended practices to solve recurring design problems in software systems. A design pattern has four parts: a name, a problem description (a particular set of conditions to which the pattern applies), a solution (the best general strategy for resolving the problem), and a set of consequences.Two development stacks that have become popular ways to build Node.js applications are the MEAN stack and the MERN stack. The MEAN stack is made up of the MongoDB database, the Express and Angular.js frameworks, and Node.js. It is a pure JavaScript stack that helps developers create every part of a website or application. In contrast, the MERN stack is made up of MongoDB, the Express and ReactJS frameworks, and Node.js.Both stacks work well, which accounts for their popularity. But it doesn’t mean the software generated runs as fast as it can—or as fast as it needs to.In this post, we share one popular design pattern that developers use with Redis to improve application performance with MEAN and MERN stack applications: the master data-lookup pattern​. We explain the pattern in detail and accompany it with an overview, typical use cases, and a code example. Our intent is to help you understand when and how to use this particular pattern in your own software development.For the purposes of this post, our demonstration application showcases a movie application that uses basic create, read, update, and delete (CRUD) operations.The movie application dashboard contains a search section at the top and a list of movie cards in the middle. The floating plus icon displays a pop-up when the user selects it, permitting the user to enter new movie details. The search section has a text search bar and a toggle link between text search and basic (that is, form-based) search. Each movie card has edit and delete icons, which are displayed when a mouse hovers over the card.This tutorial uses a GitHub sample demo that was built using the following tools:You can follow along with the details using the code on GitHub.One ongoing developer challenge is to (swiftly) create, read, update, and (possibly) delete data that lives long, changes infrequently, and is regularly referenced by other data, directly or indirectly. That’s a working definition of master data, especially when it also represents the organization’s core data that is considered essential for its operations.Master data generally changes infrequently. Country lists, genres, and movie languages usually stay the same. That presents an opportunity to speed things up. You can address access and manipulation operations so that data consistency is preserved and data access happens quickly.From a developer’s point of view, master data lookup refers to the process by which master data is accessed in business transactions, in application setup, and any other way that software retrieves the information. Examples of master data lookup include fetching data for user interface (UI) elements (such as drop-down dialogs, select values, multi-language labels), fetching constants, user access control, theme, and other product configuration. And you can do that even when you rely primarily on MongoDB as a persistent data store.Consider this pattern when you need toThe image below illustrates a standard way to showcase a UI that is suitable for master data lookups. The developer responsible for this application would treat certain fields as master data, including movie language, country, genre, and ratings, because they are required for common application transactions.Consider the pop-up dialog that appears when a user who wants to add a new movie clicks the movie application plus the icon. The pop-up includes drop-down menus for both country and language. In this demonstration, Redis loads the values.The two code blocks below display a fetch query of master data from both MongoDB and Redis that loads the country and language drop-down values.Previously, if the application used MongoDB, it searched the static database to retrieve the movie’s country and language values. That can be time-consuming if it’s read from persistent storage—and is inefficient if the information is static.Instead, the “after” views in the code blocks (on right) show that the master data can be accessed with only a few lines of code—and much faster response times.The master data-lookup pattern​ is not the only design pattern you can use to improve application performance.To learn about other effective techniques when using Redis for performance improvement, including the “cache-aside” design pattern and the “write-behind” design pattern, download our ebook, Three Design Patterns to Speed Up MEAN and MERN Stack Applications."
92,https://redis.com/blog/database-trigger-features/,Expanding the Database Trigger Features in Redis,"November 9, 2022",Thomas Caudron,"Among the features of RedisGears 2.0 is the V8 JavaScript Engine. You can experiment with it in the RedisGears 2.0. Here’s what to expect.Redis is famous for its ease of use, its low latency – and the company’s dedication to improving product features and code quality. For example, we added scripting and functions so programmers could execute business logic within a database, with triggers called by an external service or user. Now it’s time to add more.With triggered functions in RedisGears, you can register business logic that reacts to Redis keyspace notifications. Triggered functions are similar to stored procedures that run when specific actions occur, such as when changes are made to data. An event, such as adding a key-value, fires the trigger’s execution. The difference between a triggered function and a user-defined function is that a trigger is automatically invoked when a triggering event occurs.RedisGears brings the trigger and execution of your business logic into your database, so it does not have to be duplicated across multiple services and applications. The functions are executed synchronously and without latency.For example, when an order is placed in an e-commerce application, RedisGears can react to the changed data and update the number of available items in inventory. Or when a database receives data from multiple applications, a developer can embed the transformation logic in RedisGears to normalize the data. That ensures all those feeder applications apply the exact same logic. Consistency is a good thing, especially for code maintenance.With the second milestone release of RedisGears 2.0, we are introducing support for the V8 JavaScript engine. V8 is Google’s open source high-performance JavaScript and WebAssembly engine, written in C++. This JavaScript Engine, once generally available (GA), will be available in Redis Cloud and will become part of Redis Stack. It will make triggered functions available by default for Redis Cloud instances.V8 enables you to use Redis to run code where your data is without network overhead using JavaScript. In the coming weeks, we will publish several blog posts in which we introduce RedisGears’s new capabilities and concepts as we work towards RedisGears 2.0’s general availability.The initial milestone release focuses on functions that are executed on database triggers so that your code can react when Redis fires an event. The most common events that are apt to trigger a function are Redis keyspace notifications. But its usefulness goes far beyond that; most advanced data structures in Redis Stack also publish their events.Two special events are not always triggered by a command:To deploy a function, the developer provides a single JavaScript file. This is easy enough for small projects, but it gets harder to maintain as projects grow and become more complex. Happily, the JavaScript community has excellent deployment tools, as explained in a tutorial on how to set up webpack for RedisGears2.0.Make sure you use JavaScript libraries that are compatible with the V8 engine. Browser and nodejs-specific global objects are not available in the V8 engine.We start with the prototypical Hello World example. We use a simple script with the shebang (#!) that defines the library name and a function where we define the logic.Now, load that data into Redis:And execute it with the library and method name:We need to register a notification consumer with Redis before the software can react with a function on a database event.The consumer expects three arguments:The callback function can include two arguments:The key_raw provides an ArrayBuffer for the key that triggered the event, when the key_raw can be decoded to a string the key is filled in. The event type is returned in event_name.The full signature looks like this:To run a function on all changes, we include an empty string in the keyprefix.In this example, we make sure that the event was fired for a hash data type. If not, we stop the function. If it is a hash data type, we use the hincrby command to create or increment the value of the version property on the hash.This shows that we can retrieve information from the key and command that was executed to trigger the function and execute Redis commands within RedisGears functions.Redis v7.0.3 or above is required to start using RedisGears2.0. A Docker image is available, so you can begin using it immediately.If you encounter an issue or potential improvements in the milestone release, please get in touch via the RedisGears GitHub repository. We want to hear about your experiences!Experience Redis Enterprise Cloud for free."
93,https://redis.com/blog/future-proof-your-dev-career/,9 Ways to Future Proof Your Software Developer Career,"January 11, 2023",David Gaule,"You need to keep your tech skills current. But a productive career depends on several additional practices.Life is always uncertain, but sometimes it’s more uncertain than usual. That’s definitely the situation for tech industry employees right now, some of whom are nervous about their future. The good news is that the job market will continue to favor software developers in 2023, according to Forrester. The better news? You can make yourself more valuable to your company and (if that fails) to other employers.Looking for practical advice? Consider these nine concrete ways to future-proof your software development career and add in-demand skills to your resume. These practices help you stand out to your boss—both your current boss and your next one—as someone worth keeping around.Our list goes beyond technical skills, although we share important ones to add to your tool belt. We also offer advice on long-term career survival, such as soft skills that go beyond programming.Let’s get the obvious advice out of the way: Don’t let your tech skills go stale.Pay at least some attention to market demands so that you’re ready to tackle new projects. If you need inspiration for the current hot tech to learn, look in job postings for the sort of work you do or that you want to do next. If lots of those employers are looking for Kubernetes experience, it’d behoove you to learn Kubernetes.One straightforward way to grow your skills as a software developer is to add another language to your tool belt. As for which one, that depends greatly on your current company as well as your career aspirations. Language popularity changes over time, and so do pay rates based on expertise in each one, but the rule of thumb is “Never stop learning.”The web is swimming with advice about the most important languages to master, and Python, Java, JavaScript, and C and C++ routinely are at the top of those lists. According to a recent StackOverflow developer survey, the three “most loved” languages right now are Rust, Elixir, and Clojure, with salaries reflecting that popularity. But whichever new language you decide to pursue, it’s a good idea to know both a frontend and backend language, advises Lee Atchison, cloud expert and author of Caching at Scale with Redis.So how to get started? While computer books are still a thing (check out O’Reilly), particularly when there’s one topic you want to learn about in-depth, online courses help you learn languages at your own pace. Good places to start include Udemy (courses run about $100 or less), Coursera (many courses are free with the option to buy a certificate), and Codecademy (free access to basic courses with monthly subscriptions for more advanced study). LinkedIn Learning also offers courses on a wide variety of technical skills.While it’s important to stay current, strengthen your expertise in underlying computer science concepts and software development processes. Some skills never become obsolete. Among them: admirable programming habits, design patterns, software testing, debugging, agile development practices, and data structures.These essentials aren’t all tech knowledge, per se. Learn to recognize when enough is enough when you’re facing YAGNI and when you’ve gotten into the realms of diminishing returns (such as spending months optimizing performance on a rarely used application function). This helps you build a reputation for quality and reliability. Someone who produces clean work that doesn’t require re-work is always in demand.Languages and development environments change. However, with a firm technical foundation, you can confidently tell a would-be employer, “I’ve learned dozens of languages in my career. Adding another one to my skill set is no big deal.”Plenty of developers think about their code, whether that means skill with their favored programming language or improving application speed. But software runs in a larger system, whether that’s your own company’s deployment process or on the internet in general. Your reputation is affected by your understanding of the world outside the applications you create and the people who are responsible for those systems.For example, it’s a good idea to get more familiar with DevOps. While most developers know what DevOps is, true expertise builds alliances with people in the operations department—and also makes your resume stand out.To pick up these skills, here again, LinkedIn Learning offers DevOps courses on everything from DevOps foundations to more advanced topics. But you don’t need a dedicated class. Begin reading reputable online sources, such as the DevOps Institute, DevOps.com, and InfoWorld. There’s even a best-selling DevOps novel (yes, really): Gene Kim’s The Phoenix Project.Similarly, learn more about cloud-native architectures. Atchison urges developers to learn as much as they can about cloud-native technologies such as microservices, containers, and Kubernetes. Udemy offers a great, free Basics of Microservices course. Find similar courses and tutorials on containers and Kubernetes—from introductory 101 courses to advanced concepts—on Udemy, Coursera, LinkedIn Learning, and elsewhere.It’s important to understand how the work you do helps the business achieve its goals. Many developers don’t consider the importance of learning more about the business beyond the cubicle walls of the engineering department. As a longtime software developer, Jim Mischel points out, software developers write programs to solve business problems, but almost anyone can write programs: “If you want to stand out, advance, you have to understand the businesses we serve and the problems they face.”Developers are hired to create business value, not to program things. Find ways to solve problems regularly, and you won’t ever have to worry about your career.Position yourself as part of a bigger operation. Learn what managers and stakeholders care about, and do your best to serve them. Being good at working with domain and business experts helps you become more valuable to them and vice-versa.To get started, set up informational meetings with leaders in other departments—sales, marketing, finance, and so on. Ask them for their thoughts on the business, what they think the biggest challenges and opportunities are, and how the business can best serve its customers. Far from being a bother, most people are happy to impart their knowledge; also, everyone wants an opportunity to tell their story. And you just might make some new friends in high places who can help you in the future!Tech certifications rarely make a difference to other developers, who prefer to look at your code than at a test score. But they sometimes make a difference to HR departments and to the salary a company offers you. Certifications and related training also can be self-affirming and reduce imposter syndrome because the external confirmation assures you that you do meet a baseline of domain knowledge.Currently, one way to increase your value to current and future employers is to get a basic cloud certification, recommends Atchison, probably an AWS certification or Google Cloud cert. Both providers offer a general foundational cloud certification, but you can pursue a number of advanced certification levels, such as cloud architect, solutions architect, and cloud DevOps engineer. The AWS exam, which costs $100, consists of 65 multiple-choice questions that you must answer within 90 minutes. Cloud Academy offers quizzes and hands-on labs to help you prepare for the exam. Coursera offers good advice on the best DevOps certifications to consider.Learn to work with others in a team environment. You develop code, but you work with people. While traits such as empathy are built-in for some people, everyone can learn and improve their abilities in time management, conflict resolution, and active listening. In a world of increasing automation for business tasks, having a skill that can’t be easily automated, such as effective problem-solving, ensures that you remain valuable.Personal networking is a crucial soft skill. Learn how to network effectively, even if you’re shy. Nurture your connections and friends as if they matter. Because they do.Don’t burn bridges. You never know when someone you dislike will luck into a fantastic startup and be in a position to hire or recommend you. Kindness has its own karma.And once you establish a network, tend it. Stay in touch with former colleagues. Find out what they’re up to. Send a short “congrats” when you see they start a new job. Write an unsolicited LinkedIn recommendation. Help people when you can—and ideally before they ask for it. The less you take your professional network for granted, the more you get back from it when you really need it.When you’re ready for your next adventure, tell your connections and friends. Be honest (but professional) about why you’re leaving your current gig and what you hope for in the next one. If you’ve followed the advice above—earning a reputation as a problem-solver who delivers good work on time—it won’t be a hardship for your network contacts to recommend you. If you were a mentor who taught them a trick or three that stuck, and they think positively of you, it pays off.Nobody is asking you to create your own blog or become a public speaker at conferences. That’s nice, but an extra. It’s far more important for developers to learn to get their points across to non-engineer audiences such as sales and executive leadership.Work on effective communication, especially with non-technical colleagues. This means speaking in such a way that the listener gets the point. Give people the appropriate level of technical detail, explain why it matters to them and their work, and avoid adding irrelevant details.Do work on your writing ability, even if it’s just learning how to write a coherent email message. Consider taking a business writing course, such as the ones offered at the Business Writing Center.Even if you aren’t yet a solid writer, offer your services to your company’s Content team. Maybe you can contribute to the company blog or write technical how-tos. Content marketers are always looking for writers who can write about technical topics in a clear, engaging way, and those editors are delighted to mentor anyone who wants to improve that skill. Those blog posts raise your profile within the company as well as with your industry peers.Keep detailed notes about your work. Note the key decisions that were made during a project, the roles you played, and the eventual business outcome. One reason is purely personal: If you don’t recognize your own mistakes, you’re apt to repeat them.Another benefit: You have a non-vague resume that can stand up to deep discussions in job interviews—particularly for the opportunities that come along when you aren’t looking.Even if you aren’t looking for a job right now, at some point in your career, you will be. So it makes sense to grow your job search skills, including your interviewing skills before you need them. Interviewing is a separate skill on its own. You should interview often enough that you’re not stressed out by the process. Think of it as a “Hello, world!” project; it’s best to make the dumb mistakes when they don’t matter.When you care about a job, you want to come across to interviewers as calm, collected, and well-spoken. This is much easier to do when you’re not desperate for the position or even that interested at all. So take job interviews even when you are “meh” on the prospect. It’s great practice, and sometimes it leads to a better opportunity than you expect.Curiosity is one of the most important traits you can have, says life and career coach Diana Allen. “Stay open to new thoughts, ideas, and perspectives. Ask questions to learn,” she advises.Strive to cultivate a growth mindset, which means believing you are limitless and that your intelligence and ability can be developed further. “Carol Dweck, one of the world’s leading researchers of motivation and mindset and the author of Mindset: The New Psychology of Success, says one important characteristic of this type of mindset is the willingness to learn from your mistakes and to find value in criticism,” Allen says.“We all have identity capital,” says Allen. “It is everything that makes you who you are.” This includes tangible things such as work experience, degrees, and courses. “It also includes intangible things such as your personality traits, how empathetic you are, how you solve problems, whether you are introverted or extroverted,” she points out. “The better you know yourself and what you bring to the table, the more confident and successful you’ll be in your career or in any future job search.”Always keep learning! And practically speaking, in the short term, that means building on your existing technical knowledge.Gain Redis mastery with our multi-week courses that teach you how to build robust applications using the entire Redis feature set. You can also demonstrate your mastery by completing the Redis Certified Developer Program. Visit Redis University to enroll for free."
94,https://redis.com/blog/how-redis-enterprise-powers-transnexus-microservices-architecture-to-help-fight-robocallers/,How Redis Enterprise Powers TransNexus’ Microservices Architecture to Help Fight Robocallers,"May 21, 2020",Haley Kim,"(As organizations look to modernize their applications, many are turning to a microservices architecture to deconstruct their legacy apps into collections of loosely coupled services. This profound change inspired us to reach out to Redis users in various stages of this journey to microservices architectures. We are telling their microservices stories in a series of blog posts, which began in late 2019.)You’re on the way to the grocery store when your phone rings. It’s a number you don’t recognize. Do you pick up?If the idea of ignoring your phone—even for an unknown caller—invokes apprehension and concern, congratulations: you’re a normal 21st Century human being, and also a great target for robocallers.Last year the average U.S. consumer received 178 robocalls. And as everyone who’s ever experienced one understands, robocalls are huge annoyances, and they have real negative consequences. When consumers grow frustrated with constant robocalls, for example, they may stop picking up the phone altogether for numbers they don’t recognize. That means they could miss important connections, and legitimate businesses can’t contact their customers.Fortunately, disruptive companies on the front lines of the robocall wars are building new technologies designed to restore our trust in telephone services. Alec Fenichel, Senior Software Architect at telecom software provider TransNexus, is one such superhero working to modernize telecom services for consumers, enterprises, and telephone service providers around the world. In an industry known for its complexity, TransNexus’ telephone software simplifies critical functions like preventing robocalls, denial-of-service attacks, and toll fraud as well as enabling least-cost routing, call authentication, and more.TransNexus—founded in 1997—provides telcos with two products: NexOSS, on-premises VoIP telecom applications, and ClearIP, a telecom software platform hosted in the cloud. Key technologies to drive the elimination of robocalls include:Redis Enterprise is critical to TransNexus’ success, as a real-time database for the cloud product. “When we receive a telephone call, we need to make a decision on how a call should be routed, like whether it should be allowed or blocked. All of that process is done via interactions with Redis,” Fenichel says, “and it has to be done in real time.”Reliability, security, and high performance were the focus of ClearIP’s design, Fenichel explains, and are inherent features of microservices architectures. TransNexus’ call processing, which includes a significant amount of work, is typically completed in about 20 milliseconds. The aim is to do the work quickly, so users never notice.From the beginning, TransNexus’ ClearIP was designed to perform complex call processing within a microservices architecture. “Microservices architecture makes it very easy to scale as needed, and it’s also very easy to maintain and be reliable. If a server fails, it will be automatically replaced and no one even notices,” he says.TransNexus currently has hundreds of millions of keys in Redis databases. Some of TransNexus’ customers are making thousands of calls per second. Since performance is so critical for TransNexus’ operations, the team has been careful to limit the extent of Redis’ operations. The team relies almost exclusively on Strings and Hashes but uses Lists in a few instances and Sorted Sets for one particular use case.Redis Enterprise acts as a real-time database, storing configuration data and helping ensure that TransNexus’ services can respond instantly to meet the company’s aggressive application requirements. “Redis is the only database we need online to perform call processing. It is the number-one most important database,” Fenichel says. “Our other databases are used to power the user interfaces and things of that nature. But when a call is processed, it never touches anything but the Redis database. This is by design to minimize the impact of any other database failure. That makes Redis the most important database by far.”But that’s not all. Redis also acts as an intermediary queueing system that assigns jobs to cloud servers: Once a record is processed, it’s placed in a queue in Redis, which is then flushed into a data warehouse. Redis is also a historical information store for call histories and accounts. “For example, if these incoming calls are abnormal for whatever reason, Redis stores information important for making determinations going forward from an anomaly detection standpoint,” Fenichel says.Looking to learn more about microservices? Check out how Mutualink uses Redis for a life-saving microservices architecture and how software agency Z3 Works uses it for a variety of projects across a variety of industries. Hear Redis Developer Advocates Kyle Davis and Loris Cro discuss their new free e-book, “Redis Microservices for Dummies,” on The New Stack podcast."
95,https://redis.com/blog/google-cloud-spanner-for-global-real-time-applications/,Turbocharge Cloud Spanner with Redis Enterprise Active-Active for Global Real-Time Applications,"January 26, 2023",Gilbert Lau and Abhishek Srivastava and Derek Downey,"Redis Enterprise’s Active-Active Geo-Distribution and Google Cloud Spanner’s replication work together to provide a unified real-time data layer for geo-distributed applications. Here’s how they connect and the benefits the tech provides.Designing and developing a geo-distributed application can be complicated. Applications that span multiple geographic locations have multiple requirements, and sometimes those get in the way of one another. Geo-distributed applications need high availability, resiliency, compliance, and performance across remote distances.Ensuring data consistency in these database systems across multicloud regions should be a critical feature in the underlying database technologies. And it should not be a developer’s concern.In this blog post, we discuss the combination of Redis Enterprise’s Active-Active Geo-Distribution and Google Cloud Spanner’s replication, which together provide a resilient, globally-consistent data layer for geo-distributed real-time applications.Cloud Spanner is a fully-managed database service. It’s part of Google Cloud’s effort to bring ACID-based consistency, SQL relational database capabilities, and synchronized replication among geographical locations to the masses. Cloud Spanner supports mission-critical applications, and it complies with relational database services by offering transactional consistency at a global scale, schemas, SQL (ANSI 2011 with extensions), and automatic, synchronous replication for five-nines SLA high availability.A data layer generally consists of multiple database technologies. They may include an in-memory database serving as the front-end cache to improve speed and performance, and also a traditional relational database or a NoSQL database to persist data in the backend system of record.Redis Open Source is powerful, but it falls short on various enterprise-grade features like multi-region deployments, particularly when it’s used in a front-end cache for multi-region Cloud Spanner topology.As the following schematic diagram illustrates, an application deployed in two different geographic regions (us-east1 and us-west1) may behave erratically in the event of simultaneous cache reads and writes.Imagine that this application architecture is meant to support a globally distributed real-time event booking and ticketing system. Outdated information could cause losses in business and poor customer experience.It’s not a hypothetical example. Not long ago, one ticketing service provider experienced a major setback when its system acted on outdated information. As a result, concertgoers from different regions received contrived ticket prices due to an inconsistent number of available tickets for correct dynamic pricing calculations.How can that happen? Here’s the process.Let’s say a popular concert is coming up for, say, Ariana Grande. Both the U.S.-west and U.S.-east databases have the correct number of tickets listed in the inventory. So far, so good.But then there’s a caching mismatch. Someone on the west coast buys 20 tickets, but the east coast database doesn’t get the update. The west coast database says there are 80 tickets available, but the east coast still says 100.That doesn’t matter much if nobody buys the tickets. But this is Ariana Grande, so you know the concert will sell out.In our cache-aside example, the value of x has an outdated value of available tickets for sale (still 100 seats) in the Redis OSS instance in the us-east1 region. Now, a problem arises on the next read operation on x (available tickets for sale) in the us-east1 region when someone on the east coast tries to buy those seats. That’s because x still has the old and outdated value even though the Cloud Spanner instances in both regions have the correct value.With our solution in place, this just won’t happen. The two of us keep things in sync. Nobody has to cry — or get into a drunken fight at the concert over those seats.Redis Enterprise can solve this problem thanks to the native support of active-active geo-replication. Using this feature, Redis Enterprise can serve as a cross-region distributed cache to ensure data consistency across regions. That guarantees stronger consistency both in the caching layer and in the system of record.Better yet: No additional development work is required to use Redis Enterprise’s native Active-Active Geo-Duplication. Each Redis Enterprise cluster supports local read and write operations with under one-millisecond latency, and it works harmoniously with the underlying Cloud Spanner instance in each cloud region. Without you doing anything special, data is automatically replicated among Redis Enterprise clusters. Frequent data-read access is offloaded to Redis Enterprise to improve application response time as well as to optimize overall data access costs.More importantly, Redis Enterprise’s Active-Active feature in Google Cloud Marketplace supports five nines SLAs as Cloud Spanner does in multi-region deployments. End result? Better price performance with no maintenance downtime.Active-Active Geo-Distribution is achieved by implementing conflict-free replicated data types (CRDTs) in Redis Enterprise using a global database that spans multiple clusters and conflict-free replicated databases (CRDBs).You can use this kind of deployment topology across different industries to solve well-known problems. For instance:Since its inception in 2017, customers in retail, financial services, and gaming industries have used Cloud Spanner to support applications requiring robust consistency and unlimited scalability. Which, admittedly, is everything.One such example is the retail industry, where electronic commerce has been reinventing itself since Pizza Hut set up the first online ordering website in 1994. The COVID-19 pandemic forced retailers to rethink how retailers could reach and serve customers, such as quickly implementing curbside delivery systems. And despite the increase in online purchases, shoppers expect real-time responses.Those business endeavors require a software foundation on which to build new retail features. New players are democratizing the e-commerce software stack with headless e-commerce platforms that support microservices architecture, API-first, cloud-native, and headless (MACH) principles. And it has to work across all geographic regions, from retailer warehouses to the supply chains that support their inventory.The financial services industry has its own challenges, such as ever-changing regulatory requirements, cybersecurity woes, and business model adjustments – each of which affects technology budgets. Interactions between bank institutions, fintech, and regulatory bodies must run flawlessly.Here, too, the industry must respond to increased demands. The pandemic increased digital payments usage, according to The World Bank. About two-thirds of adults worldwide now make or receive digital payments, with the share in developing economies growing from 35% in 2014 to 57% in 2021, according to the report.The video gaming industry is healthy and growing – but it, too, has to deal with market shifts, such as changes in spending habits pre- and post-COVID lockdowns. Again, the companies that bring out super-popular games (such as Splatoon 3) present technical challenges to the underlying application infrastructure platform. It takes a lot of computational power to support a worldwide game launch with millions of online players, multiplayer support, and in-game purchases.Redis is used extensively in these three industries, among so many more. The enterprise capabilities provide the data models required to build software that stands up to user demand. That’s true whether it’s to leverage native data structures like string, set, and hash as a simple cache for storing application data, hash for storing user profiles, sorted-set for tracking top 10 spenders during an online sale event, hash for externalizing the sessions in a microservices architecture, set or hash for tracking the buying behavior of a user. The possibilities are endless as the business requirement at hand stimulates the power of Redis Enterprise and drives its consumption.For instance, RedisJSON, along with RediSearch, can be used to model product catalogs, shopping carts, and order details in a retail application. They can also model retail bank accounts, securities portfolios in trading applications, and nominee details for a retail banking application. In a game, the same combination of JSON and search modules can be used to store players’ game inventory, player profiles, and player matchmaking data.One well-known Google Cloud customer is using Redis Enterprise to store user information and application critical data along with Google Cloud Spanner as primary storage deployed in two regions. By deploying Spanner and Redis Enterprise in multi-region topology, they are achieving two objectives: Global datastore and caching engine for the organization’s distributed workload, and disaster recovery concerns with ultra-fast failover time, reduced complexity, and best Recovery Point Objective (RPO) and Recovery Time Objective (RTO) guarantees.The scale-out and the scale-in process is also seamless. By adding new nodes, it is possible to scale from 100,000 requests per second to 1 million requests per second, even during high-demand days such as Black Friday. You just pay for the capacity you use.We have extensive information about Redis Enterprise’s Active-Active Geo-Distribution and Google Cloud Spanner. To learn about the state of your current cache strategy, take a five-minute assessment to receive practical recommendations."
96,https://redis.com/blog/kubernetes-secret/,How To Make Kubernetes Secrets Truly Secret,"January 5, 2023",André Srinivasan,"Kubernetes Secrets are often used to share secrets used by applications deployed in the Kubernetes cluster.  The caveat is that Secrets are not as secure as their name may imply. Below, we highlight both the challenges posed by Kubernetes Secrets and a strategy to decouple secret management from a Kubernetes cluster.This post, and with the help of a companion repo, describes how to create a secure deployment of Redis Enterprise on Kubernetes that leverages the power of Vault and highlights the difference between secret management and Kubernetes Secrets.A Kubernetes Secret is used to share sensitive data such as passwords, keys, credentials, and authentication tokens. Secrets keep secret data separated from an application’s code, allowing for the management of secrets without needing code changes.In its own documentation on Secrets, Kubernetes highlights this caution:“Kubernetes Secrets are, by default, stored unencrypted in the API server’s underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.”With that in mind, here are a few factors that can make secrets management a challenge:What is a DevOps team to do? Most likely the team delegates to another service such as HashiCorp Vault.HashiCorp Vault is a platform-agnostic, identity-based, external secrets operator. Unlike Kubernetes Secrets, Vault authorizes all access before sharing sensitive data.A Vault secret is available from anywhere. There’s no assumption where an application has been deployed; access control is managed through Vault, where Secrets are encrypted and stored and require credentials and authorization to gain access.Independent of whether Redis Enterprise is installed as software, fully managed in cloud, or deployed on top of Kubernetes, users need authentication; end-to-end security is usually enabled in support of those capabilities. In the case of Kubernetes, a number of Secrets are required for both the data plane and the control plane of a Redis Enterprise cluster.In September 2022, HashiCorp announced vault-k8s v1.0, an exciting development in the data space, as Redis Enterprise on Kubernetes now uses the Vault Sidecar Agent Injector, a Kubernetes mutating webhook controller. It also uses the resulting sidecar pattern to include a Vault Agent container, granting Redis Enterprise pods access to Vault secrets.In the context of Kubernetes and Redis Enterprise, all Redis Enterprise secrets exposed by the Redis Enterprise Kubernetes Operator can be delegated to Vault through the sidecar.Below is a walkthrough on how to create a secure deployment of Redis Enterprise on Kubernetes that uses Vault’s power. Here’s a checklist of steps necessary for a successful deployment.The prerequisite to get started is a Kubernetes environment with enough resources for Vault and Redis Enterprise. You will want at least three nodes, each with at least six cores and 25GB of memory for Redis Enterprise. However, for sandbox purposes, you can get away with four cores and 10GB of memory.Be sure you have enough capacity for both Redis Enterprise and Vault. The lack could potentially call for an additional node. You also need the right admin privileges from Kubernetes to install all these components.Please note all of the following steps are completely documented in the companion repo. The first step from the repo README is to source a shell script in order to set up environment variables. Basically, when coordinating configuration across two systems, these environment variables capture the common elements. It’s important to highlight the use of separate namespaces as well as the path for storing a secret file in Vault.Following along in the companion repo, the environment variables are substituted into templates via envsubst to avoid typing errors.You probably already know Kubernetes is managing a set of TLS identities for itself. What’s not as clear is that the same infrastructure can be used to create a new TLS identity. Using openssl or cfssl to bootstrap the process with a private key and certificate signing request is still necessary.To start, create two TLS identities to bootstrap the environment, one for Vault and one for the Redis Enterprise Cluster (REC). In other words, TLS is used to secure communication with the Vault, the Admission Controller API used by the REC, the data plane to the REC and, by extension, databases hosted by the REC.In all cases, the pattern goes as so:Create the private keyCreate the CSRSign the CSR with the private keyRequest and approve a certificate through the Kubernetes clusterA complete guide to setting up Vault is outside the scope of this post. The Vault tutorials are an excellent resource to explore. For the purposes of this walkthrough, a simple instance of Vault running in the Kubernetes cluster is needed (see the first steps of the companion repo). To create the sandbox environment, setting up Vault with TLS was made possible with the help of some examples from HashiCorp.HashiCorp was kind enough to provide a Helm chart to install the Vault in the Kubernetes cluster. For this exercise, install Vault in the same Kubernetes cluster as Redis Enterprise, though in a separate Kubernetes namespace.Of course, Vault could be in a separate Kubernetes cluster, could be deployed on bare VMs, or could be fully managed in the cloud via HCP Vault. For this configuration:Enable TLS and the injector.Ensure there are enough resources for Vault.Link the Kubernetes environment to the Vault configuration.Refer to the companion repo for the complete configuration file.At this point, Vault is ready to manage Redis Enterprise secrets, and Redis Enterprise knows where it wants to manage those secrets in Vault. Redis Enterprise still needs permission to access those secrets.Specifically, Redis Enterprise should not only have full CRUD access to secrets on the agreed-upon path, but Redis Enterprise should also be able to explore the keys:Redis Enterprise uses the service account that was issued by Kubernetes.The complete documentation for the Redis Enterprise Operator includes additional details worth reviewing. The companion repo provides the remainder of the configuration needed for Redis Enterprise and depends on the assumptions made throughout this post. For now, the simplified steps are:Install the Redis Enterprise Operator:Create the Redis Enterprise Cluster:Add the TLS identity for the REC to Vault:Once the REC deployment has been completed, create your database:Connect:While there are a number of moving parts to this discussion, these details would usually be hidden in the automation of a real DevOps environment. In addition, general security policies, as well as specific Kubernetes policies, typically inform different strategies for creating deployment of services for keeping sensitive information secure.The Redis Enterprise Operator Kubernetes is the product of what we have learned after millions of cluster deployments. Read more to understand how you can optimize the overall performance of your cluster."
97,https://redis.com/blog/the-marketing-buzzwords-that-developers-hate/,The Marketing Buzzwords That Developers Hate,"January 30, 2023",Esther Schindler,"Nobody is motivated to buy a development tool when the marketing pitch is “leverage synergy.”Tell me if this sounds familiar: An online buddy listens to you describe a programming problem and suggests a tool that might fix it. You want to learn what the development tool can do, whether it works with your existing infrastructure, how much of a pain it is to use, and whether your company can afford it.So, you go to the vendor’s website to learn more. Perhaps you download a brochure, or you view an “About our product” webpage.Three minutes later, you go screeching into the night, running away as though the hounds of hell are pursuing you.It’s not because the software is terrible. It’s because the marketing language chases you off.Some marketing buzzwords are more painful than others. Maybe a vendor uses meaningless, trite descriptions. Or it presents a conclusion (“best-in-class!”) instead of describing the product features that help you reach that conclusion yourself (“The Turbo Ninja Plus is the only tool that has this unique feature – I must have it, or I will surely die!”).How painful are these buzzwords? When I surveyed developers online, more than 800 people told me which marketing terms most make them want to scream.For transparency: I must confess that Redis is guilty of using these marketing buzzwords. I do my best to stomp them out (I’m the editor, hereabouts), but you are sure to encounter these terms. (I do hope you poke around to learn more about Redis because we do have lovely software that, I promise you, does cool and unique things.) We’re working on fixing the buzzword problem, k?Also, please be compassionate towards marketing professionals; it’s hard to be both creative and descriptive. Everyone tries to spiff up their verbiage, but it’s a losing proposition. Sportscasters soon learn that there’s a finite number of ways to say, “The count was 3-and-2 when the pitcher threw a fastball down the middle.” The intention is to make the copy interesting, but overused buzzwords become stale and abstracted terms that obfuscate, distort, and (most of all) bore.Software developers understand that vendors must tout their products, but they are less understanding when the marketing language is an immediate turnoff.But which marketing buzzwords tick developers off the most? I won’t make you scroll to the end. The most often mentioned, with the most fervor:But oh, there are so many more buzzwords used in business and IT. I offer them to you for schadenfreude and desk-banging exclamations of agreement.Several require an explanation, however, so I categorized the results.Vendors attempt to hijack meaningful terms to support their own needs. It doesn’t work.Many terms have a genuine meaning where the distinction matters. End-to-end is an easy example. In security, it’s important to distinguish between security at an endpoint (prevent someone from picking a lock) and how security works through an entire process (encryption throughout). But I’ve seen vendors discuss software as “an end-to-end solution” where it’s just noise.When used correctly, telemetry refers to useful state data that helps IT professionals analyze, diagnose, or identify trends in a computing environment. When it’s a buzzword, developers and other techies hear telemetry as “a way for vendors to have far too much information about your environment.”If it’s a technical description, it’s fine. If it’s hyperbole, it loses a developer’s interest.Digital transformation is at the top of this list. Sometimes, clever people invent something new that becomes a game-changer. We can all point to tools and technologies that genuinely transformed our lives: GPS. HTML. The camera phone.We also can admire the first development team at a bank that brainstormed, “Hey, if we connect our systems in such-and-such a way, people could scan paper checks to deposit into their bank accounts.” We admire the efforts of web development teams that built curbside delivery systems seemingly overnight. Those people can use terms like “digital transformation” without blushing.But lots of marketing campaigns describe a trivial new feature as part of a digital transformation project. So do CIOs who want to present themselves as forward-thinking. I was going to list examples, but you don’t need them.Best practices also is a real thing. It might be defined as “lessons you learned the hard way,” “the canonical method,” or “recommendations based on extensive experience.”That isn’t how it’s mostly viewed in the wild, though. Often, it’s a vague arm wave. In the eyes of many software professionals, “best practices” suggests that a problem has only one right answer, which is not usually the case. When they see the term, they quit reading unless it’s from a trusted source.However, we all know what’s meant by “best practices” – and so do search engines. Even if we dislike the term, it isn’t going away.Agile, too, is real. The Agile Manifesto disassembled the waterfall development model. Of course, Agile has its detractors, as does any development methodology. I won a journalism award by explaining why users hate Agile development (and what developers can do about it), in which I shared wisdom gained the hard way (so that I wouldn’t have to write “best practices”).But Agile as a buzzword has nothing to do with daily standup meetings or iterative development. Corporate management likes to use “agile” as a synonym for “flexible,” but they may as well write “supports capricious decision-making.”Artificial intelligence (AI) and machine learning are important technical endeavors. They are changing the world in important ways, such as in finance, health care, and creating cool avatars to use on Facebook. They also keep ethicists busy.But it seems as though any software that searches a knowledgebase is described as “AI-enhanced to deliver the best results,” and any application with an IF statement is lauded as machine learning. “It’s AI-infused!” they promise. Next up: “the new trashcan 5K, powered by AI!”Special mention: Anything at scale, unless it’s backed up with clear descriptions about the steps to cope with growth. Scalability is fine. “At scale” often means nothing.I admire the first person to invent the expression single pane of glass instead of writing “status dashboard” or “administrative interface.” That first use was imaginative.The three millionth use of the phrase? Not so much.When I asked on Reddit, Twitter, mastodon, and other online communities for the most-hated buzzword, so many developers spat out “single pane of glass” that I can assure you that using the term is the fastest way to chase away a would-be customer. Instead, use dashboard, status screen, or another descriptive term.Similarly, the first person to make a “leading edge” pun by saying bleeding edge may earn a wordsmithing hat-tip. But that expression (and its sibling “cutting edge”) assumes that the technology or product is at the forefront of its field, which is rarely the case.Then there’s tech recruiting. Once, in IT circles, rock star meant “master of this domain.” Hiring a “python rock star” or “code ninja” suggested that the developer’s skill went beyond coding, such as a conference speaker or book author. Today, some might say 10x developer (though that implies productivity rather than social status).Over time, though, “rock star developer” came to mean, “We want a miracle worker because we have no idea what we’re doing.” And it’s said by people who forget that rock stars earned a reputation for poor behavior.Some “they once were cool” buzzwords are less common nowadays, but they still give developers the heebie-jeebies. Among them:Most humans aim to communicate clearly. And then there’s the other people.Some obfuscation is an attempt to sound more impressive. For instance, a writer chooses a fancier-sounding word. Most of us don’t leverage a tool, nor do we utilize it; we use it. (There is a technical distinction between utilize and use and an appropriate time for leverage, but people fall asleep during my copyediting lectures, so I won’t impose it on you.)In other situations, the marketing copywriting gets carried away, such as the developer experience. Not everything has to be an experience. The “login experience,” the “opt-out experience.” None of these warrant the kind of transformative sensory profundity their names suggest.Yes, a developer’s experience does matter; it makes or breaks job satisfaction. But the product isn’t an experience. The way it’s used may be as part of the culture of the organization and the tool choices that a developer is given. Maybe the user interface matters – but I won’t go further than “the user experience” and only in the context of “software that makes you curse at the screen” or say to the computer, “Nice bot!Similarly, accelerate innovation may be a well-intentioned expression because the vendor wants its customers to do cool things faster. But even amazing tools are just tools. It’s up to the humans to innovate.And furthermore:Fortunately, nobody is using “metaverse” in marketing to developers. Yet.Some buzzwords are just noise. They make you wonder what the copywriter intended to say. Perhaps nothing.For example, synergy is used to describe the benefits of combining technologies, products, or teams to create something greater than the sum of its parts. As an intention? Sure. As a marketing or corporate buzzword? No. Just no.Finally, there are the unsubstantiated claims to superiority, including:You can entertain yourself for hours (presumably during one of those meetings that should have been an email) with Buzzword Bingo, the classic Tech Bullshit Generator, Buzzword Ipsum, and the new-and-improved web economy bullshit generator. And, of course, for your musical enjoyment, you should listen to Weird Al Yankovic’s Mission Statement.Let’s ditch the hyperbole and describe the product instead: what it does, whom it serves, and how it makes a developer’s life better.I like to think that Redis achieves all those goals with database tools that deliver what they promise and make its users say, “Wow, that’s so useful!” Please do look at the tools we created. We’re mighty proud of them. (And we’re working to omit the buzzwords wherever we can.)Try Redis now to find out why we think it’s so wonderful – and why we don’t need buzzwords to show the reasons why."
98,https://redis.com/blog/optimizing-redis-compiler-flags/,Optimizing Redis’ Default Compiler Flags,"February 1, 2023",Maria Markova and Filipe Oliveira,"Redis and Intel teamed up to find out whether applying more aggressive optimization options would improve overall Redis baseline performance. Our conclusion: Yes! By changing the compiler behavior, we measured a 5.13% boost overall and more in some cases.Redis assumes the default GCC compiler on supported operating system distributions for reasons of portability and ease of use. The build compiler flags, present since the early stages of the project, remained quite conservative across the years, with -O2 as the default optimization level. These flags have worked well. They provide consistent results and good performance without significantly increasing the compilation time or code size of Redis server binaries. Nobody wants to spend a lot of time on compilation!However, could we do a bit better? We are optimization specialists. We always ask this question!In this post, we describe our analysis of the impact on Redis performance by changing compiler versions (GCC 9.4, GCC 11, and Clang-14) and compiler flag optimizations. We evaluated the performance impact of compilers and flags using performance automation, as we discussed in Introducing the Redis/Intel Benchmarks Specification for Performance Testing, Profiling, and Analysis. As a result of this work, we updated the default Redis compiler flags because they ensure better performance.Let’s compare two popular open source compilers, GNU Compiler (GCC) and Clang/LLVM.GCC is a classic optimizing compiler. It is well-known as a C/C++ compiler but also has front ends for other languages, including Fortran, Objective-C, and Java. GCC is a key component of the GNU toolchain, and it plays an important role in Linux kernel development, along with make, glibc, gdb, and so on.GCC is the default compiler of many Unix-like operating systems, including most Linux distributions. As an open-source product, GCC is developed by many people and organizations, and Intel is among them.For our experiment, we chose two GCC versions:Clang/LLVM, or simply the Clang compiler, is a combination of the Clang front-end and LLVM backend. Clang translates source code to LLVM byte code, and the LLVM framework performs the code generation and optimizations. Clang is GCC compatible and positioned as fast compiling, low-memory using, and extremely user-friendly with expressive diagnostics. Currently, Clang is the default compiler for Google Chrome, FreeBSD, and Apple macOS.Much of the power of Clang is in the LLVM community, as many IT companies and individual developers are involved in it. In particular, Intel developers are active community contributors. The Intel ICX compiler is based on an LLVM backend, and Intel contributes enhancements to LLVM back to the community.For our performance testing, we chose the latest available major version at the moment of experiment, Clang 14.Compilers have hundreds of configuration settings and flags that developers can toggle to control how the compiler runs and the code it generates. These affect performance optimizations, code size, error checking, and the diagnostic information emitted. While it’s common to copy and paste the default settings, adjusting them can make a difference. A big difference.-O2 is a basic performance optimization that increases both performance and compile time. -O2 optimizes work with strings (-foptimize-strlen), includes simple loop optimizations (for example, -floops-align and -ffinite-loops), and partial inlining (-fpartial-inlining).  This optimization level includes vectorization with a very-cheap cost model for loops and basic block on trees (very-cheap allows vectorization if the vector code can entirely replace the scalar code that is being vectorized). More information about cost models and other optimization flags inside -O2 can be found in the compiler’s optimization flags list.-O2 has been the default optimization level for Redis source code. We use -O2 as our baseline, traditionally. In the project, we compared it with more aggressive optimization options.-O3 is a more aggressive compiler optimization flag. It includes all the -O2 optimization behaviors as well as additional loop optimizations, such as loop unrolling (-floop-unroll-and-jam). It also splits loops if one part is always true and the other false (-fsplit-loops). In -O3, the very-cheap cost model is replaced by a more accurate, dynamic cost model with additional runtime checks. With this change, the whole application can be faster because the compiler defines which parts of code are slow (e.g. slow scalar loops) and optimizes it, so parts that were good become better.These -O3 flags are not limited to just these options. For a full list, see the Compiler Options That Control Optimization.There are a few things to keep in mind:The -flto flag stands for link time optimization (LTO). This optimization is performed by the compiler at the point where it links application code. Without this option, each file is optimized separately and then linked together. With LTO, files are concatenated first, and optimizations are applied second; that can improve optimization quality.This option can be really helpful when application files have a lot of connections to each other. For example, let’s say you define a function in a file. You may or may not use that function in other files. The compiler’s linker uses this knowledge as it builds the executable. The used functions are inlined (which makes the application run faster); unused functions are excluded from the resulting binary file.LTO helps to eliminate dead code and conditions that are always TRUE or FALSE (“Is chocolate good? Duh!”). Global variables used in code are also inlined when you use the LTO compile option.All such changes positively affect the execution time of the built binary file. Or in English: We make Redis run faster.We conducted our experiments using the performance automation framework, which included 50 test cases. We aimed to achieve good coverage across Redis usage models and to ensure that we do not adversely affect the performance of important use cases.Using the automation framework, the joint team at Redis and Intel created multiple build variants to represent the combinations of compilers (gcc v9.4, gcc v11, and clang v14) and compiler flags to evaluate. In addition to Redis, we also experimented with modifying compilers and flags for Redis dependencies (for example, jemalloc and lua) by using the REDIS_FLAGS or just FLAGS options during the build stage. In total, these variations gave us 24 different binaries or build variants.To estimate the effectiveness of each build variant, we run all 50 test cases with the Redis server, built by a particular build variant (e.g Clang 14 + “O3”). For these tests, we calculated the geometric mean (or just geomean) on repeated runs for three times (everyone loves reproducible results!) and calculated the average value of three runs. This one final number, described in operations per second (ops/sec), indicates the performance of a build configuration. Providing these steps for each pair compiler and options, we got a set of numbers to compare. We show the percentage of the difference between them and the baseline below.We performed these tests on four Intel Xeon Platinum 8360Y processor-based servers.Figure 2 provides a summary of our experimental results. As a baseline, we compiled Redis using GCC 9.4 with default optimization flags. We judged success based on ops/sec on average across all runs and in geomen by 50 test cases, given as redis-server built by that or another build variant. More operations per second are better, suggesting that a Redis server is faster.Overall, GCC 9.4 O3 + flto provided the best performance of 5.19% (with dependencies included) and 5.13% (without dependencies) geomean speedup versus the baseline.The impact of the compiler and flags was much more pronounced in some use cases (see figure 3). For instance, with GCC 9.4 -O3 -flto, there is no performance degradation versus the baseline, and four tests improved by more than 10%.The results vary quite a bit, in other words – which shows that changing the Redis optimization flags can make a significant performance difference.In other configurations, some tests showed worse performance than the baseline. Yet some were boosted more than 20% over the baseline. This is because the O3 flag enables a number of aggressive optimization techniques to improve efficiency. The result is that the compiler can reorder instructions and make other changes to the code. While these optimizations can often be beneficial, they can also cause the code to run more slowly in some cases, particularly if they introduce additional overhead or they make the code less cache-friendly.In short, changing those flags makes a difference in the execution speed of OSS Redis.Based on the results of this experiment, the Redis core team approved our proposal to update the default flags to -O3 -flto (PR 11207). This configuration showed a 5.13% boost in geomean across all measured use cases and zero tests with decreased performance.Our work on tuning the compiler does not end here. We have additional opportunities to make Redis run (even) faster. For example:Want to see how all of this shows up in the software? Try Redis for free to explore its myriad benefits."
99,https://redis.com/blog/7-redis-worst-practices/,7 Redis Worst Practices,"March 23, 2020",Redis,"Click here to get started with Redis Enterprise. Redis Enterprise lets you work with any real-time data at any scale, anywhere.Your scientists were so preoccupied with whether or not they could, they didn’t stop to think if they should.—Dr. Ian Malcolm, in Jurassic Park“Best Practices” has become a trope in technology. Sure, you can do something with a given tool, but is doing so really a good idea? The fact that this topic comes up again and again, speaks to the flexibility of our tools. Best practices are great for beginners to learn the right things from the get-go. The problem is that, sometimes, as software engineers, we have imperfect memories of these best practices. Other times we accomplished what we need to do by not reading the manual and just hammering that square peg into the round hole, not realizing we were inadvertently playing the game on hard mode.So let’s take a different approach: instead of looking at the best practices, let’s look at the worst. We’ve seen customers, open-source users, and even tools implement patterns that cry out for a disapproving headshake. Granted, we haven’t centralized this kind of wisdom before, so let’s start now with seven Redis ‘worst practices.’Based on the number of code examples I see floating around the web (indeed, probably even some of my own from years ago), a lot of people don’t bother to set a password on their Redis instance. For this to be a truly worst practice in current versions of Redis, you have to try really hard in redis.conf to open up a password-less Redis instance to the whole internet. Older versions, however, did allow for this practice. Why is forgoing a password a bad idea? Without a password, your server will be found. Once it is found, all sorts of shenanigans can occur, from flushing the database to stalling Redis by running high-complexity commands, all the way to altering files (via CONFIG SET/GET).TL;DR: You will be h4x0r3d without a password.Best-practice alternative: Set a password and use AUTH.Weirdly, KEYS is one of the first commands people learn in Redis, even though using it is terrible (in production). For those who are enlightened enough to not know KEYS, it does a full iteration of all the keys (or a pattern) in a given database. Granted, this can be useful, especially for debugging, and not really a big deal if you have only a small number of keys. However, KEYS is a hidden killer as you scale. Consider four facts:So, writing an application that depends on KEYS is fine when you have dozens of keys—but this operation takes longer and longer with more and more keys. During this time, Redis is doing nothing but churning through the keys in the database. Imagine having to do 4,294,967,295 of anything, and you can understand why it will not be fast. Finally, KEYS is a synchronous command, so building up a response of all these keys—especially if they are large keys—is going to take a while, not to mention the time it takes to send it over the wire.TL;DR: Redis gets bigger than you expect, and KEYS can clog your Redis server for a long time.Best-practice alternative: Use SCAN, which spreads the iteration over many calls, not tying up your whole server at one time.Salvatore Sanfilippo, the author of Redis, once called numbered databases the worst design mistake he ever made in Redis. This design choice is a cautionary tale in building something that looks like it does one thing but actually does another. Thankfully, while this is becoming less common in the wild, Redis still ships with the ability to switch between different “databases” with the SELECT command. Each database is isolated from a key perspective. So, key foo:bar on database 0 can be completely distinct from foo:bar in database 9. This all sounds rather nice, right? The problem is that these databases are not isolated in any other way. Running KEYS on database 0 will still freeze database 9. In effect, it looks like you can run independent workloads on each database, but in reality, they aren’t independent at all.A bummer, but not really the worst practice, right? Well, the problem is that numbered databases are not well supported throughout the ecosystem. The first, and probably most dire, nail in the coffin of numbered databases is that they aren’t supported by any clustering system (open source nor Redis Enterprise clustering). In effect, you will never be able to leave a single node of Redis. Also, some modules do not support numbered databases.TL;DR: Numbered databases don’t do what you think they do—and then paint you into a scaling corner.Best-practice alternative: Run isolated instances of Redis—it has a low overhead, so why not? If you’re running Redis Enterprise, databases are isolated/multi-tenant by default.These commands fall into an interesting bucket: useful and benign most of the time, but cursed devils other times. The hash data structure in Redis allows you to set a series of field/value pairs under one key—HGETALL is a simple command that lets you retrieve everything in a hash all at once. This is fine, as most of the time you’re dealing with maybe up to a three-digit number of fields. Like keys, you can have 232 fields and values per hash. In most circumstances, you won’t have anywhere remotely near this many, but in some situations, you can accumulate high numbers of fields and values by the nature of your code (or logical error), additively increasing the number of fields over time. Then you run HGETALL and receive thousands of fields and values that may be up to 512MB each, meaning you have virtually the same problem as you do with KEYS.Things are perhaps worse in LRANGE. LRANGE gets items out of a list in a given range; to get all the items LRANGE 0 -1 will do the trick. Lists in Redis are effectively linked lists, meaning each element has to be visited sequentially (to get the pointer to the next element). By now, you might have guessed that 232 elements (up to 512MB each) is the maximum, and you can accumulate very high numbers of elements. If you’re using Lists as a queue, just having a worker offline for a few minutes can cause the size of a list to grow quickly.The story is more or less the same with Sorted Sets and Sets. They can store a ton of pieces of data, and each piece can be quite large. When you request all of them, this can take time.TL;DR: Redis can store very large data structures. Expect the number of results to be 232 unless you know the number.Best-practice alternatives: Run a command that sanity checks the size of data structures (HLEN for hashes, LLEN for lists, SCARD for sets, and ZCARD for sorted sets).Many databases use the concept of REST as a primary interface—send a plain old HTTP request to an endpoint with arguments encoded as POST. The database grabs the information and returns it as a response with a status code, and closes the connection. Redis should be used differently—the connection should be persistent, and you should make requests as needed to a long-lived connection. However, well-meaning developers sometimes create a connection, run a command, and close the connection. While opening and closing connections per command will technically work, it’s far from optimal and needlessly cuts into the performance of Redis as a whole.Using the OSS Cluster API, the connection to the nodes is maintained by the client as needed, so you’ll have multiple connections open to different nodes at any given time. With Redis Enterprise, the connection is actually to a proxy, which takes care of the complexity of connections at the cluster level.TL;DR: Redis connections are designed to stay open across countless operations.Best-practice alternative: Keep your connections open over multiple commands.Redis can easily become the core of your app’s operational data, holding valuable and frequently accessed information. However, if you centralize the access down to a few pieces of data accessed constantly, you create what is known as a hot-key problem. In a Redis cluster, the key is actually what determines where in the cluster that data is stored. The data is stored in one single primary location based on hashing that key. So, when you access a single key over and over again, you’re actually accessing a single node/shard over and over again. Let’s put it another way—if you have a cluster of 99 nodes and you have a single key that gets a million requests in a second, all million of those requests will be going to a single node, not spread across the other 98 nodes.Redis even provides tools to find where your hot keys are located. Use redis-cli with the –hotkeys argument alongside any other arguments you need to connect:$ redis-cli --hotkeysTL;DR: Don’t create a small number of frequently accessed keys.Best-practice alternatives: When possible, the best defense is to avoid the development pattern that is creating the situation. Writing the data to multiple keys that reside in different shards will allow you to access the same data more frequently.Redis is often used as a primary storage engine for applications. Unlike using Redis as a cache, using Redis as a primary database requires two extra features to be effective. Any primary database should really be highly available. If a cache goes down, then generally, your application is in a brown-out state. If a primary database goes down, your application also goes down. Similarly, if a cache goes down and you restart it empty, that’s no big deal. For a primary database, though, that’s a huge deal. Redis can handle these situations easily, but they generally require a different configuration than running as a cache.TL;DR: Redis as a primary database is great, but you’ve got to support it by turning on the right features.Best-practice alternatives: With Redis open source, you need to set up Redis Sentinel for high availability. In Redis Enterprise, it’s a core feature that you just need to turn on when creating the database. As for durability, both Redis Enterprise and open source Redis provide durability through AOF or snapshotting so your instance(s) start back up the way you left them.There you have it—seven worst practices of Redis. Did we cover all of the bad practices out there? Of course not. Keep an eye on our blog or sign up for the Redis Watch newsletter to discover more things you absolutely, positively don’t want to do in Redis.Did this post give you the flop sweats because you may be guilty of one (or seven) of these worst practices? Let us know on social media. As always, we love feedback on Twitter @Redis.Watch our recent Tech Talk on Buy vs Build: Clustering & Provisioning in Redis Open Source vs Redis Enterprise!"
100,https://redis.com/blog/5-basic-steps-to-secure-redis-deployments/,5 Basic Steps to Secure Redis Deployments,"February 2, 2023",Quincy Castro,"Recent security reports identified the risk of attacks on misconfigured Redis databases. Here are five basic steps to secure your Redis deployments.Recently the cyber research community highlighted how attackers have been abusing insecurely configured Redis databases. As one of the world’s most used NoSQL databases, the huge installation footprint of Redis––both open source and commercial––makes it a natural target for attackers.However, there are a number of basic steps Redis users can take to reduce the risk of attacks like the recent HeadCrab malware campaign identified by AquaSec.We should note that there are no signs that Redis Enterprise software or Redis Cloud services have been impacted by these attacks.Since version 3.2.0, Redis Open Source (Redis OSS) enters a special mode called protected mode when it is executed with the default configuration and without any password required to access it. In this mode, Redis only replies to queries from the loopback interfaces. When clients connect from other addresses, Redis OSS replies with an error that explains the problem and how to configure Redis properly.We expected protected mode to decrease the security issues caused by unprotected Redis instances executed without proper administration. But system administrators can ignore the error emitted by Redis; those administrators can disable protected mode or manually bind all the interfaces. It appears that there are plenty of these kinds of deployments which are being targeted by attacks such as HeadCrab.Here are some basic recommendations for how to begin securing your Redis deployments:For more details on how to securely configure, deploy, and use Redis, visit our open source and commercial software documentation sites."
101,https://redis.com/blog/build-on-redis-hackathon-winners/,The $100K “Build on Redis” Hackathon Winners Announced!,"June 14, 2021",Raja Rao,"Developers are at the core of what makes Redis one of the most popular and most loved databases. This year at RedisConf 21, we hosted the “Build on Redis” Hackathon, our largest one to date! Today, we are thrilled to announce the winners of the $100K in prizes!At the Hackathon, participants explored a wide range of possibilities within the Redis portfolio, leveraging the advanced capabilities of our real-time data platform. Specifically, we encouraged them to explore the ecosystem of modules such as RediSearch (4x–100x times faster than the market leader), RedisJSON (10x faster), and so on. We were eager to see what they came up with and boy, did they deliver!The below pie chart shows the distribution of different modules the winners used to build their apps.In our estimation, virtually all the teams spent anywhere between two to four weeks building these projects. They went above and beyond to build really useful and interesting apps that amazed all of our judges. Don’t take our word for it, just watch the video, look at the code, or look at the in-depth documentation!It’s truly humbling to see how passionate the Redis community is and how much time and effort they put into building these apps.Congratulations to all the winners! Your check is in the mail!“I had worked with Redis as a cache before, I had no idea you could do so much more with it! It’s easy to use and it’s FAST.”—Niek Candaele, Software Engineer, Stampix (Project: Feature Creep)“It was a lot of fun learning all the new features of Redis while participating in the hackathon! It’s so much more than just a cache now.”—Tinco Andringa, Lead Engineer, AeroScan (Project: Topscorio)“The hackathon was an awesome opportunity to learn more about Redis, notably the powerful modules I haven’t used before. Building my first ever graph database was easy using Redis, its documentation and its client libraries were great.”—Mitch Ward, Sr. Engineer, Datadog (Project: NYC Bike)“The hackathon was so much fun and I learned a lot. Thanks, Redis!”—Matteus Hemström, Software Engineer (Project: Pizza Tribes)“Redis and Redis’ ecosystem has a lot of offer for data scientists and engineers taking their ideas into production.“—Dr. Alexander Mikhalev, Tech Lead, National Building Society (Project: The Patterns)"
102,https://redis.com/blog/understanding-redis-enterprise-software-support-packages/,Understanding Redis Enterprise Software Support Packages,"January 3, 2023",Vanessa Hoying,"We ask for a lot of information when you contact our support teams. We’re not being capricious or demanding. There are good reasons why we ask for this information.You’re driving down the road. The car’s “check engine” light turns on. Uh-oh.But the diagnostic test also suggests you take the car to a mechanic. When you do, the mechanic says the car has problems beyond your catalytic converter. Your brake calipers are shot, the brake pads are worn down, and the brake rotors are nearing the end of their life. While the mechanic’s bill may be high, it’s a lot worse than a car breakdown on the highway. You probably wouldn’t know about the extent of the car problem if you were not looking for it.This process is comparable to the support package that you get with Redis Enterprise. You may find a few low-performance indicators on your own. However, our Customer Success team can dig deeper and look for underlying issues. When you send in the data, our team analyzes the proprietary Redis Enterprise software using our internal tools and extensive technical knowledge.Maybe it seems like overkill. Why does the Redis Customer Success team request a support package after you provide screenshots and detailed reports about an issue? But the data we ask for truly does help us provide better answers faster.A support package contains all the essential information to help troubleshoot reported issues within a cluster or to perform health checks. It contains three types of information bundled together (thus a “package”) in a tar.gz file:We look at the overall health of your clusters. We aim to find areas for improvement, fix problems before they arise, and guide you on best practices. Your account success is our account success, and your business is our business.Here is what we check from a support package:There are two ways to obtain a support package. You can do this through the Redis Enterprise GUI or the Redis command-line interface. Both accomplish exactly what you need; it’s only a matter of preference.If you prefer to use your terminal or PowerShell, you can aggregate a support package by Redis command-line interface. When you create a support ticket, our support team may suggest creating a support package by this method.When you submit a support ticket to our support team, our first step is to learn how you encountered the issue. Yes, screenshots and detailed information about the issue help; however, every issue has multiple ways of being diagnosed.These logs provide further insight for the team to troubleshoot the issue. The logs help our support team to identify the issue and to make Redis Enterprise an even better product.We do this all in a timely manner, so you don’t have to spend hours (or days!) looking at the logs and trying to guess what’s going on. Please do provide this information, as it helps us help you – and gives you the feedback to make your system run smoothly.We have experts in this field to ensure you’re in good hands. Just as a car mechanic is an expert in their field, our Customer Success team works to find all the nooks and crannies that could be missed.We understand that privacy and security are important to every business. We stress: We never collect any database dumps or content.Interested in getting your diagnostics checked out? Read the documentation on how to create an updated support package.Experience Redis Enterprise Cloud for free."
103,https://redis.com/blog/securities-portfolio-data-model/,Exploring a Securities Portfolio Data Model Using Native JSON and Query Capabilities in Redis,"March 1, 2023",Abhishek Srivastava and Prasanna Rajagopal,"This tutorial, which shows how to optimize a brokerage application, demonstrates what you can accomplish using Redis with its JSON data structure and enhanced query capabilities.In the financial sphere, a brokerage’s success is tied to its investors’ engagement with the trading application it provides. Brokers are motivated to create better investor engagement because it leads to more assets under management, more trades, and (the brokerage’s favorite part, surely) more fees and commissions.These mobile apps also serve as a useful way to demonstrate system design in general and technologies that Redis Enterprise provides in particular. The broker’s applications are all about timing, and that is the essence of Redis as a real-time data platform, allied with the documents store (JSON) and fast query capabilities. So let’s examine this as a case study for mobile developers who serve a financial services audience.Some brokerage apps are designed to handle a certain number of client logins at any time. These apps face scalability and performance challenges when trading activity surges. These technology failures and poor client experiences are bad for a brokerage’s revenue, profitability, and reputation.Any brokerage application has a long list of technology must-haves, including high availability, low latency with fast response time, strong consistency, scalability, security, and consistent performance. Guaranteeing all these attributes is a challenge. Software developers often use a mix of technologies and databases, doing their best to balance all those parameters along with issues of interoperability and company budgets. Their application landscape comprises SQL databases, NoSQL databases, and caching to create a veneer of real-time performance and scalability.The brokerage app we describe needs an in-memory database that can ensure high availability, seamless scalability, and diverse data modeling capabilities. Redis Enterprise is an in-memory database that supports JSON, graph, and time series data structures. Querying using the unique Redis index and search capabilities can meet these requirements for a brokerage application. Redis Enterprise offers plenty of features to support high availability and scalability, such as Active-Active Geo-Distribution, which can meet the most demanding needs for a brokerage app.Herein, we present a sample implementation of a brokerage application using JSON Data structure and Redis indexing to store and retrieve securities portfolios.You can follow along with the sample code if you like.We modeled various brokerage entities (Exhibit 1) using JSON. Each investor’s personal details are modeled as a JSON document titled investor. This document stores information such as the investor’s legal name, address, date of birth, social security number (U.S.) or Aadhar (India) card number, and the taxpayer id (TIN in the U.S. and PAN in India).(Although each investor can have multiple accounts within a brokerage, for this sample implementation, we model each investor owning a single account.)Exhibit 1: Securities Portfolio Data ModelEach JSON document has a corresponding key, the format of which would be:A sample investor JSON document looks something like this:The application can extend the data model to accommodate an investor’s ownership of multiple accounts.Each account is modeled as a JSON document in Redis. The account document can also capture important data, such as the approved level of options trading and whether the account is permitted to trade on margin.The key format for the Account JSON document looks like this:The corresponding account JSON document is:Each account can have dozens of assets in its portfolio. These assets could be stocks, bonds, mutual funds, exchange-traded funds (ETFs), and options.Each asset has unique data elements that are reflected in the JSON document. For example, an entry for a stock owned by an investor in an account would include the price paid, the purchase date, and the quantity. In contrast, a call option bought by an account needs the strike price, expiry date, and transaction type (sold or bought the option) along with the purchase date, the price paid, and the number of options purchased (quantity).A typical investor purchases stocks in lots at a given date and time. Therefore a lot is unique to the account, security, and time of purchase. Investor accounts may also get new security lots assigned when dividends are paid as stock. For example, investors may register their brokerage account or individual security in the dividend reinvestment plan (DRIP). The brokerage creates new security lots for each stock that’s enrolled in a DRIP when the company pays dividends. That’s just a sample. There are many scenarios under which a brokerage can create new security lots, such as company spin-offs, mergers and acquisitions, and automatic investment plans.The key format for each such security lot is as follows:A sample security lot JSON document looks like this:In Redis, the purchase date of the security in the lot is stored in Unix date/time format.Finally, we store the details of each traded security in JSON,  the key for which looks something like this:The corresponding stock could have the following JSON elements:This data model lays the foundation for a brokerage to handle millions of customer accounts.Everything is set up. Let’s write some queries on Redis to implement a few scenarios of retrieving JSON objects and full-text capabilities.To do so, we need to add data in Redis using the programming language of our choice. The JSON as data structure in Redis is supported by official Redis clients. For instance, for Python, we have redis-py; in Java, we have Jedis. For accessing enterprise modules in Spring, we can use the Redis OM Spring library.Here’s the code snippet to create a document using the Python Redis API:The corresponding code using Java and Jedis library is as follows:Imagine that a retail investor wants to view a particular security or a subset of securities they hold. It’s a typical scenario. During peak trading hours, The underlying data platform has to handle these queries for millions of accounts concurrently.Every query should return the result in real-time so the underlying application provides a consistent performance and user experience. This can be achieved using a Redis capability that enables querying, secondary indexing, and full-text search for Redis. For that, we need to create suitable secondary indexes on the JSON documents first.In Redis, the Index has a unique capability to follow the data writing path, so as soon as you create the Index using FT.CREATE and define how the JSON document should be mapped using the SCHEMA, the secondary index is populated. New data coming, the index is updated, and you can query straight away the new documents.Here’s the RediSearch index for the account documents:And here’s the index for the security_lot documents:We created indexes on the documents of security lots(idx_trading_security_lot) and accounts (idx_trading_account). These indexes can be queried in different ways to serve an investor’s real-time, milliseconds needs.Let’s build the queries for a few scenarios:Let’s build the queries for a few scenarios:Retrieving all the security lots owned by an account may be the simplest query to display a portfolio. The query looks like this:Its output appears thusly:It is easy for the investor to filter the view of their portfolio and view their position in a certain security.The output for the above command looks like this:For now, it’s focused on retrieving specific keys that match a query predicate and need. The Redis query capabilities don’t stop there. You can use FT.AGGREGATE to group, sort, filter, and make arithmetic operations SUM. The application could get the total quantity of securities or the price paid using an aggregation query similar to the following:And its output looks like:A query like this may help an investor know the total quantity of securities owned at certain points in time, such as the end of the month or year.Expect output that looks like this:To find the average cost price of each stock owned by an account at a particular date and time, we first calculate the lot value of all the securities held in an account. Then we aggregate the total quantities of these securities. Finally, we calculate the average cost price of these securities.The output for the above command is as follows:We hope this example piques your interest in these Redis Enterprise capabilities and encourages you to learn a little more about them.Redis document store capability provides full support for JSON, a JSONPath syntax for manipulating the JSON elements, fast access to data, and atomic operations for JSON values. Multiple benchmark tests suggest that RedisJSON performs better than its competitors on metrics such as latency and read-write throughput.The Query and Search on Redis allow you to quickly create indexes on HASH and JSON documents and use real-time indexing to query documents instantly. The indexes let you query the data at lightning speed, perform complex aggregations, and filter by properties, numeric ranges, and geographical distance.Want to learn more? You can choose one of the following ways to install and get started with Redis Stack:"
104,https://redis.com/blog/now-you-can-deploy-active-active-redis-databases-with-terraform/,Deploy Active-Active Redis Databases With Terraform,"March 6, 2023",Noam Stern,"We are happy to announce that Redis Enterprise Cloud has published its Terraform resources for managing multi-region Active-Active Redis databases.Terraform is a popular, flexible open-source tool for infrastructure automation that helps DevOps configure, provision, and manage infrastructure as code (IaC). IaC lets organizations manage infrastructure (such as virtual machines, databases, load balancers, and connection topology) using a descriptive cloud operating model. Terraform makes it easier to plan and create IaC across multiple infrastructure providers using the same workflow, which aids reliability by ensuring that deployments remain consistent.And now Redis Cloud is making it easier to work with Terraform.Redis Cloud Active-Active databases synchronize data across multiple geographies. This technology makes it possible for applications to write data simultaneously to any region, with the confidence of strong eventual consistency. Developers can build globally distributed applications without needing to solve complex technical challenges, such as handling cross-region write conflicts. Redis Enterprise takes care of it for you.Active-Active databases are most valued where business requirements include disaster recovery, geographically redundant applications, and situations where it’s important to serve data closer to users’ physical locations.Managing data across multiple regions can be complicated. This is where the Redis Active-Active architecture and Terraform work together brilliantly. With Redis Cloud’s new Active-Active Terraform resources, you can create, update and delete databases in any region at a push of a button.In addition, you can easily define different configurations in each region. For example, you can consume 50,000 ops/sec in one region while only consuming 1,000 ops/sec in another region.To create an Active-Active Redis Cloud subscription in Terraform, you need to create three Terraform resources:Specify each database’s read and write operations within every region as part of this resource.Here is how a subscription with one database might be deployed across two regions:The creation_plan block allows the API server to create a well-optimized infrastructure for databases in the cluster. The provider uses the attributes inside the block to create initial databases. Those databases are deleted after provisioning a new subscription, after which the databases defined as separate resources are attached to the subscription.You can use the creation_plan block only for provisioning new subscriptions. The block is if you make any further changes or if you try to import the resource.For more information, consult the Redis Enterprise Cloud registry documentation.Need more help with creating your first Active-Active Redis subscription using Terraform? This video shows how to deploy your first Active-Active resources."
105,https://redis.com/blog/outgrowing-logical-databases/,4 Signs You Might Be Outgrowing Logical Databases,"March 8, 2023",Helene Brulin,"The purpose of Redis OSS’s logical databases, either self-deployed or launched as a managed service such as ElastiCache, is to simplify a developer’s work by reducing administrative needs and by providing out-of-the-box one-size-fits-all defaults.However, in production, there might come a time when your functional and operational needs change, and a single Redis instance is no longer sufficient.Here are four signs you might be outgrowing Redis OSS’s logical databases.Let’s imagine you are a developer at a gaming company. You use three Redis logical databases: one for caching and leaderboards, one for matchmaking, and one as a message broker. Your company recently released a successful new game, and you have peak matchmaking requests every evening. But your leaderboards show stale data during that time period, and your message broker’s latency is increasing.This is likely because a single Redis instance uses a single thread from the point of view of command execution, and it serves each request sequentially. Because logical databases all share the same instance, this thread may be slowed down or even become blocked by operations executed against a specific logical database and thus impact the other ones. This can cause problems if you have throughput-intensive use cases or your application uses O(n) Redis commands.In another scenario, you might have a bug. In the context of microservices, for example, where each service reads and writes to a dedicated logical database, all of the services’ databases could fail at once because of a bug in a single microservice. Centralizing multiple use cases in one single Redis instance is not fault tolerant.What if you used dedicated instances instead of logical databases? Processing each microservice’s requests with a dedicated database would result in better performance for each service, as well as make your application more resilient.One way to avoid noisy neighbor shortcomings is to scale your database. To do this, you can use OSS Redis Cluster, which allows you to cluster databases across multiple nodes.However, doing so is only supported on the logical database located in Index 0, which means you can scale only one of your logical databases. This might lead you to store data relative to your more significant use cases in the same logical space, negating the initial intent of keeping separate namespaces.What if you used dedicated instances instead of logical databases? You could then scale each database as needed without restriction.Let’s imagine now that you are a software developer at an e-commerce company. You use one logical database for caching and another one for session management. You have the following requirements:Despite these requirements, your two logical databases must share the same high availability and durability configuration because they both share the same redis.conf files.The same goes for eviction policies and memory limits, which are specific to caching use cases, as well as TLS certificates, passwords, and, more generally, all configuration options of Redis OSS’s redis.conf file.What if you used dedicated instances instead of logical databases? No more compromises. You could configure each database as your business requires.Because logical databases share the same Redis process, you might find monitoring and troubleshooting to be tedious.A first example is the monitor command, which streams back every command processed by the Redis server. Whichever logical database you run it from, it returns all commands sent to all logical databases running on the server, albeit displaying the database index for each command.Another example are the slowlog commands. Here, no distinction is made between the logical databases in which the logged commands were run. For example, to artificially create some slow execution commands:The same goes for logs, latency subcommands, or if you want to grep or get any values from the Redis info command: number of connected clients, used_memory, current IOPS, number of evicted keys, etc.If you use a third-party tool to monitor Redis, such as Grafana, you have the option to specify the database number when you define your Redis data source. However, the data displayed in the dashboard is not necessarily exclusive to the database index you defined. You do get the right number of keys in the keyspace, but command statistics, client connections, and IOPS are not specific to the selected index; those values are common to the entire Redis instance.Finally, let’s imagine that, despite the complexity of reading dashboards and logs, you identified that the latency on your caching logical database comes from the fact you enabled AOF upon every write because your session store database needs it. Then what can you do outside of relaxing the persistence requirements for your session database? That goes back here to two of the earlier signs that you are outgrowing logical databases: noisy neighbors and unique configuration requirements.So what if you used dedicated instances instead of logical databases? It would be easier and faster for you to monitor the performance and identify issues of each database, saving you operational time and effort.Well, you can start by using separate Redis OSS instances to address your different needs. Or, you could leverage Redis Enterprise’s cluster-level multi-tenancy, which addresses noisy neighboring, fault-tolerance, and generic configuration concerns.Whichever option you choose, it requires you to migrate your logical indexes to different dedicated databases. Since all logical databases are persisted in the same RDB file, the first step of such a migration is to manually extract the data of each logical database into a separate file. Doing so requires a repetitive process of loading, flushing, and restarting a secondary Redis server.To save you the trouble, this script automates the process. It loads your data into a secondary Redis server launched as a child process and uses this server to create one RDB file per logical database: 0.rdb, 1.rdb, and so on.The Redis technical teams are happy to assist you with planning your migration. When you contact us, mention that you want to migrate your logical databases based on what you read in this article."
106,https://redis.com/blog/developers-guide-to-fraud/,What’s Changing in the World of Fraud Detection (And What Developers Need to Know About It),"March 13, 2023",Pam Baker,"Developers are expected to level up their baked-in security measures, but that’s easier said than done. Here are some helpful tips.Motivation to improve fraud detection is at an all-time high for financial institutions. It’s not just common-place fraudulent transactions that are costing banks money, although those are costly enough. According to an ABA Banking Journal report, “For every dollar of fraud lost in 2021, U.S. financial services firms saw $4.00 in costs, up from $3.64 in 2020 before the pandemic.”Staying on top of all the ways crooks cheat financial systems and software is a full-time job. But developers are expected to grok what’s happening now and to build in safeguards to prevent it, too.Thankfully, there are tools and software to assist developers with this task, but it’s still a tall order to fill.To that end, here are tips to help bring developers up to speed on what’s new in current fraud detection – and the actions they should be prepared to take.Few developers are ignorant of the importance of security practices, but they may not realize the extent that it’s needed in financial technology (Fintech) or the urgency of finding workable practices. The bad guys are busy innovating too.Fraud has always been a huge problem for banks and other financial institutions. But Fintechs are feeling the squeeze continuing to tighten. It’s hard to deliver flawless service for the 53% of the global population expected to be using digital banking by 2026 (Juniper Green, 2021) and keep the 48.6% of fraud reported to the FTC locked out. What greases the wheels for customers also lets fraudsters slide in with ease.Synthetic fraud, which refers to the result of building a false identity from one or two stolen data points from real people, costs U.S. banks upwards of $20 billion, according to another ABA Banking Journal report. Banks are under pressure from regulators to curb money laundering, and they face increasingly high penalties when they fall short. That’s a problem, given that about $2 trillion is laundered globally each year (according to a Deloitte report), and roughly half of it goes undetected across the entire financial industry.Protections built on static checklists and data are doomed to failure in today’s fast-moving world. Designing more fluid user identification and authentication methods that can be checked in real-time is a better strategy. But it’s also more difficult to pull off without some help. Assistance can come in the form of internal or third-party tools or extra hands, but it can also result from tightening relationships with existing business partners. A lot can be accomplished by sharing more information that leads to building better inputs for fraud detection algorithms.As with other cybersecurity issues, the key to prevention requires accurate pattern detection. But first, developers need an idea of where to look for emerging or evolving patterns.“There is no crystal ball as to what is going to happen next in the world of fraud. But we can see some patterns getting stronger over the last year and a half,” says Baber Amin, chief product and operating officer at Veridium, an identity authentication company. At the top of the list are account takeover fraud, synthetic identity fraud, and card not present fraud.In injecting security protections in code to detect and reject fraud, developers often rely on their own experience or on company protocols to guide them. However, these practices are good only until the bad guys change the game.“The number one thing to avoid is rule-based detection. That should be put out to pasture at this point,” says Amin.So what can developers do to get a better idea of what protections to build into the software? Try looking outside of your organization for clues.“Fintech companies need to work with other major vendors, like airlines, car rental companies, and major retailers, to take in signals that raise or lower the potential risk of a transaction,” says Amin.Rule-based detection isn’t the only thing that isn’t working well. Much of the tried and true is going the way of the tired and trounced.“The Fintech world has learned a lot of hard lessons over the last couple of years, and it has resulted in significant increases in fraud and data breaches,” warns Ted Miracco, CEO of Approov, a security provider for mobile apps. “Implementation of 2FA and use of technologies like code obfuscation for API keys and other ‘secrets’ has proven to be woefully inadequate for preventing fraud, as these approaches are easily circumvented by the determined hacker.”Moving API keys to the cloud is safer than trying to obfuscate APIs that are hardcoded into the application. Another good tactic is mobile app attestation, which only allows genuine apps on untampered devices to access APIs, according to Miracco.“This approach can both stop bots, emulators, and hacking frameworks from abusing APIs with stolen credentials acquired from the dark web and also stop the increasingly popular man-in-the-middle (MITM) attacks,” Miracco adds.Rules-based detection has always struggled with distinguishing between risky and normal behaviors. AI is far better at pattern-checking and behavior labeling.“Integrating AI and machine learning should be a given at this point,” says Stanislav Khilobochenko, vice president of customer services at Clario, an antivirus provider. “It’s the most reliable way to detect patterns and anomalies indicating fraud, and there’s really no way around it: you’ll need to adopt it at some point,”.“You will have faster detection response, more accurate detection, and the ability to process more data. You will also have more scalability and customization to suit the needs of your business and customers,” Khilobochenko adds.While not entirely new as concepts, some items require greater attention from developers than before. Examples include making allowances for freelancers and gig workers whose income and payment behaviors may deviate strongly from other banking customers.“Geographic biases, racial biases, gender biases, or even only being trained with English data could mean fraudulent activity is overlooked or misidentified. For example, the irregular income patterns of freelancers could be flagged as suspicious if the system is trained mostly on traditional income patterns,” explains Khilobochenko.For example, banks are using AI-based systems as part of identity authentication and transaction risk-scoring models. The huge behavior-based and identity-info datasets need to be updated frequently, with worldwide access, which may require technical upgrades to keep the architecture responsive.“Obviously, this is a huge undertaking and why partnering with an AI firm might be the best way to integrate customized, effective machine learning into your applications,” Khilobochenko adds.Put another way, start looking for ways to catch fraud at its root. After all, the sooner you can detect a possible vulnerability, the more likely your block won’t miss.“Deploy AI to look for Identity theft and synthetic identity account creation. A majority of fraud is rooted in fraudulent identities,” says Amin.Identity theft is tricky to detect, so look for multiple ways to authenticate users. Two-factor authentication is not foolproof. Especially if the device or email is one of the authenticators.Add more layers of user authentication where you can and look to automate much of it so there’s no additional burden on your users. One good way to do that is to leverage AI’s ability to scale complex models.“Deploy AI to detect patterns and context around payment and purchases. Look for cohort or family member spending to detect [more] patterns,” says Amin.To learn specific ways to improve your organization’s fraud detection, read Combat Fraud with Redis Enterprise."
107,https://redis.com/blog/redis-hackathon-2022-winners/,Meet the Redis Hackathon 2022 Winners,"October 20, 2022",Redis,"Three winners. Three unique applications – and plenty of innovation in real-time application performance.The 2022 Redis Hackathon launched in collaboration with DEV in August 2022. Redis called upon the dev.to community’s most engaged, creative minds to build applications using Redis Enterprise’s many functions, modules, and products. Redis’ strengths go well beyond caching, so the contest challenged the community to showcase what’s possible in building applications backed by enterprise-grade performance.Download our e-book Enterprise Caching: Strategies for Optimizing App Performance.We received 91 valid submissions in four project categories:The Redis Hackathon selected five grand prize winners, 20 runners-up, and 25 extra participants that included a video of five minutes in length or longer with their submission.All of them achieved remarkable things, so we asked three of our grand prize winners to share the technical details of their projects, including their plans to expand their applications with Redis.Hi, I’m Subham Sahu, an engineer who loves exploring and building things. I’m a recent undergrad from the Indian Institute of Technology Ropar.What does OneSocial do?It’s an app for creators and their audiences. With OneSocial, anyone can share their thoughts on their blog, manage an active newsletter, organize events, and make their content more discoverable. Want more? One can sell digital offerings, such as notion templates, design illustrations, or stock images, as well as services like video consultation, mock interviews, resume reviews, etcHow will OneSocial make creators/influencers’ lives easier?Creators have to use multiple websites and social platforms to run their businesses. As a result, creators’ audiences are scattered across many social media channels. To ensure that the latest announcement reaches them, the creator has to post the same thing repeatedly on various websites. It is time-consuming, cumbersome, and possibly destructive to one’s brand, as asking loyal followers to subscribe to your content on multiple platforms could be a big ask.The aim of OneSocial is to build a single platform that provides creators with all the necessary tools and functions from top to bottom, so they can have their entire audience under a single roof.Techie details: How did you build your application?The communication between server-to-server, server-to-client, and client-to-server is done via GraphQL. The front end is built using Next.js, and the back end uses Express (Node.js library). We’re using Cloudinary and Google Cloud Storage to store images and files securely.Stripe powers the payments module, while Redis queue serves as the notifications system. The chat module uses Redis Streams, WebSockets, and GraphQL subscriptions to keep persistent connections and deliver messages instantly.The data is stored as Redis JSON, and we use the Redis-OM Node.JS library to interact with the database. All major fields and attributes are indexed or sorted for best performance based on the design requirements. We’re also using RediSearch to perform a full-text search on the data in Redis. Finally, we’re using Redis Streams to publish and subscribe to events across multiple services.Stay connected with SubhamView Subham’s submissionPortfolio: subhamx.devFollow Subham on Twitter: @subhamxHello. I’m Nabil, a contract developer based in Nigeria.Pic-Placeholder is a stylish image placeholder with more than 500 images in six categories: animals, cats, dogs, houses, landscapes, and people.What was the lightbulb moment that led to Pic-Placeholder?When I heard about the Redis hackathon, I didn’t know what to build, but I still wanted to participate. I’d been hearing about Redis for a while but never had a chance to work with it.I was helping a friend with a real estate app, and we wanted to use lorem picsum as a base for placeholder images, but we couldn’t because it was too generic, so I ended up building my own.What can Pic-Placeholder users hope to accomplish with this app?The project’s goal was to create a tool for quick prototyping when building image-centric sites.How did you use Redis to develop your app, and what other components were used along the way to create the final product?I started development by getting images from Unsplash in all my categories and converting them into JSON as the metadata. RedisJSON and RediSearch were used to house this data; the images were stored on AWS S3.The entire app was built using Next.js, the client that served as the demo and server for calling the endpoints. The endpoints were:Get more details about Nabil’s submission:View the Pic-Placeholder demoSee the Pic-Placeholder repoMy name is Fred Spencer, and I am an Experience Architect in the United States. I’m the founder of Elm Story, a design tool and platform for interactive storytelling.Project: Meatballs.live, powered by Redis Stack and HackerNews, is an automated recommendation network and web app for discovering interesting conversations across social news.What technological components helped you realize this project?The entire Meatballs.live project is written in TypeScript, utilizing Redis Stack features, including RedisGraph, RedisJSON, RedisTimeSeries, Pub/Sub, and caching. I used RedisInsight extensively to visually surface insights as data was collected in real-time. Redis Cloud hosts the primary Redis database.Get more of the technical details on dev.to.What professional pain points is Meatballs.live designed to solve?You can think of this project as a “super-aggregator” designed to ingest, remix, and blend data from disparate aggregation sources.By connecting and analyzing large amounts of structured, temporal data, meatballs.live can generate daily collections of top stories with a bias for the comment section. Daily collections serve as recommendations for the most engaging comments and related stories. The app experience can save time and cognitive load for users by reducing addictive scrolling. Temporal aspects provide context.The live chat feature enables real-time engagement between meatballs.live users. The latest comments from Hacker News are blended with the stream, providing links to relevant threads.Are you planning to do more with this project? If you had/have time to improve it, what would you add?My number one priority is to warehouse historical data. With one data source, Meatballs.live generates 1 GB/mo. Given the current app’s feature set, it is unnecessary and inefficient for all this data to live in-memory. Next, I hope to add support for additional data sources, such as Reddit and Twitter.I’d also like to port ingest services from Node to Rust, enhance live chat to support more real-time insight, and expand user features to include collection reactions.There are no limits when remixing big data with the Redis Stack.Stay connected with FredView Fred’s submission.Follow Fred on Twitter: @r1tsuke / @elmstorygamesWe want to thank everyone who participated in our Hackathon with dev.to. We saw some truly inspired ideas come to fruition and hope to see these Redis-built applications go to market soon!To see more applications built with Redis, please visit our Launchpad site, full of engaging demos and inspiration for anyone building applications."
108,https://redis.com/blog/legacy-database-migration/,Legacy Database Migration: What To Know Before You Start,"October 28, 2022",Redis,"Legacy database migration. Those three little words can strike fear in the hearts of IT professionals — but they don’t have to! Let’s break down the process of migrating legacy data, outline the pros and cons, and highlight considerations that industry professionals wish they knew before moving legacy data to an advanced system.For IT teams tasked with migrating data from legacy systems to a more advanced environment such as a cloud database, a successful migration can have an enormous payoff. Still, the consequences of a failed execution could be dire. There’s no one-size-fits-all approach to database migration, but there are some key factors anyone attempting a migration should consider for a relatively painless experience. Let’s explore legacy database migration in both concept and execution.Database migration is the process of transferring data from one database management system to another, or from one version of a database management system to a newer version. This process typically involves several steps, including extracting the data from the source database, transforming it if necessary to match the schema of the target database, and loading it into the target database.Database migrations track the controlled transfer of a schema from its existing state to a new environment. In any database migration, historical data is transferred from an older system to a new (and usually modern) database. Among the important goals are keeping the existing functionality working, optimizing performance, and improving maintainability.With open source and relational systems such as MySQL and Oracle databases, it can be challenging to make schema changes without jeopardizing data integrity. SQL databases infamously crunch data to fit their structure. Migrations are common for many reasons – developers regularly try out new tools – but when an application and its data store are both outdated and well-entrenched throughout an organization, moving data from one system to another presents additional difficulties. Quite often, these legacy database migrations are accompanied by a transition from a relational system, such as a MySQL database, to a NoSQL, or serverless environment, including the cloud.Old applications rely on older software ecosystems. Even if a legacy application appears to work – if it ain’t broke, don’t fix it, as the saying goes – the ecosystem may fail. Perhaps the host operating system is no longer supported. The hardware can’t keep up. The application (and its data) depends on APIs that are no longer supported, and it can no longer interact efficiently with newer systems. Or you inherit an application whose code and database design are arcane… but the programmer who knew the system retired.Sometimes, the reasons for the change are due to business success. As the user base scales, so does the amount of data. It’s essential to make provisions for more users and more data, ensure that there’s no failover or latency, support a global user base, maintain speed, and deliver data in real-time.Use cases change over time, too. Remember, Netflix started with DVDs. What happens when a business taps into a new revenue stream, but the legacy system can’t keep up? Older databases sometimes cannot take advantage of the potential of a new architecture.Besides, there are technical advantages to adopting the newer technology, such as taking advantage of features that didn’t exist in the legacy system. The new-and-improved application can accomplish much more with an in-memory, multi-model, real-time database.Every database migration is unique, but people have been upgrading database systems since the second database was released. It’s wise to plumb the experience of those who have gone before.To provide actionable guidelines on how to survive the experience, we solicited advice from dozens of IT professionals, developers, and database administrators who performed legacy database migrations. They recommend you include these items on your data migration checklist. (Identities are obfuscated because an individual’s historical experience may not reflect a current employer.)Learn about the new-to-you technology before you start. You need to become confident using the new database. If you run into a snag, you want to know if you encountered a technical problem or whether you’re discovering the depths of your own ignorance.You need to understand how and why things were done the way they are in the old system. And you also need to understand the new system’s capabilities and processes before you start the migration. Only then can you match the approach taken with the old system to the features and best practices used in the new system.This isn’t just a data migration. It’s also a problem-solving migration.Be open to new approaches. Sticking to “the old way” can result in a poorly designed new system. Users often blame the new system for that, not the migration.Before you touch any data, model each query and compare how it’s used in both the old database and the new one. That can help you spot problems when they’re still on the whiteboard before anyone begins typing.Some data won’t map well from one application to another. It’s best to find out how to address those issues before an entire team is committed to the project.Quality Assurance should get involved early. They need to address the challenges in both data integrity and the applications that access the database in its new home. And they’re used to thinking about all the ways that things can go wrong – and to recommend what to do in failure scenarios.Do you have a way to confirm that the data matches in both the legacy database and the new one? And to ensure that any system integration works correctly with old not-yet-migrated applications and the new ones that inspired the migration project. Think about that now instead of later.Create full-stack integration tests that can test both “before” and “after” results against your endpoints, API, etc., to ensure the data is the same. This isn’t easy; subtle changes against IDs and dates can be challenging.Run tests against a test database whose configuration is an exact copy of production or as close as you can make it. That means you need to create those tests.You may need to schedule lead time to build a custom test framework if the system is unusual. That may be time-consuming, but experienced database professionals insist that building these tests and tools helps you to move tables to different schemas without a problem.Make sure your data is up-to-date and accurate in your current system before trying to move it to a new one. A data audit helps you make sure the data is up to date. Wasting resources on migrating erroneous data “builds rework into the process from the beginning,” says Karen, a software analyst.Otherwise, you will start with garbage. Plus, everyone blames the new, unfamiliar technology rather than the data quality.Don’t think about migration until you clean up your data. “Anything else is sloppy and will build rework into the process from the beginning,” says Karen. “This is an equally critical first step when moving any application to the cloud.”“Always eliminate superfluous data,” says Karen. “Confirm that you are only requesting and sending the data required for the running processes.”Find out how much data you actually need to migrate and in what time frame. It may be feasible to address the legacy database in stages or to use interim tools – though that can raise additional issues.This process may be a project on its own. Your organization may need to ascertain the data retention compliance requirements, especially for historical data. Take care of that before you shift data from one database to another.Data is the lifeblood of every organization. Don’t skip on the design or the protection: security, backups, and verifications.Make sure you have backups from before the migration starts. “I’ve seen companies trying to roll back to standby servers only to realize they’ve already replicated their bad data,” says one sadder but wiser database specialist.Have a rollback plan for when things go wrong. Test that plan before you start making database changes.You can lose time. But you don’t dare lose any data.The challenges in data migration generally aren’t technical challenges. They are business challenges.A legacy database migration project is not trivial. If you find out that you lack the political capital to change processes, suggests one experienced developer, “Find another contract now before you waste 18 months of your life and come down with a stress-induced illness!”If you encounter resistance from Management for any of these pre-migration tasks, take note, says an IT professional who learned to identify red flags. Be sure you get a signoff, in writing, on how much data loss is acceptable when restoring from backup. Expect to be told, “No data loss is acceptable!” Just tell them that’s fine, suggests the IT pro. “But lay out the time and costs involved in developing the tools necessary to ensure (and test) that replaying data from the new system onto the standby is reliable. Quite often, you’ll find they’re more accommodating to data loss, or they’ll identify a subset of data that must not be lost.”Once everything is set up, it’s time for the actual migration. Just be sure you pay attention to these suggestions.Keep a table to map the legacy identity alongside the new identity for every row you insert so that everything is auditable. You have to be able to map any subsequent child rows to the appropriate parent.Don’t migrate to a target database in one fell swoop, or what’s called a “big bang migration.” That’s a lesson Andy, a DevOps specialist, learned the hard way. He regrets opting for that choice when his team moved from a SQL Server database to a cloud database. “We bit more off than we could chew, falling short of a seamless migration,” he says.Andy wishes that he had connected the old database to the new one, then synchronized each piece of data in real-time, little by little. Then he could use the new database whenever possible, with an easy way to switch back and force the prioritization of the data. “The danger here is to migrate so slowly that you end up with the databases never completing the migration,” he admits. A general plan of attack and some deadlines are important.Joseph, whose team managed the migration from a Microsoft SQL server to the cloud, warns of the same pitfall but also addresses that a balance is recommended. “You need to migrate in small increments,” he says, “but you can’t go so slow that neither the legacy SQL server nor the new cloud database gets the updated data.”Hurrah, the data is transferred to the new database. You’re done, right? Not hardly.Verify that the data matches before and after the migration says Sue, who runs the computer systems for a small, family-owned contractor. Her company’s computerized data goes back to 1985, with at least four major database migrations since then.Depending on the urgency of the legacy database migration, it may make sense to run systems in parallel. “Inject a shim into your data layer so that any activity on the legacy database also runs on the new database,” suggests Zack, a database administrator. “The layer can compare results before returning the app tier.” This method allows you to run the new and old databases side by side for a while and to confirm that the new database is operating correctly before you cut over.“We definitely had a few issues post-migration,” reminisced Guillermo. “Some required us to learn more about the new system, such as monitoring, tracing, and telemetry. Some issues required us to upgrade to a newer version of software than we’d chosen, which meant reading patch notes. We also needed to create a few custom log analysis tools.”One database specialist has made an entire career performing legacy database migrations. For Elena, a key factor is creating a dedicated test environment for the new system (both front end and back, if possible) so that stakeholders can log in and review the results in the test environment.Make sure the stakeholders sign off on their acceptance. “Never run a line of code in production that hasn’t had the results tested and approved in test,” Elena advises. “This gives stakeholders ownership over the results.”If you encounter data that just doesn’t translate, don’t just say it’s impossible, Elena says. “Sit down with the stakeholders and ask them why they need that data and have them walk you through what they’re going to use it for. Once you have the actual business use case, you can almost always find a workaround to accomplish what they need.”Custom systems aren’t always necessary. At least, you can build on existing migration deployment strategies, tools, and products.The task of migrating from a legacy system to a target database is akin to a physical move into a new home. In that analogy, data ingestion would be the movers who package up all your assets, store the items safely for transport, deliver your goods, and decorate the new place (if you did your data audit right, that is).Learn 6 Ways To Speed Up Your Application when you migrate to the cloud using data ingestion.Perhaps a business needs a little bit of column A and a little bit of column B. A full migration of an on-prem to a new system doesn’t work for all. Introduce a caching layer into the existing stack to create a hybrid environment that softens the cost of scaling operations entirely to cloud.Read more on Enterprise Caching: Strategies for Caching at Scale.Is it worth it? Most of the time, yes.Some legacy databases rely on third-party technology — technology that could become obsolete at any moment, leaving the data’s security in jeopardy. But there’s more besides the danger of archaic systems and getting trapped by vendor lock-ins.Read Top 5 Benefits of Adopting a Cloud Migration Proof Data Layer to understand the full value of the cloud.The first step to a legacy database migration is testing the waters. Remember, it’s not all or nothing—test drive data in a real-time cloud environment for free with Redis Enterprise Cloud.Looking for definitions to other related concepts?  Check out our glossary section."
109,https://redis.com/blog/object-relational-mapping-misconceptions/,3 Common Misconceptions About Object-relational Mapping,"September 13, 2022",Will Johnston,"Most software developers are familiar with object-relational mapping (ORM), a coding technique that creates an abstraction layer between object-oriented programming languages and databases. But despite its value, ORM isn’t ideal in all situations – particularly when programmers make wrong assumptions about its use. We debunk several such mistaken beliefs so that you can use ORM the right way.Don’t get the wrong message: We appreciate ORMs. Redis supports the technology enthusiastically, with support for several programming languages. But it’s important to get off on the right foot.In its purest form, an ORM serves as a translator that bridges systems. They offer a lot of flexibility by allowing programmers to streamline communications through their preferred programming language rather than relying directly on Structured Query Language (SQL) or other database-specific languages. Because systems often depend on different programming languages and store data differently, it’s all too easy to insert complexity. Without ORMs, many object-oriented applications would struggle to communicate with databases.Still, programmers can get misguided about the role of ORMs, which leads to confusion and sometimes frustration.These three misconceptions can prevent you from using ORMs correctly and getting the most benefit from the technology. That’d be a shame.Surely you’re familiar with SQL, the standardized language for storing, manipulating, and retrieving data in a relational database. You interact with the database using specific queries: INSERT, SELECT, UPDATE, DELETE, DROP TABLE, and so many more.ORMs help programmers to interact with databases in their preferred language (say, Python) rather than by manually writing SQL queries from scratch. That’s a significant plus. With an ORM, data is modeled in a structured way, which helps programmers interact with the database’s layout and content.You don’t have to use SQL, as the ORM does a lot of work for you. That saves time and energy, particularly for people who don’t know SQL or lack confidence in their skills. Only 56.9% of developers use SQL worldwide, according to a 2020 StackOverflow survey. But a shortcut is not the same thing as a justification for ignorance. Some developers assume that “I don’t need to know SQL in depth” is the same as “Learning database concepts  is unnecessary because ORMs do everything, from start to finish.”This is not true.ORMs are great for straightforward SQL queries. However, when you do anything complex, you need to know what SQL can do to wring the last bit of functionality out of it. This shouldn’t be a surprise. You do the same with other development tools. Tools are supposed to support our work, not replace our understanding.Yet, developers sometimes generalize, “I have a tool that helps me” into “If you have a tool, you don’t need to know anything about database design.” High-level abstractions don’t always write the best code. They are susceptible to errors, and you need to understand SQL to recognize when that’s the case. If you don’t understand SQL, debugging, tuning for performance, or collating complex data becomes more tedious and frustrating.Knowing SQL gives you more control over your application and allows you to tune SQL queries to optimize performance. Use ORMs to help you work faster– not to keep you from thinking.Some anti-ORM folks adopt the attitude that it’s always better to write SQL queries yourself. They assume that the only functionality ORMs offer is to write the SQL code.But the software goes beyond writing queries. The ORM gets involved with the object-oriented model and how data moves between the database and that model; this is orthogonal to actually writing SQL queries.ORMs also promote code maintainability and reuse, as they enable the developer to think in terms of objects instead of atomized pieces of data.The result of a SQL injection can be catastrophic, leading to unauthorized database access.  Fortunately, ORMs are more effective in shielding applications from SQL injection attacks. They can map objects and actions into database-related code, so you don’t have to manually write security-specific code.However, “it helps” is not the same as “…so it’s foolproof.” ORM packages do not make your database application immune from a cyber security attack. As with any software, ORM can have bugs (even Hello World is vulnerable), leaving data-driven applications exposed to another area of attack.The primary way ORMs shield against SQL injection attacks is by using parameterized queries. However, not all ORMs work this way. Before you assume a particular ORM is safe, you should at least verify how it translates your actions into SQL.For better protection, implement all the database security techniques you would employ if the code was written by hand. No matter how the application is created, make sure that your software QA process includes a full security test suite (not only for SQL injection attacks). ORMs may reduce the likelihood of security vulnerabilities, but it’s best to be safe.At Redis, we’re enthusiastic about any tool that makes application development faster, more accurate, and more effective – leaving the programmer to contemplate innovative ways to help a data-driven application better serve its users. So sure, we’re fully behind ORMs. They promote code reusability and provide a cleaner separation in the data access layer.The Redis OM client Libraries introduction explains how object mappers help you get the most out of Redis’ modules. Redis OM’s goal is to provide a toolbox of high-level abstractions that make it easy to work with Redis data in the programming environments with which you’re most familiar. It contains four high-level client libraries for Redis, which we invite you to explore:Looking for definitions to other related concepts?  Check out our glossary section."
110,https://redis.com/blog/detecting-fraud-and-securing-financial-data-with-a-real-time-database/,Detecting Fraud With a Real-Time Data Platform,"March 20, 2023",Henry Tam,"Financial institutions are struggling to keep up with the ongoing onslaught of fraudulent transactions and cybercriminals’ changing tactics. As the global financial services landscape evolves, fraudsters are moving in tandem with the multitude of digital transaction channels, finding innovative ways to steal or fake customers’ identities and commit payment fraud.Since the pandemic, 35% of retail banking customers increased their use of online banking. Indeed, by 2026, 53% of the world’s population is expected to use digital banking.Online fraud has increased as digital banking usage increases, and the situation is getting worse. According to a 2022 PwC report, 51% of businesses have experienced fraud in the last two years, robbing them of an estimated $42 billion.Banks and payment processors are working hard to detect fraud before it occurs but they have to keep up with the criminals’ evolving tactics. There’s a lot to worry about.For some years, banks and investment companies have had to follow know-your-customer (KYC) regulations, part of which entails collecting customers’ credentials to verify their identity and statistically evaluate the risk for suspicious account activity. Standard identity verification relies on static data (for example, social security numbers don’t change, and addresses rarely do). That information isn’t updated frequently enough to be reliable or safe. The data breaches at Equifax and Capital One have shown that identity data can be stolen and used for fraud and account takeovers.Instead, banks and financial services organizations are turning to digital identities. Document verification and biometric records are combined with intricate behavioral patterns to create a digital identity for each user. These digital identities are dynamic and complex, made up of a number of different sources and data types that are constantly changing. The challenge is updating information quickly enough to stay one step ahead of criminals without hampering the genuine user’s experience.Financial services firms traditionally used rules-based systems to detect simple, non-changing, known fraudulent patterns such as validating black lists or user purchase profile histories. However, they do struggle to distinguish risk from normal behavior.Machine learning (ML) algorithms and artificial intelligence (AI) predictive models can evolve and learn as they analyze and detect payment fraud based on historical and real-time transactional information. For example, transaction scoring algorithms consider transaction details, user profiles, behavioral biometrics, geolocation, IP/device metadata, a user’s financial information, and more. However, due to the size and complexity of data pipelines, successful AI/ML implementation depends not only on accurate models but also on the performance and resiliency of the underlying machine learning operations (MLOps) database, known as an online feature store.We live in a fast-paced environment where criminals have discovered savvier ways to steal identities and credit card info to commit fraud. Financial services enterprises need to adapt to the digital age and move away from rigid and slow legacy relational database management systems (RDBMS) that cannot support modern AI/ML-based fraud detection and dynamic digital identities.For more information, read Combat Fraud with Redis Enterprise. We go into quite a bit of detail about the ways that Redis Enterprise enables faster and more accurate fraud detection, reduces costs, and scales."
111,https://redis.com/blog/build-intelligent-apps-redis-vector-similarity-search/,Build Intelligent Apps with New Redis Vector Similarity Search,"November 16, 2021",Ed Sandoval,"On a recent family trip to Switzerland, my son took a photo on his smartphone as we walked through the lovely Lavaux Vineyard Terraces.Using Google Lens, he was able to use the image to quickly look up the fascinating history of this UNESCO World heritage site. Google Lens allows users to “search what they see” around them by using a technology known as Vector Similarity Search (VSS), an AI-powered method to measure the similarity of any two pieces of data, images included.VSS empowers developers to build intelligent applications with powerful features such as “visual search” or “semantic similarity”—all with just a few lines of code. The best part is that you don’t need to be an Artificial Intelligence (AI) or machine learning (ML) expert to do it. In fact, it’s easier than ever with Redis Vector Similarity Search, a new capability we just released that turns Redis into a vector database.What is VSS? Want to try it out? Read on!As noted, VSS is an advanced search method used to measure the similarity between different pieces of data. While it works well with structured data, VSS really shines when comparing similarity in unstructured data, such as images, audio, or long pieces of text. For example, using VSS and a touch of AI/ML (more about this in the next section), you can take a pair of images and generate vectors for each of them. These vectors—or, more precisely, vector “embeddings”—encode features of each image as a 1-dimensional array of numbers. These embeddings can be compared to one another to determine visual similarity between them. The “distance” between any two embeddings represents the degree of similarity between the original images—the “shorter” the distance between the embeddings, the more similar the two source images.Here’s where AI/ML come into play.The wide availability of pre-trained machine learning models has made it simple to transform almost any kind of unstructured data (image, audio, long text) into a vector embedding. For example, Hugging Face, a startup focused on natural-language understanding (NLU), provides free access to hundreds of state-of-the-art models that transform raw text data into its vector representation (embedding).The clever trick of these models is that the embeddings generated for two sentences will be “close” to each other only when the meaning of the sentences is semantically similar.In Fig. 1 above, you get a sense of how the embeddings for sentences are related. If you look at embeddings generated for sentences related to a “mobile phone,” you’ll notice that they are “close” to each other (see top left part of the diagram). More important, both of these embeddings are noticeably distant from the embedding generated for a sentence that relates to a food supplement product (lower right part of the diagram). The “distance” between the embeddings acts as a proxy for their semantic similarity. There are even models that capture semantic similarity for sentences in multiple languages.In the computer vision world, there is an equivalent: Torchvision, a PyTorch library for computer vision, offers a number of pre-trained models that can be used to generate a vector for a given image. In a similar fashion to Hugging Face models, the Torchvision-generated embeddings for two images will be close to one another only when the images are visually similar.Developers can leverage these freely available models into their applications.But generating the vector representations or embeddings is only the first step. You need a database to store the vectors, index them, and perform Vector Similarity Search.At the core of Vector Similarity Search is the ability to store, index, and query vector data. These are the essential capabilities needed in a vector database.Our VSS capability is built as a new feature of the RediSearch module. It allows developers to store a vector just as easily as any other field in a Redis hash. It provides advanced indexing and search capabilities required to perform low-latency search in large vector spaces, typically ranging from tens of thousands to hundreds of millions of vectors distributed across a number of machines.In short, this new RediSearch feature turns Redis into the powerful real-time, in-memory vector database your app needs.OK, so how do I build visual or semantic search similarity into my application?If you have Docker and a little knowledge of Python, you can take Redis VSS for a spin on a container and play around with a realistic dataset. The Redis VSS demo provides step-by-step guidance to get you started.You will start a Docker container with Redis VSS and go through several Jupyter Notebooks showing you how to generate, load, index, and query vectors generated from product images and text descriptions. You will be using the Amazon Berkeley Object (ABO) dataset, which contains text and images for hundreds of thousands of products listed on Amazon.In the demo you will find four Jupyter notebooks illustrating the key elements needed to build visual and semantic similarity into your app. These are:Give it a try and have some fun with it. We’d love to hear how your first experience with Redis VSS went.Please join our #VSS channel on Discord. We’ll be happy to answer any questions you may  have about VSS and its potential applications.Sign up for our hosted private preview program.During private preview, we are working with a selected number of customers with a clear use case and lots of data. We’ll provide you with resources to try VSS out at scale. In return, we’ll ask you to share your feedback about the experience.Reach out to us on Discord and let’s get the conversation started."
112,https://redis.com/blog/real-time-product-recommendation-docarray/,How To Build a Real-Time Product Recommendation System Using Redis and DocArray,"November 8, 2022",Alaeddine Abdessalem,"This tutorial helps you build a real-time product recommendation system for an e-commerce system using content-based filtering and vector similarity search. Follow along to learn the essential steps and how it works.Recommendation systems are an important technology for most online businesses and for e-commerce sites in particular. They’re an essential element in generating good conversion and maintaining customer loyalty.A recommendation system typically shows items to users based on their profiles and preferences and by observing their actions (such as buying, liking, or viewing items).Consider the challenges involved in building a recommendation system for a modern e-commerce site. This is just a subset of the issues to consider:As these requirements evolve, approaches to building recommendation systems need to evolve as well. In this blog post, we show you how to build a real-time product recommendation system with respect to user-defined filters using the latest vector search technology. The tool suite includes Redis and DocArray, but the methodology is relevant no matter which tools you employ.As with any other computing problem, there are multiple approaches to building recommendation systems and tools to support each effort. They include:This blog post focuses on improving the content-based filtering approach. If you’re new to this topic, it may help to read Google’s overview of content-based filtering first.There are two important considerations when implementing content-based filtering:First, when you model the user and items as a feature vector, it’s important to exploit all modalities of the data. Simply relying on keywords or a set of engineered features might not efficiently represent complex data.That’s why state-of-the-art AI models are important, as they represent complex, multimodal data as vector embeddings.One of the best-known models to represent both text and image data is CLIP. Therefore, in this tutorial, we use CLIP-as-service as the inference engine that powers our recommendations.Also, computing vector similarity can be slow and costly if not performed efficiently. Our application requirements (to respect user filters and deliver low-latency recommendations) make it impractical to pre-compute similarity between items and user profiles in batch jobs. That’s why it’s crucial to compute vector similarity in real-time, using efficient techniques such as Hierarchical Navigable Small World (HNSW).These techniques are implemented in vector databases. Redis offers vector search capabilities in RediSearch 2.4. And since Redis is an in-memory database, recommending items is both fast and performed in a real-time context.With feature representation and computing vector similarity covered, we still need a data structure to bridge the gap between our multimodal data and the vector database. For that, we use DocArray. Think of DocArray as a universal vector database client with support for multimodal data. It has a Pythonic interface that makes it easy to build a recommendation system in just a few lines of code.We have assembled the tools for this application: Redis for Vector Similarity Search, CLIP-as-service to encode visual data, and DocArray to represent multimodal documents and connect to Redis. In this tutorial, we apply these technologies to build a content-based filtering recommendation system.The procedure is as follows:In the instruction that follows, the data we use for product recommendations comes from the Amazon Berkeley Objects Dataset, a dataset of Amazon products with metadata, catalog images, and 3D models.The first step is to provision a Redis instance. You can create a local Redis instance using Docker:Next, we need to install DocArray, Jina, and clip-client:That’s all the setup we need. Now we’re ready to start exploring with data.The Amazon Berkeley Objects Dataset consists of product items accompanied by images and metadata such as brand, country, and color. It represents the inventory of an e-commerce website.For the purposes of this tutorial, we can download a subset of this dataset from Jina Cloud, pre-processed in DocArray format.First, authenticate to Jina Cloud using the terminal:Next,  download the dataset:This returns a DocumentArray object containing samples from the Amazon Berkeley Objects dataset. We get an overview using the summary() method:Or we can display the images of the first items using the [plot_image_sprites()](<https://docarray.jina.ai/api/docarray.array.document/#docarray.array.document.DocumentArray.plot_image_sprites>) method.Each product contains the metadata information in the tags field.Let’s take a look at the content of tags:Later, we use this metadata to filter recommendations according to the user’s preferences.To create the vector embeddings for our dataset, we first need a token for CLIP-as-service inference:Then we can start to encode the data. Be sure to pass the created token to the Client object:Encoding the dataset takes a few minutes. When it’s done, we proceed with the next steps.At this point, our data is encoded and ready to index.To do so, we create a DocumentArray instance connected to our Redis server. It’s important to specify the correct embedding dimensions and filter columns:For more information, see Redis Document Store in DocArray.In order to understand the recommendation logic, consider the following example: Eleanor decided to add a scarf to her wardrobe and looked into several ones in our shop. Her favorite color is navy, and she has to keep the budget under $25.Our recommendation function should allow specifying those requirements and recommend items based on the view history, with emphasis on the most recently viewed items.Thus, we combine embeddings of the latest viewed items by giving more weight to the recent items:Let’s implement a function that recommends products based on a weighted average of the embeddings of recently-viewed items, taking user filters into account. That is, in recommended items, we want to emphasize the items the shopper viewed most recently.Thus, we combine embeddings of the latest viewed items by giving more weight to the recent items:To show relevant recommendations when a customer views a product, we need three steps:We can achieve that with the following function:Let’s try it out a few times. First, let’s view the first item in the store and the recommendations for it:That displays an attractive scarf, labeled as ” Thirty-Five Kent Men’s Cashmere Zig Zag Scarf, Blue”:… and the accompanying recommendations:How well do they meet the user’s filter?Let’s check the third item in the recommendation list and apply a filter color=’Navy‘ to ensure a better match:…which generates a better item display and recommendations:Thirty-Five Kent Men’s Cashmere Zig Zag Scarf, Blue:Now the recommendation function returns the most visually similar items to scarves that also satisfy the filter color='Navy'.Success! And, possibly, a new e-commerce sale.The instructions above are a brief overview to demonstrate the building blocks for a process to recommend products in an online store.You’re welcome to take it further. We created a GitHub repository with source code for a product store interface with the same dataset and technique we just showed.This demonstration just showed you how Vector Similarity Search can offer low-latency real-time recommendations that respect user preferences and filter selection.But how fast is it? Let’s look at the latency numbers for recommendation queries. You can find logs in the command line console:This means computing recommendations takes about 10 milliseconds!Of course, there’s still room for improvement, especially when it comes to quality. For example, we can come up with more sophisticated ways to model the user’s profile and interests. We could also incorporate more types of data, such as  3D mesh and video. That’s left as an exercise to the reader.Vector Similarity Search is an essential technique for implementing recommendations in a real-time context.Your next steps:Use DocArray as a convenient data structure for handling multimodal data as well as for interfacing with the vector database. Consult the DocArray documentation to get started and get connected with the Redis AI/ML team for more information on Redis Vector Search.Get started with the Redis Cloud free tier."
113,https://redis.com/blog/rediscover-redis-for-vector-similarity-search/,Rediscover Redis for Vector Similarity Search,"May 18, 2022",Ed Sandoval,"At RedisDays NY 2022, we announced the public preview of our new Vector Similarity Search (VSS) capability. VSS is part of RediSearch 2.4 and is available on Docker, Redis Stack, and Redis Enterprise Cloud’s free and fixed subscriptions.In this article, I’ll walk you through the basics of vector similarity, and its applications and share resources to get you started with Redis VSS!"
114,https://redis.com/blog/scaling-entity-matching-at-the-room-with-scribble-enrich-and-redis/,Scaling Entity Matching at The Room with Scribble Enrich and Redis,"June 29, 2021",Pieter Cailliau and Dvir Dukhan and Guy Korland,"The Room’s mission is to connect top talent from around the world to meaningful opportunities. Envisioned as a technology-driven, community-centric platform to help organizations quickly find high-quality, vetted talent at scale, The Room will host tens of millions of members in its system and have a worldwide presence.At the core of the technology challenge is a mathematically difficult entity-matching problem. Each entity in the system—individuals, organizations, opportunities, and content—must be matched to other entities with high accuracy and relevance, context-sensitivity, and timeliness. The application has multiple instances of this problem.After experimentation, The Room’s ML team settled on an approach using vector space embedding of raw text data describing entities, and a combination of vector similarity matching and evolving business logic to surface similar entities. When testing the implementation, it was discovered that the core vector similarity computation was driving the compute and memory needs of the algorithm. Using Redis’ high-performance key retrieval based on nearest neighbour vector lookup, the team was able to achieve more than 15 times improvement in the core similarity computation loop without any memory overhead. In turn, this created headroom to increase the complexity of the algorithm and also deliver it on the real-time path.The overall flow of data is shown in the architecture diagram in Figure 1. While fairly standard, the key challenge was to put the entire production system in place with a small team, tight timelines, and the need for flexibility.Scribble Data’s Enrich feature store and its apps provided the overall framework for implementation of The Room Intelligence Platform which is responsible for entity matching. Enrich handled integration with approximately 15 data sources, data quality, batch, and streaming-feature engineering including vectorization, as well as integration with in-memory database backends such as Redis, cataloging, and compliance. Enrich’s pipelining and orchestration was also used for modeling, and future versions of implementations will use a standalone model database such as MLFlow.Profiling of the early versions of the matching code for The Room’s entity matching engine indicated that more than 95% of the time consumed and 90% of memory was spent in the vector similarity computation. This resource intensiveness hinted at the need for compute distribution and cumbersome optimizations. The three main challenges were:Furthermore, it was necessary to optimize for the following key constraints:Redis made available a private-preview version of their vector similarity functionality and, to demonstrate the capability, leveraged RedisGears to support key retrieval using high-performance vector similarity lookup. The interface was quite simple, involving storing and lookup using similarity matching, with straightforward integration. Cosine similarity and Euclidean distance metrics were supported to find k-nearest neighbors (k-NN) vectors stored in Redis given a query vector. Standalone tests showed that the top-k closest match retrieval time was sub-millisecond, and the duration is almost constant over a large range of vector counts. The current test implementation has some limitations such as only supporting fixed length of vectors and ability to match all vectors stored in Redis. The production version is expected to address these issues.The entity matching problem at The Room was modeled as a k-nearest neighbour (k-NN) retrieval on vectors representing entities. The number of candidate vectors (N) is large and expected to grow rapidly. The number of dimensions of each vector is on the order of a few thousand. The testing only focused on the core similarity score computation portions in the application and the speedup reported is based on this.Three approaches were tested:For approaches (1) and (2) above, a key sort was performed on the scores to get the indices of the top-k similar candidate vectors. Various values of N and k were tested. All three approaches used the same dataset of vectors. The results are shown in Table 1.We see a consistent 15 times speedup in top-k vector retrieval performance. The larger the data size, the greater the speedup. Our hypothesis is that this speedup is due to elimination of Python and Pandas overhead in compute and memory, and that this speedup can be expected in production consistently.There are a number of value propositions in using Redis’ functionality at The Room:Redis’ low latency and high-performance vector similarity computation, integrated with Scribble Data’s Enrich feature store, is being extended to address other problems. Beyond a datastore for serving online features, there are two near-term problems that Redis Enterprise is being evaluated for, including use of graphs to identify opportunities and streams to process real-time events. The Room’s own data intelligence platform benefits from this integration as it develops state-of-the-art low-latency data applications for its members and internal staff alike.The RoomPeter Swaniker, CTO of The RoomScribble DataAchint Thomas, Data Architect, Scribble DataThe Room is hiring data scientists and data engineers! See more at https://www.theroom.com/careers/Learn more about the Enrich Feature Store at www.scribbledata.io/product, and to get started developing with Redis visit https://developer.redis.com/, including with RedisAI for real-time serving at https://redis.com/modules/redis-ai/."
115,https://redis.com/blog/register-for-redisdays-new-york-2022/,Register for RedisDays New York 2022,"March 30, 2022",Henry Tam,"RedisDays 2022’s three-stop virtual tour comes to a close in New York on Thursday, March 31st, after stops in London and San Francisco. Where London’s RedisDays sessions zeroed in on the power of sub-millisecond latency and San Francisco dove deep into developer innovations, New York will wrap things up with a day focused on the power of artificial intelligence/machine learning (AI/ML) in the world of real-time data.Register to watch all the sessions below, plus, attendees will be automatically entered in a raffle for Oculus Quest 2™ VR systems and Nintendo Switches™ at the conclusion of each RedisDays. Plus, win gift cards (with mystery amounts ranging from $5 to $500) when you complete an event survey after watching the keynote or a customer session.How are financial services organizations using in-memory technologies to drive artificial intelligence/machine learning (AI/ML) innovation? Watch our New York City keynote with Redis’ Chief Marketing Officer Mike Anand, and Chief Business Officer Taimur Rashid, alongside special guest Pascal Belaud from Microsoft as they discuss how real-time AI/ML can be used to infuse IQ into applications and why a modern data infrastructure and architecture is vital to support the entire MLOps Lifecycle. Then watch the enhanced RedisFi demo that shows how you can use vector similarity search to find buying signals across tens of thousands of SEC 10K and 10Q filings.Traders live with their eyes on the clock. Real-time data is of the essence in the world of trading. The RedisFi demo shows an example of how traders can make investment decisions leveraging the signals from corporate SEC filings (10-K/Q) by using artificial intelligence/machine learning (AI/ML).In this session, Ed Sandoval, Sr Product Manager for AL/ML at Redis will be joined by Charles Morris, Head of Enterprise Data Science Financial Services team at MSFT to deep dive into how the RedisFi Vector Similarity Search demo was created. Redis and MSFT will discuss how they worked together to create an AI/ML pipeline that combines Microsoft Azure’s ML services with their SEC embeddings and new Redis vector similarity search technology.What is operational machine learning (ML) and how are businesses leveraging it to create true business value? Redis’ Chief Business Development Officer Taimur Rashid, and guest speakers Mike Gualtieri, VP, Principal Analyst at Forrester Research, and Mike Del Balso, Co-Founder and CEO of Tecton, a leading ML platform company, will discuss how businesses running on legacy technologies can infuse ML into their architecture to build the applications of the future.What challenges are companies facing as they transition from monolithic apps to microservices at scale? Allen Terleto, Field CTO at Redis, and Viren Baraiya, CTO at Orkes break down how to overcome challenges businesses face by incorporating orchestration-based sagas into their architectural designs to overcome issues around orchestration, state management, observability, error handling, and more. Listen in and learn how to scale microservices with a reliable in-memory data layer and workflow orchestrator.RedisDays 2022 comes to a close with a fireside chat between Redis CMO Mike Anand and “The People’s Shark” himself, Daymond John, CEO of FUBU, original Shark Tank cast member, Presidential Ambassador for Global Entrepreneurship under President Obama, and one of the most successful entrepreneurs in the world, Daymond John. Listen in and hear them chat about what success looks like in the age of real-time data and what it takes to be successful in business and in life.Register for RedisDays New York"
116,https://redis.com/blog/announcing-active-active-kubernetes/,Public Preview: Active-Active Deployment With Redis Enterprise for Kubernetes,"March 16, 2023",Angel Camacho,"We recently introduced Redis Enterprise 6.4.2-30 at its core, which has an emphasis on security features such as including extended client certificate validation and pub/sub access management.But it also has additional enhancements for our Kubernetes-based release. One that’s worth highlighting is how we manage Redis Enterprise inside Kubernetes clusters. We’re making that easier, and you can experiment with the features before they are publicly available. That is: We are proud to introduce the public preview of Active-Active database deployments with Redis Enterprise for Kubernetes.Redis Enterprise has had the capability to extend a database beyond one single region or single data center for some time. We call it Active-Active Geo-Distribution, though it is also known as geo-replication.What’s new here is that Redis Enterprise for Kubernetes can extend a database to include Active-Active configuration in a simplified fashion, one that fits in with the Kubernetes declarative mannerisms.Related content: Redis Enterprise Operator for KubernetesAs you surely are aware, deploying a Redis Enterprise cluster requires preparation and planning. The operator uses the Kubernetes API to automate the deployment and management of Redis clusters. The task gets more complicated when you introduce data replication that reaches beyond a single data center or, in the case of cloud infrastructure, beyond a single region or availability zone.Until now.You currently use the Operator and the new Active-Active controller to provide a repeatable and predictable deployment model for your Redis Enterprise databases. The Operator constantly monitors your cluster’s health to provide automated high availability and scalability for your data layer.But until today, deploying cluster replication across regions or data centers was a challenging endeavor, both because of the cluster itself and also because of the database and associated resources and manual processes.What’s that mean, practically? Here are a few new things you can now do declaratively, in a few lines of YAML, in an Active-Active controller scenario:Here’s a quick preview of how you would express this configuration in Kubernetes. In this example, we create an Active-Active database named example-aadb-1. It has copies on two clusters, and the database has three shards on each cluster.This feature is in public preview and has some configuration to turn it on for your environment. We encourage you to explore our Redis Enterprise for Kubernetes documentation and learn how to configure an Active-Active database. Contact us with any questions or feedback."
117,https://redis.com/blog/the-redis-partner-program-is-launched/,The Redis Partner Program is Launched!,"January 24, 2023",Ash Vijay,"Redis has improved our already-impressive program to support its business partners. The new launch offers several new features to help companies that work with us, whether for related products, cloud platforms, or software. That means good things for Redis end-users.Redis is one of the most loved and used databases in the world, with millions of downloads daily. This creates a community of open source users that spans the globe, encompassing small organizations and Fortune 1000 companies. These deployments are becoming more complex as the environments cross the boundaries of on-prem and cloud. Customers transitioning from open source software to enterprise-grade use cases, whether on-prem, hybrid, or across the cloud, need access to a stable of partners to help them navigate these deployments and operations. And that’s where the Redis Partner Program comes in.The Redis Partner Program offers marketing resources, technical support, and financial compensation programs. Redis has worked with over 200 partners around the globe to deliver our products in concert with their solutions.It’s worked well, and now we are expanding it. Our customers want a broader partner ecosystem to help them build, deploy, and run real-time applications in complex environments.As our customers’ need for real-time information grows, so do the size and complexity of their deployments. The Redis Partner Program enables our partners to grow with our customers’ success by providing our admirable real-time data platform, sales enablement, technical training, revenue sharing, and more.There are two tiers: Community and Enterprise. You can see much more information about the Redis partner program, which we invite you to explore.We’d love to partner with you. Get in touch, and let’s see what we can accomplish together."
118,https://redis.com/blog/redis-cloud-expanded-programmability/,Expanding Programmability With Redis Enterprise Cloud,"March 29, 2023",Tim Hall,"We updated Redis Enterprise Cloud with Redis Open Source 7.0 compatible features, including Redis functions and Pub/Sub ACL changes, in a limited release.Developers love Redis’ simplicity and ease of use, and we are eager to continue extending server-side processing functionality for their applications. Redis Open Source version 7.0 was made generally available nearly a year ago, and it was the first major release of Redis since 2019. One significant change to improve server-side programmability is Redis functions.Today, we are announcing the availability of Redis functions within Redis Enterprise Cloud.This first phase is a limited release. These updates are only available on Amazon Web Services (AWS) and Google Cloud Platform (GCP) for Fixed and Free subscriptions. The regions supported, as of today, are GCP Tokyo and AWS Singapore.The most significant new feature that was introduced is Redis functions, an API for managing code to be executed on the server. This feature, which became available in Redis OSS 7.0, superseded the use of EVAL in prior Redis versions.As part of this limited release, we now support Redis functions in Redis Enterprise Cloud.Redis functions have several advantages over scripting with Lua. As our developers know, Lua has limitations.First, with Lua, all client application instances must maintain a copy of all scripts in order to guarantee consistency. That means that the developer needs to create some mechanism that applies script updates to all of the application’s instances.Second, scripts are ephemeral, and a script can’t call another script. This makes it nearly impossible to share or reuse code between scripts, short of client-side preprocessing.Functions solve these problems. They serve as a single source of truth of the executable logic and they operate as first-class citizens in Redis. Functions are also persisted to the append-only file (AOF) and replicated from primary (formerly known as ‘master’) to replicas, so they are as durable as the data itself.To learn more about Redis functions and how you can take advantage of them, consult the documentation.Another important change is the default ACL access to Pub/Sub channels.ACLs for Pub/Sub channels were originally introduced as part of Redis OSS 6.2, with permissive access by default. That meant that if you didn’t explicitly block access in your Pub/Sub ACL, the Pub/Sub channel remained accessible.Starting with Redis OSS 7.0, the default access is more restrictive in response to developer feedback. Unless you explicitly allow access in your Pub/Sub ACL, the Pub/Sub channel is not accessible.To ease the upgrade process from version 6.x to 7.x, Redis Enterprise Cloud continues to support the permissive approach by default. We provide additional support within the ACL Rule Builder UI to allow you to adapt to this change at your own pace.You are not required to take immediate action when upgrading your applications from Redis 6.x to Redis 7.x. However, if you use ACLs and Pub/Sub channels, we recommend that you make your Pub/Sub ACL rules explicit for increased security. For more, see the Redis Cloud ACLs documentation.We have additional information about the Redis OSS 7.0 changes that are relevant to Redis Enterprise Cloud, and you may want to consult the complete Redis OSS 7.0 release notes.If you already have an account with Redis Cloud, create a new Fixed subscription.When you get to the vendor/region selection section, you’ll notice a new toggle for Redis 7.0 preview. Selecting it narrows the region list to regions that support Redis 7.0. (We will gradually add more regions to the list.)All the databases you create with this subscription will be compatible with the features and capabilities of Redis OSS 7.0.Don’t have an account yet? No worries! Create a new free account, and you can choose Redis 7.0 there as well.Need additional help? We’re here for you. Contact us at support@redis.com.We will expand the rollout and availability of Redis 7.0 commands, functions, and capabilities to new regions.We also plan to continue programmability improvements for Redis OSS, Redis Enterprise, and Redis Enterprise Cloud. Specifically, we are exploring triggered functions and you can catch up on the latest innovations here. We want to support the development of JavaScript-based functions that can be invoked, triggered, or scheduled based on events (keyspace notifications) driven by changes to underlying data. Expect triggered functions to be included as a preview within Redis Enterprise Cloud later this year, as well as efforts to gather feedback from developers on the ease of use across development, debugging, and provisioning.We are excited about what possibilities this unlocks for developers! Stay tuned."
119,https://redis.com/blog/learn-json-from-redis-experts/,Learn JSON From Redis Experts,"April 3, 2023",Redis,"These videos can bring you up to speed swiftly on using JSON with Redis.What makes JSON such a popular choice for building applications? Its user-friendliness and its ability to be parsed by both humans and computer systems have made it a fan-favorite among developers. Though, of course, it’s not without its pratfalls, and it’s a good idea to learn both the basics and advanced features.These webinars, livestreams, and demonstrations illustrate JSON’s core concepts. They also even put those concepts into practice so you can apply what you learn.JavaScript Object Notation (JSON) is JavaScript-native text-based syntax used to exchange and describe data in applications. One reason developers use  JSON is that it uses conventions similar to popular programming languages, such as Java and Python.In this video, Justin Castilla, a senior developer advocate at Redis, shows a simple, real-time demo of creating various JSON objects. He does so by organizing his favorite food trucks in Oakland, “but adding some extra spice with JSON path syntax and querying documents with search,” he says.Castilla sets out to map his favorite food trucks in Oakland by creating and storing JSON objects for vendors, locations, and events. “Each food truck JSON object, known as a vendor, has a name, an array of cuisines offered, a primary cuisine, and an address,” he explains. Castilla uses that information, along with location, event, and vendor data, to show how to use JSON object types. And then Castilla puts it together to demonstrate how you can use JSON to discover which food trucks will be at certain locations during specific events.It’s a simple and concise walkthrough for anyone looking to efficiently query and index JSON documents.Also, if you’re hungry and don’t want to write a new application from scratch, note that there’s an entire website to help you find Oakland food trucks.This session, recorded during RedisDays Seattle 2020, was presented by Jay Won of Coupang, who unpacked the question: Why JSON?Though this session covers how Asian e-commerce vendor Coupang has benchmarked RedisJSON and its findings, Won does a wonderful job of navigating the conversation to show how the company uses JSON to assist Coupang with properly executing its advertising platform. He then goes into how JSON works, including when to choose popular commands, such as Strings versus. Hashes when using JSON to build applications.“Our data model is JSON-formatted, so if you ask ‘Why JSON,’ the answer would be simple – because JSON is human-readable and it’s a universal format,” Won explains. “So most of the programming languages and tools support JSON; [it’s] very convenient to carry around.”Watch the video to hear the key findings from Won’s benchmarking exercise.Redis Launchpad is an application hub for developers and architects in the Redis community to explore high-quality sample applications that operate various “architectures, data modeling, data storage, and commands, allowing you to start building fast apps faster,” we noted in our introductory blog post. Many of the applications in this hub were built using JSON.This Redis Launchpad video demonstrates how to build a self-hosted, real-time JSON document store (cheekily named Redis Store for the purposes of the demonstration). It has the same functionalities as Firebase’s Firestore, a tool that helps build applications without a single line of server code.The caveat, the speaker notes, is that “Firebase is a great tool as long as you structure your code to avoid exorbitant bills, and even then, your project and data are permanently locked into Google Cloud.”Looking to replicate your own cost-saving solution? Follow along to create a JSON document store where “you can build a basic real-time chat app from scratch in less than five minutes.”How many of today’s applications are restricted by the strict schemas of relational database management systems (RDBMS) and the sluggish speed of disk-based document databases? RedisDays London 2022 was all about showcasing the advancements and new use cases for real-time data, with one session, in particular, focusing on the evolution of JSON document stores.In this RedisDays London session, Ash Sahu joins Pieter Cailliau, Senior Director of Product Management at Redis, to demonstrate common JSON usage patterns. Among them: real-time inventory management for product cataloging, financial services for fraud detection, online gaming for establishing user profiles, digital mobility to match drivers with riders, and cybersecurity to establish endpoint protection. They also point out the object mapping libraries that can be used in conjunction with JSON to build applications.Hear about different ways you can solve business needs using a query accelerator (used to store frequently accessed JSON data), how developers can use a JSON document store as a front-end database (to accelerate read and write queries), or as a primary database (as a distributed, in-memory JSON document database).As a fun JSON bonus, watch this livestream demo in which Justin Castilla takes a large group of data containing bird sightings, with species and location data points, and converts it into an easy-to-digest document database.Need a solid JSON primer to keep on your bookmark tab? Take a look at A Cat Lover’s Guide to Understanding JSON Databases, which breaks down how JSON databases function and how they’ve simplified the application development process – with examples from a cat’s viewpoint!If you’re ready to experiment with JSON, give this tutorial a try: Exploring a Securities Portfolio Data Model Using Native JSON and Query Capabilities; it demonstrates how to fine-tune a brokerage app using a JSON data structure. Or try your hand at Building a Fast, Flexible, and Searchable Product Catalog with JSON."
120,https://redis.com/blog/redisinsight-new-diagnostic-and-support-features/,RedisInsight Introduces Diagnostic Features and Support for Search Capabilities,"January 25, 2023",Olga Lopaci,"RedisInsight is an ideal tool for developers who build with any Redis deployments – including Redis Open Source, Redis Stack, Redis Enterprise Software, Redis Enterprise Cloud, and Amazon ElastiCache – and who want to optimize their development process. RedisInsight lets you visually browse and interact with data, take advantage of the advanced command line interface and diagnostic tools, and so much more. Best of all, RedisInsight is free for everyone.The latest version of RedisInsight has new UI controls to perform full-text search across your data, a new database analysis tool, data formatters, visual support for Redis streams, bulk deletion, and Slow Log tool. We’re excited about all of these, and we think they can help you a lot.Use the database analysis tool, its dashboards, and its recommendations to optimize the performance and memory usage of Redis databases. Among its features:If you’ve been using Redis for indexing, try the latest Redis Stack, which includes – among other strengths – impressive query and search capabilities.We added more functionality in this release. RedisInsight now complements Redis Stack with UI controls to quickly and conveniently run search queries against a pre-selected index. You can also create a secondary index of your data in a dedicated pane.You can view, validate, and manage key values in a format suitable for humans, not just for computers. The Browser tool has new formatters to prettify and highlight data in different formats, including Unicode, JSON, MessagePack, HEX, and ASCII.Take advantage of the visual support for Redis streams to create and manage streams. You can add, remove, and filter entries per timestamp. You can see and work with new entries and enable and customize refresh rates.That’s just the start; we expanded the number of ways to view and control the way you work with Redis Streams. You can view and manage consumer groups. You can see existing consumers in a given consumer name as well as the last messages delivered to them. Plus, you can inspect pending messages, explicitly acknowledge processed items, or claim unprocessed messages.The new RedisInsight makes it fast and easy to clean up databases. You can delete multiple keys of the same type or with the same key name pattern in bulk.One frustration in troubleshooting performance issues is determining which tasks are bogging down the system. The Slow Log tool displays the list of logs captured by the SLOWLOG command to analyze all commands that exceed a specified runtime. You can specify both the runtime and the maximum length of Slowlog to configure the list of commands logged. You also can set the auto-refresh interval to automatically update the list of commands displayed.Want to learn more? To get the latest version of RedisInsight:Learn more about the ways RedisInsight can help you from the latest RedisInsight release notes. We’d love to hear your feedback."
121,https://redis.com/blog/microservice-architecture-key-concepts/,Microservice Architecture Key Concepts,"April 5, 2023",Redis,"What role do microservices play in creating applications? We offer a foundational understanding of what microservices are, how they differ from monolithic structures, and what to consider when you evaluate microservices for your own adoption.Microservice architecture is a framework that represents different parts of a workflow or process, usually involving one area of a business. For instance — take a retail app that involves a shopping cart, order management, payments, checkout, product catalogs, back-end finance/accounting integration, and customer services support. These services are uncoupled, representing different knowledge domains, though they do need to intersect with one another to have a fully-functional microservices application.Independent, autonomous software development teams manage, test, and deploy these uncoupled services without affecting the rest of the overall system. The result of these independent-yet-connected services is a framework that works as a larger architecture. When it is implemented well, this compartmentalization makes operations more flexible by allowing teams to create their own release cycles, quickening production, and reducing time-to-market by simplifying operations with reusable artifacts.A monolith is a single massive unit. Think Ayer’s Rock in Australia (its aboriginal name, “Uluru,” means “giant monolith”), the Great Sphinx, or Michelangelo’s “La Pieta.” The point is: monoliths often are hefty structures.In that vein, a monolithic architecture is one that connects all the different business units of an entire application into one single stack that shares the same code base. This structure makes it particularly difficult to scale any separate components independently. Take a publishing site, for instance, where users request articles very frequently, but new articles are published infrequently by comparison. In a monolith, you can’t effectively isolate and scale article reading from publishing.A microservice architecture will segment disparate business domains, allowing independent teams to choose the tech stack they believe they need for continuous successful releases.Microservices enable teams to scale their entire application and to answer customer demands faster with updates that don’t require entire stack overhauls. These are some of the key characteristics that define how developers can establish microservices.In any company, each business unit has its own set of business challenges and objectives associated with that specific application. In most cases, each development team has the autonomy to choose the tools, technology, and resources that help them achieve their scope of work within their bounded context. This architecture works perfectly well with a polyglot technology stack, so developers have ample flexibility when building microservice applications the way they want in order to meet the performance, availability, continuous release cycle, and automated testing goals. In microservices, the “master of your domain” adage rings especially true, as each team is segmented by functions and ultimately responsible for what they build. Creating modular teams working on specific services within the overall microservice application helps to accelerate time to market.One successful practice is domain-driven design, which encourages developers to understand their domains in-depth so that the software development serves their users’ needs. In this case, the users are not always other humans, though; other related services need to be served, too. While a development team may have a lot of autonomy, it does need to work seamlessly with other development teams.The pieces have to fit together, and they need consistent interfaces. Many companies implement decentralized governance to ensure that software stays in compliance across the organization, often with the help of subject matter experts (SMEs), such as enterprise architects.Sometimes, parts of a large microservice application fail. Even with loosely coupled functions deployed as multiple microservices, it is important to keep the communication lines open. If a single microservice fails, the microservice architecture needs a fallback behavior that is implemented at the code level to account for this disruption. Otherwise, a “minor” failure can create a domino effect of failures with all other microservices.A fault can arise from any number of causes, from an unavailable server to an inaccessible database. Fortunately, there are ways to sidestep these challenges. Among them: replicating instances, adding timeouts on requests, and using libraries that use fault-tolerant design patterns to fortify the resiliency of a microservices application. One way or another, design your microservices with the assumption that things will fail at the worst possible time – and ideally, only one component goes down.According to Conway’s Law, coined by computer scientist Melvin Edward Conway, “Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.” Expect your stack to reflect how your company is organized and factor that into your design process.In particular, do your best to keep systems in your microservices architecture as loosely coupled as possible. You don’t want changes to the design, implementation, or behavior of one element to cause changes in another. It’s a bad idea to change to one microservice to enforce an almost immediate change to all other microservices that collaborate with it directly or indirectly.If multiple microservices are built with the heavy dependencies that occur at an organizational level, domains lose their autonomy and, with that, the ability to produce their own release cycles. It’s important to strike the right balance.The inter-service communication methods between a monolithic and a microservice architecture are quite different. Components are hard to scale independently in monolithic applications and are highly coupled. Microservices are loosely coupled, and they can use different inter-service communication mechanisms to exchange information, such as REST APIs or some type of message queue system.A monolithic architecture already has all components working together with possibly one or two primary databases. Each microservice has its own instance, and they don’t necessarily share the same data stores; that’s part of domain-driven design and polyglot persistence — you can select the right data store for each microservice.Microservices use either synchronous or asynchronous communication, meaning the component waits for a reply (synchronous) or it doesn’t (asynchronous). An example of a synchronous communication scenario would be an authentication service for validating a login and password; the service requires a response in order to allow the user into the microservices-based application.On the other hand, asynchronous communication isn’t bound by time, and the sending service requires no reply from another service(s) For example, in e-commerce, when a customer checks out, you want them to go about the rest of their day. They don’t need to wait for you to process the order, receive payment, and eventually fulfill the order. This is all done asynchronously, and the customer is informed of progress through notifications (in-app push notifications, emails, or an order status page).Generally, you can use a message broker for asynchronous communication between services, though it’s important to use one that doesn’t add complexity to your system and possible latency if it doesn’t scale as the messages grow.Version your APIs: Keep an ongoing record of the attributes and changes you make to each of your services. “Whether you’re using REST API, gRPC, messaging…” wrote Sylvia Fronczak for OpsLevel, “the schema will change, and you need to be ready for that change.” A typical pattern is to embed the API (application programming interface) version into your data/schema and gracefully deprecate older data models. For example, for your service product information, the requestor can ask for a specific version, and that version will be indicated in the data returned as well.Less chit-chat, more performance: Synchronous communications create a lot of back and forth between services. If synchronous communication is really needed, this will work okay for a handful of microservices, but when dozens or even hundreds of microservices are in play, synchronous communication can bring scaling to a grinding halt. Perform a microservices audit to determine where synchronous is required and where it isn’t. Asynchronous methods, such as using a message broker, will help reduce dependencies, improve overall fault tolerance, and alleviate performance sluggishness.Note every change (and keep tidy): A service registry is a manifest that notes the locations (through IP addresses, for example) of any available instances. IP addresses are amorphous – they can change with rapid cadence, from one minute to the next – so document the system and its changes.A monolithic application has only a couple of entry points, notes Prabath Siriwardena and Nuwan Dias, the authors of Microservices Security in Action. But applications written with a microservices architecture offer far more. “As the number of entry points to the system increases, the attack surface broadens too,” they write. “This situation is one of the fundamental challenges in building a security design for microservices. Each entry point to each microservice must be protected with equal strength. The security of a system is no stronger than the strength of its weakest link.”Writing secure code is important for even the most trivial application, but it’s even more of an issue with a microservices architecture. The broader the attack surface, the higher the risk of attack.Unlike in a monolithic application, each deployed microservice has to carry out independent security screening – and that has an effect on performance, the book authors point out.Test in a controlled environment: A sandbox is a safe and controlled environment where developers can test their code before it goes into production. That makes a difference in security testing as well as helps you discover hiccups in the connection between services.Deploying with Kubernetes? Use an operator: Kubernetes is a popular choice for deploying containerized applications and spinning up additional microservice instances but can require extensive configuration from DevOps to properly deploy and secure. Having a Level III Kubernetes operator for a particular software component, e.g. database can help automate and secure deployment. Even Kubernetes Secrets, which help keep sensitive information separate from an application’s code, are stored unencrypted in the API’s data store, readily accessible for anyone with authorization to create a pod in that same namespace.Hopefully, the above serves as a decent primer for understanding the basic concepts of microservices. We covered how a microservices architecture differs from monolithic applications, how software development teams communicate under both models, and the autonomy microservices enable for developers and best practices for fortifying data with proper API design practices.If you’re new to working with microservice architectures, read our 5 Microservices Misconceptions post to help you sidestep the most common microservice trappings in application development."
122,https://redis.com/blog/5-microservices-misconceptions/,5 Microservices Misconceptions,"February 28, 2023",David Gaule,"Many misconceptions about microservices persist, often to the detriment of companies hoping it’s the silver bullet to solve all their problems.A drive to be perceived as cool and trendy isn’t reserved for awkward teenagers. That motivation can also cause full-grown business executives to chase the latest shiny, new technology just because the cool (and successful) companies are using it.That’s not to say that any given technology lacks value. However, sometimes the new item—such as microservices, Kubernetes, Lambda functions, or [insert your own cool, new tech du jour]—is embraced or rejected for all the wrong reasons. Too often the adoption comes from people who know the terms only as buzzwords rather than the technology’s benefit as an actually useful solution that applies to a genuinely annoying problem. Which may not be theirs. That leads to hyperbole, and that’s unfair to the tech at hand.Microservices is today’s victim of corporate mythology, where software assumptions create barriers to sensible adoption and usage. It’s time to clear the air with the help of technologists who have hands-on experience. These are the microservices misconceptions they encounter most often.Microservices are great – as long as they solve a problem your company actually has. The old adage is relevant here: “When all you have is a hammer, everything looks like a nail.”“Many companies think, ‘We have this application landscape, and microservices are the answer,’” says Lars Rosenquist, a partner solution architect at Redis. “But first you need to understand what microservices are; next, you need to understand the problems of your own application architecture; then there needs to be a match between the two. And there rarely is.”Many technologies solve specific problems at a specific scale. “If you’re a company like Netflix and your applications consume a third of internet traffic, you’re going to have to do things with your architecture to make that work,” explains Rosenquist. “But if you’re a smaller company, such as a local bank, you’re not operating at that scale—so you don’t need to fix that problem. People often optimize for the problems they want and not necessarily for the problems they have.”Most use cases do not require microservices. Avoid the “monolith is bad, monolith is the old way of doing things” mentality, suggest several programming experts. Instead, try to prove yourself wrong about that premise before you adopt a microservices approach.In fact, a microservice architecture almost always adds complexity. Worse: If left unchecked, it turns into a distributed monolith – and that gets messy.Microservices do not fix system design problems. “If you can’t create a monolithic design with high cohesion and low coupling, you’re likely to do worse with microservices,” says George Dinwiddie, a software development consultant, and coach at iDIA Computing.Start with a monolith, and then extract services from it as you add teams and suggest several developers. Conway’s Law applies.“With a monolithic application you have all the logic, communication, and complexity in one application,” says Rosenquist. “With microservices, you turn this inside out.” The result: All these communication patterns and such are now part of your infrastructure.Many companies don’t really understand this problem, Rosenquist believes. They think they can make the complexity go away with microservices. But you don’t really make it go away; you just move it to a different layer of abstraction.“Let’s say you have two application components communicating with each other. They live in the same application, in the same process, in the same CPU. That application is pretty fast and is not very likely to fail,” says Rosenquist. But with microservices, you put those application components on different machines, with a network in between them, a whole battery of infrastructure, and maybe even different clouds. “That becomes a completely different challenge—not just in terms of communication and failure modes but also in terms of performance.”Managing the complexity has become easier with the tooling that’s now available, but don’t discount that the complexity still exists. For one thing, complexity makes it harder to debug applications. To make managing that complexity somewhat easier, ensure that your monitoring and observability work is in place.While a microservices approach certainly makes your code less complex, Rosenquist points out that it’s not always easier when it comes to deploying or managing it. “Instead of having one application to debug, you now have six or ten. Those applications are also running on multiple instances, which are also load-balanced across the entire architecture.” To assemble all that, pay attention to things like log aggregation and observability. All of which are generally more complex than having a single application.Less complex code does not necessarily translate to a less complex system. “From a code standpoint, individual services might be slightly easier to understand in a silo,” points out William Johnston, developer growth leader at Redis. “But the complexity of the system you create with microservices becomes way more complex than in a monolith.”That also means more development time, particularly in smaller teams that are already overloaded. That’s not always a bad thing because it forces developers to learn the application domain and the entire system. That can make refactoring easier in the long run since it makes the whole system less coupled. However, that comes with a high cost, and developer productivity might drop significantly.That affects quality assurance, too. Typically, a microservice has so many dependencies that it makes it harder to test.Taking it slow isn’t always possible for teams that need to meet tight deadlines or that may lack the background to adopt the technology quickly. “[Microservices] can be overwhelming technology for a smaller group of engineers or groups unfamiliar with it. It’s quite a different way of operating,” says John Obelenus, a software engineer at Boston-area startups.Don’t imagine that you can take one architecture and apply it to your entire landscape. Nor should you assume that you can immediately begin refactoring your old applications. Your old applications may not support these kinds of patterns, Rosenquist says.Distinguish between monoliths, microservices, and functions. A function does only one thing. They are really for specific architectures because they are more event-driven, says Rosenquist. That’s the case, especially with enterprise processes. If you decompose all your systems, you end up with a thousand things, and all your advantages are lost.The best approach really depends on the specific use case. A bank, for example, is apt to have quite a few monoliths, says Rosenquist. “But if you look at the most run-of-the-mill applications that are powering your mobile banking app, that’s going to be microservices—say around 50% to 60%. Functions make up the remainder. So a typical enterprise will end up with a diverse landscape in terms of architectures, not a single one.”At the end of the day, it really comes down to the people who build your software. “A great engineering team finds a way to be effective regardless of what software architecture or pattern you’re using,” says Johnston, “And a dysfunctional team will find a way to screw something up, whether it’s microservices, monolith, or anything else.”And speaking of teams …“One of the most common misconceptions about microservices is that they solve a technology problem,” says Johnston. “But really they solve a business or organization scalability problem.” If microservices don’t accomplish that goal, it’s a waste of time—and not the technology’s fault.“I see a lot of people who are starting a new application, even as part of a large business, and they believe they need to start by separating the domains and start with microservices,” Johnston points out. “But if you don’t have a clearly defined business domain, you don’t want to start with microservices.”To once again cite Conway’s Law, the communication patterns in your software landscape mimic those of your organization as a whole. “Say you want to go from a monolithic architecture to a microservices architecture,” says Rosenquist. “Look at how your organization is organized. Split up the teams to make sure that one microservice belongs to one team, for example.” This is a management issue more than a technology challenge.Make sure that everyone understands what microservices are and how the technology is envisioned to solve the business problem. Different departments often have differing viewpoints on the nature of microservices. The advantages of adopting a microservice architecture are going to depend on the problem space and the organization adopting the technology.This isn’t a full-on misconception, according to Johnston and Rosenquist, because there is some truth to this perception. It’s just not always a good idea for teams to use different platforms.One common perceived benefit of microservices is that everyone can use the languages, tools, and platforms they prefer. That does make sense in some ways, says Johnston. “If you have a very large organization with several teams distributed throughout the world, some teams may excel at .NET, for example, and others at Java. Then depending upon the business domain for their particular service, it might benefit them to use one technology over the other.” But, he cautions, support for additional development environments comes at the cost of increased complexity in the system. “This is why enterprise architects exist,” says Johnston. “They need to understand the system complexity.”“Yes, you can choose a different technology for every microservice you have, but I really suggest you don’t,” advises Rosenquist. “If you have 500 microservices, you shouldn’t have 500 different technologies. Somewhere around five would be a better number.” Those are examples, of course; there is no one-size-fits-all approach. Use common sense and find a middle ground between what’s manageable and what holds developers back.While the misconceptions we shared in this post are definitely some of the more prevalent ones floating around, they are by no means the only ones. Some other microservice myths that our experts raised include:If you didn’t harbor any of these misconceptions, congratulations—you’re an expert yourself. But if you did, don’t feel bad. You’re not alone. And we hope that you can now move forward with a higher degree of confidence, speak with more authority on this topic in your company, and embrace the benefits that microservices can offer.Ready to adopt microservices in your shop? Learn how Redis can help."
123,https://redis.com/blog/redis-2021-gartner-magic-quadrant-cloud-database-management-systems/,Redis Recognized in 2021 Gartner® Magic Quadrant™  for Cloud Database Management Systems,"January 6, 2022",Steve Naventi,"Over the past two years, the database market has transformed dramatically. As we started 2020 we were still in the early stages of moving data processing and analytics to the cloud. Within a few months, businesses have accelerated from the crawl and walk phase with cloud initiatives to running and sprinting in modernizing their existing apps and launching new greenfield apps.To quantify this shift, Gartner reports, “By 2022, cloud database management system (DBMS) revenue will account for 50% of the total DBMS market revenue.” This has been pushed up a year earlier than Gartner forecasted in the inaugural “Magic Quadrant for Cloud Database Management Systems.”With this as a backdrop, we’re honored Redis has been recognized by Gartner for the second consecutive year as a Challenger in the December 2021 “Magic Quadrant for Cloud Database Management Systems.” Subscribed members can access the report here.We believe this acknowledgment is a reflection of our “One Redis” vision––to deliver a platform that can provide our customers not only with the speed, but also the ability to build their apps seamlessly, flexibly across cloud providers, and scale them across the globe. We’ve designed Redis Enterprise Cloud to be a true real-time data platform that allows developers, cloud architects, and DevOps practitioners to deploy both enterprise caching and primary database capabilities as a single solution that can address a myriad of use cases.As Gartner states, “The initial move by vendors to the cloud is now nearly over, with developments now more about exploiting the cloud. The cloud-native vendors began by offering simple cloud offerings and are now expanding them by providing more-sophisticated features.”Gartner delves into greater detail in the accompanying December 2021 report “Critical Capabilities for Cloud Database Management Systems for Operational Use Cases.” It evaluates vendors for traditional transactions, augmented transactions processing, stream/event processing, and operation intelligence use cases. These use cases capture usage scenarios for end users for operational needs.Redis Enterprise Cloud not only met the inclusion requirements for all four use cases, but ranked within the top 10 vendors in each use case in Gartner’s evaluation. We also were in the top five for the Operational Intelligence use case with a score of  3.47 out of 5 (as of December 1, 2021). We believe this validates one of our key initiatives and customer use cases in 2021 and beyond.Ultimately we’re taking what inspired the creation of Redis––speed, simplicity and ease of use––and applying it to a company’s data infrastructure in their application architecture. The last thing developers and operators need is the complexity of running several different pieces of infrastructure; each piece having a learning curve, configuration issues, operational quirks, and more as they are asked to move faster, while keeping their apps available with 99.999% uptime.Respected third-party evaluations and recognitions are important, but ultimately what matters is the feedback and input from our customers. We’re thrilled to have achieved over the past 12 months a 4.6 out of 5 rating and 100% “would recommend” on Gartner Peer Insights™ as of December 15, 2021. This positive customer feedback plays an important factor in Gartner’s evaluations for the “Magic Quadrant and Critical Capabilities” reports.As an Associate Director in Enterprise Architecture and Technology Innovation for a $500M+ Services company states, “Our application required a robust caching solution as part of a new rollout and Redis was the obvious choice for us as it supported all of our requirements and met our needs for flexibility in deployment. I also really appreciated their pre-sales team who did not try to pressure us to buy solutions we didn’t need and were quickly able to answer our numerous questions.” (Read the full review here.)In another review an Analyst for a $30B+ enterprise comments, “The system is well-structured and performs properly. In my perspective, it is straightforward and convenient to use. The solution and service appeal to me. In my perspective, it is simple to work. It has a lot of benefits and none of the disadvantages. Redis Cloud makes scaling your demands a breeze with only a few clicks. It is quite inexpensive, the cost is reasonable, and the service is simple to use and widely available.” (Read the full review here.)Finally, a Software Engineer from a $10B+ Services firm shares, “We were in need of a cloud-based database management system. Redis Enterprise Cloud provides the best possible solution and data storage capability. It’s a reliable, consistent and secure application over cloud. Must go for it if you need one.” (Read the full review here.)We appreciate our customers’ support and feedback, so please keep the reviews coming!Gartner does not endorse any vendor, product or service depicted in our research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of the Gartner research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.Gartner Peer Insights reviews constitute the subjective opinions of individual end users based on their own experiences and do not represent the views of Gartner or its affiliates.Gartner, Magic Quadrant for Cloud Database Management Systems, Henry Cook, Merv Adrian, Rick Greenwald, Adam Ronthal, Philip Russom, 14 December 2021Gartner, Critical Capabilities for Cloud Database Management Systems for Operational Use Cases, Merv Adrian, Rick Greenwald, Adam Ronthal, Henry Cook, Philip Russom, 14 December 2021GARTNER and Magic Quadrant are registered trademarks and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. All rights reserved."
124,https://redis.com/blog/what-is-dbaas/,What is Database-as-a-Service (DBaaS)?,"January 7, 2022",Ajeet Raina,"Database-as-a-Service (DBaaS) is a hosted or managed cloud service model that lets users and companies easily access database services without managing software or infrastructure as service providers handle the database management.Relational and non-relational databases (or NoSQL databases) constitute the two main database types. Redis, for instance, is a NoSQL database.Relational DatabasesRelational databases are a mainstay based on their long history, however a more traditional relational database like a SQL database does not allow users to work with data as close to the application as a NoSQL database.Non-Relational DatabasesNoSQL databases were initially developed as open source and were designed for large-scale data storage.  In addition they allow for parallel, high-performance data processing.Read more about the difference between a relational database and non-relational database and why your SQL server needs Redis.DBaaS providers host all your database infrastructure and data while enabling access through API endpoints. They follow best practices and operate all the databases, which means they take care of rapid provisioning, scalability, resiliency, failover, backup, and restoration.In addition, DBaaS providers normally offer various features such as monitoring, alerts and notifications, round-the-clock support, and geo-replication for availability and backups. All maintenance and administrative tasks are handled by the service provider, freeing up users to benefit from using the database without the overhead of managing it.DBaaS provides a zero-management and cost-effective solution that gives developers and companies faster, scalable deployments while reducing IT costs. The major benefits of DBaaS include:Business agility refers to the distinct qualities (such as adaptability, flexibility, balance) that allow organizations to respond rapidly to changes without losing momentum or vision. Cloud DBaaS applications are agile in nature, so they adapt seamlessly to any upgrades according to business or technology advancements. DBaaS allows rapid provisioning of database resources to provide new computing resources and storage facilities in the minimum possible time. Together, this adds agility and flexibility to development teams, no matter their size or industry.The result is that developers and DevOps teams don’t have to bother with complex and time-consuming database provisioning and management, which can take anywhere from several hours to days to complete. Instead, they can focus on delivering on the business’ goals.Security is one of the most critical challenges in the DBaaS domain. As more and more enterprises host their data in the cloud, it’s crucial for DBaaS providers to prevent unauthorized access to data resources, disallow misuse of data stored on third-party platforms, and ensure data confidentiality, integrity, and availability.DBaaS providers typically offer enterprise-level security that supports encryption and multiple layers of security to protect data at rest, in transit, and during processing. It also improves your data center security by adding strategic layers of protection that keep your data and systems safe. DBaaS providers offer SLAs that guarantee integrated access management and control of regulatory compliance standards. They also offer end-to-end network security with micro-segmentation and virtual private networks.The DBaaS model provides automated and dynamic scaling. DBaaS providers adapt to workload changes and are able to manage load variations by increasing resources during peak hours without any service disruption, or by allocating fewer resources during periods of non-peak usage to help reduce costs. Users can quickly add storage and computing capacity to meet high processing demands while also defining usage threshold policies for how the system should behave during demand fluctuations.Modern applications are, by design, created in a more modular style. They can span multiple cloud providers or consume services from multiple clouds. Most of the time organizations focus their investment in a single vendor’s technology stack and hence rely on a single cloud provider. However, to increase agility and improve performance, your data layer has to span bothRedis Enterprise for example works on all the clouds to preserve operational flexibility. Read more about the growth of cloud computing and Kubernetes cloud adoption.In today’s fast-paced digital world, maintaining 24×7 operational uptime is a must for any modern business. Outages are directly proportional to the loss of revenue. As digital transformation becomes more and more essential, it’s increasingly important that your application service should remain up 24/7 without any downtime. If there’s any kind of system failure, the system should be intelligent enough to recover from the loss in no time.The DBaaS model provides maximum high availability and runs at peak performance. It provides zero or no-data loss tolerance and eliminates a single point of failure. In case of any kind of failure in a single database instance, the platform automatically reroutes traffic to a replica/standby instance and maintains uptime. The model is intelligent enough to accept the database connections and allow enterprise developers to perform database queries even in the case of a system failure.  For developers building modern applications leveraging microservices with serverless and containers architecture, it is critical to consider the impact on application availability.Not all Database-as-a-Service providers are the same. They differ significantly across a wide range of characteristics and capabilities. It’s important to select the right DBaaS provider for your organization, one that meets the technical requirements of your application workloads.Choosing the correct Database-as-a-Service (DBaaS) environment is critical to the success of any cloud-based database management system (DBMS). And this is an increasingly significant decision: The global cloud database and DBaaS market is expected to grow from an estimated $12 billion in 2020 to $24.8 billion in 2025, according to a recent Research and Markets report.What’s driving that growth? The ever-increasing demand to process queries with minimum latency.But when you look at the increasingly competitive DBaaS market, along with rapid advancement in cloud database architecture, technology, and features, it becomes clear that organizations must perform a detailed analysis of the competitive offerings and select the most appropriate DBaaS for their technology stack.Redis Enterprise Cloud is a cost-effective, secure, and fully managed Database-as-a-Service solution. Whether your organization is fully hosted in the public cloud or in its own virtual private cloud (VPC), Redis Enterprise Cloud is perfectly suited to power the modern cloud native data layer.Each of the major cloud providers today offers its own Redis managed database service, with those versions based on the open source Redis. Redis Enterprise Cloud is based on the proven Redis Enterprise technology, which serves the thousands of customers of our Redis Enterprise software products. Redis Enterprise can be deployed as a fully managed DBaaS over Amazon Web Services (AWS), Microsoft Azure, or Google Cloud; as a managed Kubernetes service over Amazon Elastic Kubernetes Service (EKS), Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE); as software on bare-metal, virtual machines, Red Hat OpenShift Container Platform, or Pivotal Kubernetes Service (PKS); or in a hybrid model to preserve operational flexibility and avoid vendor lock-in. With Redis Enterprise Cloud, you can deploy quickly on any major public cloud and create Redis databases that are fully compatible with open source Redis clients.Running Redis Enterprise as a managed service is the fastest way to deploy Redis Enterprise and get immediate time to value. Redis Enterprise is available via many cloud vendor marketplaces, including the AWS Marketplace, Microsoft Azure Marketplace, and Google Cloud Marketplace.With modules, Redis Enterprise Cloud eliminates the need to operate and maintain specialty databases for every use case. Redis Enterprise incorporates ten data structures and several purpose-built modules to provide best-in-class performance across use cases. Additionally, RedisGears, a serverless in-database engine, supports transactions and trigger-based events across both Redis core and Redis modules with sub-millisecond latency.Get started for free today"
125,https://redis.com/blog/importance-of-database-persistence-and-backups/,The Importance of Database Persistence and Backups,"June 17, 2022",André Srinivasan,"We recently had an opportunity to work with the Cohesity team to verify integration between Helios and Redis Enterprise as a step in onboarding into the Redis Technical Partner Program.  Cohesity SmartFiles, a service that runs on Helios, provides a single view and global management of unstructured data, irrespective of where the data resides. In the context of Redis Enterprise, SmartFiles provides a single view of database snapshots.Why do we bother with persistence and backup? It is true: If Redis is used as a cache and can be rehydrated from a backend database with minimal system impact, there is no need for either. On the other hand, consider when Redis is used as an operational database to provide fast access to slow data (think: transaction history, inventory count, customer lookup). The Redis database needs to quickly recover from hardware issues or even failover to another datacenter.All data in a Redis database is stored and managed exclusively in either RAM or RAM + Flash Memory (Redis on Flash); it is at risk of being lost upon a process or server failure. There are several strategies that can be followed to mitigate this risk: High Availability (HA) databases with primary and secondary data shards, geo-distributed active-active databases, and of course disk based persistence. From a performance perspective, it is always preferred to failover from a primary database shard to a synchronized secondary database shard, and this is how Redis Enterprise works; only when the primary and secondary shards are lost is the database restored from disk. Similarly, in the case of a datacenter failure, failing over to a synchronized active-active database in another data center will be faster than recovering from disk.To the question of persistence, Redis supports append-only files (AOF) for better durability, though requires more resources, and snapshots (RDB), which are less durable while requiring fewer resources. As with all things, there are tradeoffs that can be further explored in the Redis Enterprise documentation. For the purposes of this article, we will assume we want AOF for better durability.Redis Enterprise relies on persistence for recovery when both the primary and secondary database shards are lost. If active-active is not used and the racks housing the databases along with attached storage are lost, then we are left with restoring from a backup. In the case of Redis Enterprise, a backup is always a snapshot, which, when coupled with AOF for persistence, creates a resilient and durable data platform.This is where Cohesity SmartFiles comes into play; Redis Enterprise utilizes the S3 compatible endpoint conveniently provided by SmartFiles for managed backups across the clouds. To set this up, we use the Redis Enterprise REST API with the following assumptions:Enable database persistence.Configure the cluster to use the S3 object store by setting the S3 URL. Note the bucket name is not included in the S3 URL.Configure the cluster S3 backup for privacy only.Validate the backup location. Note the only response on success is an HTTP 200 OK.Create a backup. Note the only response on success is an HTTP 200 OK.Configure a recurring backup every 30 minutes. Note the only response on success is an HTTP 200 OK.In a frictionless world, Redis Enterprise configured with HA and Active-Active saves you from worrying about persistence and backups. We don’t live in a frictionless world and we need both persistence and backups (along with HA) when Redis is expected to recover from system failures. It is common to see an S3 backup endpoint as can be seen by the appliance offerings from the likes of Dell EMC, NetApp, or Pure Storage. Fortunately, the verified integration with Cohesity SmartFiles makes it easy to utilize this endpoint and to include backups in operational strategies."
126,https://redis.com/blog/redis-enterprise-release-using-helm-charts/,An Introduction to the Helm Tool and Helm Charts,"September 12, 2022",Angel Camacho,"Manual deployments of Kubernetes clusters can be a real hassle. Installing and maintaining a cluster is a complex task unless there are automations to fast-track the process, and the potential for error is high. To mitigate these complications is Helm, a Kubernetes package manager, and Helm charts, a blueprint that defines how clusters are deployed. We explore the pros and cons of a Redis Helm chart as well as how the new Redis Enterprise Operator for Kubernetes solves most of the pain points that arise with using the Helm package manager.Developers rely on open source interfaces like Kubernetes to manage and deploy containerized applications. Applications and services run in a container and are deployed in a cluster. Keeping everything working right is a process that requires attention to detail. There’s a lot to keep track of: managing configurations, using the proper commands, choosing the correct values, and packaging the right application files. It’s easy to get wires crossed.Helm is an open source Kubernetes package manager that uses configurations and the defined values within manifest files to enable the efficient deployment and installation of an application or service in a cluster.Developers use these manifest files to specify every configuration necessary for a successful deployment. These are executed with commands using a command line interface (CLI).But to deploy a single container, a developer only needs a simple manual process. Once the need for multiple containers arises, it’s probably best to begin executing with configuration files. If your architecture requires a cluster (which may need multiple containers or pods), those configuration files are best folded into a Helm chart.Get started by performing a Helm install.To properly install an application in a Kubernetes pod or cluster, you need the set of pre-configured resource definitions that come packaged in a Helm chart. This chart contains the customized information for many scenarios: everything from running anything from a small application to a complex database within a cluster.Helm charts are a sound choice for development teams. The developers can sidestep deploying from scratch and start with a bespoke configuration that reduces the risk of misconfiguring resources.Helm charts are stored in a repository. A chart repository enables Helm to break down the workplace silos that encumber development teams by storing the resource definitions in a central hub.A chart may even depend on another chart. Nesting a chart within another creates a synchronized dependency. Helm makes it easy for developers to manage any dependency in a chart, which provides the URL for the chart repository server storing templated files.Once a chart runs an instance (when it adds a pod to the network), it creates a release. For every running instance, there is a release. (We explore a Redis instance and release in more detail below).In short: A chart enables streamlined deployments of regularly used and repeatable applications and services in a cluster.Redis has deployed hundreds of clusters in Kubernetes. From the beginning, we realized that a manual process would not cut it. The first issue is to make Redis cluster deployments easier to reproduce; it’s also a matter of scaling operations. We needed the ability to deploy a Redis cluster in a single Helm CLI line.So far, we’ve covered how charts accelerate productivity, allowing developers to cut corners guilt-free. Here are a few other benefits worth highlighting.It’s not uncommon for an application to need a cluster for every stage of its lifecycle – and those lifecycles only grow more challenging the more intricate applications become.The Helm package manager is an excellent tool for setting a workable, reproducible foundation for a cluster. Still, it won’t provide automatic failover when a cluster has maxed out its resources. Clusters change over time. Perhaps it’s an added Redis node, pod, or Kubernetes application… but clusters rarely stay in a fixed state. These state changes are impossible to reflect in a Helm chart unless you have charts for everything, and again it becomes a manual process.We took everything we learned while deploying applications on Kubernetes containers and translated it into the Redis Enterprise Operator for Kubernetes.The operator is a tool for businesses that need to manage a Redis instance in Kubernetes at scale. It controls the provisioning, scaling, and availability of the Redis cluster and manages the containers’ lifecycle in any infrastructure.The Redis Enterprise Operator understands the lifecycle of an application and creates new resources to sustain the application. For instance – ever set up a gift card to automatically reload funds once they’ve been depleted?  The operator automatically provisions a cluster with the resources it needs to keep running, be it an added Redis node or load balancing a Redis cache to maintain sub-millisecond latency.With Helm charts, teams can get an application up and running quickly and depend on the operator to ensure that each stage of the application’s lifecycle has the resources it needs to keep going.In short, the Redis Enterprise Operator for Kubernetes keeps the proverbial balls in the air by ensuring a cluster always has what it needs for optimal performance.Find more answers to your Redis Enterprise on Kubernetes questions in these FAQs."
127,https://redis.com/blog/redisdays-2022-bangalore/,4 Reasons To Attend RedisDays 2022 Bangalore,"October 30, 2022",Redis,"It’s time to meet your peers in person! We’re excited to bring together the Redis community.Architects, DevOps, and engineering leaders will meet on November 4 for RedisDays Bangalore. The free, in-person event can help attendees discover how to create software that gives its users joy. Or, if you prefer, to seamlessly provide real-time digital experiences to your customer base. But really, let’s go for joy.There are plenty of technical reasons to attend, from Redis tips and tricks to learning about new-to-you features. You might be a Redis novice and need basic information to give you a head start. Or you may be experienced and ready to learn advanced techniques. We all want to unlock the full potential of our tools.So it’s worthwhile to invest a day learning about real-time computing, database search capabilities, and software architecture design.Attending a conference is the ideal way to learn about the things that interest you, as well as new-to-you topics you hadn’t even thought to investigate. And there’s no better way to gain technical know-how than by listening to subject matter experts explain what something is, why it matters, and how to use it.This event’s focal point is the design and implementation of real-time data layers. A range of speakers will demonstrate the how and why, with a demo station so you can ask follow-up questions from authoritative technologists.Among the topics you can expect to hear about:There are plenty of reasons to meet other humans in person – even if it means putting on grownup clothes once again. This event is not just about learning new techniques; it’s also about meeting and learning with your peers.Attending RedisDays gives you an opportunity to meet people who care about the same things you do, whether it’s a shared passion for optimizing code or for a sports team. With all of the talent congregating at RedisDays, the event will be a melting pot of ingenuity. Connect with like-minded individuals, and expose yourself to an array of business opportunities that can propel your career forward.Sit next to someone whom you don’t know. Mingle. Chat. Maybe your new buddy’s company solved a technical problem that you’re struggling with. Perhaps a lunchtime conversation will inspire a design alternative. Or (just between us) a job change.Striking new, genuine friendships in the industry is just as rewarding as it is conducive to career advancement. Having someone who is a friend as well as a professional to turn to for advice can keep you on track to achieving your goals.This is an opportunity for you to grow your personal brand. Perhaps it can help you get on the radar of industry professionals you admire.Every year, RedisDays attracts tech talent who are making waves in the industry.  RedisDays Bangalore is no different.There will be no shortage of brain power, so take advantage by discussing ideas, connecting with new ones, sharing key findings, asking questions, discovering new solutions, and unearthing new techniques.It’s an invaluable opportunity to see things in a new light. Rubbing shoulders with other great thinkers in the tech-sphere can only inspire you to grow and make greater strides in your career.We’re here to learn, but we’re also here to have fun. RedisDays Bangalore has dinner on the menu and entertainment on the schedule to tickle your funny bone. A comedy act and a meal to round out the day and inspire conversation into the evening!  You’ll learn a lot, and you’ll laugh a lot, too.Discover how to build and scale real-time digital experiences. Build your network. Make new connections that can open new doors. Create new friendships. Get inspiration. Generate new ideas. Live, laugh, and learn at a truly incredible event that will propel your career forward."
128,https://redis.com/blog/redis-enterprise-google-cloud-t2d-benchmarks/,Benchmarking Performance on Redis Enterprise and Google Cloud T2D Machines,"November 30, 2022",Gilbert Lau and Brandon Liu,"Google and Redis are working together to ensure the best possible server performance across new CPU architectures. Case in point: Our benchmarks show that T2D has up to 75% better price performance than older-generation Rome Google Cloud machines.Google Cloud’s virtual machine (VM) family, T2D Tau VMs, is based on third-generation AMD EPYCTM processors. It provides customers with impressive price performance on scale-out workloads without the need to port x86-based applications to new processor architectures.Benchmarking system performance helps us deliver the best value and experiences. How else can we measure what we aim to achieve? In this blog post, we explore how T2D Tau VMs bring the best price performance to Redis Enterprise software workloads.On both workloads, T2D has up to 75% better price performance (throughput per dollar) compared to older-generation Rome Google Cloud machines. The benchmarks also show up to 40% better price performance compared to same-generation Milan Google Cloud machines.To accurately benchmark performance on this workload, we used the open-source PerfKit Benchmarker (PKB) tool, which provides data that helps to compare cloud offerings. PKB wraps hundreds of industry-standard benchmarking tools in an easy-to-use and extensible package. This includes Redis’s own memtier benchmark, which is what we use for workload generation for Redis Enterprise. PKB handles all of the provisionings of cloud resources, package installation, workload execution, and cleanup for the benchmark.The overall benchmark flow is as follows:The metrics collected include:We performed the testing using two configurations:The first setup is single-VM performance. To get good coverage of performance across a variety of NUMA configurations, we use the following vCPU counts: 2, 4, 8, 16, 30/32.The second setup is similar to what an average Redis Enterprise customer would use. The difference in the configuration is that it introduces multiple databases on a single cluster with replication. We focus specifically on clusters of three 16 vCPU servers for this setup.This benchmark is non-trivial. There are a lot of knobs to tune regarding server configuration and workload configuration. Our goal is to ensure that the end results are stable and consistent and that we report the best achievable throughput for each machine type. This section lays out the benchmarking principles for this experiment.The goal is to not bottleneck on the client, so we choose clients that are significantly larger than the server. For example, for a single 16 vCPU server, we provision two 32 vCPU clients.We test on GCP AMD machines:T2D (Milan), N2D (Rome/Milan), E2 (Rome)The choice of Guest OS greatly affects server performance. For comparison purposes, these servers run CentOS 7, which is popular among Redis Enterprise users.Servers install and run Redis Enterprise Software version 6.2.4-54 on CentOS 7 for all tests.We don’t enable transparent hugepages for benchmarking, as Redis incurs a latency penalty when these are used.Performance can be affected by guest security mitigations to some degree. For comparison purposes, however, we keep all GCP machines with the default mitigations. Turning them on and off did not have a noticeable effect on results.Placement groups affect how closely VMs are physically placed; they affect latency depending on configuration settings. For these experiments, we run with a clustered placement group policy.We present optimized configurations for servers based on a varying number of shards (in the single-VM case), proxy threads, and client threads. There is an optimal number of each of these which varies among machine types, which the benchmark optimizes to find the best throughput.This example database creation command allows us to vary the number of Redis shards running on the server.curl -v -k -u user@google.com:a9a204bb13 https://localhost:9443/v1/bdbs -H'Content-type: application/json' -d'{""name"": ""redisdb"",  ""memory_size"": 10000000000,   ""type"": ""redis"",   ""proxy_policy"": ""all-master-shards"",   ""port"": 12006, ""sharding"": true,   ""shards_count"": {_SHARD_COUNT.value},   ""shards_placement"": ""sparse"",   ""oss_cluster"": true,   ""shard_key_regex"": [{""regex"": "".*\\{(?<tag>.*)\\}.*""}, {""regex"":""(?<tag>.*)""}]}'}Here’s how we go about the benchmark testing.Load generation and benchmarking are done using the memtier benchmark (v1.2.15), which comes preinstalled with Redis Enterprise. This industry-standard benchmark is also used to benchmark managed/unmanaged Redis and Memcached.We preload the cluster:This preloads about 100MB of data onto the server to minimize the effects of cache before the test starts. In our experiments, raising the initial data size to a scale of about 8GB did not affect the results.To run the benchmark, we increment the number of memtier threads until we get an average latency that is higher than the 1ms latency cap. After reaching this threshold, we record the maximum throughput (ops/sec) that the server could sustain. This metric can be thought of as an indication of practical throughput.This command is run on each of the client VMs. The results are logged, measured, and aggregated.For instructions on how to set up PKB, refer to the Getting Started page.For the single-VM configuration, we run:For the representative-cluster configuration, we run:The following are optimized results for each machine type in terms of shard count, proxy threads, and client threads. T2D is generally close to the lead or leading in the single-VM per-vCPU comparisons in terms of throughput and price performance. T2D has exceptional price performance in the representative cluster configuration as well.In the single VM 16-vCPU throughput comparison, the T2D beats N2D Milan machines by about 50% and previous generation N2D Rome by about 75%.In the single VM 16-vCPU price-performance comparison, the T2D beats previous generation E2 Rome machines by about 40% and N2D Rome/Milan by about 75%.In the representative cluster configuration, we achieved over 3.9 million ops/sec throughput under 1ms latency on T2D-standard-16. That translates to about 5.8 million QPS/dollar (based on cost per hour) on T2D, which is over 40% improvement over other AMD machine types. It’s a price-performance winner.Google Cloud’s T2D has exceptional throughput and price performance on this workload and is the best-performing GCP AMD machine.We’re excited about this collaboration between Google Cloud and Redis Inc. to track the performance of new CPU architectures. It enables us to deliver the most helpful changes and bring customers the best user experience. Benchmarking with PKB allows us to partner on important metrics and keep results reproducible for customers.We look forward to continued collaboration. Redis will continue to challenge the status quo with the latest and greatest virtual machine types that Google Cloud brings to the market  – and together, we can provide the most cost-efficient real-time database solution."
129,https://redis.com/blog/redis-cloud-alerts-via-slack/,How to Receive Redis Cloud Alerts via Slack,"November 22, 2022",Nic Gibson,"Several Redis Cloud customers have requested a way for alerts to be delivered on Slack channels. While these notifications are currently available only through email, there is a workaround with similar results. This how-to outlines the process step-by-step.Staying on top of your Redis Cloud performance is mission-critical. While thresholds and configurations are established in the Redis Cloud interface, the important alerts that signal high memory usage, for example, are delivered by email.You want to stay on top of some situations and act quickly. For example, a database is affected by latency, or a team wants an alert once a database has crossed a 90% capacity threshold. The current notification system requires an administrator to decide who receives email alerts and for what purpose.However, many people rely on Slack for timely notifications rather than email. Here’s how to integrate them with Redis Cloud – as long as you have a paid Slack account.To use a Slack channel, an email address is required. Redis customers can take the email address associated with the Slack account and use it to receive alert emails. The instructions are the same for both the Slack desktop and web UIs.Note: A paid Slack account is required.Establish a password.…And done! Redis Cloud alerts should now be enabled in Slack.Test the settings to verify everything is in order and get started on your app performance optimization team strategies."
130,https://redis.com/blog/redis-enterprise-software-6-4-2/,Redis Enterprise 6.4.2 Highlights Client Validation and Access Management Features,"February 23, 2023",Adi Shtatfeld and Yoav Peled and Maayan Agranat,"In Redis Enterprise 6.4.2, we enhanced existing security features: extended client certificate validation and publish/subscribe access management. Here’s what they mean to you.All of us know how important it is to protect company data. At the top of our priority list for Redis Enterprise 6.4.2 was to add two enterprise-grade security features.Security is always on your minds, and it’s on ours, too.Transport Layer Security (TLS) is a cryptographic protocol widely used to secure communication between computer applications. One common use is a web application connecting and authenticating a server, after which all the exchanged data is encrypted before being sent over the network. TLS was proposed by the Internet Engineering Task Force (IETF), which describes it as “the core security protocol of the Internet.”Mutual TLS, or mTLS, is a method used for mutual authentication. mTLS ensures that both the server and the client authenticate each other at the same time. This is achieved by the server providing a server certificate to the client and the client providing a client certificate to the server. mTLS is performed during the TLS handshake at the beginning of the session.In Redis Enterprise, a database can be configured to use mTLS. In this case, when a client attempts to connect to the database, Redis Enterprise authenticates the client’s certificate during the TLS handshake before allowing it to connect to the database.But what happens when several clients are all given a valid client certificate, and you want only a subset of them to access a certain database?This is where the new feature, additional certificate validations, comes in handy.Starting with version 6.4.2, Redis Enterprise lets you perform additional validations on authenticated client certificates.It uses the public key certificate’s subject field, which contains additional information about the identity of the client to which the certificate belongs. This way, before allowing a client to connect to the database, Redis Enterprise performs two steps:For example, one of our customers, a large U.S.-based financial institution, wants to better control which clients can access which databases based on their client certificates. Their clients are all using valid client certificates but with different subject values. Once the “Additional Certificate Validations” option is enabled for a database and the list of allowed subjects is properly configured, the customer can now control which subset of client certificates can access the database.Using mTLS in Redis Enterprise involves a few steps:Publish/subscribe (pub/sub) is a messaging method that allows non-direct communication. A client or application can publish messages to a shared resource, and other clients/applications can subscribe to the resource and receive these messages.In Redis, this resource is called a channel, and it provides a fast, lightweight, and scalable solution. Pub/sub channels operate like a radio station, meaning you need to be connected to receive the message. Their synchronous nature makes pub/sub channels useful for implementing real-time notifications, sending messages between microservices, or communicating between different parts of an application.And naturally, the software has to protect these resources.Access Control List (ACL) is a list of rules, each of which grants or denies access to resources or actions. ACLs are a powerful tool for organizations to restrict unauthorized users from accessing sensitive business information or performing unauthorized operations.To accommodate users’ needs, Redis is continuously enhancing its ACL functionality and coverage. With the release of Redis Enterprise 6.2, ACLs were enhanced to allow and disallow access to pub/sub channels.Why would anyone want to protect pub/sub channels? An invalid message sent to a channel disrupts the application. It may cause data corruption, data loss, or even an outage. By restricting all access and permitting only the relevant users to a specific channel, we can reduce the chance of someone accessing or sending messages when they are not supposed to, whether maliciously or unintended.There are two primary methods for protecting resources.The first is implicit access, where you have access to everything, and restrictions are put in place to limit access. An example of this is entry into a restaurant: anyone is allowed to come in unless someone has been placed on some type of denied entry list.The second method is explicit grant: you do not have access unless you are granted permission explicitly. An example of this is boarding an aircraft. Nobody is allowed onto the aircraft until their credentials have been verified to allow for entry. The second method is the more secure method for protecting an asset.This is the approach that Redis has taken, choosing to restrict all pub/sub channels except the ones you permit using ACLs. With Redis Enterprise 6.4.2, this policy can now be enforced by configuring a cluster-wide default option that applies to all the channels in all the databases. To avoid a breaking change, the 6.4.2 installation-provided value of acl-pubsub-default is permissive (allchannels) to comply with previous Redis versions. Once all databases in the cluster are in Redis version 6.2 (or higher in future versions), we recommend setting this value to “restrictive” (resetchannels).If you are using ACLs and pub/sub channels, we recommend reviewing your databases and ACL settings. Plan to switch to restricted mode, which will be the new default of acl-pubsub-default in future Redis Enterprise releases.Our emphasis in this release was the aforementioned security features. But we added more than that!Redis Enterprise provides self-signed TLS certificates. That allows customers who don’t use trusted CA signed certificates to use Redis Enterprise securely. Those self-signed certificates are valid for one year by default; we advise customers to renew those certificates before they expire.We now provide a user-friendly script to create new self-signed certificates in one go. You can create new self-signed certificates quickly and easily, with no prior knowledge required. You then load these certificates into Redis Enterprise in a few simple steps.Redis Enterprise offers a rich variety of services, but sometimes you don’t need all of them. Redis provides the tools to remove services so you can save memory resources and focus on what’s important to you. We recently added the ability to disable the alert manager, the service that is responsible for sending email alerts:rladmin cluster config alert_mgr [<enabled | disabled> ]Disabling this service can come in handy if you have an alternative alerting system. We recommend using this feature with discretion.With Redis Enterprise 6.4.2, creating applications is faster, more efficient, flexible, and above all, safe for users throughout their journey. Download the 6.4.2 release and start a free 30-day trial today!"
131,https://redis.com/blog/redistimeseries-performance-optimizations/,The Gorilla in the Room: Exploring RedisTimeSeries Performance Optimizations,"December 6, 2022",Martin Dimitrov and Ofir Moskovich,"Intel and Redis are working together to investigate potential performance optimizations for the RedisTimeSeries compression/decompression algorithm. Here’s how that exploration works.Time-series data management can be critical in data analytics. Many applications need to store time-stamped data, such as stock prices, IoT sensor data, and CPU performance metrics. By definition, data accuracy is a significant factor in how this information is stored, as is the speed of storage and access.RedisTimeSeries is a time-series database (TSDB) module that keeps large amounts of streaming data completely in-memory. That makes data available for fast processing, thanks to a state of art compression/decompression algorithm called Gorilla. In this blog post, we review the Gorilla algorithm implementation in RedisTimeSeries as well as potential ideas for further optimization. While at this time we chose to keep the existing compression/decompression implementation unchanged, we want to share our exploration process in our shared pursuit of continuous improvement.The Gorilla algorithm was developed by Meta for its large-scale and distributed TSDB. One of Meta’s key TSDB requirements is to maintain at least 26 hours of streaming data in-memory. It has to be available for fast analysis. Moreover, newer data is typically of higher value than older data for detecting events, anomalies, etc. That made compression ratio and query efficiency of paramount importance.Gorilla tailors the compression algorithm specifically to the characteristics of time-series data. In particular, its authors recognize that sequential data points are closely related in value and that using a “delta” style algorithm can achieve an excellent compression ratio with low computational overheads. For example, data points may arrive at regular time intervals – say, every 30 seconds. Thus, the delta between the time stamps is frequently the same or a similar value. The delta between sequential floating point data values is also expected to be closely related.The authors propose to use a “delta of deltas” algorithm for the integer time stamps and an XOR-based delta encoding for the floating point values. The encoded time stamps and data values are inserted one after the other into an in-memory buffer. See the original paper for the details of the encoding/decoding algorithms.Overall, the Gorilla algorithm achieves about a 10X compression ratio, with more than 96% of time stamps compressed to a single bit and more than 59% of floating point values compressed to a single bit.The RedisTimeSeries database implements the Gorilla algorithm for compressing and decompressing time-series data.It could be faster. (Then again, when it comes to performance optimizations, it could always be faster! That’s what Redis is here for.)While Gorilla achieves excellent compression ratios and computational efficiency, in some situations – e.g. when computing filtering operations across the time-series database – the decompression routine takes up a significant amount of time. For example, in the flame graph below, we see that decompressChunk takes up a large portion of the execution time, up to 85% in one instance.Given that decompression takes up significant CPU resources, Redis teamed up with Intel to investigate. We instrumented RedisTimeSeries to separate the integer timestamps and the floating points values into two separate memory buffers. That permitted us to study their characteristics independently and apply potential optimizations to each data type independently.Our first idea was to explore vectorizing the compression/decompression computation. A good overview of the techniques used to vectorize compression algorithms can be found in “A general SIMD-based approach to accelerating compression algorithms.” In general, in order to operate on multiple elements concurrently, some regularity must be imposed on the bit widths used for encoding the values. Such imposed regularity may result in sacrificing the compression ratio.To evaluate, we tested vectorized implementation of different compression algorithms available online at TurboPFor: Fastest Integer Compression using as data the input from the Time Series Benchmark Suite DevOps use case. In our analysis, we discovered that the decompression of the floating point values was the most time-consuming (the integer timestamps were much quicker to decompress), so that’s where we focused our attention. We extracted a trace of floating point values from a dataset and tested it with TurboPFor as well as with our Gorilla implementation.In Figure 2, we show the best-performing algorithms in terms of compression ratio from the TurboPFor library on our dataset. It shows that the TurboFloat DFCM was the best algorithm in terms of compression ratio: The values were compressed to 11.15% of the original. However, the compression ratio with Gorilla for the same dataset was still significantly better at 9.3% or 6.01 bits on average for every 64-bit float.Since Gorilla offers a better compression ratio than TurboPFor, we decided to stay with the existing algorithm and turn our attention to further fine-tuning the Gorilla implementation.Next, we delve deeper into the data patterns observed in our dataset for clues for other possible optimizations.First, we instrumented RedisTimeSeries to print out the number of bits used to encode each integer timestamp and floating point value in a 4K chunk. Most of the time, integer time stamps are compressed to a single bit, which is consistent with the findings in the Gorilla paper. On the other hand, the bit patterns used to encode floating-point values are much more varied and irregular.These results lead us to conclude that focusing on optimizing double delta floating point decompression through vectorization would not pay off in terms of performance and code complexity. Based on that realization, we decide to delve into other potential optimizations that would not be based upon vectorizing the floating point xor encoding.We printed out the floating point values in our test dataset and analyzed their characteristics. Doing so helped us discover that in many cases (and also noted in the Gorilla paper), the floating point values are actually integers represented as doubles (e.g. 72.0, 68.0, 71.0). However, as compared to the timestamps, these integer values do not sequentially increase or decrease.Furthermore, after generating a histogram of the frequency of occurrence of different values, we discovered that only a small number of values (fewer than 128) represent the overwhelming majority of observed data points in our example dataset.Based on these observations, we believe that we may achieve better performance and/or compression ratio by using a combination of Gorilla and a dictionary and/or run-length encoding. However, we still have to do further analysis with a more extensive set of datasets.Floating point data may be overkill or an inefficient way to store information. Data usually measures a physical quantity (such as temperature) and sensors have a given range and accuracy. Even stock prices have a resolution of plus or minus one cent.So an alternative in creating a time series may be to instruct the user to specify a range (e.g. 0-100) and resolution (e.g. 0.01).The external interface would remain floating point, but internally we could convert the measurements into integers in the most efficient manner (size- and performance-wise).Based on our analysis of the compression ratio of Gorilla and other fast compression algorithms, our Gorilla algorithm is currently the better compression ratio option. We also explored the characteristics of a typical time-series dataset for ideas about how we might further improve the Gorilla implementation.Our conclusion: So far, so good. We decided not to make changes to the existing implementation. However, we continue to study the characteristics of different datasets and to evaluate enhancement ideas, such as extending the usage of vectorization to accelerate data processing with RedisTimeSeries, reducing the serialization overhead of our protocol, and even improving the way we store and access our data structures.This is just an example of the performance explorations Redis is engaging in, made possible by the collaboration between Intel and Redis. If you like what we described so far, we encourage you to check how we’re extending Redis’s ability to pursue core Redis performance regressions and improve database code efficiency, which we detail in Introducing the Redis/Intel Benchmarks Specification for Performance Testing, Profiling, and Analysis."
132,https://redis.com/blog/podcasts-for-database-professionals/,5 Podcasts for Supervillains Who Are Also Database Professionals,"April 10, 2023",Carol Pinchefsky,"These database-related podcasts offer technology and development career advice based on what job you have…and what kind of supervillain you are.Do you work with databases? And are you just a little bit villainous? Well, think bigger. You’re not just some engineer, administrator, or analyst. You’re [insert name here]. You have the power of millions, nay, billions of inputs at your fingertips. You can conjure databases or statistics from thin air. When you give database commands, the database leaps to respond.Your quest for ultimate power doesn’t have to be boring. Stop and treat yourself to a podcast. The ones recommended here act like a virtual henchperson, giving you insights to chew over with sharpened teeth and aiding you on your quest to become The Most Powerful Datamancer Of Them All.Feast your ears on the six podcasts we recommend for database professionals…with varying degrees of supervillainy.If you’re a database engineer who thinks that data is your personal plaything and chuckles softly while solving problems that have foiled lesser mortals, The Data Engineering Show is for you. “Data engineering? Hah!” you may say. “I can convert raw data in my sleep. I can even invade your puny user mind and convert it in your sleep.”The Data Engineering Show seems to recognize megalomaniacal supervillainy in its listeners, too. In more than one podcast, hosts Eldad and Boaz Farkash recommend that engineers keep customers in mind when designing data-centric applications. Consider adding “Win the love of customers and other common people” to your villainous checklist. After all, if it weren’t for these lesser mortals, you wouldn’t have anyone to shower you with tribute.Recommended episode: Database observability with millions of usersRecommended villain: Victor Von Doom/Doctor Doom, The Fantastic FourSupervillains, take note. The Data Engineering Podcast covers a wealth of topics, including working with real-time data, data lakes, and optimizing cloud costs. It also takes security seriously, devoting several episodes to the need to keep your data safe. The Data Engineering Podcast, hosted by Tobias Macey, has everything you need to implement your best-laid, possibly nefarious plans.Any data engineer could learn from this podcast. But as The Data Engineer Podcast reveals solutions to thorny data problems, it’s of particular interest to supervillains who reveal their schemes to the hero while gloating. Of course, the hero couldn’t possibly foil your plot: You carefully craft your plans and you listen to the Data Engineering Podcast. Truly, you are invincible.Recommended episode: Aligning data security with business productivity to deploy analytics safely and at speedRecommended villain: Dr. Julius No, Dr. NoDrill to Detail focuses its efforts on database analytics and reporting. With episodes on real-time stream processing, data quality, metadata, and more, this podcast helps analysts glean all-important insights that make you your client’s, ahem, “hero.” Host Mark Rittman and his interviewees offer strategies to the data analyst who sees patterns that no one else sees because their tiny brains cannot grasp the data’s meaning.Although it can take years for data to reveal trends, this podcast is for the best kind of supervillain: the mastermind whose far-reaching schemes take years to unfold. Think, “I created a clone army, then, years later, manipulated events for which I would need a clone army.” Thanks to Drill to Detail, future analysts will be able to analyze your morally questionable work.Recommended episode: Oracle analytics, Luke Skywalker, and the remarkable return of enterprise analyticsRecommended villain: Sheev Palpatine, the Star Wars universeHosts Kostas Pardalis and Eric Dodd use The Data Stack Show to interview an impressive number of CEOs in the data industry. Databases aren’t the main thrust of this podcast, but the hosts recently carved out air time on the topic, so engineers who design storage engines are covered. In particular, database analysts who are also supervillains can grow their careers by casually peppering boardroom conversations–boardrooms they secretly plot to overthrow–with the many industry trends this podcast addresses.The Data Stack Show isn’t just a podcast that covers business intelligence. It’s a siren call for the supervillainous strategist who plots to wrest a company from the hero. It’s especially true if you started out from nothing, applied yourself, and recognize the hero is too weak to run his company. No, your company.Recommended episode: Materialize origins: a timely dataflow storyRecommended villain: Obidiah Stane/Ironmonger, Iron ManRuns as Radio is dedicated to All Things Microsoft, and while databases aren’t quite the thrust of this show, this podcast has you, an “innocent” database administrator, covered. Maintaining a SQL database? Implementing a migration to Azure? Host Richard Campbell and his array of guests offer tips and tricks to keep your operations running smoothly, leaving you free to design your volcano lair.Although Microsoft isn’t the Evil Empire it used to be, Microsoft is an industry-standard, and to paraphrase an old adage, nobody ever got fired for buying it. Because of this, it’s the perfect system of choice for a particular breed of supervillain: the one who hides in plain sight. Therefore, this podcast is for the supervillain who, when exposed to a mad cackle, has been there all along.Recommended episode: Query performance tuning strategiesRecommended villain: Agatha Harkness, WandaVisionWhile the other podcasts recommended here are, at this time of writing, currently up and running, here’s a shout-out to the fabulous RedisPods, which ran from 2020 to 2021. And while you’re at it, check out Redis’ former podcast, The Data Economy. The advice is useful, though, even if the podcasts are not ongoing.Trying to improve your database skills? We recommend video lessons about JSON, the best database conferences to attend, and how to use leaderboards in your applications even when they aren’t games."
133,https://redis.com/blog/datasets-for-test-databases/,Groovy Datasets for Test Databases,"April 4, 2023",Esther Schindler,"When you experiment with a new-to-you data science skill, you need some sort of data to work with. Why be boring?Teaching yourself new tech skills often requires a “starter project” and data to support that project. Your motivation for learning the new skill could be anything: preparing for a career upgrade, curiosity about a hot, new programming language, or an intent to better exploit features in an existing development environment.Good starter projects – once you’re done with “Hello, world,” – accomplish something, however trivial, even if they have nothing to do with work. You need to experiment with realistic coding scenarios, including edge-cases, so the starter project should represent the way you’ll use the tool in real life. On the other hand, you don’t want to spend months debugging a practice application.Which is to say: Why not have fun? Choose a starter project that lets you play. In my past, such projects have included creating dungeon master tools, food co-op ordering systems, and software developer market research.With that in mind, I offer several entertaining datasets for inspiration – from astronomy to science fiction to parking meter revenue – many of which support a range of data types. I like to think you’ll use them as you teach yourself about Redis features.These datasets are all free to access, though a few require you to create a site login. They are downloadable (most are CSV) or accessible via an API. A lot of cool archives are designed for interactive search (such as the Women and Gender Marginalized Composers Repertoire Database, Baseball Reference, or the Tulsa Historical Society’s photo archive), However, this list is for developers, not for people who like to scan fascinating data collections.I haven’t explored the data in any depth, nor do I vouch for their accuracy. This is purely a pointer to useful resources and a source of many, many internet rabbit holes.The Star Trek API provides a read-only model of all things Star Trek, including characters, performers, species, episodes, spacecrafts, books, astronomical objects, and video releases. For an idea of its scope, this dataset has information about 7,560 characters, 3,207 technology pieces, 2,497 locations, and 2,348 astronomical objects. Similar information is available from a shared Airtable with “every official Star Trek book, audiobook, comic, episode, movie, and more.”If your science fiction fandom lies elsewhere, you might choose the Mutant Moneyball project, which tracks comic book market data for individual X-Men characters’ financial value. The project’s dataset has decade-by-decade statistics for 26 X-Men characters drawn from sales histories and pricing guides.If you prefer to geek out with tech relevance, the programming language database describes several thousand programming languages, including their file formats, communications protocols, and other related concepts. You get information on the year the language was announced, its technical features, creators, countries and communities of origin, relevant books and URLs, and popularity metrics.For a different set of data characteristics, consult the Global Jukebox, an interactive map, and its accompanying compilation of datasets. It’s focused on traditional songs from around the world, based on information collected by musicologist Alan Lomax. The core dataset, called Cantometrics, encodes “37 aspects of musical style for 5,776 traditional songs from 1,026 societies.”This is my favorite find: the extracted acoustic signal features of tall Philippine coconut fruits. In the Philippines, it turns out, coconuts are classified manually into their maturity levels. “Traders often use their fingernails, knuckles, or the blunt end of the knife to tap the coconuts before assessing the sounds produced,” write the study’s authors, who developed hardware and software to emulate that process. They used it to collect acoustic signal data from 129 premature, mature, and overmature coconuts, each mechanically knocked on each of its three ridges.This may be a good data source for AI or machine learning experimentation, particularly if you are interested in digital-signal processing or audio signal processing. Though really, we know the reason to look at this dataset is that you want to tell your friends, “I’m working on an application evaluating coconut acoustics.” I don’t blame you.The University of Hawai’i released what it claims is the largest catalog of exploding stars.” The largest data release of relatively nearby supernovae (colossal explosions of stars), containing three years of data from the University of Hawaii Institute for Astronomy’s (IfA) Pan-STARRS telescope atop Haleakalā on Maui, is publicly available via the Young Supernova Experiment,” reports the university. The data contains information on nearly 2,000 supernovae and other luminous variable objects with observations in multiple colors, and also extensively uses multi-color imaging to classify the supernovae and estimate their distances.If you want a bookish application for your sample project, build on the Post45 Data Collective’s dataset of major literary prizes. It has more than 7,100 “winners and judges of prizes for prose, poetry, or unspecified genre between 1918 and 2020 with a purse of $10,000 and over.” The data represent 50 awards and fellowships, plus the Library of Congress’s poet laureateship. This dataset is heavily text-based, with entries including the prize name, institution, type, genre, year, and dollar amount, among other fields.If you are looking for less literary text-based data, consider the Dad Jokes API managed by the national responsible fatherhood clearinghouse.The Fjelstul World Cup Database covers 22 men’s World Cup tournaments from 1930-2022. The database includes 27 datasets that cover all aspects of the event, accounting for about 1.1 million data points. (I’d say more about this one, but you all know that I’m a baseball girl. I do note that Brazil is the only team with five World Cups.)Maybe you plan to work with spatial and architectural data? The Swiss Dwellings dataset contains detailed data on over 42,500 apartments (250,000 rooms) in about 3,100 buildings, including their geometries and room typology as well as the apartments’ visual, acoustical, topological, and daylight characteristics. It also has location-specific characteristics for the buildings, including climatic data and points of interest within walking distance.If you’re interested in gem quality, diamond pricing, or merely a good-sized dataset for your sample application to chew on, consider this diamond dataset. It has information on about 220,000 diamonds, with 25 columns of data including fluor (measuring the effect of longwave UV light), the stone’s measurements, and the total sales price. That should add a bit of sparkle to your analysis.Vendors sometimes offer (anonymized) data for public use and analysis. For instance, in addition to a cool world map that shows you live bird pictures that are taken with the company’s smart bird feeder, you can download Bird Buddy’s monthly datasets with longitude, latitude, and species name. Surely you can build a geospatial application that incorporates a northern cardinal, tufted titmouse, and red-headed woodpecker?Another example that’s less visually attractive comes from BackBlaze, which regularly provides reports about true hard drive failure rates based on its extensive hardware use – 231,309 hard drives at the end of 2022. In addition to its own in-depth analysis, the company also provides its source data.Open-data policies made it easy to find and download datasets that government agencies collect or generate. And it’s a lot of data. The U.S. government has a data search site where you can look for statistics on a wide range of topics, such as healthcare, car sales, and sensor data gathered from agricultural farm use. Whether any of these qualify as “cool datasets” is an exercise left to the user – but they often are large enough to be useful for experimental programming, and some have unique data types.For instance, if you are exploring geospatial database features, you might want to use a data set that includes location data. One such example is the 31 million parking meter receipts collected since 2015 by the city of Arlington, Virginia, which includes where the meter is as well as the monies paid ($68.6 million dollars in revenue, if you are keeping score).Similarly, the City of Los Angeles publishes the location and orientation of more than 50,000 stop signs; you can find similar information for Houston, San Francisco, and Detroit. Some datasets, such as from OpenStreetMap, are available through an API as well as downloadable files; if you can think of something to do with information about 1.4 million stop signs around the world, you can do so with ease.I like to think that these datasets can help you expand your database skills – particularly as you explore what you can accomplish with Redis features such as search, gaming leaderboards, vector similarity search, time series, and geospatial capabilities. Choose datasets that match the application domain you want to learn about.For example, if you want to experiment with database processing that incorporates geospatial analysis, your sample data need location data (birds! Stop signs!). To expand your knowledge of database search features (because ultimately, you want to speed up internal searches in production databases), choose a huge dataset (stars! diamonds!); your performance testing needs something to work with. Pick a numbers-heavy dataset when you want to learn how to create data visualizations that make everyone say, “Oooh!” And so on.Don’t feel foolish about choosing one of these datasets. It’s a bad idea to use existing internal data for a starter project. Banging on real customer data raises privacy concerns, particularly when you aren’t using it for the reasons it was gathered.You certainly cannot use real information when you speak at an industry conference. But you can entertain and engage an audience when you describe the graph database essentials in the context of Dungeons and Dragons. Mental models help us reframe our knowledge with familiar examples, turning abstract functions into practical analysis.And, speaking from personal experience, if your intent is to show the boss a technology proof-of-concept (“Here’s what we could accomplish if we deployed this database feature!”), they could be distracted by the “real” data. (True story. A user saw the output of a “play with the tool” experiment – “show a graph of hotel reservations made, displayed by the day of the week” – and said, “Oh, can I get a copy of this report monthly?”)If you’re interested in collecting datasets (oh look, a dataset of datasets!), I highly recommend the Data is Plural newsletter, which I drew on liberally to inform my suggestions. You also should visit and subscribe to ResearchBuzz, which shares dataset descriptions as well as archive-related news and tools (a recent example: Turn Wikipedia into an RSS Search Engine With WikiRSS). Google Research maintains a search site for test datasets, too, if you know what you’re looking for.If you use any of these datasets in your personal projects, please tell me about them!"
134,https://redis.com/blog/cloud-dataflow-pub-sub-to-redis/,How To Use Google Cloud Dataflow to Ingest Data From Google Cloud Pub/Sub to Redis Enterprise,"April 11, 2023",Gilbert Lau and Virag Tripathi,"Google Cloud Dataflow provides a serverless architecture that you can use to shard and process very large batch datasets or high-volume live streams of data in parallel. This short tutorial shows you how to go about it.Many companies capitalize on Google Cloud Platform (GCP) for their data processing needs. Every day, millions of new data points, if not billions, are generated in a variety of formats at the edge or in the cloud. Processing this massive amount of data requires a scalable platform.Google Cloud Dataflow is a fully-managed service for transforming and enriching data as a stream (in real time) or in batch mode (for historical uses), using Java and Python APIs with the Apache Beam software development kit. Dataflow provides a serverless architecture that you can use to shard and process very large batch datasets or high-volume live streams of data, and to do so in parallel.A Dataflow template is an Apache Beam pipeline written in Java or Python. Dataflow templates allow you to execute pre-built pipelines while specifying your own data, environment, or parameters. You can select a Google-provided template or customize your own. The Google Cloud Dataflow pre-built templates enable you to stream or bulk-load data from one source to another such as Pub/Sub, Cloud Storage, Spanner, SQL, BigTable, or BigQuery with an easy-to-use interface accessible via the Google Cloud console.Redis Enterprise is used extensively across the Google Cloud customer base for many purposes, including real-time transactions, chat/messaging, gaming leaderboards, healthcare claims processing, real-time inventory, geospatial applications, and media streaming. As an in-memory database, Redis Enterprise consistently delivers millions of operations per second with sub-millisecond latency. Thus, Redis Enterprise perfectly complements many of the native Google Cloud managed services that drive real-time user experiences.To give you a practical introduction, we introduce our custom template built for Google Cloud Dataflow to ingest data through Google Cloud Pub/Sub to a Redis Enterprise database. The template is a streaming pipeline that reads messages from a Pub/Sub subscription into a Redis Enterprise database as key-value strings. Support for other data types such as Lists, Hashes, Sets, and Sorted Sets will be built over time by Redis and Google experts, and perhaps by open-source community contributors.We want developers to have a great experience with Google Cloud Dataflow and Redis Enterprise.Using a pre-built template offers many benefits:It helps to see how the process works, so here we walk you through the high-level workflow to show you how to configure a Dataflow pipeline using our custom template.In this example, we process a message arriving at a pre-defined Pub/Sub subscription and insert the message as a key-value pair into a Redis Enterprise database.From the Dataflow GCP console, enter a pipeline name and regional endpoint, and then select Custom Template. Enter gs://redis-field-engineering/redis-field-engineering/pubsub-to-redis/flex/Cloud_PubSub_to_Redis for the template path.Next, enter the Pub/Sub subscription name that holds the incoming messages. Add the Redis Enterprise database parameters (e.g., Redis database host, Redis database port, and the Redis default user authentication password).Choose Create Pipeline. The pipeline is now set to receive incoming messages. You may cheer, if you like to celebrate small victories.You’re ready to publish a sample message to the Pub/Sub topic. Type in some sample data, and choose Publish.Confirm that your sample data was published, if only for your own reassurance. To verify that the data was inserted into the Redis Enterprise database, you can use Redis Insight, a Redis GUI that supports command-line interaction in its desktop client.The support model for this custom Dataflow template currently uses a community-based support mechanism. This means it is supported by the open source community on its GitHub repo.You are welcome to check out the open-source code and provide your feedback. We encourage you to add new features. Fork our GitHub repo and create a pull request when you are ready to contribute. Your support will make this project far more successful and sustainable."
135,https://redis.com/blog/how-to-build-a-knowledge-base-platform-using-redis/,How To Build a Knowledge Base Platform Using Redis,"October 19, 2022",Mirko Ortensi,"Building multi-model applications with Redis Stack is surprisingly straightforward! Follow along with a tutorial that shows how to build a knowledge base in Python that incorporates powerful search features.Companies produce an overwhelming amount of data daily. It’s a huge challenge to organize it, filter out obsolete information, and make it available whenever it is needed. That is not a trivial problem.Making sense of data is a different task than storing it. Accomplishing that goal includes documenting the data’s organization as well as the process by which it is created, edited, reviewed, and published, with awareness of the relationship between editors and consumers.Knowledge base systems serve as libraries of information about a product, service, department, or topic. In such systems, documents are dynamic assets that can be referenced, improved, classified, shared, or hidden from unauthorized users. Typical examples include FAQ databases, how-to guides, and onboarding material for new hires. A knowledge base should be tailored to a company’s needs, which means it needs to interface with existing tools and processes.The internet made knowledge base systems popular. Documents could be structured and connected using hypertext, the core idea behind the internet itself. Organizing and linking content in such a way changed forever the way producers and consumers would create value out of knowledge.In this article, I demonstrate how to use Redis’s data structures and search capabilities to build a web knowledge base platform with basic functionality. This project is well suited to use Redis’s powerful capabilities for real-time full-text searches and queries because it fulfills the main purpose of such systems: providing several methods to retrieve the information a consumer is looking for and as soon as possible.This tutorial helps you build a knowledge base that can store and serve documents for typical use cases, such as customers who want to learn about a new product feature, troubleshoot a technical issue, or serve as a single, reliable source of truth for internal departments.The project’s source code is available in a GitHub repository, licensed under the MIT license.I named this project “Keybase,” and this is a preview of what it looks like:In this tutorial, I use the following components to develop a working prototype:The popular Nginx web server is used together with Gunicorn, which implements the web server gateway interface and serves the Flask application.The knowledge base backend is implemented as a Python application developed over the popular Flask framework. Flask provides good separation among the views. The application is implemented using the Jinja template engine and the controllers delivered by the Werkzeug WSGI toolkit. User authentication is based on Okta, the popular Identity as a Service platform.The front-end is styled using the CSS framework Bulma; JQuery is the Javascript library for Ajax communications and user interface manipulation together with JQueryUI, and Notify.js is used for UI notifications.An additional service, deployed as a cron job, indexes documents for document similarity recommendations. Document indexing is conveniently decoupled from the sessions in order to avoid impacting the user experience.Finally, in the data layer, the project uses a Redis database plus two Redis modules: RediSearch and RedisTimeSeries, which in the tutorial, I use for document indexation and search, and analytics, respectively.Documents in the knowledge base are modeled as Hashes. Here is how the data is stored:The Hash data structure stores:Got all that? We’re ready to dive into the Keybase features and how to implement them.We are extremely proud of the Redis database’s powerful real-time search features; this tutorial lets us show it off.You probably think of search as a brute-force lookup, but in fact, there are no slow scans when we use RediSearch indexing capabilities. The options we cover briefly here are:For a more in-depth explanation of search functionality in Redis, see an introduction to the RediSearch module.RediSearch creates the index using this syntax:The schema of the index document_idx enables the three types of search using the corresponding field types:The search input field in the web interface is used to perform a full-text search, which looks at the name and content of the knowledge base documents.Consider this example of a real-time full-text search using the redis-py client library. It performs the search on the query string received from the client. Note that this command excludes documents in draft state; such documents must be kept hidden and not included in the output:This neat code sample performs a real-time full-text search and returns a batch of per_page documents starting from the specified offset (useful to paginate documents in the UI). It fulfills the search criteria specified: all the documents that satisfy the query the user provided in the input field, it removes the documents that are in draft state, and it sorts the documents by creation timestamp.An example of real-time querying by tag in Python, where the search is combined with the filter on the tag and also excludes documents that are in draft state, is the following, which returns all the documents tagged with the “troubleshooting” tag:This snippet of code is a pure real-time query, including all the documents tagged using the troubleshooting keyword but also excluding those documents that are not public and still classified as private drafts. This syntax uses the indices on several fields in order to filter, order, and paginate the results.You’re surely familiar with this sort of search, especially on e-commerce websites. Personalized recommendations direct users to similar content with prompts such as, “You may also be interested in reading” or “people who bought this also purchased. “This sample application doesn’t need to increase product sales, as the knowledge base site has nothing to sell, but we do want to provide users with meaningful recommendations.You can add recommendations using the VSS feature of Redis real-time search.If you haven’t used this type of search in your projects, start with Rediscover Redis for Vector Similarity Search.The short instructions: To use the VSS feature, you first create a model, then store it, and finally query it.The core concept behind recommendations using VSS is to transform the document content into its corresponding vector embedding. The vector is the entity that describes the document and is compared against other vectors to return the best matches. The vector embedding is stored in the Hash data structure (in the content_embedding value).In this example, we use the Python SentenceTransformer library to calculate the vector embedding using the all-distilroberta-v1 model (from the Hugging Face data science platform) as follows.In this sample, the content of the document is vectorized and serialized in binary format, the format that VSS accepts. This operation is triggered twice: at document creation time and for any subsequent document update. In particular, when updating the document content, the vector embedding needs to be recalculated, or the index becomes obsolete and returns imprecise results. To trigger an offline vector embedding recalculation, as described in the architecture, it is sufficient to activate the processable flag and periodically list those documents whose vector embedding needs to be refreshed and then process them. For example:Using the same syntax as in the previous tag search example, this command executes a search to include those documents with the tag processable set to 1. Documents returned by this search get the associated vector recalculated and stored.Once the vector embedding is generated, you can store it as usual in the document Hash data structure ((in the content_embedding field).Finally, set the processable flag to zero. No further update is needed to the vector embedding until the next document change.Every time the user sees a document, the sidebar proposes a list of recommendations. That means the Keybase application has to discover what the recommendations are.To accomplish that, we fetch the vector embedding from the Hash of the document currently presented (that is, the one the user is looking at).The hget returns the value associated with the field content_embedding.To retrieve the most similar documents, compare the vector to the rest of the stored vectors.In this example, the query is configured to execute the powerful vector similarity search to return the six most similar documents by retrieving the k-nearest neighbors (KNN).For more VSS syntax examples, see the client library documentation.Every business application needs to establish who can and cannot access the software. For our knowledge base authentication, we use the popular Identity as a  Service platform Okta. Users are stored in the Redis database as Hashes prefixed by the string “keybase:okta:”. I tested the integration with the Okta Developer Edition.Here’s an example of a user profile:To search the knowledge base users, we create an index on the user Hash data structures:The group formalizes the implementation of Role-Based Access Control (RBAC). I created three user types for this example application, although additional roles can be added. The current roles are:Finally, it is also possible to bookmark documents by storing their IDs in a collection. Once more, the Hash is the best option because of its flexibility; you can store the document ID and also a personal comment on the bookmark.Here’s an example of a collection for the authenticated user:Many business scenarios require the organization to monitor a knowledge base’s activity. For instance, you might want to evaluate document popularity by subject area, minimize repeated questions, or mine for new feature ideas.The RedisTimeSeries capability is a natural choice for analytics functions because it is optimized to store and aggregate large amounts of data.The code to track the overall visits to the knowledge base, as an example, is:This code calculates the visits in the last month, aggregated by day. It is formatted as JSON, ready to feed to the Chart.js visualization library:Then, to render the chart in the Jinja template, use this Javascript:The chart is rendered in an HTML canvas element.This method lets you monitor additional metrics, such as views per document or user activity. You can use Redis time series aggregation features to compute averages, variance, standard deviation, and all kinds of statistics.To round out the sample application, we need to ensure that the knowledge base can be managed.The administrator can grant or revoke roles to users with the RBAC policy. Access control is implemented using Python decorators and a custom User class. (Refer to the repository for details.)For example, the signature of a protected method to perform backups might be:You can generate Redis database backups natively using the asynchronous BGSAVE command. In addition to the native backup method, the knowledge base also implements logical data import and export routines. Exporting data is achieved by iterating the Redis keyspace with the non-blocking SCAN command in batches of 20 elements.This project is a proof-of-concept to showcase Redis features when employed as a primary database. The source code in a GitHub repository, under the MIT license, is shared for demonstrative purposes and is not to be used for production environments. However, the repository can be consulted, cloned, and/or forked and all the examples reused to better understand the search and indexing features discussed in this article. I encourage you to do so.I developed Keybase to demonstrate how easily Redis can replace a relational database for these types of web applications. And it was surprising to discover how rewarding it was to model the data. Hash data structures are neat and compact! I was also happy to learn that Redis’s unique features for similarity searches and time series provide full out-of-the-box solutions to standard problems that otherwise would require multiple specialized databases. Building multi-model applications with Redis Stack is surprisingly straightforward!You could add many additional features to complement this project, such as improving the authoring experience with multi-user drafts, revisions, and feedback collection. This project could also be integrated with Slack, Jira, ZenDesk, or any platform that organizes teamwork. Additional features such as scanning the documents’ sensitive words, detecting broken links, and user auditing capabilities could be designed to take this software from the utility to the enterprise level.I invite you to clone or fork the Keybase repository and set it up. You can run it on your laptop with a local Redis Stack installation (or a free Redis Cloud database) and in the context of a Python environment. I hope you have fun with it!"
136,https://redis.com/blog/aws-autoscaling-groups-with-terraform/,Updating AWS Autoscaling Groups With Terraform and Preserving Desired Capacity,"June 21, 2020",Ariel Peltz,"Terraform is probably the best tool for deploying infrastructure, so it is an obvious choice to deploy an autoscaling group into AWS. The project we are working on requires us to deploy a service on instances in AWS. The service is stateless and has simple configuration that is easy to configure using cloud-init.Our goal is to configure everything with Terraform. Every time we need to make a configuration change, we update the Terraform configuration and apply again. We want to have clean instances when we make any configuration change and not update in place.First create the launch template:The launch template defines the critical elements of the instance and also loads the configuration of the service from the user data file. The user data file is a template that evaluates to a valid cloud-init configuration file.Now let’s create the autoscaling groupThere are a few things to note here:This is great and does exactly what we need. Or does it?What happens when we have scaled out the group to have more instances and we change the configuration? With the current configuration we will replace the running group of, 5 instances, for example, with a group of only 1 instance. We want to be able to create a new group with 5 instances.In order to keep the current value of desired capacity we need to somehow get it first.Let’s get the current launch template, if it exists:Now let’s see if we have autoscaling groups that use this launch template`s latest version:The idea here is that either data.aws_launch_template.current.latest_version has a value or it is null. If there is a value, we expect to have a list of at most size one. If it is null then we are looking for a non-existent autoscaling group and get a list of size zero.Now we can get the info on the current autoscaling group:Using count essentially creates either one or zero resources, per the size of data.aws_autoscaling_groups.current.names.Now we can update the autoscaling group resource:Here we define the desired capacity by checking if we can find any current autoscaling groups. If we can, then we use the first (and only) autoscaling group value of desired capacity. If however we cannot find any current groups then we use the default value. We also remember to add the tag we need to query for the autoscaling group.Terraform is pretty cool and can be used to do many things. Some require a little more jumping through hoops but it is worth it to have everything in a single tool."
137,https://redis.com/blog/redis-modules-into-arm-land-part1/,Getting Redis Modules Into ARM Land – Part 1,"November 13, 2019",Rafi Einstein,"Here at Redis, our motivation to bring Redis modules into ARM land was RedisEdge. Redis, of course, has long been native in this land, in both glibc and alpine/musl variants. Redis modules have already been on the multi-platform scene, running on various Linux distributions and supporting macOS mainly for the sake of development experience. However, it was more enterprise/data-center-oriented until RedisEdge, which targets IoT devices. In this series of posts, I’ll describe our vision of ARM platform support and the developer user experience, as well as the steps we took to get there.If you follow along, you’ll end up with a fully functional ARM build laboratory.Let’s take a look at RedisEdge. RedisEdge is not a Redis module but an aggregate of three Redis modules: RedisGears, RedisAI, and RedisTimeSeries. It is distributed as a Docker image, which is based on Redis Server 5.0. Thus, one can simply pull the image, run it, and start issuing Redis commands; load models into RedisAI; and execute Python gears scripts on RedisGears. Although one can easily remove Docker from the equation by installing a Redis server and copying Redis modules files, we’ll see that Docker actually provides significant added value and is worthwhile to keep.Inside RedisEdge: modules structureLet’s now take a look at each component of RedisEdge to figure out what would it take to have them ported to ARM. First, RedisTimeSeries.  It’s a simple C library built with make. Not even a configure script. No problems there. Next is RedisGears. It’s a C library built with plain make, and it also uses an embedded Python 3.7 interpreter, which is built from source. This requires running automake to generate a platform-specific makefile. Finally, RedisAI. It’s a C library built with CMake, and it includes modular “engines” that allow abstraction and encapsulation of AI libraries like TensorFlow, PyTorch, and ONNXRuntime, in their C library form—most users typically use them in Python, with PyTorch and ONNXRuntime not officially supporting ARM. So the build requirements, as it seems, have deteriorated quickly. It went from building an innocent C library to compiling massive source bases with convoluted build systems.In the following sections, we will fit each component with its proper build method.At this point, we’ll pause and take stock of what is required to build software for ARM. The obvious way is to use an ARM-based device, like Raspberry Pi. Once set up, you’ll be able to build and test in the most natural manner.The testing medium is important: even though you can build almost every ARM software without a physical ARM device, there is no way to reliably test the outcome without such a device, especially with non-standard devices. Therefore, while a virtualized/emulated/containerized test may be useful, you should always test your software on its designated target device.A Raspberry Pi 4 with 4GB RAM (and a 1Gbps NIC, no less important) and a fast microSD card look promising.Regarding the OS selection on ARM: the offerings are limited, and the rule of thumb is to go for the latest release of Raspbian (which is customized Debian distribution for RPi), Ubuntu, or Fedora, the latter two offering easy-to-install ARM systems. Do not worry about immaturity: it was proven time and again that newer systems work better and old ones can’t keep up.Installing an OS brings us to the first dilemma: If we install Raspbian, we get a 32-bit OS (i.e., arm32v7 platform). If we choose 64-bit Ubuntu, we get an arm64v8 platform (we discuss ARM platforms in detail later). If we intend to support both platforms (as we do with RedisEdge), we can either get two SD cards and install each OS on its own card while taking turns on the device or get two RPi devices (which is not terribly expensive). I recommend the latter.And now, for the principal principle of OS selection: our goal is to have a stable system with the newest Docker version. Any OS that will satisfy these requirements will do. That’s right: we’re going to use Docker to fetch the OS we really wish to build for, using the underlying OS as infrastructure. This will also help keep our build experiments properly isolated.If you have an RPi device at your disposal, you can now proceed with making it functional. In the next post, we will present build methods that do not require a physical ARM machine.Very good and detailed instructions on how to download and install the latest Ubuntu Server for ARM on RPi can be found here for Linux, Windows, and macOS. However, you should follow the following instructions to install Ubuntu 19.04 rather than the latest released version:By “workstation,” I’m referring to a Linux or macOS host that holds one’s development environment and git repositories. As a side note, I highly recommend using a desktop PC (that’s another blog post), though most people use laptops. In either case, I also recommend having some virtualization infrastructure on your workstation, such as VMware Workstation/Fusion or VirtualBox.So, we need to establish a network connection between the RPi and the workstation. As mentioned before, RPi 4 has a 1Gbps NIC, which is a great improvement over the RPi 3 with its 100Mbps NIC. Connecting the RPi to a network is simple: get any gigabit Ethernet unmanaged switch and two Ethernet cables, then hook the RPi to the switch and connect the switch to your gateway Ethernet port. You can hook your workstation to the switch as well.At this stage, we need to gather some information from the workstation.First, we need to determine UID & GID of the workstation user that owns the views:We’ll call them MY-UID & MY-GID.Next, we need to find out what’s our time zone:We’ll call it MY-TIMEZONE.Finally, we’ll need your workstation’s IP:We’ll call is MY-WORKSTATION-IP.During the initial setup, you’ll need to have the RPi connected to a monitor and a keyboard. Once setup is complete, it can be controlled from your workstation via SSH.Another good practice is to avoid cloning source code into the RPi, instead sharing it between your workstation and RPi via NFS.For convenience, I assume we operate as root (via sudo bash, for instance).So let’s get on with the configuration:Add the following to etc/hosts:Also, add the following line to etc/fstab:Regarding git repositories, I’ll use the following terminology and structure throughout the discussion. This is not essential and one may organize matters differently. The term “view” refers to a set of git repository clones that one uses in a certain context. For instance, if we work on RedisEdge modules in the context of ARM compilation, we’ll end up with the following directory structure:Here ‘arm1’ is the view name, and the directories it contains are the result of the corresponding git clone commands. There may be other views serving other contexts. The idea is to share this structure among all hosts and containers to avoid the hassle of moving code around and git key management.For even more convenience, I add the following link:So, finally, we get to set up NFS.Now proceed as root:Note the MY-UID and MY-GID values.Note the MY-UID and MY-GID values.Note that if you’ve got your workstation firewall enabled, it might interfere with NFS. Consider turning it off for wired connections.Now that we have NFS set up on the workstation, we can mount the views directory into the RPi:Finally, we can install Docker and Docker Compose.In the next post, I’ll present the developer experience we’re aiming for, discuss ARM platforms in detail, present methods for building for ARM without using ARM hardware, and start putting theory into practice with RedisEdge modules.Stay tuned!"
138,https://redis.com/blog/using-the-redis-allocator-in-rust/,Using the Redis Allocator in Rust,"November 12, 2019",Redis,"While developing redismodule-rs, the Rust API for writing Redis modules, I encountered the need to set up a custom memory allocator.Normally, when a Rust program needs to allocate some memory, such as when creating a String or Vec instance, it uses the global allocator defined in the program. Since Redis modules are built as shared libraries to be loaded into Redis, Rust will use the System allocator, which is the default provided by the OS (using the libc malloc(3) function).This behavior is problematic for several reasons.First of all, Redis may not be using the system allocator at all, relying on jemalloc instead. The jemalloc allocator is an alternative to the system malloc that includes many tweaks to avoid fragmentation, among other features. If the module uses the system allocator and Redis uses jemalloc, the allocation behavior will be inconsistent.Secondly, even if Redis always used the system allocator, memory allocated directly by the module would not be visible to Redis: it would not show up in commands such as info memory and would not be influenced by cleanup operations performed by Redis such as eviction of keys.For these reasons, the Redis Modules API provides hooks such as RedisModule_Alloc and RedisModule_Free. These are used much like the standard malloc and free calls but make Redis aware of the allocated memory in addition to actually passing the call on to the memory allocator.Rust provides the option to define a custom memory allocator by providing a custom implementation of the GlobalAlloc trait:We can use it by implementing the GlobalAlloc trait with our own methods that delegate the allocation to Redis. For this, we need a way to call the Redis Module API functions from Rust. That is a topic for another post, but in short, we achieve this by using the bindgen crate to generate Rust bindings from the redismodule.h C header file.The header file defines the functions as follows:These functions, like the rest of the Modules API, are defined as function pointers. When calling the functions from Rust, we need to dereference the function pointer first, which we do use the unwrap() method. We also need to do some casting to match up the pointer types. Finally, we need to use the unsafe keyword since we dereference raw pointers, which is not allowed in safe Rust for good reasons:Unfortunately, it’s not that simple. When we build a module with this custom allocator and load it into Redis, it crashes on us. Redis does print a nice stack trace when it crashes, so let’s look at it:So, it looks like we had a null pointer dereference here (3 ??? 0x0000000000000000 0x0 + 0), but what are all these weird symbols starting with _ZN…?After a bit of searching, we find that this is the way Rust does name mangling: Unlike in C, and similarly to C++, in Rust, multiple functions with the same name can coexist since there are various namespace mechanisms such as modules and traits to distinguish them. To generate unique symbols that are C-compatible, the compiler mangles these into long and ugly unique names. To detangle these names back into the original, we can filter the output through rustfilt.This gives us the following stack trace (uninteresting parts removed):It still took me a lot of head-scratching and experimenting to figure it out, but here’s what happened:The functions of the Redis modules API are accessed via C function pointers. Instead of relying on the dynamic linker to initialize these pointers, they are initialized explicitly by Redis as part of the module initialization process.As the stack trace shows, during the loading of the module, we call the CString::new function. This standard library function allocates memory for a string. This, in turn, calls our allocator, which would then call RedisModule_Alloc.unwrap()… to actually perform the allocation. This causes a chicken-and-egg problem. The Redis module is not ready yet, meaning our function pointers have not yet been initialized, so we can’t call the relevant API to perform the allocation.I try various approaches to solve this, but there seems to be no clean way to avoid the allocation during module initialization. The second best thing would be to use the standard allocator until the module is ready and then switch to the custom one. However, Rust doesn’t allow changing the allocator at runtime, so we can’t do that.I end up adding a flag to the custom allocator that causes allocations to be passed through to the system allocator at startup. After the module initialization is complete, the flag is toggled so that further allocations are then performed via the Redis allocator. This solution still has edge cases—most importantly requiring that all previously allocated memory is freed before switching, otherwise, that memory would leak. However, it’s good enough for our purposes.Here is what the final code looks like:We add a static flag named USE_REDIS_ALLOC that determines whether we should use the Redis allocator or the system one. It’s important to guarantee safety when mutating static data, so we use an AtomicBool here that is false by default.In the module initialization code, we call use_redis_alloc when the module is ready to use. At this point, we can safely start using the Redis allocator, and all future allocations will be accounted for by Redis.This takes care of the crash and ends up in the redis-module crate. Feel free to check it out and let me know how you like it!"
139,https://redis.com/blog/redis-short-lived-tls-certificates/,Redis Cloud Introduces Short-Lived TLS Certificates,"March 21, 2023",Jonathan Salomon,"We’re changing some of our security practices. Here is what you need to know to ensure a smooth transition.As part of our commitment to continuously improve security measures at Redis, we will soon start replacing the TLS certificates for all Redis Cloud database services in favor of the short-lived certificates issued by the publicly trusted GlobalSign Certificate Authority.In short: better security.The somewhat longer answer is that the TLS certificates in use today are issued by a self-signed Certificate Authority (CA). This means that no trusted third party verified that these certificates were issued on behalf of Redis.There is no reason for concern, though. You can – and should! – still download our public certificate bundle, which is published in the Redis Cloud admin console. That bundle can assure you that Redis genuinely signed the certificates presented by our database services.However, the new TLS certificates are issued by a publicly trusted CA called GlobalSign, which means that you don’t have to take our word for the trustworthiness of the certificate.In this context, “publicly trusted” means at least two things:Once we introduce the GlobalSign certificates, they will be issued as “short-lived” certificates. That’s considered a security best practice; ask your CISO! In our case, it means that the leaf certificates presented to your Redis client by our database services will be valid for three months. We will automatically rotate those certificates well ahead of their expiration date.Since your Redis client should already trust the GlobalSign CA (more about that later), after the rotation of these leaf certificates, any new connections should establish seamlessly; existing connections should not be affected.If you have not enabled TLS on any of your Redis Cloud databases, then you have nothing to do.If you currently have TLS-enabled databases in Redis Cloud, then you should make sure that your Redis client will continue to accept the database certificates before and after the change. (See below for how to do that.)So when should you expect these new certificates? That depends on your Redis Cloud subscription tier and when your subscription was provisioned:If your Redis client is configured to validate the certificate that is presented by the database (which is not always the case), then it tries to confirm that the certificate was signed by a chain that it trusts. Typically, this trust is either established by providing the public chain of the database certificate to your Redis client directly or adding it to your client’s trust store (depending on your environment, this could be your operating system or Kubernetes trust store or a jks file).For example, with redis-cli you can provide the database’s public chain to the client, as shown in this example. The cacert parameter should point to a file containing the public database certificate in PEM format:As mentioned above, the public certificates of Redis Cloud’s databases are published in the Redis Cloud admin console, which is available both from the “Account Settings” and the “Database Configuration” pages. (For more details, see the TLS documentation.) If you downloaded this public chain anytime after August 2022, the PEM file contains a bundle of the old self-signed certificate chains, as well as the root CA of the new GlobalSign certificates. If you downloaded this public chain before August 2022, make sure to download the latest version and update your Redis Client trust store.To ensure you are safe, you can inspect this PEM bundle with the keytool that comes with Java. On Linux, you can use certtool. It would look something like this:The PEM bundle should contain four certificates: the GlobalSign Root CA, plus three self-signed certificate chains used for the old certificates for Fixed and Flexible tier subscriptions.If your client trust store contains all four certificates, then you should be safe; expect a smooth transition once we start the migration to the GlobalSign certificates. If your Redis client trust store does not contain the GlobalSign root CA certificate, take action now to ensure that your client will not reject TLS connections after the migration.If you have any questions, review the FAQ section below. For additional information, contact Redis customer support at support@redis.com.Alternatively, on Linux, you can also use certtool.Now compare the SHA1 fingerprint of the certificate in the PEM bundle to the thumbprint of the R3 GlobalSign Root Certificate on the official publication of the GlobalSign Root Certificates. If the fingerprints match, then you can be sure that the certificate in the PEM bundle is indeed the public certificate of the R3 Root CA published by GlobalSign."
140,https://redis.com/blog/vendor-established-cloud-partnership/,3 Reasons to Choose a Vendor With an Established Cloud Provider Partnership,"April 20, 2023",Redis,"When you assess a new software vendor for your business, be sure to factor in the benefits of the company’s partnerships.When you set out to build a new cloud application, you want to make sure the process goes as smoothly as possible. This means that any development hiccups are kept to a minimum—or better yet, they don’t happen at all. It’s essential to have the right tools for the job, especially when you are implementing complex architectures such as microservices or a multicloud strategy.However, choosing a software provider to rely on for programming tools or an application hosting service can be stressful. There are so many options! Developers and IT teams sometimes find it difficult to settle on the right one for their specific use case or to feel 100% confident that the provider they’re gravitating toward really is the best choice after all.There are a number of factors to consider when assessing software vendors: functionality, pricing, security, reputation, client reviews, and much more. But be sure to also consider the value of choosing a vendor that has an established partnership with a large cloud provider, such as Amazon Web Services (AWS). A formal alliance between your vendor candidate and one of the big three cloud companies provides a number of benefits that you may not have considered, but which can be extremely valuable.Here are three ways that working with a vendor that’s already partnered with a large cloud vendor can make your life a whole lot easier.Who likes learning new things anyway? Well, a lot of us do, to be honest, but nobody wants to deal with a steep learning curve when they’re under pressure. You’re already busy, and likely on deadline; picking up new skills may not be the best use of your time.If you’re a software developer, a DevOps engineer, a cloud architect, or anyone else who contributes to building applications that require high availability and scalability, you are likely already familiar with large cloud providers like AWS—and how they work. Being able to access a software vendor’s offerings in the AWS environment, where you know how to configure systems and fix problems, removes a huge roadblock to getting started with a new tool.In addition, if you’re trying to build something new, or you have a complex project where you personally lack expertise, it’s invaluable to work with an experienced partner. You can rely on your software vendor, with whom you have a close relationship, as a trusted advisor; and you also benefit from the immense knowledge and expertise the large cloud vendor brings to the table.Anytime you work with a new tool or database vendor, you wonder how well their solution works with your existing systems. You can relieve that worry by choosing a vendor that has an established relationship with a cloud vendor you know.If you’re already an AWS shop, for example, you can get your tools directly on AWS. For instance, Redis and AWS have an alliance. You don’t need to integrate Redis into your legacy systems, and you don’t need to read another manual (that “learning new stuff” hassle we discussed earlier). You can simply sign up for Redis Enterprise Cloud on AWS: a fully managed, real-time database-as-a-service and cache that’s trusted by thousands of customers because of its unmatched performance, high availability, scalability, and support. As partners, AWS and Redis work closely together to ensure that customers have the best possible experience running Redis Enterprise Cloud on AWS.This also means you don’t need to worry about buying anything “new.” No new purchase orders! There’s less need to convince your boss—or whoever it is that needs to approve your purchases.Instead, you can have Redis Enterprise Cloud as a line item on your unified AWS bill, with the expenses counting toward your AWS commitment. You can provision according to your business needs, paying monthly without any budget commitment, and change your provisioning at any time as those needs evolve.When vendors have an established relationship, the technical contacts know their partners’ tools better than you do, they know the magic words to type into the help desk software, and they have a Rolodex with the correct human contacts if all that fails.In addition, you don’t need to worry about a lack of expertise among your colleagues. Using tools on an established cloud platform that everyone is familiar with means any shared work with others on projects should go more smoothly, with less friction and more collaboration.The last thing you need is to choose a lesser-established software solution that you’re convinced will do the trick, then have your project go south and get blamed for placing your bets on an unknown company.Ready to experience the power of partnership?  Check out Redis on the AWS Marketplace today."
141,https://redis.com/blog/redis-2022-retrospective/,"Looking Forward, Looking Back: The Redis 2022 Retrospective","December 29, 2022",Allen Terleto and John Noonan,"Recession, personalization, and ChatGPT, oh my! 2022 was a wild ride!The tech industry was not immune to economic swings and, in many ways, seemed at its epicenter. So let’s reflect on what we learned in order to draw inferences on what awaits in 2023.This year slowed tech’s long winning streak and, in some cases, brought it to a complete halt. The past few years gave us favorable macroeconomic conditions: a strong job market, a shift from the physical world to digital channels, cheap access to capital in a low-interest rate environment, and a generally favorable investment environment. However, current global and domestic events slowed down tech’s momentum.Even though many economists aren’t officially using the “big R” word, the tech industry is feeling the effects of a recession. More than 90,000 tech employees were laid off in 2022. And companies will likely continue with more layoffs starting early in the new year.These are stark indicators that growth has stalled. Companies are tightening their belts and making difficult decisions about how to navigate choppy waters ahead. Businesses in the tech industry, including many of our customers, are forced to answer difficult questions:While each business will make decisions based on its unique circumstances, our customers have widely indicated that the momentum of their digital transformation initiatives will continue into the new year. While they won’t be at the same frenetic pace as immediately following the COVID pandemic, their 2023 initiatives will focus on areas that drive meaningful business outcomes through innovation focused on real-time personalization, fraud detection, and AI.Our experiences in 2022 showed us that digital acceleration is not just a flash in the pan. Though most customers no longer see the same peaks as during the height of COVID lockdowns, it’s clear that digital is here to stay. Digital channels drive new revenue channels, meet user demands, and foster innovation.  However, digital is not a free lunch. Digital channels require performance beyond the capabilities of legacy technology and introduce new market competition — often cloud native. Hence, enterprises now treat real-time latency at scale as a competitive advantage.We have seen this ourselves, as Redis has helped customers perform real-time data analysis to improve product recommendations, fraud detection, and financial transaction scoring.Real-time performance enables businesses to provide customers with a seamless and personalized shopping experience. By instantly analyzing data across multiple e-commerce domains, a shopper’s purchasing behavior is analyzed in microseconds. Using that data, online retailers can provide a personalized customer journey that is curated to the individual, which increases the likelihood of purchase conversion. The result: a better shopping experience for the customer and increased revenue for the retailer. It’s a win-win.As an example, our Ulta Beauty case study showcases all of these elements.felt that their security initiatives had not kept up with digital transformation and introduced new business risks. Introducing new digital channels also introduces new cybercrime attack vectors and security complexity that legacy solutions were not built to handle.Because cybercrime is a cat-and-mouse game, real-time analysis can help financial institutions build better (and faster) mousetraps. Millions of transactions can be analyzed in milliseconds, as they occur to prevent fraudulent transactions at the point of purchase.  When transaction-risk scoring and anomaly detection is performed in-flight, as part of the transactional path against known malicious patterns, these technologies can block the purchase before it’s committed or immediately refer it for forensic analysis. This saves financial institutions the expense, and consumers the headache, of remediations after fraud has already occurred.Read our Simility case study on fraud detection and transaction scoring.AI has come so far that you might have been asking if this blog was written by ChatGPT. (Just kidding, I’m not a robot.). That said: AI is now so advanced that it can write essays as well as an undergraduate student, negotiate lower utility bills, and even delve into a realm once thought untouchable by machines: creating works of art. AI is now everywhere, and its adoption continues at a rapid pace.There are also tremendous business implications for artificial intelligence and machine learning. Machine learning can be used to analyze patient healthcare data and to power interactive customer chats. But to deliver on the promise of artificial intelligence and machine learning within an interactive digital environment, you must store vast quantities of data to be accessed in real-time.To see how AI works in practice, see the iFood case study.2022 was a wild ride, but through the ups and downs, there were many lessons. Let’s carry this insight into the future and work together to make 2023 a happy and successful new year.Learn more about how Redis Enterprise can prepare your business for the times ahead.Book a Meeting"
142,https://redis.com/blog/serverless-databases-as-a-service/,What Serverless Databases as a Service Accomplish – and Why They Matter,"April 24, 2023",Yiftach Shoolman and Filipe Oliveira,"We could lead with the technical advantages: flexible scaling, easier management of clusters and nodes, and offloading complicated resource analysis. But we know we can get your attention by saying Serverless Databases as a Service prevents overspending.Selecting the right cloud database has become a progressively intricate challenge. To optimize performance and cost, developers must sift through a wide variety of instance types, determine the optimal number of cores, and consider a variety of pricing options.The emergence of serverless Database as a service (SDBaaS) offers a promising resolution to these complexities. SDBaaS promises streamlined management, adaptable scaling, and cost-efficient operation. In particular, it can have a positive outcome on real-time use cases.SDBaaS refers to cloud-based database services that eliminate the need for developers to manage clusters or nodes. Instead, developers can create a database with a simple click or API call. Upon creation, an endpoint for database access (i.e., IP:Port represented as a URL) is received, and thereafter, the database adjusts itself (throughput and memory/storage-wise) to accommodate the application’s load – without requiring any administrative input.SDBaaS providers are responsible for ensuring their infrastructure is appropriate. That’s unlike instance-based DBaaS, where developers must select an instance type from a list of dozens or even hundreds of options to create a cluster to host their dataset.SDBaaS pricing is generally determined by two factors: the size of the dataset and the cost of database operations. Some vendors charge separately for reads and writes.A variety of billing options are available:Many factors influence the decision to use SDBaaS, but the following are the most crucial.Choosing the right instance type for your workload is difficult. In instance-based DBaaS, developers must choose among dozens or hundreds of cloud instances without understanding how the service works. Some instance-based DBaaS vendors fail to disclose on their pricing pages that the size of the dataset that can be stored in their service instances is much less than what they actually provide. The result is that developers choose the wrong instances, scale up (or scale out) sooner than they’d planned, and pay more than they should. Next time, that developer may choose larger instances to host the database, just to be safe, and it’s well-known that overprovision costs significantly more.Most developers don’t know how many cores a database workload actually requires, and it could be argued that they should not need to. That calculation is a complex one, which most developers have no chance of solving even if they have previous experience with a database. Whether the service is based on an open-source project or whether you used to run commercial software on-premises, in many cases, you will find that the DBaaS vendor created a fork of the software. Again, that leads to spending more money and scaling up (or scaling out) sooner than planned.A developer who uses instance-based services is responsible for scaling and deciding when to use new instances. In some cases, they are forced to change how their application works with the database, for example, when moving from a non-cluster mode to a cluster mode database. To choose a managed cloud service but be forced to operate it without the tools or knowledge seems absurd.SDBaaS ensures flexible scaling (to cope with the ups and downs), eliminates the need to manage anything, and prevents you from paying your DBaaS vendor a fortune.Architecting real-time applications goes beyond infrastructure costs. You must also ensure that your chosen database can handle the load with a latency that guarantees end users have responsiveness.As a rule of thumb, every millisecond spent in the database is a hundred times this number for the end users.A typical real-time application must respond within 100 milliseconds, preferably less, for every user request, which means the database should handle any query (read or write) in less than a millisecond. How are your applications stacking up against that metric?Redis Enterprise Cloud was built on a serverless architecture from the start, allowing developers to dynamically configure their data (dataset size plus replica if needed) and throughput (operations per second) limits and to be billed just for what they set. Using a comprehensive algorithm, we select the appropriate instance type for the cluster and we distribute Redis shards so that any workload can be processed in an average of less than a millisecond.Redis relies on DRAM, which is significantly more expensive than SSD. However, the majority of the cost for an SDBaaS is determined by the number of operations executed rather than by the amount of data stored. That point is particularly relevant in real-time scenarios in which one database can manage thousands or even millions of requests per second. Because Redis is engineered for high-speed performance, a single core can handle more operations than dozens or hundreds of cores in other databases. This makes each operation carried out in Redis highly cost-effective.This isn’t an academic exercise. We benchmarked read and update performance by running common workloads with a fixed dataset size of 50GB (plus replications) and multiple throughput levels (measured by operations per second) across two popular SDBaaS solutions: Amazon DynamoDB and Redis Enterprise Cloud. We found the following results:The read performance:The update performance:No matter which workload we tested, Redis Enterprise Cloud maintained an end-to-end latency of 0.5-0.6 msec.In contrast, DynamoDB performed no faster than 2.7ms (for read) and 4.4ms (for update), even with only a few requests per second (not shown). At 18,000 operations per second, DynamoDB latency increased to 6msec for read operations and close to 8msec for update operations, 12- and 16-times slower than Redis Enterprise Cloud, respectively.Finally, for this workload, we found that DynamoDB cannot scale beyond 18,783 operations per second using the default throughput quotas per table.Our next step was to create a cost/performance graph comparing the cost of each SDBaaS solution at different requests per second levels, as well as the performance advantage Redis Enterprise Cloud provides over DynamoDB (averaged results for read and update).The results speak for themselves:When it comes to 1,000 requests per second, Redis Enterprise Cloud is already 15% cheaper than DynamoDB (as well as 6.44 faster). At 18,000 requests per second, Redis Enterprise Cloud is less than 2% of DynamoDB’s costs and over an order of magnitude faster.Obviously, we at Redis are convinced that SDBaaS Redis is ideal for real-time applications. It is easy to manage, flexible, scalable, and extremely cost-effective. The cost of operation is much more meaningful than the cost of storing the data, and each operation in Redis is extremely cheap. Moreover, it ensures real-time experience for your end users, as any database operation can be executed in less than 1 msec.Do your own testing, if you like, and see if our performance data matches yours.Get a serverless Redis for free if you don’t already have one."
143,https://redis.com/blog/azure-cache-for-redis-enterprise-tiers/,New Database Scaling and Security Features in Azure Cache for Redis Enterprise Tiers,"April 26, 2023",Shreya Verma,"As applications encounter rising data volumes and user counts, developers can struggle to scale their databases and maintain security. Database scaling can be a complex operation. When you get it wrong, the result is downtime or service disruption, resulting in frustration for end users, embarrassment for the tech staff,  and loss for the business.It isn’t just a matter of coping with user demand. Databases can become a target for cyberattacks or fraud, particularly as a company becomes more prominent and its data volumes increase. Maintaining database security can become a real challenge.To help address these common challenges, we added some new preview features to the Azure Cache for Redis Enterprise tiers. Here’s what changed.Database scaling is always top of mind for application developers. Imagine that you launched a new game, and it went viral. Within a few days, the number of gamers grows from a few hundred to millions. Your database needs to handle the increased volume of data, connections, and user requests.The new scaling feature in Azure Cache for Redis Enterprise tiers allows you to scale your Enterprise database with a click of a button. With the new in-place scale operation, you can both scale-up and scale-out a database. As this is an in-place operation, applications do not incur downtime. The scaling operation completes with minimal interruption to the database.With Redis Enterprise software, you can scale up by moving your database to a larger virtual machine (VM) with more shards. This is useful when there is enough under-utilized (memory and CPU) capacity on the machine to host more database shards or Redis processes.If you intend to scale a database, we recommend scaling-up to a higher cache type on Azure Cache for Redis Enterprise before you start to scale-out. With a larger cache, you get more memory and compute power. Unlike Redis Open Source, Redis Enterprise software uses the additional compute power to give an optimized shard placement for the database, which results in better overall performance and throughput.Scaling-out refers to adding nodes to the cluster followed by rebalancing, resharding, and then reoptimizing the shard placement within the database. This is useful if you are already using a larger cache type and need more physical resources to scale the database.Microsoft offers documentation on how to scale-up or scale-out on Azure Cache for Redis Enterprise tiers. As it explains, the scaling feature is available in preview.It is crucial to monitor and prevent unauthorized access and to guard against data breaches. The following two preview features, introduced on Azure Cache for Redis Enterprise tiers, allow developers to put strong security measures in place.One way to protect data from theft or interception is to use encryption. Azure Cache for Redis already offers Platform Managed Keys (PMKs), also known as Microsoft Managed Keys (MMKs), to encrypt the data on disk, and it does this by default. Additionally, the Enterprise and Enterprise Flash tiers support the ability to encrypt the operating system disk and persistent storage disk data using Customer Managed Keys (CMK). You can store keys in Azure Key Vault, which allows you to keep the keys used to encrypt the data separate from the data itself.Azure Cache for Redis Enterprise tiers now supports customer-managed keys in preview. Here’s how to configure CMK encryption on Enterprise tiers.Connection auditing is the process of monitoring database access. A real-time log captures who accessed the database, for how long, when the connection was established, and what authentication events were sent during that time. If someone gains unauthorized access to a database, the connection audit logs can provide an entire trail of events.Azure Cache for Redis Enterprise tiers now supports connection auditing in preview.  The connection auditing on the Enterprise tiers uses the built-in audit connection events functionality in the Redis Enterprise software.Here’s how to enable connection auditing on the Enterprise tiers of Azure Cache for Redis.Maintaining data requires both scalability and security. Businesses can ensure that their databases can handle rising data volumes and remain secure from cyberattacks by putting in place strong security measures and choosing the right scaling approach. To confirm that the organization’s data is kept safe and secure, use Azure Cache for Redis Enterprise tiers to stay current with the most recent trends in database security and scaling."
144,https://redis.com/blog/kubecon-database-trends/,Database Trends Spotted at KubeCon Europe,"April 26, 2023",Steven J. Vaughan-Nichols,"Database vendors need to become more active in the cloud-native community — particularly to counter the scaling issues related to Kubernetes and stateful applications.Containerization and Kubernetes continue to revolutionize the development and deployment of cloud-native applications, largely because these technologies provide enhanced scalability, portability, and flexibility. But integrating databases into these environments raises a new set of challenges.I’d like to say that those challenges are being overcome. But, after attending last week’s KubeCon + CloudNativeCon Europe 2023 in Amsterdam, I have to conclude that we’re still a long way from fixing these problems.The database/container issue all boils down to questions about data persistence and storage. Containers are ephemeral. They’re easily created, destroyed, and replaced. While this is beneficial for stateless applications, it poses a significant challenge for stateful applications that depend on databases, which, in turn, require persistent storage to maintain data integrity.True, Kubernetes offers Persistent Volumes (PV) and Persistent Volume Claims (PVC) to provide a durable storage solution for stateful applications. PV and PVC allow databases to maintain their data even if the container running the database is replaced or destroyed.But that’s not enough. This is not a solved problem.“I think it’s one of the most important problems to solve right now,” said Petter Sveum, Veritas senior distinguished engineer, in a KubeCon panel discussion. “There are lots of different solutions available to the market, but database scale remains a real problem.”“You have larger workloads being driven into Kubernetes with multi-terabyte volumes across multiple namespaces, perhaps even multiple sites,” Sveum pointed out. “Solving it requires a lot of attention to detail in traditional architecture. So why would it be any different than Kubernetes?”Instead, Sveum has run into several teams that were really surprised. They go, ‘Oh, my 10-terabyte volume can’t be snapshotted within the time I expect!’ They don’t get that ‘It’s not fast, right? It takes minutes, hours.'”Guillaume Savage de Saint Marc, a Cisco VP of engineering, agreed with Sveum’s assessment. “It’s awkward dealing with edge computing that needs the combined resources from stateful and serverless services in a cloud-native environment.”Progress has been made with databases and cloud-native computing. But the bottom line, said Xudong Ren, Huawei’s chief open source liaison officer, “There’s no mature solution.”Everyone agreed that DBMS vendors need to do more with Kubernetes.“We’re still relying on the legacy infrastructure component instead of investing in porting it to Kubernetes,” Sveum said. In his view, DBMSs remain in virtual machines (VMs) and servers and are called upon, as needed, from Kubernetes-orchestrated containers. We need more and better integration among DBMSs, containers, and Kubernetes, he asserted.That isn’t stopping companies from running databases in containers. According to the Data on Kubernetes (DoK) community, many organizations are already running data services via Kubernetes operators – software extensions that use custom resources to manage applications and their components. Ideally, an operator encapsulates a real-world operations team’s knowledge and expertise and codifies it into software.Notice the ominous word “custom”? The top problem for using data on Kubernetes is a lack of integration with existing tools. As a DoK report notes, “There remain few known good practices for running data on Kubernetes.”Melisa Logan, DoK director and CEO of Constantia summed it up during a KubeCon DoK panel. “To handle Day-2 operations for data workloads on Kubernetes, organizations rely heavily on operators. But they present several challenges, including lack of integration with existing tools; lack of interoperability with the rest of their stack; varying degrees of quality; and lack of standardization.”Yet most people use at least 20 operators, according to the 2022 Data on Kubernetes Report. “For those evaluating their options, the challenge is further complicated by choice; the number of operators continues to grow, with Operator Hub currently listing over 270,” said Logan. “Without operator standards, how can end users possibly evaluate each one to know whether it meets their needs?” Clearly, much more work needs to be done.Now, if only there were more people who were qualified in both Kubernetes and databases! This is a common refrain heard by Kubernetes users, not just from those who work with databases. There is not enough Kubernetes expertise to go around, never mind DBAs who know their way around Kubernetes.As Sveum remarked, “We have a real shortage of people with an understanding of systems and infrastructure, how they actually are executed, and how they can leverage automation platforms as well as stacks like Kubernetes.”We need people who can be the glue between the developer team that makes grandiose requests and then expect the platform to deliver.That’s not to say that Kubecon had no database news. There were a few announcements. For example, Cisco introduced a new open-source project, VMClarity, for securing VMs in cloud-native environments. VMs, of course, host both containers and DBMSs.Alternatively, some database news is narrowly implemented. For instance, Fermyon Technologies added local stateful storage capacity with its Fermyon Cloud Key Value Store, With this, users can persist non-relational data in a key/value that remains available for a serverless application via WebAssembly framework Spin. A developer can make what appear to be ordinary API calls to DBMSs such as Redis, PostgreSQL, or MySQL. Naturally, this only works in the Fermyon ecosystem, but it’s still an interesting step forward.Taken all-in-all, the message I got from KubeCon is that more work – much more work –  remains to be done with DBMSs and Kubernetes. Yes, you can do useful things today with stateful workloads and cloud-native computing. But it requires custom programming, which doesn’t scale well. Bridging this gap requires more effort both from the cloud-native and DBMS industries and communities.Redis is at the forefront of database companies taking advantage of the container ecosystem, as we explain in Running Redis on Kubernetes."
145,https://redis.com/blog/chatgpt-memory-project/,Introducing the ChatGPT Memory Project,"May 2, 2023",Shahrukh Khan and Navdeeppal Singh,"ChatGPT Memory responds to context length limitations in large language models (LLMs) used in AI applications. The ChatGPT package uses Redis as a vector database to cache historical user interactions per session, which provides an adaptive prompt creation mechanism based on the current context.ChatGPT, the AI chatbot created by OpenAI, has revolutionized the realm of intelligent chat-based applications. Its human-like responses and capabilities, derived from the sophisticated GPT-3.5 and GPT-4 large language models (LLMs) and fine-tuned using reinforcement learning through human feedback (RLHF), took the world by storm since its launch in November 2022. The hype machine is in full force.Some ChatGPT interactions are entertainingly silly, some uses are worrisome, and they raise ethical concerns that affect many professions.However, everyone takes for granted that this technology inevitably will have a significant impact. For example, Microsoft is already using these models to provide an AI-based coding assistant (GitHub Copilot) as well as to support its search engine (Bing). Duolingo and Khan Academy are using them to power new learning experiences. And Be My Eyes uses these tools to offer an AI assistant that helps visually impaired people.Despite the success of these language models, they have technical limitations. In particular, software developers who are exploring what they can accomplish with ChatGPT are discovering issues with the amount of context they can keep track of in an ongoing conversation.The context length is the amount of information from a previous conversation that a language model can use to understand and respond to. One analogy is the number of books that an advisor has read and from which they can offer practical advice. Even if the library is huge, it is not infinite.It is important to make good use of the context length to create truly powerful LLM-based applications. You need to make clever use of the available context length of the model. That’s especially so because of cost, latency, and model reliability, all of which are influenced by the amount of text sent and received to an LLM API such as  OpenAI’s.To resolve the issues with limited context length in AI models like ChatGPT and GPT-4, we can attach an external source of memory for the model to use. This can significantly boost the model’s effective context length and is particularly important for advanced applications powered by transformer-based LLM models. Here, we share how we used Redis’ vector database in our chatgpt-memory project to create an intelligent memory management method.Let’s start by taking a deeper look into why context length matters.ChatGPT’s context length increased from 4,096 tokens to 32,768 tokens with the advent of GPT-4. The costs for using OpenAI’s APIs for ChatGPT or GPT-4 are calculated based on the number of conversations; you can find more details on its pricing page. Hence, there is a tradeoff between using more tokens to process longer documents and using relatively smaller prompts to minimize cost.However, truly powerful applications require a large amount of context length.Theoretically, integrating memory by caching historical interactions in a vector database (i.e. Redis vector database) with the LLM chatbot can provide an infinite amount of context. Langchain, a popular library for building intelligent LLM-based applications, already provides such memory implementations. However, these are currently heuristic-based, using either all the conversation history or only the last k messages.While this behavior may change, the approach is non-adaptive. For example, if the user changes a topic mid-conversation but then comes back to the subject, the simplistic ChatGPT memory approaches might fail to provide the true relevant context from past interactions. One possible cause for such a scenario is token overflow. Precisely, the historic interactions relevant to the current message lie so far back in the conversational history that it isn’t possible to fit them into the input text. Furthermore, since the simplistic ChatGPT memory approaches require a value of k to adhere to the input text limit of ChatGPT, such interactions are more likely fall outside the last k messages.The range of topics in which ChatGPT can provide helpful personalized suggestions is limited. A more expansive and diverse range of topics would be desirable for a more effective and versatile conversational system.To tackle this problem, we present the ChatGPT Memory project. ChatGPT Memory uses the Redis vector database to store an embedded conversation history of past user-bot interactions. It then uses vector search inside the embedding space to “intelligently” look up historical interactions related to the current user message. That helps the chatbot recall essential prior interactions by incorporating them into the current prompt.This approach is more adaptive than the current default behavior because it only retrieves the previous k messages relevant to the current message from the entire history. We can add more relevant context to the prompt and never run out of token length. ChatGPT Memory provides adaptive memory, which overcomes the token limit constraints of heuristic buffer memory types. This implementation implicitly optimizes the prompt quality by only incorporating the most relevant history into the prompt; resulting in an implicit cost-efficient approach while also preserving the utility and richness of ChatGPT responses.ChatGPT Memory employs Redis as a vector database to cache historical user interactions per session. Redis provides semantic search based on K-nearest neighbors (KNN) search and range filters with distance metrics including L2, Inner Product (IP), and COSINE. These enable adaptive prompt creation by helping to retrieve the semantically-related historical user interactions using one of the distance metrics.Furthermore, the ChatGPT Memory project takes advantage of the vector indexing algorithms that Redis supports, including the FLAT index (which employs a brute-force approach) and the optimized hierarchical navigable small world (HNSW) index. Redis supports real-time embedding creation/update/delete (CRUD) operations for managing this process in production.In addition to mitigating the shortcomings of the heuristic memory limitation, ChatGPT Memory allows real-time management of concurrent conversational sessions. It segregates the history of each session, which relates to the user’s past interactions with the chatbot, for each chat session. Once the ChatGPT assistant responds to a user query, both the query and the assistant’s response are embedded using OpenAI’s embedding service. Then the generated embeddings are indexed in a Redis index for later retrieval.The subsequent interactions are carried out as follows:This empowers users to get better, personalized answers because the system has more information to draw on.Before you use ChatGPT Memory, you need to clone the repo and install the dependencies. You also need an OpenAI API key and access to the Redis cloud-based vector database (which you can try for free).Once you obtain these credentials, set them as environment variables named OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, and REDIS_PORT by plugging in the value in the following bash script.After that, you can start using ChatGPT Memory by writing just a few lines of code.Create an instance of the Redis datastore connection.Instantiate the OpenAI embedding client for vectorizing conversation history.Create the memory manager for orchestrating semantic prompt creation based on the current context.Then connect to the ChatGPT API.Finally, you are all set to interact with the ChatGPT client with an infinite contextual and adaptive memory powered by the GPT and the Redis datastore.So, what’s it look like? Consider these two examples of conversations with ChatGPT. They illustrate the difference in performance between the two modes of operation.When the memory feature is not activated, the ChatGPT model can’t retrieve any information provided by the user in previous interactions – even from only a few sentences back.The two conversations presented have a similar message flow. However, the conversation where the memory feature was not enabled showed that the ChatGPT model could not recall any information that the user had provided. In contrast, when the memory feature was enabled, the model remembered specific details about the user and offered a personalized and customizable conversational experience.We believe that ChatGPT Memory is a valuable addition to the growing ecosystem of tools designed to improve LLMs’ capabilities. It presents a great opportunity for developers to build powerful and contextually intelligent AI applications.ChatGPT Memory’s solution also highlights the essential nature of the Redis vector database, which caters to a multitude of AI use cases, including but not limited to LLMs.To try ChatGPT Memory yourself, head to our GitHub repository and follow the instructions in this post. You should be able to spin up an instance within minutes. Finally, if you’re interested in learning more about building LLM apps with Redis and tools like LangChain, register for this upcoming RedisDays virtual session."
146,https://redis.com/blog/provision-manage-redis-enterprise-cloud-hashicorp-terraform/,Provision and Manage Redis Enterprise Cloud Anywhere with HashiCorp Terraform,"January 5, 2021",Aviad Abutbul,"Today we are pleased to announce the availability of the HashiCorp Terraform Redis Enterprise Cloud provider. While development teams embrace more and more DevOps principles such as continuous integration/continuous delivery (CI/DC), the need to manage infrastructure as code has become almost a must-have capability for any cloud service. A leading tool in the infrastructure as code space is HashiCorp Terraform, supporting the major cloud providers and services with its providers and modules cloud infrastructure automation ecosystem for provisioning, compliance, and management of any cloud, infrastructure, and service.If you’re not familiar, Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently, managing everything with code. Configuration files are used to describe the components needed to run your application. Terraform can manage almost any infrastructure component, including bare metal, virtual machines, cloud instances, and many other popular Infrastructure-as-a-Services (IaaS) services. Components in your application stack are represented as resources in Terraform. A Terraform provider is responsible for understanding API interactions of the relevant resource and exposing the resource’s features out to the world.To meet this increasing demand, Redis has developed a Terraform provider for Redis Enterprise Cloud. The HashiCorp Terraform Redis Enterprise Cloud Provider allows customers to deploy and manage Redis Enterprise Cloud subscriptions, databases, and network peering easily as code, on any cloud provider.To demonstrate how the Redis Enterprise Cloud’s Terraform Provider works and what it can do, let’s create a sample subscription with two databases, one of them using the RedisJSON module.Prerequisites:Let’s start by storing our Redis Enterprise Cloud API keys as environment variables:$ export REDISCLOUD_ACCESS_KEY=<your API access key>$ export REDISCLOUD_SECRET_KEY=<your API secret key>Now let’s create a new folder and place the Terraform HCL file shown here. Let’s call the file rediscloud-tf-example.tf:The next step is to initialize the terraform environment by running this command:$ terraform initThe result should like like this:After Terraform has been successfully initialized, run the following command to have Terraform create the plan:$ terraform planThe output should look similar to this:Now let’s have Terraform do its magic and provision our databases:$ terraform applyThe terraform apply command might take several minutes to complete once finished, it should produce a result that looks like this:As you can see, we have now provisioned two new databases. All you have to do is to grab the database’s endpoints and plug them to your application. You can also use RedisInsight, the GUI for streamlined Redis application development, to connect and explore your data.The Redis Enterprise Cloud console displays the new subscription and database:Once you are done having fun with your database you can tear down everything by running this command:$ terraform destroyWe’ll continue to extend the capabilities of the HashiCorp Terraform Redis Enterprise Cloud Provider. If you have an idea or feature you’d like us to support in the provider or you find a bug, please let us know by opening an issue in our GitHub repository. You can also drop us a line at pm.group@redis.com.For production use cases, we also recommend using Terraform Cloud, which is HashiCorp’s cloud-based offering for Terraform. This will enable you to store credentials in a secure environment out side your configuration file and source control. Terraform Cloud also comes with collaborative features such as free remote state storage, custom workspace permissions, version-control system integration, as well as policy as code. You can sign up for free at terraform.io/cloud."
147,https://redis.com/blog/scott-mcnealy-managerial-courage/,How Scott McNealy Learned About “Managerial Courage”,"August 27, 2020",Fredric Paul,"Scott McNealy is a Silicon Valley legend. As the helm of Sun Microsystems from 1984 through 2006, he oversaw the development of the Java programming language, the Solaris operating system, ZFS, the Network File System (NFS), and SPARC—while also selling millions of Sun workstations and servers. Since leaving Sun in 2006 (the company was acquired by Oracle in 2009), McNealy has been CEO of Wayin and co-founder and board member of Curriki.For the inaugural issue of Rediscover Magazine, we chatted with McNealy on his hard-won experience on how to manage a company during a crisis. Unfortunately, we couldn’t fit all his insights into the magazine, so this blog post covers how McNealy learned to deal with difficult situations, including the people who who mentored him.The full McNealy interview includes illustrative anecdotes from his time at Sun, his thoughts on how leaders can rediscover their management skills in times of crisis, and why he describes being a CEO as “being in the piñata.”Redis: How did you learn to manage through crises? Did you have mentors or did you learn on the job through trial and error?Scott McNealy: Mostly through error, but I have had some really great mentors in my life.My dad was vice chairman of American Motors and president of AMF. You know, Archie Manning trained his kids how to be quarterbacks. I learned at the foot of my dad’s chair at night as he’d have a Manhattan and read his mail.My first boss was Roger Penske. And I worked for Roger for a buck-seventy-five an hour washing cars in Penske Chevrolet in Southfield, Michigan, as my first job. And funny story, a couple decades later, he and I are on the GE board together, which was kind of fun.Another big mentor of mine was Ken Oshman. He was the O in ROLM, the old telecommunications company. And he was maybe one of the most underappreciated and under-the-radar giants of Silicon Valley. He was Silicon Valley culture.In board meetings, Ken would ask, “What do you think I should do?” He’d say, “Here are your three options. Here’s the one that’s the easiest to do, but you know the right choice is to do this.” And I’d look at him and say, “Oh, you son of a gun. That is so true. The easy choice is just kicking the can down the road. I know which one is the right one. That’s hard. That’s not fun. I don’t wanna do that. But you’re right, that’s the one I should do.” He really taught me the power and the importance of managerial courage.And then there was Jack Welch, when I was on the board of GE. I got to spend only a couple years with him. But there’s books been written about what you can learn from Jack Welch.I had a lot of really good mentors. I’ve had a lot of really great board members. But mostly, I’ve had two really good VPs of HR, Crawford Beveridge and Bill MacGowan, who taught me the power and importance of culture and treating people right.Redis: What do you see as the legacy of having to make all those tough choices?I think if you talk to the 235,000 ex-Sun employees, we get a very, very high percentage of people saying it was the best place they ever worked. And we got a lot done. Yeah, we had a pretty awesome financial outcome, but we weren’t Amazon or Google.But we changed the industry in a big-time way. And we spawned hundreds of CEOs who went off to run other companies. And the technology that we created is still incredibly widely deployed everywhere. And we never cheated. We never lied. We didn’t have any scandals. I never had to do the perp walk and explain to my boys why I’m in Club Fed. Those are all good things.Read the full interview with Scott McNealy in Rediscover Magazine, available free (online and in print). The premiere issue features more than a dozen stories on rediscovery, the power of data, real-time financial services, database trends, serving artificial intelligence, and managing remote workers, plus interviews with Redis creator Salvatore Sanfilippo and more tech leaders."
148,https://redis.com/blog/database-developers-conferences-2023/,The Top Conferences for Database Developers to Attend in 2023,"February 7, 2023",Redis,"If you’ve been staying home with your cat and avoiding public events for the past two years, that’s perfectly understandable. But 2023 may be the year when things start to get back to normal. It’s time to get back to doing the fun things we’ve been missing out on—namely, attending tech industry conferences!Conferences and other tech events offer the ideal opportunity to build your career skills, learn about new technologies and new approaches to software development, and network with other engineers and tech industry folks. You can also learn how other companies are solving the problems you may be struggling with.No matter your specialty or your location, there’s an event for you. Attend one nearby, pick an exciting destination to visit, or even just attend online. Most are hybrid events with a virtual track, and some are fully virtual events.In many cases, the details are limited—particularly those scheduled for later in 2023, when the venue and agenda are still scribbled on the back of an organizer’s envelope. However, the information here should help you organize your schedule and travel budget.See you out there in 2023!Dates throughout all of 2023Multiple locationsCost to attend: TBDDevOps Days is a worldwide series of technical conferences run by local volunteers and dedicated to the world of DevOps—software development, IT infrastructure operations, and the intersection between them. Most events feature curated talks along with self-organized open-space content on topics such as automation, testing, security, and DevOps culture. The first 2023 event will be held in Goiânia, Brazil, on March 3-4, with additional March events in Los Angeles and Salt Lake City, Utah.DevOps Days happen all over the world throughout the year. There are 2023 events planned in Tokyo, Geneva, Amsterdam, and Cairo, just to name a few. Check out the DevOps Days website to find the event nearest you—there’s sure to be one in your area.Brussels, BelgiumFebruary 4-5, 2023Cost to attend: FreeFOSDEM is a free, non-commercial event for developers of free and open source software to “meet, share ideas, and collaborate.” This conference is extremely busy and offers something for everyone. Recent events have featured 400 speakers, 500 events, and a wide variety of topic tracks. The FOSDEM schedule page contains an overview of everything going on this year. No registration is required to attend this event, and you can also watch all the talks live online. For more information, check out the FOSDEM FAQ page.In addition, FOSDEM is now accepting presentation proposals for developer room talks, lightning talks, and even online presentations; the proposal deadline is December 15, 2022.Oakland, CaliforniaFeb. 15-17 (live); Feb. 21-23 (virtual)Cost to attend: PRO Pass $995; PREMIUM Pass $1,595; student and government discounts availableThe world’s largest engineering and developer conference with a focus on new technologies, Developer Week 2023 will be a hybrid event with three days onsite at the Oakland Convention Center and three days online. The event will offer at least 15 conference tracks on topics such as API and microservices, AI/ML, containers and Kubernetes, and more, and feature 250 workshops, sessions, and keynotes by business leaders from companies such as AWS, Netlify, and Puppet. Check out the full Developer Week schedule, and don’t miss its online hackathon for a chance to win cash prizes.Redis Developer Day in Tel Aviv is an opportunity to learn about the latest tools, technologies, and solutions from Redis and to network, not just with Redis experts, including our Co-Founder and CTO Yiftach Shoolman, but with other professionals in your field. Enjoy the breakfast and networking, stay for the insightful keynote, and the numerous customer sessions that break down Redis’ most powerful use cases, including a detailed dive into the evolution of Redis Stack and RedisInsight.Tel Aviv, IsraelFebruary 21, 2023Cost to attend: FreeSan Francisco, CaliforniaMarch 7-8, 2023 (Bootcamp dates TBD)Cost to attend: $999 (conference pass); $3,998 (conference plus Bootcamp)TrailblazerDX is Salesforce’s developer conference featuring 100 expert-led breakout sessions, hands-on workshops, certification opportunities, and a four-day pre-conference bootcamp. The event unites developers across Customer 360, MuleSoft, Slack, and Tableau, bringing together developers, admins, architects, and others, culminating with the Trailblazer Celebration, an epic catered party with a performance by a special musical guest.San Francisco, CaliforniaMarch 20-24, 2023Cost to attend: $188 (Expo only); $1,101 (GA); $1,784 (all-access pass)The game industry’s premier professional event, GDC attracts programmers, artists, game designers, and business leaders for an exciting five days of education, inspiration, and networking. Conference topics include game narrative, VR/AR, game career development, and more (consult the list of scheduled GDC 2023 sessions). The GDC Expo showcases the latest game development tools and services from companies such as Epic, Nvidia, Oculus, and Sony. The main stage will host a lineup of entertainment and prominent industry leaders, and there will be several award ceremonies, including the can’t-miss Game Developers Choice Awards.Atlanta, GeorgiaApril 4-6, 2023Cost to attend: $595 (conference pass); $895 (conference and workshop pass)Devnexus is the largest Java platform conference held in the United States. This year’s event is expected to draw 2,000 professional software developers eager to learn about the latest technology trends, hear from acclaimed industry experts and technologists, and network with their nerdy peers. The first day of the three-day conference will be a full day of hands-on workshops on topics such as Java programming, software design, and Kubernetes, followed by two more days of presentations and live demonstrations. While this year’s schedule is not yet set, check out the 2022 presentation schedule for a taste of the kinds of presentations you can expect at this premier developer event.Amsterdam, The NetherlandsApril 17-21, 2023Cost to attend: TBDThe Cloud Native Computing Foundation’s flagship conference is where technologists and experts from leading open source and cloud-native communities come together to network and share their expertise on these bleeding-edge technologies. The event draws a wide variety of attendees, including application developers, IT ops folks, and product managers—in fact, anyone looking to learn more about cloud-native technology. An early peek at the 2023 schedule is available online and will be firmed up as the event approaches.Salt Lake City, UtahApril 19-27, 2023Cost to attend: TBDThis year’s PyCon 2023 will be the 20th anniversary of the development conference dedicated to all things Python. Join your Python-coding peers from all over the world for a lively expo, fascinating keynote presentations, community talks, and PyCon’s “famed” lightning talks. In addition, there will be a job fair, summits, PyLadies auction, and more. Following the conference days, there are four additional days of sprints, free and open to the public, offering an opportunity for anyone to collaborate and contribute to a project—even first-timers! For those who can’t attend in person, live streams of all talks and keynote sessions during the main conference days will be available online.Location: VirtualApril 20, 2023Cost to attend: TBD. This may be free; current data is uncertainPresented by online tech conference producer Conf42, this year’s Golang event is for anyone maintaining a project written in Go, using Go in production, or who just loves the language. This event is 100% virtual and will offer presentations and talks on topics such as cool Go libraries, frameworks, and tools; new Go techniques and tricks; adopting Golang for the first time; and getting started with Go when coming from another language. If you’d like to present at the conference, you have until March 20, 2023, to submit your proposal.London, UKApril 24-27, 2023Cost to attend: £1,599 – £699 (early bird prices available until 12/15/22)Billed as the “conference for continuous delivery, microservices, containers, cloud, and lean business,” DevOpsCon is more than anything, the event for all things DevOps. This is your chance to meet internationally recognized DevOps thought leaders and benefit from their expertise. Check out the DevOpsCon 2023 session program for an early look at the keynote, workshop, and session titles. Those who can’t make it to London can participate online but must purchase a remote ticket. There will also be additional DevOpsCon events in Berlin and New York City later in the year.North America and Europe Virtual-HybridMay 9-12, 2023Cost to attend: $99 – $149Unite with 100,000 other women in tech to “foster innovation with purpose and impact” at this women-focused technology conference. This hybrid event brings together women, minorities, and allies from all over the world for live keynotes and panel discussions, technical workshops and training sessions, and both in-person as well as virtual networking opportunities. Conference themes include career-focused topics as well as more technical tracks such as software engineering, web development, data science, and emerging technologies—there’s honestly something for everyone. The first day of the conference is dedicated to a “Chief In Tech Summit” for senior leaders, VPs, and C-level executives in technology-driven companies. If you’d like to present a talk at the virtual segment, the application deadline is Feb. 1, 2023.Vancouver, CanadaMay 10-12, 2023Cost to attend: TBDOrganized by the Linux Foundation, the Open Source Summit is the place for thousands of open source developers, technologists, and open source community leaders to collaborate, solve problems, and share their knowledge about all things open source. It’s an umbrella conference composed of many different events dedicated to the open source community and the latest open source solutions. If you’d like to present at the Summit, the CFP opens in late December 2022. (The European version of this event will be held in Bilbao, Spain, September 19-21, 2023.)Location: TBD May 2023 (final dates TBD)Cost to attend: TBDDockerCon is a free event for developers who are building the next generation of modern applications and want to learn more about Docker for application development. The event also draws engineering managers and cloud architects and offers content targeted at all levels, from those brand new to Docker to advanced Docker users. In fact, developers who are new to Docker can attend pre-day, instructor-led workshops with hands-on training focused on getting started with Docker. For a sense of what you’ll learn at this event, watch the DockerCon 2022 presentations on demand (signup required to access content).Various global locations and onlineMay 2023 (final dates TBD)Cost to attend: in-person ticket price TBD; online attendance is freeMicrosoft Build is Microsoft’s hybrid event focused on developers and software development, where attendees get an early look at the newest Microsoft products and features. At Build, you’ll have the opportunity to learn about the latest innovations in code and application development, meet Microsoft engineers and connect with your coding peers, and learn new technologies and techniques to take your career to the next level. This is a global event. In addition to English, conference content will be presented in French, German, Japanese, Mandarin, and Spanish. There will be something for everyone: “ask the expert” sessions, intro to tech skills classes, product roundtables, and a “cloud skills challenge.” Watch the 2022 Build presentations on demand for a sample of what you’ll experience at this conference by developers, for developers.Location: Virtual (not live)June 5-9, 2023 (likely)Cost to attend: FreeThe Apple Worldwide Developer Conference (WWDC) is for software developers, engineers, and Apple fans of all stripes to connect with those who build the Apple products, services, and software they love. Apple often makes several important product announcements at this annual event, which has been 100% virtual since 2020. Along with the much-anticipated keynotes and product presentations, WWDC also features the Apple Design Awards (see 2022’s award winners). In addition, you can request an appointment with an Apple engineer, designer, or other experts for a one-on-one developer lab to get answers to your questions.Las Vegas, NevadaJune 26-29, 2023Cost to attend: $2,195-$2,395 (early bird pricing available until April 30, 2023)This June, thousands of Snowflake customers, partners, and industry peers will descend on Las Vegas to “make it snow in the desert” at the annual Snowflake Summit 2023. This in-person event is ideal for data engineers, data scientists, app developers, and data professionals of any kind. Learn about all the latest innovations coming to the Snowflake Data Cloud, emerging trends in data and analytics, and how to build and monetize data, tools, models, and applications in ways that were previously unimaginable. The event will offer an opportunity to take SnowPro Certification exams to boost your professional credentials. If you’d like to present at the Summit, the CFP deadline is Jan. 31, 2023.Tel Aviv, IsraelJune 2023 (final dates TBD)Cost to attend: TBDThis in-person event, which is hosted by Intel Software, draws architects, designers, engineers, and engineering managers from all over Israel to hear informative keynote speeches and presentations on topics such as enterprise and technical computing, AI/ML, and cybersecurity. There are also deep-dive technical sessions for developers and data engineers. Check out the 2022 agenda for a sense of what to expect at this year’s event.Las Vegas, NevadaSept. 18-21, 2023Cost to attend: TBDCloudWorld is the place to learn everything you need to know about building apps and running workloads with Oracle Cloud Infrastructure (OCI) for hybrid, multicloud, and dedicated environments. The 2022 event attracted more than 20,000 Oracle customers, business owners, IT administrators, and other industry professionals (you can watch the 2022 keynotes on demand). Attendees will get an early look at the newest Oracle products and features, along with exciting learning sessions and insights from Oracle experts from all over the world. The event culminates in a huge party at the world-famous Venetian with a special musical guest (to be announced). Subscribe to get updates about CloudWorld 2023 registration.San Francisco, CaliforniaOctober 2-6, 2023Cost to attend: Ticket prices range from $875 – $4,995QCon is an educational conference specifically for professional software developers, designed to help you “understand the emerging software trends you should pay attention to.” It’s a great opportunity to learn from practitioners driving innovation in software as well as leaders of innovative companies, discover emerging tools and trends, and validate your software development roadmap. In addition to presentations, the event offers a number of workshops and training sessions to help take your skills and career to the next level. The 2022 event featured 15 different topic tracks and over 75 technical talks, all of which you can watch on demand for a sense of what to expect at this practitioner-focused event.Location TBDOct 2023 (final date TBD)Cost to attend: FreeGoogle Cloud Next is a global, hybrid event and your opportunity to get “hands-on” with the newest Google Cloud products and technologies as well as meet the engineers who build Google Cloud. There are all sorts of special features and events designed to appeal to devs, including the Innovators’ Hive, DevFest, and the Drone Racing League. Developers watching the event online can sign in using their Google Developer Profile emails to gain access to exclusive content, badges, and more. With session tracks like Build, Modernize, Operate, and Collaborate, there are sure to be a number of interesting, informative presentations that you’ll want to check out, no matter your role. For a taste of what the event is like, watch any of the 2022 talks on demand, including the keynote with Google CEO Sundar Pichai.Chicago, IllinoisNov. 6-10, 2023Cost to attend: TBDThis event is the North American version of the Amsterdam event held in April, also organized by Cloud Native Computing Foundation. Right now, the event website is a placeholder, with details on speakers and topic tracks to come later. As with the European event, keynotes and sessions will be live-streamed on the virtual platform and available to watch on demand.Various global locations and onlineNov. 15-16, 2023Cost to attend: $1,895 in person; online attendance is freeIgnite is a Microsoft-hosted conference for engineers and IT professionals, and the company uses the event to give attendees a sneak peek at its newest products and features. While Microsoft Build in May is focused on developers, Ignite is more geared toward sysadmins and ops folks. More than 200,000 people attended the 2022 event virtually, with 7,500 in-person attendees at various satellite locations. Ignite is a truly global event, with presentations offered in six languages and a content program highlighting six spotlight locations around the world, including China, Japan, and Latin America. This event is a great opportunity to gain world-class training skills on various Microsoft technologies such as Azure, Microsoft 365, and the Microsoft Intelligent Data Platform. For a sample of what attendees can learn, you can watch a number of 2022 presentations online, including Microsoft CEO Satya Nadella’s 2022 Ignite keynote.Orlando, FloridaNov. 2023 (final dates TBD)Cost to attend: $650 – $3,575 (various conference packages available)SQL Server Live is an event focused on training for database administrators, analytics experts, SysAdmins, and other IT professionals. This event is a great opportunity to learn specific technical skills and abilities, such as how to integrate cloud-based data services, run SQL Server technology in the cloud, and plan for SQL Server recovery and availability. It features nearly 30 different tracks on topics such as database development, intelligent apps, and even soft skills for IT pros. The event is co-located with four other events, giving you a lot of (conference) bang for your buck.Las Vegas, NevadaNov 2023 (final dates TBD)Cost to attend: $1,799AWS re:Invent has become one of the largest cloud computing conferences in the world, and you can probably count on one hand the people you know who aren’t going. However, if you’re unable to attend in person, you can still register for free to watch the keynotes and leadership sessions online. In addition to all the informative sessions, you can participate in AWS Builder Labs to build practical skills in an AWS sandbox environment or expert-led AWS training bootcamps to help you prepare for an AWS certification exam. AWS re:Invent is a sprawling event that is also known for a variety of fun onsite activities such as a 5K race, a ping pong tournament, and the epic re:Play party (2022’s party featured performances by Chromeo and Thievery Corporation). Finally, a reason to visit Las Vegas!If you can’t make it to Vegas for the AWS flagship event, smaller AWS Summits are held throughout the year all over the globe. Keep your eye on the event page and watch for one near you in 2023.Of course, if you’re a Redis user, the place you most belong is with us at one of our many RedisDays events held throughout the year in a variety of locations. For details on upcoming RedisDays events, check out our RedisDays page. And sign up for the Redis newsletter, where we share events throughout the year where we have our own experts speaking or available on the show floor.NOTE: All ticket prices are subject to change without notice."
149,https://redis.com/blog/mobile-banking-app-design-tips/,From Databases to Dashboards: Mobile Banking App Design Tips,"May 8, 2023",Pam Baker,"Mobile apps are super easy and convenient for banking customers to use. But to build and maintain them? Not so much. Here are a few tips to make a developer’s job easier.Mobile banking apps conveniently deliver real-time account information for each customer on demand. This data often spans multiple distinct banking products that are consolidated into a mobile app’s account dashboard. Making that bit of wizardry happen requires a technical feat of unfailing precision and skill. We offer a few tips and tricks that can help developers who work on financial applications deliver the illusion of magic with the solidity of real-world substance.First, it’s important to get a handle on the challenge. Banks and other financial institutions, such as savings and loans (S&Ls) and credit unions, have a lot of different product lines. The product lineup typically includes checking accounts, money market accounts, savings accounts, certificate of deposit (CD) accounts, retirement accounts, credit cards, auto loans, home loans, mortgages, and other financial instruments and transactions.Each of those products typically rises from a separate business function, such as the mortgage department, the investment department, or standard banking business line. Each function or department has its own long-standing legacy infrastructure to support it – including its databases.That’s a lot of disjointed pieces. They need to be customized and assembled instantly in each of hundreds, thousands, or even millions of consumer banking app dashboards. With all that comes a lot of issues to solve, some foreseen and some forgotten.“What developers often forget is load latency,” says Richard Luna, CEO of Protected Harbor, an IT firm that works with software and application developers. “Running a summary query against a table when only a few users are connected is quick; the result is returned immediately. The situation changes when there are hundreds or thousands of connections. Now the load of replying to the summary traffic must contend with the user load – slowing down the ability to see the dashboard results.”To solve the latency problem, developers should use tools that can prefetch account balances and recent transactions once or twice a day from each database. That gives you a way to deliver those insights to the app with sub-millisecond latency.“There are two solutions we recommend. One is to use high availability and run the summary dashboard queries against the second server in the high availability pair to avoid being directly affected by user load. Two, maintain a table that only contains the summary values. Those values are reloaded from the main table on a timer or as-needed basis,” says Luna. “This is a faster and more efficient approach.”But there are other tips developers can use to smooth the processes involved in pumping the data from databases to dashboards in mobile banking apps.In an effort to cover all the bases, you can bog down a banking app’s performance by choosing the wrong data sources or by connecting too many sources.“It is crucial to carefully select the data sources that you will connect to your mobile app dashboard,” says Mohit Maheshwari, co-founder of outsourcing firm NMG Technologies. “Consider factors such as data accuracy, reliability, and relevance to ensure that the information displayed on the dashboard is accurate and up-to-date.”At first glance, this tip appears to be a no-brainer. In practice, it requires considerable thought.“Establish clear relationships between your data sources and the product fields you want to populate,” advises Vaibhav Kakkar, CEO of Digital Web Solutions, a company that handles backend tasks for digital marketing agencies. “This involves mapping out how different pieces of information should be structured and stored, as well as defining rules for accessing and updating that data in real-time.”Optimize performance by caching data on the device.“Fetching data from multiple sources and displaying it on a mobile app dashboard can impact performance,” says Maheshwari. “Caching involves storing a copy of the data locally on the app, which can be quickly accessed without making repeated requests to external data sources. This can greatly reduce the load on the app and provide a smoother user experience.”A good user interface (UI) has structure, yes, but be careful that the structure doesn’t choke data feeds.“Ensure that your app’s user interface can accommodate multiple types of data inputs and display formats seamlessly. This means designing flexible templates or layouts that can adapt to changing user needs while maintaining consistency across all screens and devices,” says Kakkar.It always is essential to test apps thoroughly before release. With banking apps, Kakkar recommends, the quality assurance process should include running automated tests on several combinations of input values, as well as a thorough check for inconsistencies or errors in how data is displayed or processed.It’s easy to get confused over which data sources feed which input. To cut the confusion before it starts, create a data dictionary.“It’s crucial to have a clear understanding of all the data sources, fields, and attributes,” says Youssef El Achab, a cloud security and DevOps consultant and blogger. “Creating a data dictionary can help you keep track of everything and avoid confusion or duplication.”You’ve heard this mantra a million times, but it’s especially essential for banking apps where regulations literally rule: In order to be reliable, data must be cleaned, standardized, organized (labeled), validated, and integrated. It’s crucial to validate data quality before integrating it into your app.“When working with multiple data sources, it’s common to encounter different data formats, such as CSV, JSON, or XML. Therefore, it’s important to standardize the data formats to ensure consistency and ease of integration,” El Achab adds.It’s one thing to provide for growth in data sizes and user counts. It’s quite another to plan for changes you can’t quite see coming yet.“As your app grows and evolves, the data requirements may change. Plan for scalability and flexibility in your data integration approach to accommodate future changes,” advises Maheshwari. “Avoid hardcoding data sources or data formats; Instead, use configuration files or dynamic data mapping techniques to make it easier to switch or add new data sources in the future.”Redis Enterprise can help IT teams build lightning-fast, scalable, and reliable mobile banking apps, complete with cross-product account dashboards. Read Redis Enterprise for Mobile Banking for the details."
150,https://redis.com/blog/learn-clustering-from-redis-experts/,Learn Clustering From Redis Experts,"May 9, 2023",Redis,"These five videos break down the general concepts of clustering through webinars, tutorials, walk-through demos, and even a real-world customer success story from a Fortune 500 company.So many large datasets, so little time.Clustering has been a major boon for DevOps. We define a cluster as “a set of cloud instances, virtual machine/container nodes, or bare-metal servers that let you create any number of Redis databases in a memory/storage pool that’s shared across the set.”Clustering allows IT teams to parse enormous blocks of data quickly, giving them more time to work on other projects, not to mention the flexibility, cost-efficiency, and reliability it generates in your applications. These five videos run the gamut to explain clustering and show how it’s used,  from high-level information for those just getting started with clustering to hands-on demos you can follow along with.How does clustering work? In this video, Justin Castilla, a senior developer advocate at Redis, breaks down clusterings’ top concepts. He explains how it amplifies availability for applications and scales for extra memory, CPU capacity, and throughput.“Scalability is the property of a system to handle a growing amount of work by adding resources to the system,” says Castilla. To further explain how clustering works, he focuses on the two most common scaling strategies: vertical scaling (also called scaling up) and horizontal scaling (scaling out).Castilla breaks the video into chapters: sharding, resharding, hash slots, high availability, and split-brain situation, making for a high-level but thorough examination of how clusters work.Take a closer look at this video’s description on YouTube; Castilla provides a helpful supplemental tutorial and Redis cluster specification links to help you get started.Click here to watch.Are you past the basic clustering concepts and ready to create your own Redis clusters? Justin Castilla is there again.In just ten minutes, Castilla walks you through his demonstration in four steps:It’s practical information that you can put to use.Click here to watch.In this presentation from RedisConf 2021, Uber software engineers Anders Persson and Bisheng Huang present the use cases that called for enhanced application scalability. They go through the process by which they migrated to a cluster from initial evaluation to production launch.Persson and Huang detail how their team built a cluster management library in Go to automate its cluster management. They explain how this new framework could handle several operations, such as adding and removing nodes, restarting nodes, scaling clusters horizontally and vertically, as well as failure recovery protocols.Watch and listen as they outline the important lessons learned during their migration process.Click here to watch.AWS was also on hand at RedisConf 2021 to discuss clustering. Its representative explained how clustering is seen as a solid, consistent strategy to establish high availability and maximize application performance.Presented by Madelyn Olson, a software engineer for Amazon ElastiCache, this presentation covers how clustering helps achieve millions of operations per second and the opportunities for improving performance and reliability.Olson introduced her presentation by saying she hopes “not to just give a bullet point list of what [a cluster does], but to give everyone an intuitive understanding of what it is and some of the choices that were made during development.” Watch her presentation to learn how to use meshes between different nodes in a cluster and how to shard data so that each shard has its own unique data set.Click here to watch.One of the challenges of using clusters is keeping data together that belongs to the same user or application. As Justin Castilla explains in this demonstration, data is stored in different shards based on its hash slot.What’s a hash slot? As Castilla describes it, “Hash slots are a way to distribute data across shards in [a cluster]. Each key is assigned to a single hash slot, and all keys in the same hash slot are stored on the same shard. This ensures that all of the data for a particular key is always stored together, which can improve performance and scalability.”Castilla  notes that hashtags are the piece that “allows you to group your data in a given slot since we don’t want to force you to reverse engineer the crc16 to make sure all the data ends up together.”Castilla demonstrates how hash slots work, provides examples of clusters, cluster slots, and cluster visualization, covers a section on primary shards, using hash slots, and reviews clustering limitations.Click here to watch.How do clusters work with key-value stores? Dive into Redis Clustering Best Practices With Multiple Keys to understand the value of working with keyspaces, avoiding CROSSSLOTS errors, and evaluating MULTI/EXEC transactions.Looking to understand how clusters power today’s microservices? Download our e-book Redis Microservices for Dummies for more information."
151,https://redis.com/blog/redis-om-node-js/,What’s Up With Redis OM for Node.JS?,"August 4, 2022",Guy Royse,"Redis OM for Node.js is still in its early days, but we’re making a lot of progress. Here’s what we added in the latest version and where we’re headed.I released a preview of Redis OM for Node.js a few months back. I was pretty happy with it at the time. It did the things it was supposed to do, at least this early in a development cycle.One element that made me particularly happy was the fluent interface for searching. You know, this thing:But the best part—the part I’ve been most happy with—is how much Redis OM for Node.js has been embraced and used by the developer community. Many of you have given me a lot of useful and actionable feedback, often via our Discord server. Plus, you fixed bugs and even sent in complete features from my rather sparsely defined issues on GitHub. Your assistance is truly appreciated.I’ve incorporated your feedback and your pull requests, and your contributions. In this post, I summarize some of the changes that we—and by “we” I mean “you and I”—have made to Redis OM.Early on, some rando on the Internet suggested that calls to .createEntity should take initial values for that Entity. I thought this was a great idea and adopted it in Redis OM.So what was once a bit verbose:Became much tidier:I even added a .createAndSave method to eliminate the call to .save because this is such a common pattern:Thanks for the suggestion. It was a solid one.Redis makes an awesome database but also a pretty good cache. Many of you suggested we needed a way to create expiring Entities.Ask and ye shall receive:Not sure why you’d want to expire a Mushroomhead album, but de gustibus non est disputandum.Nobody asked for these, but I wanted them anyhow! RediSearch has good support for location with the GEO type, and dates are used in all sorts of applications. So I added them as point and date, respectively.:And, of course, they’re perfectly usable with the fluent search interface:Probably my favorite change in Redis OM for Node.js is how it works with Node Redis. In the preview, you could .open a client that used Node Redis behind the scenes:Now, you can .use an existing connection from Node Redis:This lets you connect to Redis in all sorts of ways, not just with a simple connection string. I also changed .open and .use to return the Client to allow nifty one-liners:No preview is perfect; that’s why it’s a preview. I learned a lot by watching people’s mistakes while using Redis OM for Node.js. Mistakes that were the result of misunderstanding parts of the interface to Redis OM. Parts that need improvement because I had not sufficiently thought them through. These improvements resulted in a few important but breaking changes.I noticed a lot of folks being confused when they were using a string as a TEXT versus a TAG in RediSearch. So I made this distinction explicit by breaking the string type into two types: string and text.I also learned that it was unclear that the array type could only be an array of strings. I changed it to string[]:Almost everyone using Redis OM was using RediSearch and RedisJSON. They were choosing to store their documents as JSON documents. But in the preview, Redis OM used Hashes as the default storage mechanism.This resulted in everyone having to tell Redis OM to use JSON explicitly:To avoid this step, I changed the default:This change really didn’t impact all that many developers as you were all using JSON anyhow. But it is nice to know that you can now delete just a little more code!Of course, software is never complete. I have more things I want to add to Redis OM than I probably have time to implement. Here’s a few of the things that I’m looking at for future releases:I’ve also been working on some helpful content if you’re getting started with Redis OM:So how’s that for a summary of what’s new with Redis OM for Node.js? Redis OM is what it is because of the assistance from folks in the community. So, I’d like to close with a hearty “Thank you” to some of the helpful folks who have… well… helped:I’m sure I’ve missed some of you, but know that GitHub’s commit history will never forget."
152,https://redis.com/blog/overcoming-microservice-challenges/,Overcoming Microservices Adoption Challenges,"March 9, 2023",Henry Tam,"Concerns about complexity, eventual consistency, and latency: all these can generate reservations from those who are new to adopting microservices. Our solution brief, Cache and Message Broker for Microservices, highlights the design patterns that can help you work around such common obstacles.Microservice architecture can be a game-changer that helps organizations reduce barriers for organizations’ application modernization and cloud migration journey – and lets them beat their competition to market. A microservice architecture decouples business domains into their own respective services, dependent on use case or function, while keeping the line of communication open between all services through APIs. Each domain typically has its own autonomous development team who creates, manages, tests, and deploys services independent of all other domains. This provides faster time to market as each service has its own release cycles.Here’s a real-world example:Consider the airline industry, where real-time data is exchanged through numerous applications by various business domains. The business domains – you might think of them as departments – might include the reservations system, baggage handling, and passenger check-in systems. If Joe Smith buys a ticket from Atlanta to Miami, that information goes into a reservation system. That data can be readily retrieved once Joe Smith checks in at an airport kiosk on the day of his flight. When Joe hands over his luggage to the airline, a whole other division (separate from reservations and check-in services) ensures that Joe’s luggage arrives in Miami when he does.Each domain has its own enterprise tools and resources, and it might use a unique tech stack or database platform. One microservice might use Java and a NoSQL JSON document store; another service may use a legacy mainframe system that runs on COBOL and a relational database. All these distributed services are connected via an API gateway which enables the airline to build applications that call on data from several sources, such as airport kiosks, booking sites, and baggage handling. The point is: the same data can be accessed in real-time by all these decoupled services through APIs.What are the challenges preventing the wider adoption of microservices? Despite the many benefits of a decoupled architecture, why aren’t all companies making the switch?Complexity: In the end, dispersing each viable domain from a monolithic architecture into microservices almost always creates a more complex system. As someone commented in our 5 Microservices Misconceptions post, “Companies think they can make the complexity go away with microservices. But you don’t really make it go away; you just move it to a different layer of abstraction.”When functionality is splintered into hundreds or thousands of isolated domains, team autonomy can introduce a host of different SLAs, tools, and various headcounts to troubleshoot and properly manage each domain at the data tier.How to untangle all these disparate tools in one distributed system without breaking isolation? The answer lies in the efficient use of one data platform for caching that supports the optimal data models and provides a lightweight message broker for inter-service communication.  This enhances developer productivity, provides faster time to market, and generally streamlines the architecture of your application.Eventual consistency: Maintaining strong consistency is extremely difficult within a distributed system. Though domains in a microservice architecture must be decoupled, teams need to maintain up-to-date data, even with far-flung instances halfway across the globe.It’s essential for all services to maintain optimal consistency with shared data, whether you’re dealing with a system of record write-optimized database in one microservice and a read-optimized cache in another service.  You will need to implement the Command Query Responsibility Segregation (CQRS) pattern to improve cross-domain data access with sub-millisecond query latency.  And if microservice instances are distributed to multiple data centers around the world, Active-Active Geo-Replication of the read data is required to scale and maintain availability.Latency: Latency can come from all angles. Each new domain or component can elevate the risk of performance failure, as the growing number of API calls between services can escalate and grow into unwanted latencies.Ideally, if you follow the domain-driven design principle of using an optimal database for each microservice, latency wouldn’t be an issue. However, legacy databases need to be used in a specific microservice in some industries due to technical debt constraints or regulatory reasons. However, those applications can’t meet the performance SLAs. Caching can provide those query response times, though, and setting it up requires minimal development time.A single bottleneck in the data flow in one microservice can cascade into a system-wide collapse, especially if it’s global data at the API gateway. The API gateway cannot be the single point of failure that leads to system downtime, user crankiness, and news headlines that you truly did not want to see.To optimize durability, consistency, and read performance, your application needs a powerful cache and rate-limiting solution for the API gateway.Learn in detail about Redis Enterprise solutions that can minimize the cost and complexity of microservice applications with Cache and Message Broker for Microservices."
153,https://redis.com/blog/redis-cloud-supports-pulumi/,Deploy and Manage Redis Enterprise Cloud With Pulumi,"May 16, 2023",Noam Stern,"Redis Cloud is now supported by Pulumi, an infrastructure as code (IaC) platform that allows developers to manage cloud resources using familiar programming languages. With Redis Cloud and Pulumi, you can automate the creation and configuration of Redis Cloud resources and ensure consistency across environments.Pulumi is a cloud-native IaC platform that lets developers define and manage cloud resources using their favorite programming languages, including Node.js, Python, Go, C#, and YAML. Pulumi offers a consistent, composable, and reusable way to manage resources that can be version-controlled and shared with others.When you use Pulumi with Redis, you can automate the creation and configuration of Redis Cloud resources; doing so lets you spend less time on manual tasks and more time building your applications. Pulumi also provides testing and validation tools to ensure infrastructure reliability. You can write tests to validate a Redis Cloud configuration and catch errors before they cause problems in production.The Redis Cloud provider for Pulumi is built as a bridge provider through Terraform. It provides the same functionality as the Terraform provider but with the added benefits of using Pulumi.Want to see what’s possible? Here’s a step-by-step walkthrough to show how to create your first Redis resource using Pulumi.Before you get underway, install Pulumi. And then…Set up a Pulumi project and install the Redis Cloud provider package for your chosen language; in this example, it’s Python. You can do this by creating a new directory and running the following commands:You will be asked to provide the configured credit card details and the Redis Cloud credentials as part of the Redis Cloud provider package installation.The setup is done!This creates a new Pulumi project with a basic file structure.Write the Pulumi code. Now it’s time to write the Pulumi code to create and configure a Redis Cloud subscription and database in the __main__.py file that was created as part of the previous step.This shows what the code might look like in Python:This code assembles a Redis Cloud flexible subscription in AWS us-east-1 and a database within this subscription with 10GB of memory and 20,000 ops/sec throughput.Deploy the infrastructure. Finally, you can deploy the Redis Cloud infrastructure. You accomplish this by running the following command:This creates the Redis Cloud resources and saves the private endpoint URL in a variable called endpoint so you can use it in your applications.You can now manage your Redis cloud resources using Pulumi. That means you can add and remove Redis Cloud resources and update and delete the ones we just created.Ready to learn more? Here are your possible next steps."
154,https://redis.com/blog/redis-statistics/,Redis Statistics Every Developer Should Know,"May 17, 2023",David Gaule,"How well do you know Redis?Redis is an open source, in-memory data store that was voted “the most loved database” five years in a row. But its ecosystem may be larger than you knew—and more personally financially rewarding than you realized.If your organization uses Redis, you’re in good company. Today, nearly 10,000 customers rely on Redis Enterprise to run their business. You know many of them by name, too: British Airways, MGM Resorts International, Ulta Beauty, and Viacom18. That’s just a small subset.And that doesn’t include users of the open source version. Redis has over 4 billion Docker pulls (with 6 million launches per day), its GitHub repository has nearly 60,000 stars, and it supports more than 50 programming languages.Redis Stack, which adds advanced features and multi-model capabilities to Redis OSS (released in 2022), already has over 3.9 million pulls on Docker hub. The data visualization and performance management tool, RedisInsight, is downloaded over 15,000 times per month and has more than 60,000 monthly active users.To date, 19,332 questions posted to StackOverflow about Redis have been answered. That means that over 5,000 questions are still open and waiting for a response–so if you consider yourself something of a Redis expert, see if you can’t help a fellow developer out.According to Indeed.com, on an ordinary day in April, 4,523 open job postings required Redis skills. About 1,500 of those open roles were for remote positions.So if you want to invest in a technical skill to help you get a leg up in the job market, consider adding Redis to your resume.It’s not just a matter of “lots of jobs available.” It’s lots of high-paying jobs available. According to a Dice.com report, Redis skills are among the top 10 highest-paying IT skills in today’s job market (along with Golang, Elasticsearch, Apache Kafka, and other technologies). While the average tech salary in 2022 was just over $111K a year, developers with Redis skills average annual salaries of over $140K—a 1.6% increase over the previous year.Perhaps you like the idea of adding Redis expertise to your resume. What’s the best way to learn? Redis University offers free online training classes, including “Introduction to Redis Data Structures,” “Redis Streams,” and “Redis for Python Developers” as just a small sample. Well over 10,000 people signed up for at least one of the nine courses available, and over 4,000 completed at least one.And while knowledge for its own sake is rewarding, impressing an employer by saying you’re a Certified Redis Developer is even better, because it’s probably accompanied by numbers that make a loud ka-CHING as they arrive in your bank account. About 350 people have passed that test (and its course prerequisites) in the past three years, earning them serious bragging rights.Maybe you prefer to learn by watching videos. A gold mine of Redis tutorials and presentations is available on YouTube. The official Redis channel, with more than 20,000 subscribers, has over 800 videos that you can watch for free. The most popular videos are “Reduce Complexity by Using Redis as a Real-Time Data Platform,” “Flexible Deployment of Redis to Multicloud or Hybrid Cloud,” and the high-level overview “What Is Redis?” That doesn’t include plenty of other videos created by Redis users and business partners.Statistically speaking, chances are 100%. So try Redis Cloud for free today."
155,https://redis.com/blog/taking-in-memory-nosql-to-the-next-level/,Taking In-Memory NoSQL to the Next Level,"June 14, 2012",Ofer Bengal,"We are launching today the first in-memory NoSQL cloud, which will change the way people use Memcached and Redis. This is a great opportunity to examine the state of these RAM-based data stores and suggest a new, highly-efficient way of operating them in the cloud. Memcached and Redis are being increasingly adopted by today’s web-applications, and are being used to scale-out their data-tier and significantly improve application performance (in many cases improvement is x10 over standard RDBMS implementation). However, cloud-computing has created new challenges in the way scaling and application availability should be handled and using Memcached and Redis in their simple form may not be enough to cope with these challenges. Memcached It’s no secret Memcached does wonders for websites that need to quickly serve up dynamic content to a rapidly growing number of users. Facebook, Twitter, Amazon and YouTube, are heavily relying on Memcached to help them scale out; Facebook handles millions of queries per second with Memcached. But Memcached is not just for giants. Any website concerned with response time and user based growth should consider Memcached for boosting its database performance. That’s why over 70% of all web companies, the majority of which are hosted on public and private clouds, currently use Memcached. Local Memcached is the simplest and fastest caching method because you cache the data in the same memory as the application code. Need to render a drop-down list faster? Read the list from the database once, and cache it in a Memcached HashMap. Need to avoid the performance-sapping disk trashing of an SQL call to repeatedly render a user’s personalized Web page? Cache the user profile and the rendered page fragments in the user session. Although local caching is fine for web applications that run on one or two application servers, it simply isn’t good enough when the data is too big to fit in the application server memory space, or when the cached data is updated and shared by users across multiple application servers and user requests. In such cases user sessions, are not bound to a particular application server. Using local caching under these conditions may end up providing a low hit-ratio and poor application performance. Distributed Memcached tends to improve local caching by enabling multiple application servers to share the same cache cluster. Although the Memcached client and server codes are rather simple to deploy and use, Distributed Memcached suffers from several inherent deficiencies:Amazon has tried to simplify the use of Memcached by offering ElastiCache, a cloud-based value-added service, where the user does not have to install Memcached servers but rather rent VMs (instances) pre-loaded with Memcached (at a cost higher than plain instances). However, ElastiCache has not offered a solution for any of the Memcached deficiencies mentioned above. Furthermore, ElastiCache scales-out by adding a complete EC2 instance to the user’s cluster, which is a waste of $$ for users who only require one or two more GBs of Memcached. With this model ElastiCache misses on delivery of the true promise of cloud computing – “consume and pay only for what you really need” (same as for electricity, water and gas). Redis Redis an open source, key-value, in-memory, NoSQL database began ramping-up in 2009 and is now used by Instagram, Pinterest, Digg, Github, flickr, Craigslist and many others and has an active open source community, sponsored by VMware. Redis can be used as an enhanced caching system alongside RDBMS, or as a standalone database. Redis provides a complete new set of data-types built specifically for serving modern web applications in an ultra-fast and more efficient way. It solves some of the Memcached deficiencies, especially when it comes to high availability, by providing replication capabilities and persistent storage. However, it still suffers from the following drawbacks:A new cloud service changes the way people use Memcached and Redis Imagine connecting to an infinite pool of RAM memory and drawing as much Memcached or Redis memory you need at any given time, without ever worrying about scalability, high-availability, performance, data security and operational issues; and all this, with the click of a button (ok, a few buttons). Imagine paying only for GBs used rather than for full VMs and at a rate similar to what you pay your cloud vendor for plain instances. Welcome to the Garantia Data In-Memory NoSQL Cloud! By In-Memory NoSQL Cloud I refer to an online, cloud-based, in-memory NoSQL data-store service that offloads the burden of operating, monitoring, handling failures and scaling Memcached or Redis from the application operator’s shoulders. Here are my top 6 favorite features of such service, now offered by Garantia Data:We have recently concluded a closed beta trial with 20 participating companies where all these features were extensively tested and verified, – and it worked fine! So this is not a concept anymore, it’s real and it’s going to change the way people use Memcached and Redis! Am I excited today? Absolutely so!Fig 1– DB Caching Evolution"
156,https://redis.com/blog/aws-outages-and-in-memory-datastores/,AWS Outages and In-Memory Datastores,"July 2, 2012",Redis,"For the second time during June 2012, the AWS us-east-1 region failed, this time due to a power outage caused by extreme weather conditions, according to Amazon. For those of you who use an in-memory data store like Memcached or a service like AWS’ ElastiCache, the result of such power outage is losing your entire Memcached dataset. The implications are that all database queries are now directed to your main database, which most times is not built for such load. This means your app may suffer dramatic performance degradation and in extreme cases may even crash. Recovering from a Memcached failure can take days and sometimes weeks. Furthermore, many of today’s applications and dev-platforms like Magento, WordPress, Drupal and Django store users’ sessions in Memcached. Losing such data typically means forcing all of your users to immediately logout, and if you are running an ecommerce site, flushing all your users’ shopping carts. Both events may have adverse effect on your business. In Garantia Data, we took a different approach towards operating in-memory NoSQL data stores like Memcached & Redis, making sure a dataset is never lost, while maintaining the high-throughput and low-latency of these extremely fast platforms. Our built-in replication and auto-failover processes helped us to survive the June 15 AWS outage with zero downtime! – switching in-memory datasets from all the failed nodes to the healthy nodes of our cluster in the affected zone. Although replication is a very effective way to recover from a node failure event, the latest June 30 AWS power outage affected multiple cluster nodes of our service simultaneously. However, thanks to our robust data-persistence mechanism we were able to recover from this failure without damage. Our users maintained replicas of their datasets in persistent storage (EBS) using either Append Only File (AOF) or Snapshot methods. By the way, this capability too comes without any application performance degradation. We also allowed daily S3 backups, which proved to be very effective in case of impaired EBS volumes, as occurred in the latest AWS outage. Bottom line – our robust data persistence mechanisms enabled us to successfully recover ALL of our users’ datasets from the June 30 AWS outage ! Future improvements:"
157,https://redis.com/blog/its-true-even-modest-datasets-can-enjoy-the-speediest-performance/,It’s True: Even Modest Datasets Can Enjoy the Speediest Performance,"September 27, 2012",Redis,"Those of us who use them know that Redis and Memcached were designed from the ground-up to achieve the highest throughput and the lowest latency for applications, and they are in fact the fastest data store systems available today. They serve data from RAM,  and execute all the simple operations (such as SET and GET) with O(1) complexity. However, when run over cloud infrastructure such as AWS, Redis or Memcached may experience significant performance variations across different instances and platforms, which can dramatically affect the performance of your application. As a provider of cloud services for hosting Redis and Memcached datasets, we at Garantia Data are always looking at best practices for optimizing performance, and so we recently conducted a benchmark test to compare small-to-medium size (<=12GB) Redis and Memcached datasets running over different AWS instances and platforms.Architectural considerationsThe first thing we looked at when putting together our benchmark was the various architectural alternatives we wanted to compare. Users typically choose the most economical AWS instance based on the initial size estimate of their dataset, however, it’s crucial to also keep in mind that other AWS users might share the same physical server that runs your data (as nicely explained by Adrian Cockcroft here). This is especially true if you have a small-to-medium dataset, because instances between m1.small and m1.large are much more likely to be shared on a physical server than large instances like m2.2xlarge and m2.4.xlarge, which typically run on a dedicated physical server. Your “neighbours” may become ”noisy” once they start consuming excess I/O and CPU resources from your physical server. In addition, small-to-medium instances are by nature weaker in processing power than large instances. Your instance selection may have a big effect on the performance of your Redis/Memcached, which typically uses an optimal number of code lines to run each command. Any slight interference of a “noisy neighbor” or lack of processing power that delays operations (such as memory access, network transmission or context switching) can significantly reduce throughput and increase latency.  Because of this, it’s a fair assumption that apps using small-to-medium AWS instances for their Redis or Memcached data are more prone to performance degradation. Another way of running Redis or Memcached on AWS is to use a service, such as the AWS Elasticache (for Memcached), or our own Redis Cloud or Memcached Cloud. However, when using Elasticache, the user is still required to select instance size and therefore may suffer from the same performance issues. Garantia Data’s Redis Cloud and Memcached Cloud use a different approach: we run all cluster nodes on m2.2xlarge or m2.4xlarge instances to ensure the best performance, but we only charge users based on their actual dataset size (in GBs). For all the details about how we set up the benchmark, including resources, data sets and configuration, feel free to hop to the end of the post.Benchmark test resultsWithout further ado, here’s what we found out about how the alternatives for running Redis and Memcached on AWS compare when it comes to performance.ThroughputNote: ElastiCache uses 2 Memcached threads on m1.large instance and 4 on m1.xlarge instance, while the Stand-alone Redis is based on a single threaded architecture As seen above, the RPS of the Garantia Data Memcached/Redis Cloud was better in the range of 25-100% (depending on the instance type) than ElastiCache or Stand-alone Redis.LatencyWe also saw a better than average response time with our Memcached/Redis Cloud cluster in the range of 25-50% than ElastiCache or Stand-alone Redis, while its 99%-tile response time was significantly better, in the range of x6-x30, than ElastiCache or Stand-alone Redis.Note: Our response time measurement takes into account the network round-trip, the Redis/Memcached processing time, and the time it takes the memtier_benchmark tool to parse the result.ConclusionThis benchmark demonstrates that a different architectural approach can improve throughput and significantly reduce latency of Redis and Memcached in the cloud. We believe these findings are explained by the unique architecture of our Redis Cloud and Memcached Cloud services. We at Garantia Data have developed several technologies that run on each node of our clusters and guarantee minimal interference between users, i.e.:Behind the scenes we monitor each and every Redis or Memcached command and constantly compare its response time to what should be an optimal value. In addition, we reshard datasets on-the-fly in a non-intrusive manner, which is a very challenging operation with complex synchronization mechanisms between multiple distributed elements. This architecture allows users with any size dataset (including small-to-medium sizes) to run on the strongest instances and enjoy the highest performance, while paying only for their actual GB used on an hourly basis — so that our customers get the best of both worlds.Benchmark test setupWe know you want to know about the nitty gritty details of our benchmark, so here are the resources we used:This was our setup for generating load:We ran each of the tests 3 times on each configuration and calculated the average results using the following parameters:Our dataset sizes included:"
158,https://redis.com/blog/is-amazon-piops-really-better-than-standard-ebs/,Is Amazon PIOPS Really Better than Standard EBS?,"October 4, 2012",Redis,"UPDATE: read our followup post Take #2 – Is Amazon SSD PIOPS Really Better?Two months ago, Amazon launched its EBS Provisioned IOPS (PIOPS) for running high performance databases in the cloud, and introduced EBS-Optimized instances to deliver dedicated throughput between EC2 and EBS. As the provider of the Redis Cloud service, we wanted to know whether the PIOPS EBS with EBS-Optimized instances would really perform better than Standard EBS with non-optimized instances for Redis. Note – recently, Amazon released their new RDS Provisioned IOPS. However, it is not applicable to Redis and is therefore not addressed in this post.Running Redis on AWS has been a challenge for Redis users, mainly due to EBS’ slow and unpredictable network and I/O throughput. Though Redis is a pure in-memory datastore system and serves everything from RAM, it also has two persistent-storage mechanisms for better durability: AOF (Append Only File) and Snapshotting. These mechanisms are very efficient when interfacing with persistent storage, as they only use sequential writes with no seek time. As a result, Redis can run pretty well with data-persistence even on non-SSD hardware.That being said, we decided to conduct some benchmarks for Amazon’s EBS PIOPS because slow storage devices can still be very painful for Redis. We see this when the AOF “fsync every second” policy blocks an entire Redis operation due to slow disk I/O. In addition, background saving for point-in-time snapshots or AOF rewrites takes longer, and therefore more memory is used for copy-on-write — leaving less memory available for your app. You may have seen the detailed post from Strattalux comparing the performance of PIOPS EBS vs. Standard EBS using EBS-Optimized and non-Optimized instances. While their tests were interesting, we thought we’d dig a little deeper and examine whether the performance of a simple Redis application is influenced by different storage architecture. Therefore, we were very specific and only tested the performance of sequential writes, ignoring other scenarios like random read/write and sequential read.For our benchmark, we also reviewed cost implications, and used what we believe is a more optimal performance configuration of Standard EBS (1 x 1TB EBS volume vs. 10 x 100 GB RAID0 EBS, per a very educational blog about Standard EBS performance by Adrian Cockcroft from Netflix). So, without further ado, here are the details of our tests…Benchmark Setup We used the following setup: PIOPS EBS with Optimized-Instance (i.e. m2.4xlarge with EBS-Optimized attribute set to true) vs. Standard EBS with non-Optimized Instance (m2.4xlarge with EBS-Optimized attribute set to false). We also tested another setup in which we compared PIOPS EBS vs. Standard EBS on a non Optimized-Instance (m2.2xlarge). Our intention was to see whether AWS users who cannot use the EBS Optimized-Instance for various reasons (such as cost) can get benefit from the new PIOPS EBS. Those results were very similar to the setup described above. For all tests, we used 2 x m1.xlarge instances that run our memtier_benchmark load generation tool (memtier_benchmark is an advanced load generator tool we developed for testing Redis and Memcached datastores, which we will soon share in our github account). We ran 8 tests on each configuration to simulate a Redis application, then ran each of these tests 3 times and calculated the average results. Our tests combinations included:Benchmark Results Note – we have chosen to show the results from the 1KB object size tests, which impose higher load on the EBS. RPS (Requests Per Seconds)  Latency  The RPS and Latency results aggregated from our two memteir_benchmark tools are shown above. Our latency tests take into account the network round-trip, the Redis processing time, and the time it takes for the memtier_benchmark tool to parse the result. As you can see, RPS and latency are approximately the same on all tests, though there are some differences which might be due to the fluctuations in the AWS infrastructure during the test time. Needless to say, Redis runs better with fewer disk accesses. iostatsThe results above are taken from iostats commands executed while the tests were running (here’s a useful AWS forum post that explains how to interpret Linux iostats). We ran iostats every 2.5 seconds and aggregated results from both EBS volumes. These charts only present relevant performance parameters for Redis — “read” parameters were not presented as they were constantly zero during these tests. As you can see, except from few glitches here and there, iostats show approximately the same results for both EBS configurations. Set-up cost As mentioned earlier, we wanted to understand the cost implications of the Standard vs. PIOPS EBS options, so we compared the monthly setup costs using Amazon EC2 on-demand prices in us-east-1 (N. Virginia):Conclusion After 32 intensive tests with Redis on AWS (each run in 3 iterations for a total of 96 test iterations), we found that neither the non-optimized EBS instances nor the optimized-EBS instances worked better with Amazon’s PIOPS EBS for Redis. According to our results, using the right standard EBS configuration can provide equal if not better performance than PIOPS EBS, and should actually save you money. Keep in mind that these findings represent a specific storage usage pattern (sequential write only), so if you have different storage access patterns, you may want to review the Stratalux test results (while taking into account the different Standard EBS configuration we suggest). Given these results, we plan to continue running Garantia Data’s clusters on a Standard EBS configuration. We welcome comments from the Amazon EBS Team on our benchmark, and will be sure to share any new information we discover."
159,https://redis.com/blog/does-amazon-ebs-affect-redis-performance/,Does Amazon EBS affect Redis performance?,"October 15, 2012",Redis,"As you may know, Running Redis with persistence storage on AWS has been a challenge for users. Local storage lacks data durability, and standard EBS is perceived to be slow and unpredictable. A recent discussion amongst the Redis community suggested that users change the default Redis configuration to AOF with “fsync every second” for better data durability than you can get with today’s default snapshotting policy. As many Redis users run on AWS, one of the arguments against changing the configuration is the perceived slowness of EBS, and the fact that AOF is much more demanding than snapshotting with respect to storage access. As a provider of the Redis Cloud, we decided to find out whether EBS really slows down Redis when used over various AWS platforms. Redis Data Persistence: Redis’ AOF and Snapshotting persistent storage mechanisms are very efficient, as they only use sequential writes with no seek time. Because of this, Redis can run pretty well with data-persistence even on non-SSD hardware. That being said, slow storage devices can still be very painful for Redis. We see this when the AOF “fsync every second” policy blocks an entire Redis operation due to slow disk I/O. In addition, background saving for point-in-time snapshots or AOF rewrites takes longer, which results in more memory being used for copy-on-write, and less memory being available for your app. Testing Redis with and without Persistent Storage: We conducted our latest benchmark to understand how each typical Redis configuration on AWS performs with and without persistent storage. We tested AOF with the “fsync every second” policy, as we believe this is the most common data-persistence configuration that nicely balances performance and data durability. In fact, over 90% of our Redis users who enable data-persistence use AOF with “fsync every second.” Although it is a common Redis configuration, we didn’t test a setup in which a master node serves the entire data from RAM, leaving the slave node to deal with data persistence. This configuration may result in issues like replication bottlenecks between master and slave, which are not directly related to the storage bottlenecks we were testing for. And, we’re leaving the effect of Redis fork, AOF rewrite and snapshot operations for our next posts, so stay tuned! Benchmark test results Here’s what we found out when trying to compare how each platform runs with and without data-persistence. ThroughputAll the platforms ran a bit faster without data-persistence (w/o DP), but the difference was very small: 15% with m1.xlarge instance, equal or less than 8% for other instances, and about 1% with the Redis Cloud platform. Note: By default, Redis Cloud uses a raided EBS configuration for each of its cluster nodes (see more setup details at the end of this post), but we used a non-raided configuration for all the other platforms in this benchmark. It is therefore safe to assume that a more optimal EBS configuration can reduce performance degradation. LatencyAverage response time was 13% higher with data-persistence on the m1.xlarge instance and less than 8% higher in all other instances. Again, due to the optimal EBS configuration of the Redis Cloud, the average response time with data-persistence was higher by only 2%. Significant latency gaps were seen in 99%-tile response time with m1.small and m1.large instances, mainly because these instances are much more likely to be shared on a physical server than large instances like m2.2xlarge and m2.4.xlarge, as nicely explained by Adrian Cockcroft here. On the other hand, very small higher latencies were seen with m1.xlarge and m2.2xlarge instances, and an equal response-time was seen with the Redis Cloud. Note: Our response time measurement takes into account the network round-trip, the Redis/Memcached processing time, and the time it takes our memtier_benchmark tool to parse the result. Should AOF be the default Redis configuration? We think so. This benchmark clearly shows that running Redis over various AWS platforms using AOF with a standard, non-raided EBS configuration doesn’t significantly affect Redis’ performance under normal conditions. If we take into account that Redis professionals typically tune their redis.conf files carefully before using any data persistence method, and that newbies usually don’t generate loads as large as the ones we used in this benchmark, it is safe to assume that this performance difference can be almost neglected in real-life scenarios. Why does Redis Cloud perform much better ?Behind the scenes, we monitor each and every Redis command and constantly compare its response time to what should be an optimal value. This architecture lets our customers run on the strongest instances and enjoy the highest performance, regardless of their dataset size (including small-to-medium). And with Garantia Data, they can do this while paying only for their actual GB used on an hourly basis — so they get the best of both worlds. Benchmark test setup For those who want to know more details about our benchmark, here are the resources we used:For each tested platform we ran 2 tests:We ran each test three times on each configuration, and calculated the average results using the following parameters:Our dataset sizes included:"
160,https://redis.com/blog/testing-fork-time-on-awsxen-infrastructure/,Testing Fork Time on AWS/Xen Infrastructure,"October 24, 2012",Redis,"As a provider of the Redis Cloud, we’re always testing out different configurations and options for Redis and AWS to help the Redis community achieve the best possible performance. This week, we decided to find out how fork times influence various AWS platforms and Redis dataset sizes. Redis uses Linux fork and COW (Copy On Write) to generate point-in-time snapshots or rewrite Append Only Files (AOFs) using a background save process. Fork is an expensive operation in most Unix-like systems, since it involves allocating and copying a large amount of memory objects (see more details here). Furthermore, the latency of fork operations on the Xen platform seems to be much more time consuming than on other virtualization platforms (as discussed here). Since a fork operation runs on the main Redis thread (and the Redis architecture is single-threaded), the longer the fork operation takes, the longer other Redis operations are delayed. If we take into account that Redis can process anywhere between 50K to 100K ops/sec even on a modest machine, a few seconds delay could mean slowing down hundreds of thousands operations, which might cause severe stability issues for your application. This problem is a real life limitation for Redis users over AWS because most AWS instances are based on Xen. Our test scenarios in a nutshell were as follows — We ran Redis (version 2.4.17) on top of the following platforms:A more detailed description of the test setup can be found at the end of this post. Here’s what we found out about the Fork times:Fork time per GB of memory:As you can see, there is a strong correlation between instance processing power and the execution time of fork operation. Moreover, the Xen HVM instance achieved significantly lower latency than the regular Xen hypervisor instances.   So, should Redis users over AWS migrate their datasets to Cluster Compute instances? Not necessarily. Here’s why:Fork time and the Redis Cloud We tested fork times in similar scenarios on the Redis Cloud and compared the results to those of corresponding instances in a do-it-yourself (DIY) approach. This is what we found out:Fork time per GB of memory:As you can see here, fork times are significantly lower with the Redis Cloud than with regular Xen hypervisor platforms. Furthermore, fork time per GB drops when dataset size grows. Fork times on the Redis Cloud are somewhat higher than those of Xen HVM (cc1.4xlarge instance) – for small datasets of about 1GB it is 0.03 seconds versus 0.008 seconds. However, since there is probably a correlation between the size of your dataset and the rate your app is accessing Redis, the amount of delayed requests when using the Redis Cloud should be small. For larger datasets, the Redis Cloud fork time is 0.03 seconds per GB compared to 0.01 seconds for Xen HVM. Nevertheless, we still prefer to use m2.2xlarge and m2.4xlarge instances over cc1.4xlarge and and cc2.8xlarge instances in the Redis Cloud, for the following reasons:How does the Redis Cloud minimize fork time? The Redis Cloud applies several mechanisms and technologies to maintain high performance:Conclusions We validated in this test that the fork process on AWS standard Xen hypervisor instances causes significant latency on Redis operations (and therefore on your application performance). Fork time per GB somewhat improves with stronger instances, but it is still unacceptable in our opinion. While fork time is practically eliminated when using AWS Cluster Compute instances, these instances are very expensive and are not optimized for Redis operations. The Redis Cloud provides minimal fork times at affordable infrastructure prices, and our fork times drop even further when dataset sizes grow.   Benchmark test setup For those who want to know more about our benchmark, here are some details on the resources we used:This was our setup for generating load:"
161,https://redis.com/blog/six-things-to-consider-when-using-redis-on-heroku/,Six Things to Consider When Using Redis on Heroku,"November 9, 2012",Redis,"Earlier this week, Flights With Friends wrote this post evaluating alternatives for using Redis on Heroku. Even though we were not selected as their favorite choice, we think they did a fair job and want to share what we think should be the most important things to consider when selecting a Redis add-on for your Heroku app:Redis Cloud was developed by a group of specialists in the fields of databases and application acceleration. We have been serving production databases since January 2012, many with over 100GB in size. Already, we’ve overcome three AWS outages without losing a single byte of customers’ data. So why are we still in beta? It took us time and great effort to build our novel dynamic clustering technology around standard Redis, which enables capabilities like true high-availability, infinite and seamless scalability, high performance at any dataset size, multiple databases (each in a dedicated process), and unlimited database connections. We want to make absolutely sure it works properly over time before we remove the “beta” label. In regards to questions around pricing, at this time we can only promise it will be very competitive, so stay tuned!"
162,https://redis.com/blog/benchmarking-the-new-aws-m3-instances-with-redis/,Benchmarking the new AWS M3 instances with Redis,"November 15, 2012",Redis,"Two weeks ago, Amazon launched its next generation of standard instances (M3 instances), adding twice the computational capability/cores while providing customers with the same balanced set of CPU and memory resources as M1 instances. We don’t use the M1 instances in our Redis Cloud clusters (they can’t cope with the high throughput, low latency requirements of Redis as detailed here), but wanted to know whether the M3 double extra-large instance (m3.2xlarge) would really perform better than the m2.2xlarge high memory instance that we use in many of our clusters nodes. Both instances come with similar memory configuration and use the same type of vcores, so what intrigues us most is the fact that M3 instances can run in Xen hardware virtualized mode (HVM). Therefore, they should overcome the Xen fork time issues that significantly affect Redis performance (as described in detail here). In-addition, the m3.2xlarge instance has two times the number of vcores as an m2.2xlarge instance at less than 30% additional cost:Test scenarios With all of that in mind, we conducted a test comparing Redis performance over m3.2xlarge and m2.2xlarge instances for the following:It was important to test replication time, because m3.2xlarge comes without ephemeral storage or, to be more accurate, with only 15GB of local disk size. That means it must be attached to an EBS volume even if no data persistence is required, because the Redis replication process involves writing the entire dataset to persistence storage twice (at the master and then at the slave). Since EBS is slow on sequential writes (see more details here), we wanted to see if and how Redis replication is affected by this configuration.   Without further ado, here’s what we found: Fork timeAs expected, fork time using the m3.2xlarge instance is significant lower than fork time on an m2.2xlarge instance, as it uses Xen’s full virtualization mode (HVM). Similar low fork time results are also seen over AWS Cluster Compute instances (see more info here), but at a much higher instance cost.   Throughput and Latency We compared the throughput and the latency of a single shard Redis on both instances, and this is what we got:Our insight:Replication timeTo measure replication time, we started measurement upon connecting the slave to the master, and stopped when the slave was synced. Although we configured the m2.2xlarge instance to use its local disk interface running at 80 MB/s (peak) and the m3.2xlarge to use an EBS interface running at 30 MB/s (peak), the overall replication time of the m2.2xlarge instance was better by only 13%. We believe this is because the population process is consuming most of the time at this size of RDB file. We assume the ratio of the population time to the entire replication process grows linearly to the size of the RDB file. Therefore, the larger a file is, the lower the effect of disk access throughput would be on the entire replication process. The replication of small RDB files is inherently fast, so it is safe to assume that a higher throughput storage interface does not significantly affect the full replication process.   Conclusions AWS’ new M3 instances with HVM AMI completely eliminate the Xen fork time issues that significantly affected Redis performance during point-in-time snapshots or rewrite Append Only Files (AOFs) background save processes. On the other hand, the performance of a single threaded Redis server over m3.2xlarge instance was ~15% slower than over an m2.2xlarge instance in both pure in-memory and AOF tests. Replication time was also slower with the m2.3xlarge instance, though not significant as we expected. Our recommendation is to use Redis on m3.2xlarge rather than m2.2xlarge only if you run Redis without replication or if you need to run more than four Redis processes on the same instance. For all other cases, we still recommend using the m2.2xlarge instance.   Test setup For those who want to know more details about our test, here are the resources we used:This was our setup for generating load: m2.2xlarge instance that ran our memtier_benchmark load generation tool (an advanced load generator tool we developed, which we will soon share in our github account).  For testing fork time, we took the following steps:For testing throughput and latency we ran 2 tests:We ran each test three times on each configuration, and calculated the average results using the following parameters:For testing replication, we:"
163,https://redis.com/blog/enhancing-redis-slow-log/,Enhancing Redis Slow Log,"January 17, 2013",Redis,"Redis Slow Log is one of the best tools for debugging and tracing your Redis database, especially if you experience high latency and high CPU usage with Redis operations. This online discussion in a Redis DB group is just one of many examples that show how efficient Redis Slow Log is. Because Redis is based on a single threaded architecture, Redis Slow Log can be much more useful than slow log mechanisms of multi-threaded database systems such as MySQL Slow Query Log.Unlike tools that include the software locking overhead which makes the debugging process very complex, Redis Slow Log is highly effective at showing the actual processing time of each slow command. As a provider of Redis Cloud, we use Redis Slow Log intensively for internal monitoring and to help our users solve latency issues related to their complex queries. And we often find ourselves struggling to understand the differences between two execution times of the same Redis command shown in the Slow Log – usually with complex commands such as ZUNIONSTORE, ZINTERSTORE, ZRANGEBYSCORE. Based on our experience, we thought the Redis community could benefit from adding the time complexity parameters for each command stored in the Slow Log. With this enhancement, we expect users to get a better understanding of their Redis DB operations, including differences between execution times of the same command and spikes in CPU usage. So without further ado, here’s (bold) what we’ve contributed for the Enhanced Redis Slow Log:For better understanding of the complexity, you may want to use the table below:For those who are interested, this enhancement is part of our extended Redis 2.6. version and can be found here in our GitHub account."
164,https://redis.com/blog/available-now-redis-memcached-clouds-across-multiple-availability-zones/,Available Now: Redis & Memcached Clouds across Multiple Availability Zones,"June 11, 2013",Redis,"We have just announced the launch of our Multiple Availability Zones for Garantia Data’s Redis & Memecached clouds. To learn how our Multi-AZ solution works, just keep reading. Availability Zones are used to segment the physical structure of a cloud’s data center (a.k.a. region) into independent sections in order to compartmentalize the impact of major catastrophes. Modern high availability practices use multiple availability zones from which to run several instances of the application. This is done so a failure of a single zone won’t stop the application from running from surviving zones. Depending on the actual nature of the setup that you want “zoned out,” the effort required to actually make it happen ranges from trivial to absurdly difficult. A typical application is usually a stateless engine, so running it from multiple zones requires very little effort, if any. Datastores, on the other hand, are almost always stateful and thus require some form of consistency that is harder to achieve. In the case of in-memory datastores, the challenge is greater given the main storage’s volatility and significantly higher update rates.The following description relates to our Redis Multi-AZ solution, however it is also applicable to our Memcached Multi-AZ solution. A typical Multi-AZ environment is shown in the diagram below.As can be seen, in normal state, the application’s instances either run from Zone A or from both A and B. The datastore, however, is only present in Zone A and is used to serve the application instances in both zones. That means that the datastore may become unavailable if Zone A fails, leaving the surviving application instances in Zone B without their data. In our Multi-AZ solution,  the datastore is replicated from Zone A to Zone B, as shown here:With our Multi-AZ replication, the active datastore remains in zone A. Zone B’s replica is only updated with changes and kept in standby mode as long as there are no zone failures. If and when Zone A fails, Zone B’s standby replica is automatically prompted to active state and the application is failed over to it, like so:  This allows the application’s operation to continue uninterrupted despite a major failure event. The failover to Zone B’s datastore replica is entirely managed and carried out externally to your application so no code/configuration changes are needed, and by updating the datastore’s DNS records even its URL remains unchanged. The failed-over-to environment in Zone B will continue to serve the application’s traffic with only minimal and temporary, if any, performance degradation from renewing the connections to the datastore. At some later point in time, Zone A is likely to be recovered by the cloud infrastructure provider. According to our Multi-AZ solution, once Zone A is again available for use, it will be rebuilt to restore the Multi-AZ deployment in anticipation for the next failure. Typically, the application servers will be first to be spun up in the recovered zone, and will connect to the now-active datastore replica in Zone B. Similarly, no modifications are needed at the client side since the datastore retains its URL:A little while later, the datastore’s new standby replica in Zone A will finish its synchronization from the active replica and the replication stream flow from Zone B to A will begin. At this stage, setup will have fully returned to its Multi-AZ, zone-failure-resilient state as depicted here:Following are some of the challenges that the Garantia Data team had to tackle when we prepared our service to support multiple availability zones for Redis & Memcached:All the goodness of this hard work is now available to you with the same simple setup and ease-of-use as any other of our plans. And it carries the assurance that the next time an availability-zone-crippling disaster hits, your datasets will be safely tucked away out of harm’s way while remaining fully functional. To start using Multiple Availability Zones with your Redis & Memcached Clouds, just select the “Multi-AZ Deployment” checkbox the next time you launch a new Redis DB or Memcached Bucket. Our Multiple Availability Zones plans for datasets are available today at AWS’s us-east data region, and we intend to add plans for more regions, clouds & providers in the coming months."
165,https://redis.com/blog/benchmark-shared-vs-dedicated-redis-instances/,Benchmark: Shared vs. Dedicated Redis Instances,"June 27, 2013",Redis,The TheoryThe ProofThe Inevitable Conclusion
166,https://redis.com/blog/memtier_benchmark-a-high-throughput-benchmarking-tool-for-redis-memcached/,memtier_benchmark: A High-Throughput Benchmarking Tool for Redis & Memcached,"July 2, 2013",Redis,"Benchmarking is the practice of measuring the performance of a system to identify its limits. Benchmarking is an integral part of our service’s development process and we use it both for regression testing (verifying performance has not been reduced between releases) and also as an aid for optimizing database performance. It is generally considered good practice to measure the performance of your system periodically as well as after every change made to ensure it is achieving maximal performance and to potentially uncover relevant issues. To facilitate the execution of these benchmark runs, we developed our own benchmarking tool that we fondly call memtier_benchmark.This tool can be used to generate various traffic patterns against both Memcached and Redis instances. It provides a robust set of customization and reporting capabilities all wrapped into a convenient and easy-to-use command-line interface. We use it extensively for all our benchmarking needs, both in development as well as to help share our knowledge with the broader community (for example, as we did in this blog post). We’ve released memtier_benchmark source code under the GPLv2 licensing scheme, and you can download it from our github account. The following are some highlights about the tool.For more information about these and other features of the tool, below is memtier_benchmark’s usage clause (printed with the help switch). Do feel free to give it a go and let us know what you think.EDIT: check out the description of  memtier_benchmark’s new version with pseudo-random data, Gaussian access pattern and range manipulation here"
167,https://redis.com/blog/the-endless-redis-replication-loop-what-why-and-how-to-solve-it/,"The Endless Redis Replication Loop: What, Why and How to Solve It","July 16, 2013",Redis,"Redis’ replication is an indispensable tool – it can be used both to heighten the availability of your Redis setup (you can read more about Redis availability in this post) as well as to scale it out by doing reads against read-only slaves.In implementing replication, Redis capitalizes on its core functionalities – namely RDB files – to deliver a simple and elegant mechanism that’s effective in the majority of scenarios. Replication is widely adopted, including in our own Redis Cloud service, and is yet another useful, well-proven capability and of Redis’.There are circumstances, however, when activating Redis’ replication can prove to be no simple feat. These are typically rare and extreme scenarios, such as when your dataset’s size grows significantly. But before getting into those details, let’s cover the basics of how Redis’ replication works.“In a Galaxy Far, Far Away”Replication in Redis is like to a dialog between a slave, (or an apprentice), and its master.This conversation runs along the these lines:The above exchange essentially lets the slave synchronize the contents of the master’s dataset in two phases: firstly the full, albeit slightly-out-of-date, body of data is copied; then a partial subset consisting exclusively of updates is applied during a shorter catch-up period.As mentioned above, some databases, depending on their use and purpose, grow significantly in size. Growth may be sudden or happen gradually over time, but regardless of how it got there, the fact remains – your database has gotten substantially bigger. And bigger isn’t always better, especially when trying to bootstrap a replication slave.There are several problems that surface when a slave attempts to synchronize off a master that manages a large dataset (about 25GB or more). Firstly, the master server may require a lot of RAM, even up to 3X the size of the database, during the snapshotting. Although also true for small databases, this requirement becomes harder to fulfill the bigger the database grows. Secondly, the bigger the dataset is, the longer and harder it is to fork another process for snapshotting purposes, which directly affects the main server process.This phenomenon is called “latency due to fork” and is explained here and at redis.io. Let us assume, however, that the latter are non-issues and that by throwing enough hardware at it, you’ve gotten the master server enough resources so that creating the snapshot and forking latency are negligible efforts. Remember that after all the forking is done, the file needs to be copied from the master by the slave.Regrettably, this is carried over the same interconnect that clients are using to the access database from. A large database, in most cases, is used by a lot of clients that generate a lot of traffic. Furthermore, in a cloud setup, the same network may also be used to access EBS-like network-attached storage as well. Adding a 10GB-worth of file transfer traffic to that transport medium hardly lessens any existing congestion. Quite the opposite, actually. Even assuming the existence of optimal network conditions, there are still physical limits to just how fast a chubby RDB file can travel through the wires and get written to local disks.The bottom line is that, given these factors and the compounded effects they have, it takes time for the slave to get the file ready and in place for loading. And once in place, it also takes time for the slave to load the file. You don’t need detailed models or elaborate mathematical proofs to intuitively grasp the fact that the bigger your dataset is, the longer it will take to fork, dump, copy and load it into the slave.“But what of it?” you may say, “It’s not like I need to set up a new slave every day. I have the time, I can wait it out.” “You must unlearn what you have learned” and wait you will, ad infinitum et ultra.The slave will never finish synchronization and replication will not start. That is because while the snapshot was being created, hauled and loaded into the slave, time had passed during which the master was busy serving requests (and probably a lot of them in a big and busy database). Updates were accumulated in the dedicated replication buffer, but that buffer’s size is ultimately finite and, once exhausted, it can no longer be used to bring the slave up to date.Without a valid updates buffer to catch-up from, the slave cannot complete the cycle of preliminary synchronization that is required to actively begin replicating updates from the master in real time. To rectify the situation, Redis’ behavior under these circumstances is to restart the slave’s synchronization process from scratch.And so, the Apprentice goes back to square one, forgetting all that was learned so far and returns to the Master with a single request: “I want to learn and to become like you.” However, since the basic circumstances remain unchanged, successive attempts to kickstart the replication are in all likelihood doomed to the same fate as that of the initial iteration.This scenario, while rare, is real and may occur as originally brought up here by Manohar. Upcoming v2.8 Redis will definitely improve on it and in the future it is near-certain that the open-source community will overcome it completely. In the meantime, if you are looking for an immediate solution for it, you can visit our github to download our version of Redis 2.6.14. In this version we’ve included a client-throttling mechanism to tactfully buy enough time for the slave to complete synchronization. Our throttling mechanism works by introducing a delay to the master server’s responses to application clients’ requests. While appearing counterintuitive at first glance, the added delay provides just enough “breathing room” for the slave to finish the transfer and replay the updates log before the latter runs out of space, thus allowing synchronization to complete and replication begin.In implementing this mechanism we have added the new configuration variablethat is set using the following syntax:where:So, for example, the following setting:will cause the the replication process to be played out as before with these changes:The master’s estimate of time to complete the replication cycle is done as follows:"
168,https://redis.com/blog/go-public-or-stay-private-which-aws-network-address-should-paas-users-use/,Go Public or Stay Private: Which AWS Network Address Should PaaS Users Use?,"July 31, 2013",Redis,"A little over 18 months ago we took our first baby steps in what has since then proved to be a wild and exciting ride. To the uninitiated outsider, a cloud service provider’s existence might seem dull and uneventful. However, anyone who has tirelessly searched for the one platform tweak to rule them all; who has tracked the elusive flow of the river of Through(put); who has courageously battled the Hydra Serpent of Many Heads (also known as the Ssssp-lit) will tell a different tale.Ours is a wonderful and adventurous world in which nothing is certain and there is only one truth: using the private network interface, a.k.a. private DNS, is always preferable. Or so we believed.We’ve since discovered otherwise, but first lets take a walk down memory lane… In the far reaches of time, before the Dawn of Virtualization and the Age of the Cloud, servers were physical objects, machines made of parts and pieces working in unison for computation’s sake. As networks sprang to life and grew, a distinct separation was made between private and public. Private networks were characterized by the fact they use their own address space, are accessible only to servers directly connected to them and are generally localized. The public network, as the name suggested, was global and accessible to anyone.This basic difference between networks reflected well in the role each played. The public network was used to provide a system’s service to its consumers, whereas private networks were used internally to connect the system’s component servers. Furthermore, given their nature and location, private networks were more often than not considered to be safer, faster, cheaper, more reliable and to have more bandwidth than their public counterpart. And then servers ascended and thus became The Cloud.In their new-found ethereal existence, servers shed their physical properties only to substitute them with virtualized replacements. No more servers, now there were instances in their place. Gone were actual CPUs and cores, in came compute units and vCPUs. Hard drives disappeared and gave way to ephemeral storage and “network attached persistent storage.” Even network interfaces were stripped of their corporeal essence and replaced by “IP addresses [that] are associated with the instance”. But we still believed that the private IP addresses were preferable for in-system communications and just plain awesome. But over time, experience has taught us differently.During the development and testing of our service, we’ve spent countless hours trying to define and reproduce a particularly nasty phantom. We were finally able to capture it and we learned the following: AWS’s private addresses are, at times, worse than public addresses!Once identified, we were able to easily and repeatedly demonstrate this behavior. The above graph is a live recorded representation of the time it takes a simple application to connect to one of our Redis Cloud instances. The application uses two threads, one connecting to the public address and the other to the private one. Once both connections are made, the application writes the time it took each connection to complete into that instance and is restarted.Note how after each restart of the client, connecting to the private address in most cases takes more time than connecting to the public address, by orders of magnitude, and rarely is as fast as the public address. While these symptoms are easily demonstrated, the causes for this behavior are harder to explain. We suspect this unwelcome conduct is at least partially caused by the extra protection that the private interface is given. Since AWS uses software-based firewalls to protect private instances, the former can become quickly burdened by having too many rules to enforce. That load isn’t necessarily generated by a specific EC2 instance (in our tests we used less than 10 Security Groups), but rather by many EC2 instances that share the same firewall in the AWS network.Once too loaded, the firewalls may introduce delays and even block connection attempts. This phenomenon, as shown above, is reliably reproduced when using PaaS for an application. Interestingly, it is much rarer with independent EC2 users.Therefore, our conclusion and standing recommendation to PaaS users is to avoid using the private interface in favor of the public interface – the latter may induce up to a millisecond’s worth of latency, but at least it performs more consistently than its counterpart. So there you have it, another day, another myth busted."
169,https://redis.com/blog/lets-play-master-and-servant-real-time-synchronization-tool-for-redis-migration/,Let’s Play Master and Servant: Real Time Synchronization Tool for Redis Migration,"August 21, 2013",Redis,"Especially when orchestrating the migration of a sharded Redis backend with a strict requirement for minimal application downtime.That is exactly the challenge we addressed with one of our users who wished to migrate to our Redis Cloud service from an existing setup, which is why we developed our own tool to ease this process (more details below). Many databases, Redis included, can be trivially moved from one place to another using built-in backup and restore mechanisms with a couple of file copy operations.This approach, however, requires the database to be stopped and taken off-line during the move. In some cases you can optimize the process to reduce the database’s downtime (e.g. by completing the migration in two phases, the first for the bulk of the data body and the second for deltas), but a there are use cases where this approach is technically impossible (i.e. updates come too fast and in Redis’ case that could mean tens of thousands requests per second) or simply unacceptable (because of the application’s criticality).An alternative approach uses database replication, also supported by Redis, to create a copy from the original (a.k.a master) as an exact replica (a.k.a slave) at the destination. Then, once the replication is complete, you can switch to the replica.The advantage of this method over the native backup-copy-recover approach is that application downtime can be kept to the absolute minimum because the database is not stopped during the migration.Once the slave is ready, the application only needs to switch from using the master to using it instead – a change that takes practically no time and has no discernable effect with modern application architectures.To successfully perform migration with replication, our recommended process is:Replication as a means for migration is a great vehicle, but introduces some complexity. You need to know exactly when to proceed from one step to another, because all sorts of mishaps can happen if you prematurely switch to the slave.Consider, for example, a case in which the rate of updates is high enough that the master’s replication buffer is never quite empty or even overflowing as described in The Endless Redis Replication Loop. With such cases, identifying the right point in time to make the switch between the master and slave can be a challenging task.The process becomes even more complex if the migration’s scope consists of more than one server (e.g. in a sharded scenario). Regrettably, this has been somewhat of a constant issue for Redis users up until and including v2.6. The good news is that Redis v2.8 improved–PSYNC will introduce the lag information under the replication section of SHOW INFO’s output but this will not eliminate the timing challenge.To quote the man himself:The better news is that as of today, regardless which Redis version you use, you can use our home-grown redis-migrate tool to carry out the replicated migration of one or more Redis servers without even breaking a sweat.Available here from our github, redis-migrate is an interactive Python script that displays real-time replication status information with top-like UI. The script accepts two N-sized lists of Redis URLs, via the the –src and –dst arguments, as input.Once invoked, it first prompts you to let it continue (by entering ‘c’) after displaying the total size of data, number of keys and Redis servers that will be migrated. If you choose to continue, it then executes Step 1 of the above-described migration process by setting up and starting replication between the masters (src list) and their respective slaves (dst list). As Step 1 is executed, you are presented with up-to-date information regarding the progress of each master-slave replication link.You can prompt the tool to continue to Step 2 once you have verified that everything is ready (i.e. initial sync completed and updates are flowing) by entering ‘e’. During Step 2, the tool waits for you to point your application to the replicated slaves. You can verify that your masters are no longer being used with the real-time information that the script outputs when replication buf is 0.As before, the script waits for your permission to move to Step 3 – just say ‘m’. Once Step 3 has been executed – promoting the slaves to masters – the tool undramatically exits.All you have to do now is attend to the old master.Note: Ax not included"
170,https://redis.com/blog/reflections-from-sitting-on-a-branch-of-a-falling-tree-in-the-amazon-forest-or-post-mortem-the-aws-outage-from-a-service-provider-view/,Reflections from Sitting on a Branch of a Falling Tree in the Amazon Forest or Post-Mortem: The AWS Outage From a Service Provider View,"August 28, 2013",Redis,"You’ve probably already heard about AWS’s outage last Sunday around noon PDT. The thud of that “minor” network-cum-storage issue in one of the busiest data regions of the Amazon forest cloud was heard loud and clear. Even before that fallen zone tree’s leaves touched the ground, reports about the collapse of some of the better-known social brands (i.e. Instagram, Vine & IFTTT) started coming in.As usual, AWS has not divulged the full extent of the incident’s impact, but it stands to reason that there were other, albeit smaller, casualties as well. Our Redis Cloud and Memcached Cloud services use resources from that zone, and we manage a number of fairly large clusters in it. We use network and storage heavily, given the nature of our replicated and durable in-memory databases, so naturally our dashboard flared bright red at the first sinister splintering sound.From the extent of damage we monitored in our environment, we believe it’s possible that 15-30 percent of that zone’s services were affected by the event. Even after the network started degrading, our systems were thankfully able to withstand the interruption. Our replication streams were not laggy for the most part, and the response latency impact was relatively small.Things looked a little shaky for the availability of our non-Multi-AZ instances for a while, but we held on tight. But just when it seemed that the worst was over and the shakes were subsiding, we experienced plummeting performance with the accelerated decline of our storage.Cliffhanger.Although somewhat counterintuitive at first, in-memory database do use storage. However, they use it only to store data instead of reading and writing from/to it like disk-based database.Writing the data to disk endows the in-memory database with persistency – a highly desirable trait that comes in very handy when recovering from a meltdown, but that’s a different story. However, it also means that when the storage becomes unavailable the database can’t ensure the data’s durability. It can still serve read requests, but in most cases a read-only database is like a fish on a bicycle.This is why most in-memory databases are designed to cease operating without accessible storage, Redis included.With storage failing on multiple nodes simultaneously, our systems did the only thing they could: work around the failure to recover themselves and keep the service operational.Affected nodes continued serving the non-persistent Redis Cloud instances while immediate action was taken with the persistent ones. Persistence-enabled Redis Cloud instances were quickly migrated to unaffected server and storage resources before causing any disruption to our service. Granted, the extra load we put on the network and storage in moving the data from one place to the other wasn’t meant as a resolution to the developing issues, but was aimed at making a hasty retreat to neighboring shelters.After all the dust had settled, we ran a quick survey of our users and were relieved to learn that none had experienced any issues with our service during that period (despite our logs looking like the work of a psychotic ax murderer). Of course, the scenario might have been different had this outage been marginally less minor.We’re glad that wasn’t the case, and feel validated to obtain real-life proof of how a system that’s designed for dynamic stability copes with the challenges it was built for."
171,https://redis.com/blog/an-invitation-to-an-open-rediscourse/,An Invitation to an Open (Re)Discourse,"September 18, 2013",Redis,"Arguably, one of the strongest benefits of the open source movement is that the community can get actively involved in the development of projects. Having access to the source code of an application allows anyone to modify it at will to suit specific needs (within the boundaries of its licensing). Naturally, we at Garantia Data not only use the open sourced Redis and Memcached software in our service, but also contribute our own developments back to the community.Most of our contributions are tied into the projects we use, but once in a while we also diverge slightly. One such case is that of the popular Discourse project – an open source discussion platform that’s used for operating forums. It is built using Ruby and relies on Redis for job queuing, rate limiting and caching among others.While it appears to be a solid piece of engineering, we have gotten several support calls from our users who had tried using it with our Redis Cloud service. The crux of the issue is that Discourse uses Redis’ shared databases capability.Our service, by design, deliberately blocks any attempt to use this functionality because of performance considerations – you can read more on that topic in this post. That limits our users that want to put their Redis Cloud instances to use with Discourse. To resolve this, we asked our engineers to dig into Discourse’s code and develop the fixes necessary to make it compatible with our service. They were successful, and we put in a pull request to have these fixes merged with Discourse’s sources.Until then, you are welcome to use our Discourse fork, available from our github.Do you have an open source project that you want us to take a look at? Are you experiencing any issues using our service with other software? Feel free to drop us a line and tell us about it. We can’t promise anything, but we’ll do our best to help!"
172,https://redis.com/blog/how-to-make-your-in-memory-nosql-datastores-enterprise-ready/,How to Make Your In-memory NoSQL Datastores Enterprise-Ready,"September 24, 2013",Yiftach Shoolman,"In-memory NoSQL datastores such as open source Redis and Memcached are becoming the de-facto standard for every web/mobile application that cares about its user’s experience. Still, large enterprises have struggled to adopt these databases in recent years due to challenges with performance, scalability and availability.This article will outline how to make an in-memory NoSQL datastore enterprise-ready, with tips and recommendations on how to overcome the top seven challenges associated with managing these databases in the cloud:Continue reading the full article at InfoQ."
173,https://redis.com/blog/scaling-out-redis-read-only-slaves-or-cluster/,Scaling Out Redis: Read-Only Slaves or Cluster?,"October 1, 2013",Redis,"Instead, it uses asynchronous replication “for scalability … or simply for data redundancy.”Also, “a [Redis] master can have multiple slaves … [that] are able to accept other slaves’ connections. Aside from connecting a number of slaves to the same master, slaves can also be connected to other slaves in a graph-like structure.”Setting up Redis replication is trivial and its flexibility provides great power.Classic use cases for Redis’ replication are:As a Redis Service provider, we are often asked why we do not provide access to our read-only replicas. It’s a valid question, and our answer relates to each of the use cases:Since replication is asynchronous, it can introduce inconsistencies when accessing stale data. Sharding, on the other hand, does not exhibit that property and is therefore a preferable approach to addressing read and write scaling challenges. Because our managed service addresses scalability (via sharding), redundancy (with auto-failover) and load management, there are only a handful of scenarios in which having access to the read replicas is of any value.For example, a replica can be set up to give read-only database access to interested parties (e.g. development).We are planning to support this functionality in the future, so if you have other use cases for read replicas we’d love to hear about them. Let us know if you are using replication for purposes other than scaling, redundancy or offloading – just drop us a line at info@garantiadata.com or contact our support."
174,https://redis.com/blog/why-redis-is-a-great-tool-for-new-applications-and-startups/,Why Redis is a Great Tool for New Applications and Startups,"October 15, 2013",Redis,"This article was originally published at Citizen Tekk on October 7th, 2013It can also be found here.While it has been proven time and time again that open source databases and technology are ideal for startups and application developers due, in large, to the potentially unlimited contributors that aid in perfecting the code, when it comes time to choose a database within that open source software, what makes one stand out over the other?Yiftach Shoolman, CTO of Garantia Data, which is the provider of enterprise-class Redis Cloud and Memcached Cloud services, has provided some insight on the NoSQL, open source database, Redis, and why it stands out as a great tool for new applications and startups.Open source Redis is one of the top three databases used by new applications today. According to a survey of database users by 451 Research, Redis adoption is projected to increase from 11.3 percent to 15.9 percent in 2015.But what is it, exactly, that makes Redis so attractive to startups and application developers alike?Redis’ popularity is due, largely, to its combination of high-performance, attractive data types, and commands that simplify application development. As new companies and applications emerge, they demand scalable high-performance databases to keep up with the exponential growth of their data. Redis’ unique characteristics have resulted in tremendous adoption rates — making it a database of choice for many leading companies.For example, Pinterest uses Redis for the “follower graph”, which is a breakdown of who is following whom and Twitter uses Redis for its home timeline. Redis is especially well suited to new companies and applications for several key reasons.Redis is entirely served from RAM, which makes it faster than any other datastore (most of which are served from disk) by an order of magnitude. Furthermore, it has a simple, single-process, event-driven design, meaning it does not have to deal with lock mechanisms like other databases do, which hinder performance for many applications.The diagram below presents benchmark tests carried out for several leading databases.Benchmark tests showing the fastest data store systems available todayDeveloping new applications with Redis is way simpler, more intuitive and faster than other databases, including MySQL. Redis has a rich set of data structures, which are very similar to those of today’s high level programming languages that are increasingly used by application developers. The code used to build the data structures of Redis, like sets, lists, sorted lists, etc., allows users to perform really complex tasks very easily.It also offers transactions that allow users to plan multiple commands, making it thread-safe.With Redis, developers do far less damage to the concepts of their programs, resulting in faster development, improved code quality and more beautiful code. This, combined with its top performance, it’s no wonder why Redis’ popularity is soaring."
175,https://redis.com/blog/join-redis-cloud-at-aws-reinvent-in-las-vegas/,Join Redis Cloud at AWS re:Invent in Las Vegas!,"October 31, 2013",Redis,"Here at Redis Cloud, our mission is to provide enterprise-class Redis to developers. At AWS re:Invent, developers, business leaders, and cloud visionaries will come together to share ideas and learn from each other, and we’re very excited to be a part of it again. If you’re at the event, we hope you’ll join us for any or all of the following…Our staff, customers, partners, and friends will be there to share how we empower developers with enterprise-class Redis. We’ll also have really cool goodies for you 🙂Dr. Carlson will be joining the team and available to discuss a variety of Redis and NoSQL high RAM topics as well as feature his book ‘Redis In Action’.We’ll be making several exciting and ground breaking announcements at the event. We look forward to seeing you at booth #627! Feel free to contact Cameron on our team for questions or advance bookings by email or twitter."
176,https://redis.com/blog/we-raised-9m-in-series-a-funding/,We Raised $9M in Series A Funding,"November 5, 2013",Ofer Bengal,"Today we’re proud to announce that we’ve raised $9 million in Series A funds with participation from Bain Capital Ventures and Carmel Ventures. It’s a very exciting day for the Garantia Data team, our customers, and the entire open source Redis community. We’re also very excited to welcome two new board members to our team: Salil Deshpande from Bain Capital Ventures, and Ronen Nir from Carmel Ventures.We will use the new funding to continue the development of our core offerings Redis Cloud and Memcached Cloud. We now have more than 1,000 paying customers that use Redis Cloud every day for seamless scalability, true high-availability, stable top performance and fully automated operations.We were about to change our company name to Redis and even acquired the domain redis.com for that purpose; however, respecting a request by Slavatore Sanfillipo, the Redis creator, we decided to stick to Garantia Data.Redis is now one of the fastest growing NoSQL databases and ranks among the top three chosen databases for new applications. According to 451 Research, the NoSQL market is expected to grow to $1.3 Billion in the next few years. This makes perfect sense to us as we see more and more developers choosing Redis for its many use cases, strong community and large ecosystem.Our roots are in the open source community, and we want to continue to support and foster this growth. We believe that a rising tide lifts all boats and will continue to be a significant contributor to the open source Redis community. We’ve learned very well how to adapt Redis to a variety of enterprise needs and we will continue to develop and build upon the high level functionality and service that you would expect from our team!"
177,https://redis.com/blog/great-just-got-even-better-more-free-space-new-plans-at-a-lower-price/,"Great Just Got Even Better: More Free Space, New Plans at a Lower Price","November 7, 2013",Redis,"We’re already having a whirlwind November as we gear up for AWS re:Invent next week, and we don’t intend to slow down! Today we are delighted to announce some new options and exciting changes for our popular Redis Cloud and Memcached Cloud plans:Since the beginning of our commercial service offerings, our free 20MB plan has been extremely popular with both new era and enterprise level developers. A key part of this popularity is because we have always made our free tier more useful than our competition’s without attaching any strings to it.But when something is really great, that doesn’t mean that it can’t be improved…As of today, our free tier now provides a memory size of 25MB for all instances. You can start using the extra 5MB right away without changing anything in your account or application (the update may take slightly longer for some of our PaaS users).Until today, we offered just two variants of our highly-available, enterprise-class premium services – Standard and Multi-AZ (which offers additional resilience to zone failures). However, as we’ve talked with customers, we noticed some use cases for which high availability isn’t mandatory, i.e. when our services are used for cache-like purposes. So, today we have added new plan options, called Redis Cloud Cache and Memcached Cloud Basic.These plans still provide infinite scalability, dedicated databases and a devoted team, but at considerably lower prices. Of course, they don’t feature our additional replication, auto-failover or data persistence capabilities, which means that the only way to get your data back after a failure is by restoring it from backup (that we do provide of course).Check out our new pricing page for details about our plans. That’s all for today but the week isn’t over… We hope to see you at booth #627 at the AWS re:Invent event in Las Vegas next week (contact Cameron for bookings and questions) and, as always, feel free to drop us a line on any matter at any time."
178,https://redis.com/blog/hello-%e4%bd%a0%e5%a5%bd-and-%e0%ae%b5%e0%ae%a3%e0%ae%95%e0%af%8d%e0%ae%95%e0%ae%ae%e0%af%8d-2014-garantia-data-opens-shop-in-aws-us-west-apac-data-regions/,"Hello, 你好 and வணக்கம் 2014! Garantia Data Opens Shop in AWS US West & APAC Data Regions","January 2, 2014",Itamar Haber,"Today we are pleased to kick off 2014 with the announcement of the immediate availability of Redis Cloud & Memcached Cloud services off two additional AWS data regions:We’ve added these regions to our services to meet the demand to regions other than AWS’s us-east-1 and eu-west-1, and Azure’s us-east and us-west, where we already have many active users. The details about all of our new and existing plans can be found, as always, at www.redis.com/pricing.If your application is using compute resources in these data regions and you want to use our services then there’s really nothing to it:You’re done! Now you can create a cloud instance in seconds and start using it immediately with your application just by changing its connection endpoint.This is just the beginning of what we hope and plan to be a pivotal year. If you want to hear more about our plans, have feedback to offer or are just looking to make new friends as your New Year’s resolution – contact me at itamar@garanatiadata.com, or tweet us at @Redis."
179,https://redis.com/blog/3rd-time-is-a-charm-garantia-data-is-now-redis-labs/,3rd Time Is A Charm! Garantia Data Is Now Redis Labs,"January 29, 2014",chen.w,"Today we’re thrilled to officially announce our company’s new name: Redis. Since 2012, the developer community has known us as Garantia Data or by the names of our popular services, Redis Cloud and Memcached Cloud (even better).This new name is a natural step for our continued engagement with the Redis and NoSQL community. Redis has been both a contributor to the open source Redis project and a top choice among developers for building Redis database instances with infinite scalability, high availability, stable top performance, and zero management.This corporate re-branding reflects our commitment to making Redis one of the leading databases of choice among developers for many years to come. Redis is one of the fastest-growing NoSQL databases, with a vibrant open source community and high visibility users, which require super-fast read/write capabilities. The name change aligns with our commitment to making Redis a cornerstone of every application with demanding performance requirements.In November 2013, we announced $9M in Series A funding and a name change to RedisDB. However, after listening to the Redis community’s concern about possible confusion between the open-source project name and our company name, we decided to stick with our original Garantia Data name for the time being. Two months later, after gathering additional feedback from the community on the name Redis, and confirming that no such confusion exists in this case, we’re ready to unveil our new name.The only thing that’s changing is our corporate identity – you can expect the same great service for both Redis Cloud and Memcached Cloud. Regarding our popular Memcached Cloud service, it’s important to note that we’re still committed to this offering, which is actually powered by a Redis engine and our proprietary dynamic clustering technology. This allows us to support capabilities and use cases that are not available in OSS Memcached, including storage engine, replication and auto-failover.Questions about the name change? Please email me or ping me on Twitter!"
180,https://redis.com/blog/up-to-30-price-reduction-for-redis-cloud-on-windows-azure/,Up to 30% Price Reduction for Redis Cloud on Windows Azure,"February 6, 2014",Leena Joshi,"Today we’re proud to announce a price reduction for Redis Cloud on Windows Azure. In line with Window’s Azure price reduction of up to 22% on memory-intensive compute instances across Windows, Linux and Cloud Services, we’ve aligned pricing for both our Redis Cloud and Memcached Cloud services with reductions by up to 30%.Current Windows Azure users building highly scalable apps with Redis Cloud will immediately benefit from the price reduction… no action required. See our current pricing for Windows Azure and other public clouds here.Completely served from RAM, Redis is the fastest datastore available today and one of the top databases chosen by developers. Many modern day applications use Redis because of its numerous attractive data types and commands, which can achieve uses cases such as:Many developers build high traction apps by using Windows Azure for its high performance infrastructure and Redis Cloud to fully capture the super-fast read/write capabilities of Redis. More than 1,400 Redis customers currently power their apps with use Redis Cloud for its infinite scalability, true high availability, top performance, and zero management.Sign up today at www.redis.com for a free 25MB Redis instance.Questions? Contact me at cameron@redis.com or a twitter @Redis."
181,https://redis.com/blog/the-search-for-redis-on-google-compute-engine-gce-is-over/,The Search for Redis on Google Compute Engine (GCE) is Over!,"February 12, 2014",Itamar Haber,"Update: the beta is over and we’re generally available on Goolge Compute Engine – all the details are at our pricing page.I am pleased to announce the immediate availability of Redis Cloud on Google Compute Engine (GCE). Even before GCE’s GA launch, a lot of developers had contacted us and asked whether we’ll offer our services there – this is our positive answer to that question. Naturally, we’re very excited to add this new and promising platform to our offerings. As is the case with every new cloud, we’ll be kicking it off with a beta.The new Redis Cloud Google Compute Engine 1GB beta plan is available today for free to anyone with a Redis account.[Don’t have one yet? What are you waiting for? Create your free 1GB Redis instace – minimum details, no credit card is required, no spam is sent – ever]You can find all the details on our pricing page by selecting GCE/us-central1 in the Cloud drop-down box and if you add the free beta subscription you’ll be able to have a go at your very own Google-flavored Redis Cloud instance in seconds.Apropos having a go, you can connect to your new Redis Cloud instance on Google Compute Engine with an idiomatic Go foobar:"
182,https://redis.com/blog/safe-and-informed-try-out-our-new-alerts-ssl-support/,Safe and Informed: Try Out Our New Alerts & SSL Support,"March 4, 2014",Itamar Haber,"Lately we’ve been busy establishing and expanding our service’s presence on the IBM SoftLayer Cloud and Google Compute Engine. But, that doesn’t mean that we’ve been neglecting our core service development. As a matter of fact, today, by popular demand,  I am pleased to announce the public availability of two new features for our services across all clouds: Email Alerts and SSL support.Our first addition, Email Alerts, allows you to get pertinent notifications regarding the usage and performance of your Redis resources. You can enable the new alerting mechanism for any resource from the service’s console. By default, emails alerts are turned off for your existing resources, but you can easily change the alert settings by editing any Redis Cloud instance or Memcached Cloud bucket and expanding the Advanced Options section.Each email alert trigger can be toggled on or off independently and have its threshold configured. You can set up the triggers to get email alerts for the following conditions:In addition to all of these great notification tools, we’ve officially added support for SSL authentication and encryption after a successful private beta. Standard Redis and Memcached do not feature built-in security mechanisms other than password protection. This is an omission by design, because both technologies are primarily aimed for use near the application, inside a trusted and secure network. By putting security out of scope, these open-source projects can stay focused on their primary goals, without adding functionality that would eventually bloat them.At Redis, however, we provide our services’ resources from public clouds, which of course requires more security. Our resources can be configured with Source IP and (AWS) Security Groups access control rules to restrict access to them. Password protection and access control rules are great means for authentication, but leave the communication channel unsecure. This could potentially expose your data to eavesdropping, man-in-the-middle attacks and other security threats.SSL ensures that the communication channel between your application servers and your Redis resources is secure and strongly encrypted. Since SSL is not native to Redis or Memcached, your application will need to use a secure proxy such as stunnel to connect to our resources using SSL. SSL-enabled plans cost twice as much as our Standard plans and are offered exclusively via our custom plans – send a line to support@redis.com if you’re interested in learning more about them.We are working on adding SSL support to popular Redis clients so that eventually the use of a secure proxy will be optional. Contact me at itamar@redis.com or on Twitter at @itamarhaber to let me know if you want SSL support added to a specific client and if you want to help in developing one – you’ll get a 100MB SSL-enabled subscription, free of charge for a whole year!"
183,https://redis.com/blog/the-proven-redis-performance/,The Proven Redis Performance,"April 8, 2014",Yiftach Shoolman,"Recently, a known NoSQL vendor announced the addition of in-memory capabilities to its database offering. The decision to take a hybrid approach with their database offering brings up a few serious concerns. Redis, the in-memory, open-source, key-value store, has provided many insights into the world of modern applications. One particularly eye-catching observation is the inherent need for databases with in-memory capabilities. As a result, NoSQL vendors across the board have been working diligently to harness the benefits of both on-disk and in-memory databases.At Redis, we believe this hybrid approach is wrong. Our concern stems from the fact that vendors using this approach cannot succeed in providing the advantages that databases such as Redis, with solely in-memory capabilities, have introduced to modern applications. In his recent post, Salvatore Sanfilippo listed three important factors of database performance including latency, operation speed, and operation quality. The following real-life scenarios exemplify these three factors based on the experience we have gained from supporting tens of thousands of modern applications.Before diving into each of these performance components, I would like to share the results of an in-house research study we did to compare the performance of a number of popular SQL and NoSQL databases. As seen below, there is no doubt that in-memory databases perform significantly better.NoSQL and SQL Response Performance ComparisonAll in-memory systems can be measured by three main criteria: latency, throughput and efficiency. Similar to Sanfilippo’s three important factors of database performance, excelling at these criteria enables in-memory databases, such as Redis, to perform significantly better than their on-disk rivals. Because of this, in-memory databases have become critical for modern, scalable applications and real time processing.1 – Latency: Modern applications require a latency of around 100 msec from the time a request reaches an application until it is processed and the application generates a response and returns it back to the user. In order to achieve this, the database level must perform at a consistent sub-millisecond latency, which is solely accomplished by an in-memory database that retrieves all data directly from RAM.2 – Throughput: Nowadays, applications’ performance needs to scale instantly. Take a Twitter account with 1 million followers for example. A single tweet intended to reach 1 million people instantaneously generates 1 million simultaneous writes and requires immediate scaling. These types of events occur constantly without any prior warning, creating significant challenges for the system.Scenarios such as this require infrastructure that can instantly scale out. Auto-scale will not do the job, as provisioning additional nodes to an existing cluster can take several minutes. And building an entire NoSQL infrastructure just to serve requests during peak times is very expensive. Accordingly, a growing number of developers are designing real-time apps entirely from scratch with Redis.3 – Efficiency: While an in-memory database may be extremely fast, its performance will continue to be poor if it is constantly transferring large amounts of raw data between the database and an application. Redundant processing of raw data not only wastes resources, it increases latency. In addition, this raw data transfer can potentially block the entire network infrastructure, requiring node additions in order to maintain network speed.Using Redis’ unique data types and commands, it is possible to build a schema in such a way that the database is tuned to serve application requests without any additional processing at the application level. This intelligent design significantly reduces the amount of transferred data.The figure to the right illustrates an actual scenario from one of our customers. Without Redis, they would’ve needed to build a 150+ node cluster to support a network speed of 120 Gbps. In addition, they would’ve required increased work at the application level. Redis works using a six, in-memory, node cluster, 1.5 Gbps, and no extra work at the application level.Attempting to fix problems that Redis has already solved by adding in-memory capabilities to an incompatible database is destined to fail.At Redis, we believe in a best-of-breed approach. There is no reason to settle for just one type of database, whether it be in-memory or disk based. Modern, scalable applications ought to be designed in a way that ensures each component leverages the appropriate database.(Author’s note: Many thanks to Salvatore Sanfilippo for the fruitful conversation that led to the ideas in this post.)"
184,https://redis.com/blog/security-notice-cve-2014-0160-heartbleed-openssl-vulnerability/,Security Notice: CVE-2014-0160 / Heartbleed OpenSSL Vulnerability,"April 9, 2014",Itamar Haber,"Over the last couple of days our team has been hard at work to address the newly-disclosed CVE-2014-0160 vulnerability (a.k.a. Heartbleed). We’ve completed proofing our service against the bug by upgrading our OpenSSL libraries, replacing certificates and changing the credentials. While we have no reason to suspect that our service’s security had been breached, we strongly recommend that all our users change all their passwords and certificates by following the steps in the sections below:To prevent unauthorized access to your Redis account, change your login password:Next, you should change the password of your Redis Cloud and Memcached Cloud resources. To do so, follow these steps from your account’s console:If you are subscribed to our SSL-protected Redis Cloud and Memcached Cloud plans, we strongly recommend that you replace your resources’ certificates. This can be done by editing your resource and generating or uploading a new client certificate under the SSL Client Authentication section.Lastly, once you’ve changed the password of a Redis cloud resource, you’ll need to apply the change to your application’s environment as well.To change the password of your Redis Cloud or Memcached Cloud Add-On resources, follow these steps:Once you’ve changed the password of a Redis cloud resource, you’ll need to apply the change to your application’s environment as well. To update your Heroku app’s environment with a new Redis Cloud password, use the CLI to modify the config var in the following manner for the Redis Cloud add-on:$ heroku config:set REDISCLOUD_URL=redis://rediscloud:<password>@<host>:<port>Similarly, if your app uses our Memcached Cloud add-on, use the following syntax:$ heroku config:set MEMCACHEDCLOUD_URL=memcached://<user>:<password>@<host>:<port>Please make sure you replace the placeholders (e.g. <password>, <host>, <port>,…) with the information that’s relevant to your resource. Also note that any changes to your application’s config vars will restart your application.To change the password of your Redis Cloud or Memcached Cloud Add-On resources, follow these steps:Once you’ve changed the password of a Redis cloud resource, you’ll need to apply the change to your application’s environment as well. To update the VCAP_SERVICES environment variable of your CloudFoundry app with your Redis Cloud connection details, use the CLI as follows (formatted for readability):$ cf set-env –app <appname> –name VCAP_SERVICES –value ‘{rediscloud-n/a: [{name: “rediscloud-42”,label: “rediscloud-n/a”,plan: “<plan>”,credentials: {port: “<port>”,hostname: “<host>”,password: “<password>”}}]}’The same approach should be used for your Memcached Cloud resource:$ cf set-env –app <appname> –name VCAP_SERVICES –value ‘{memcachedcloud-n/a: [{name: “memcachedcloud-42”,label: “memcachedcloud-n/a”,plan: “<plan>”,credentials: {servers: “<host>:<port>”,username: “<user>”,password: “<password>”}}]}’Important: if your VCAP_SERVICES variable consists of additional services, make sure these are not overwritten by the password update.Please make sure you replace the placeholders (e.g. <password>, <host>, <port>,…) with the information that’s relevant to your resource. Also note that any changes to your application’s evironment variables will restart your application.To change the password of your Redis Cloud or Memcached Cloud Add-On resources, follow these steps:Once you’ve changed the password of a Redis cloud resource, you’ll need to apply the change to your application’s environment as well. To update the configuration variables of your AppHarbor application, access the Configuration Variables tab in your AppHarbor console. Once at that tab, locate the REDISCLOUD_URL variable (or MEMCACHEDCLOUD_URL variable) and edit its value to reflect your database’s new password.Note that any changes to your application’s configuration variables will restart your application.To change the password of your Redis Cloud or Memcached Cloud Add-On resources, follow these steps:Once you’ve changed the password of a Redis cloud resource, you’ll need to apply the change to your application’s environment as well. To update the configuration variables of your AppFog application, navigate to the Env Variables tab. Once at that tab, locate the REDISCLOUD_URL variable (or MEMCACHEDCLOUD_URL variable) and edit its value to reflect your database’s new password.Note that any changes to your application’s env variables will restart your application."
185,https://redis.com/blog/fast-and-efficient-parallelized-comparison-of-redis-databases/,Fast and Efficient Parallelized Comparison of Redis Databases,"April 16, 2014",Yoav Steinberg,"The process of comparing two versions of a database is a fairly common practice, generally used for testing and development purposes, as well as supporting application updates and new releases. Comparing databases provides a number of advantages. Beginning by ensuring two databases are fully synced, then enabling users to assess the functionality of the backup setup and various processes, including verification of backup restoration or master-slave replication. This process is highly useful to anyone who uses a Redis database. As such, at Redis, we compare older versions of databases with our own newly developed ones, ensuring the replication mechanism between the different versions is satisfactory. In addition, we continuously validate our database synchronizations between different cloud zones and regions as a means to support customers with their fully managed and highly available Redis databases. The following information expands upon the process I underwent to develop the Redis comparison tool.Initially, we used Redis RDB Tools, which is an open source project that provides libraries to efficiently parse Redis dump files (RDB). The Redis RDB Tools library also provides tools to compare two RDB files, analyzing the amount of memory that their operations require, among other useful capabilities. By means of the dump file of a given database, Redis RDB Tools prints a text file consisting of all existing key names and respective values sorted alphabetically. Once a file has been created for each database, the two files can be compared easily using a simple text comparison tool (e.g. diff or meld).Redis RDB Tools has proven to be very flexible and useful in the past, though over time, several drawbacks were discovered.Redis’ recent version 2.8 introduced a family of commands (SCAN, HSCAN, SSCAN and ZSCAN) that are used to incrementally iterate a collection of keys. Using the SCAN function, it is possible to iterate through all of the keys in one database, and compare each key with the corresponding key type in a second database, if such a key, indeed, exists. This can provide a simple and efficient comparison solution that operates on the existing database and therefore does not require any additional compute resources or the creation of a local dump file.Even with this fairly straightforward approach, we ran into performance issues when dealing with large datasets. Comparing millions of keys, one key type at a time, while using a single Redis connection is still a very slow process. In order to overcome this, I wrote a script which consists of a pool of processes using Python’s multiprocessing library. Through this method, the main process iterates all of the keys in the source database and subsequently sends several keys to the processes in the pool. Each process then compares the key it received with the second database. Since the comparisons are parallelized, the entire process is completed much faster.The script currently prints “Key X differs key Y” when discrepancies are discovered between two databases. Improvements can be made to the tool by having a more detailed output, such as indicating the specific element that differs within a list type key.Redis keys can be optionally set with expiration values. During a comparison, it is possible that a key read from the source database will have already expired in the target database (or vice versa), however, our tool will still indicate a difference. Since the key will soon expire this difference may be irrelevant to the overall database comparison, nonetheless, our current version does not take this factor into account. Setting a threshold for minimal expiration values and comparing between different expiration values is yet another useful improvement that would enhance the tool.It should be noted that this solution could possibly fall short if the data is altered during the comparison process. This tool should be used for cases in which data is constant, or if real time data changes are tolerable.I welcome you to download this Python script that will help perform a simple and quick comparison of your Redis database."
186,https://redis.com/blog/konnichiwa-and-gday-redis-cloud-now-available-in-tokyo-and-sydney/,Konnichiwa and G’day: Redis Cloud Now Available in Tokyo and Sydney,"April 20, 2014",Itamar Haber,"Due to popular demand, I am delighted to announce the immediate availability of our Redis Cloud and Memcached Cloud services off AWS’ ap-northeast-1 (Tokyo) and ap-southeast-2 (Sydney) data regions. After having  opened shop in Singapore  at the beginning of this year, our friendly takeover of the Asia-Pacific region is finally complete!The majority of Redis and Memcached users expect and rely on top performance from their datastores to support their applications’ requirements. Network latency, however, makes it impractical to have your in-memory database reside anywhere other than in close geographical proximity to the data center where your application is hosted. Furthermore, given the potentially network-intensive nature of using our services, mere proximity is not necessarily always enough because of the often-limited bandwidth between different data centers. Therefore, the optimal and only solution is to have your database and application instances run in the same data center and leverage the LAN’s performance.I invite you to experience firsthand our enterprise-class cloud services and benefit from the only hosted Redis and Memcached that offer infinite scalability, high-availability and stable top performance. Sign up to our service and you can start with our free 25MB plan. When needed, you can upgrade to any of our plans instantly, seamlessly and without any interruption to the service, data loss or configuration changes.Questions? Feedback? Sushi and beer? Drop me a line or a tweet – I’m highly available 🙂"
187,https://redis.com/blog/re-april-23rd-aws-eu-west-1-service-interruption/,Re: April 23rd AWS EU-WEST-1 Service Interruption,"April 24, 2014",chen.w,"This week on Tuesday, April 23rd, Amazon cloud’s eu-west-1 data region experienced a service degradation. It was not highly publicized, although some reported on Twitter about AWS’ connectivity issues and node failures. Analyzing the impact of this incident on our services can provide us and our customers with valuable insights regarding the efficiency of Redis’ automatic failover mechanisms.What occurred in AWS’ EU region was a multiple node failure event, in which several nodes within the same Redis cluster became temporarily unavailable. In some clusters the nodes were affected one by one, while in other cases all nodes malfunctioned simultaneously, causing different results:Some of Amazon’s services were disrupted for several hours, with our ops monitoring the situation the entire time. We were pleasantly surprised by how few customers actually contacted us about this issue. It is encouraging to be able to observe the way that our services protect our customers from damage when failovers occur.We at Redis offer several different mechanisms intended to protect our customers from precisely these scenarios. Organizations considering using our services should take this event into account in order to make an informed decision that suits the level of protection that they require.Check out this article for more information about our high availability mechanisms"
188,https://redis.com/blog/price-reductions-from-google-amazon-and-redis-labs/,"Price Reductions from Google, Amazon and Redis Labs","April 1, 2014",Itamar Haber,"Last week, two events shook the skies in San Francisco. On Wednesday, a sunny and gentle day, Google’s Urs Hölzle announced during the Google Cloud Platform Live event that they are cutting the prices of Google’s platform services by considerable percentages. A day later, during the AWS Summit keynote, Andy Jassy answered that move with Amazon’s 42nd price reduction – this time cutting the cost for a host of the cloud’s services by 10-40% (the Ultimate Question remains unknown). The skies over San Francisco showered rain that day as the two giant clouds collided (or maybe it was just Rob McKenna attending the events).We’ve wasted no time in processing this exciting news, and today I’m happy to announce our very own Redis Cloud and Memcached Cloud price reductions. We’ve lowered our prices of all plans on AWS by 10-40%. And the best news? The new prices will be in effect right away, so all our users can enjoy the lower prices starting on April 1st. Check out the details on our pricing page.We believe that a simple pricing model is priceless and that’s exactly why we stick with ours – it just makes sense all around. However, as a fully-managed and all-inclusive service provider, Redis’ prices do take into account a wide range of factors besides the cost of RAM, such as the costs of EBS storage, EBS I/O and network among others. We try, nonetheless, to roll any positive developments forward to our users whenever we can – as in this case.Have any questions, feedback or want to share your favorite bathtime gurgle? I’m highly available on twitter and email."
189,https://redis.com/blog/the-ibm-cloud-marketplace-new-partnership-forged/,The IBM Cloud Marketplace: New Partnership Forged,"April 28, 2014",Itamar Haber,"Today I’m happy to announce our newly-founded partnership with IBM’s Cloud Marketplace. Because Redis is a launch partner of the new marketplace, customers can immediately use our Redis Cloud and Memcached Cloud services to build blazing fast and scalable applications. Our enterprise-class service can be used together with 100+ other SaaS applications and services in the marketplaceand IBM’s SoftLayer Cloud (which we’ve actually been using since earlier this year with terrific results).IBM’s expertise and track record of powering and delivering enterprise-grade solutions for more than a century are well-known. Our partnership with IBM allows us to leverage that expertise, while expanding the reach of the best hosted and fully-managed Redis and Memcached services, without compromising on our core strengths: infinite scalability, high availability and stable top performance.I invite you to visit our Redis Cloud and Memcached Cloud offers at the IBM Cloud Marketplace tp try out the services with a click of your mouse – you can start with our free 25MB plan and upgrade at any time without service interruption or configuration changes.Questions? Feedback? Email or tweet me – I’m highly available 🙂"
190,https://redis.com/blog/run-quartz-the-java-job-scheduler-on-top-of-redis/,"Run Quartz, The Java Job Scheduler, On Top of Redis","April 30, 2014",Matan Kehat,"Job scheduler programs have grown to become very popular in today’s web application world. Most commonly used to automatically run asynchronous and heavy jobs, schedulers have been utilizing Redis as a go-to backend host for some time now. This stands true for popular job management systems, such as Resque, written in Ruby to create background jobs; Sidekiq, which provides efficient background processing for Ruby; and Python users who mainly use Celery with a Redis backend option. Based on requests by the community, we recently implemented Redis to support Quartz, the popular Java job scheduler.Quartz is a mature and robust solution that can use almost any type of database as a backend for storing its data. As most relational databases feature a JDBC driver, it isn’t surprising that Quartz’s JDBC JobStore is quite popular. Relational databases, however, can be slow and difficult to manage. In fact, the Quartz FAQ has several suggestions on how to improve performance while using JDBC, ranging from throwing money at the problem and buying better hardware/network/software to traditional DBA wisdom (e.g. adding indexes on tables).It is no surprise that Redis has many advantages as a backend for job schedulers, due to the efficiency and convenient data types it provides as an in-memory database. As noted in The Proven Redis Performance, Redis’ unique data types and commands enable schema building in such a way that the database is tuned to serve application requests without any additional processing at the application level. Additionally, given that an application is deployed in the same region as a Redis dataset, the result in latency will be a mere few milliseconds.Large web applications, such as Facebook or Twitter, rely on job schedulers to perform a vast amount of background jobs. These range from heavy jobs (i.e. image processing) to light ones (i.e. sending emails). Applications these days require quality performance without the hassles of database setup and script writing. Fortunately, Redis provides the efficiency and level of performance these applications require, with an added motivation of simplicity. Moreover, using our Redis Cloud Platform, with a single click, a database can be instantly configured with Quartz.Quartz is an open source job scheduling and management library that can be integrated within any Java application. As the informal standard job management system in the Java world, Quartz has gained popularity, serving tens of thousands of applications. By utilizing Redis as persistent storage, Quartz users are granted the Redis advantages of efficiency, low latency, and simple integration with PaaS providers, such as Heroku.Redis JobStore was implemented for Quartz utilization, can now be downloaded from our Github repository, and simply added to a given Quartz project’s library. Redis JobStore utilizes sorted sets in which triggers and their respective trigger fire times are stored, making the retrieval of specific jobs and their execution quick and efficient. A sorted set value is provided with a score that is used for the job’s fire time, accelerating the entire process.An additional element used in Redis JobStore is global locking with Redis’ publish and subscribe mechanism (Pub/Sub), in which JobStore listens to an ‘unlock’ message.Download the Quartz Scheduler JobStore that uses Redis for persistent storage.Redis JobStore supports multiple schedulers with two handy mechanisms. One releases the locked triggers of a previously run scheduler, and the second monitors live schedulers. The live schedulers’ IDs are stored in Redis, and inactive scheduler triggers are released after a given amount of time.To better understand the workflow and the behavior of a Quartz Scheduler using Redis JobStore, I invite you to review the Redis schema that it uses.Check out the readme or contact me to learn more about how to get started, as well as current limitations and issues. This is a good starting point and we plan on continuing to enhance this package based on community requests."
191,https://redis.com/blog/a-success-story-worth-sharing-glide-me-and-redis-labs/,A Success Story Worth Sharing: Glide.me and Redis Labs,"May 5, 2014",Itamar Haber,"I would like to take this opportunity to share a story that wholeheartedly exemplifies the nature of Redis’ dedication to our customers. After a reminiscent conversation with one of our longtime clients, Max Rabin, the Head of Server Development at Glide, we put together a case study that suitably represents the success of our work together.>>> download the case study here <<<Glide is the world’s first mobile video instant messenger. Utilizing a massive real-time network, their users are able to send text and video messages via live stream, whilst granting recipients the flexibility to read or watch messages in real-time or at a later convenience. Redis’ platform enabled Glide with multiple layers of support to ensure that the service not only remains up and running, but also provides the utmost in customer satisfaction. That being said, I welcome you to read the full story, to get a behind the scenes look at what Redis accomplished for Glide."
192,https://redis.com/blog/now-generally-available-on-pivotal-web-services/,Now Generally Available on Pivotal Web Services,"May 7, 2014",Itamar Haber,"I’m delighted to announce that, starting today, our Redis Cloud and Memcached Cloud services are generally available from the Pivotal Web Services Marketplace. Developers who are using Pivotal Web Services to run and scale their applications on top of the open source Cloud Foundry PaaS can immediately boost performance by adding our hosted and fully-managed cloud services. In addition to providing top performance for applications, our cloud services are fault-tolerant and can scale up (or down) without interruption to the service.Redis Cloud and Memcached Cloud Marketplace pagesWe offer our services through a wide range of plans, from our free 25MB starter to a 50GB whopper, to suit every requirement and cater to every need. This documentation includes all the details you need to add and bind our services to your application from the command line, or from Ruby, Java, Python, PHP and Node.js. By letting us do your Redis and Memcached heavy lifting, you can focus on developing your applications.Questions? Feedback? Email or tweet me – I’m highly available 🙂"
193,https://redis.com/blog/tldr-rapid-14-q1-growth-23k-customers-45k-instances-4-clouds-and-10-regions/,"TL;DR Rapid `14 Q1 Growth, 23K+ Customers, 45K+ Instances, 4 Clouds (and 10 Regions)","May 28, 2014",chen.w,"We’ve just released our formal announcement about Redis’ Q1 results and the numbers speak for themselves. This is first and foremost solid proof of Redis’ amazing popularity and the central role that it has taken in the developer community. We also want to use this opportunity to say a big “Thank You” as a team to the entire Redis community, and especially to our users from all internets and clouds – you are the reason we’re here.That’s it for now – we’re going back to work. Lots of interesting developments happening in our labs that we want to get out before this quarter is over. You may have already noticed subtle changes here and there but there are bigger things yet to come. Stay tuned."
194,https://redis.com/blog/my-schedule-for-the-upcoming-2014-cloud-foundry-summit/,My Schedule for the Upcoming 2014 Cloud Foundry Summit,"May 28, 2014",Yiftach Shoolman,"In less than two weeks, Pivotal is holding its annual Cloud Foundry summit in San Francisco and our team is gearing up for the event. Cloud Foundry has been and will continue to be a pivotal element in our company’s vision, and we’re already using it extensively with Pivotal Web Services.I’m looking forward to travelling there to participate in the event, and have prepared a list of sessions I hope to attend (and that I’d recommend for anyone else who’s going):When not in the sessions, I’ll be in meetings or at our booth. I don’t want to spoil the surprise, but we’ve also prepared a new demo that is I feel is really impressive – I recommend that you come by the booth to check it out. If you want to book a meeting with me in advance, feel free to email me directly at yiftach@redis.com."
195,https://redis.com/blog/join-redis-labs-at-the-cloud-foundry-summit-in-san-francisco/,Join Redis Labs at the Cloud Foundry Summit in San Francisco!,"June 5, 2014",Leena Joshi,"The Redis team joins Cloud Foundry to celebrate cloud operations and open source on June 9th, 10th, and 11th!Redis is a proud sponsor and exhibitor at the upcoming Cloud Foundry Summit, a premier event for developers and cloud operators. Join us, core contributors to Cloud Foundry, and real world users for three days to discuss deep technical topics, the engineering roadmap, the community ecosystem and operational best practices. As you would expect, we at Redis have several exciting activities planned throughout the event:Get your #RedisGeek shirtDrop by and pick up one of our famous Redis Geek t-shirts! Meet the Redis experts and staff members behind our popular fully managed Pivotal Web Services plugins, Redis Cloud and Memcached Cloud. We also have a few surprises in store for real Redis users, so drop by to find out more!Featured Redis Session with Yiftach ShoolmanListen to Redis expert and Redis Co-Founder and CTO during his featured track on “How to Build a High Performance Application Using Cloud Foundry and Redis.” In this session, you will learn:Feel free to contact me via email or twitter for questions or to book meetings with us in advance."
196,https://redis.com/blog/how-pixlee-uses-redis-labs-to-serve-top-brands/,How Pixlee Uses Redis Labs to Serve Top Brands,"June 5, 2014",Itamar Haber,"Customer satisfaction is Redis’ highest priority. As such, it is with great pleasure that I share another success story, this time with one of our longstanding customers, Pixlee. A recent conversation with Pixlee’s co-founder, Jeff Chen, prompted the idea to compile exactly how Redis has been of service to the ever-growing visual content marketing platform.>>> download the case study here <<<Pixlee’s Personalized Visual Marketing SaaS platform sits at the intersection of photos and data, helping brands navigate the user-generated world of today. With the changes happening in consumer behavior, the rise of the visual web, the massive volume of social sharing, and increased sophistication in customer data, brands now have the opportunity to use the authentic voice of their customers to market and optimize content in real-time. Utilizing both Redis and Memcached, Pixlee’s case required top-notch hosting and monitoring support they could rely on. Accordingly, Redis has provided Pixlee with the utmost in enterprise-class Redis and Memcached solutions, along with Redis’ highly-resilient multi-zone support. That being said, I welcome you to read the full story, to have an in-depth look at what Pixlee has accomplished with Redis."
197,https://redis.com/blog/because-what-the-world-needs-is-another-memcached-cli/,Because What The World Needs is Another Memcached CLI,"June 12, 2014",Itamar Haber,"Today I’m proud to present my latest contribution to the open source community – bmemcached-cli. It is a simple Python wrapper around python-binary-memcached that provides an easy way to interact with a Memcached bucket via a command line interface. What makes it (arguably) unique is that it supports the Simple Security and Authentication Layer (SASL).Here at Redis we are quite focused on providing the best cloud-hosted Redis service. We do, however, also offer a similar service for Memcached that’s actually built on top of our Redis technology. Memcached, despite being comparatively less robust than Redis, is still a popular choice for the caching layer, and a significant number of our users are using it. In fact, there are use cases where  Memcached is preferable to Redis (gasp!).Like Redis, Memcached uses a plaintext protocol for client-server communication (that’s why you can use telnet to connect to your server). Unlike Redis, Memcached can also be configured with SASL authentication that, when used, switches communication to a binary protocol (BTW, if you’re looking for an authenticated and encrypted channel for your Redis, we also offer SSL for it). Memcached’s SASL authentication is primarily used to protect a bucket against unauthorized access. Additionally, one might argue that using SASL also makes it harder for eavesdroppers to intercept the traffic and that it helps in reducing the bandwidth.Our Memcached Cloud service allows its users to configure their buckets with SASL effortlessly and with no interruption to their service. All you need to do to enable SASL is edit your bucket’s properties, tick the appropriate check box and supply a username and password. Once you’ve added SASL to your Memcached resource, you can no longer use plaintext to connect to it. This makes sense from a security perspective, but it makes debugging the contents of your cache more cumbersome. In fact, after searching the internets high and low, I couldn’t find even one CLI that supports Memcached’s binary protocol. So… I rolled up my sleeves and wrote one.>>> go to bmemcached-cli’s repository on github <<<Ok, I admit I didn’t have to write it from scratch. I stood on the shoulders of the giant Andrew W. Gross and shamelessly hacked his labor of love to suit my needs. It was a short and fun project – I hope you find it useful. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
198,https://redis.com/blog/7-recommended-mongodb-world-sessions/,7 Recommended MongoDB World Sessions,"June 19, 2014",Itamar Haber,"Next week the Redis team is attending MongoDB World. We’re planning to spend most of our the time at the booth (#110) and we would love it if you’d stop over! We have #RedisGeek tees and stickers to give out, a drone you can win, and of course talking shop about Redis is also an option 😛As for myself, time permitting, I’ll be using this opportunity to obtain some quality education about MongoDB. I’m the first to admit that my MongoDB fu is sub-par, so I’ve prepared a list of “must attend” sessions that I plan to check out. Here are my recommendations.Firstly, to truly grok any DMBS, you need to understand how it reads and writes data. That’s why I’ve marked down these two cuties:Next, since MongoDB is disk-based, if you want it running at top speed you need to know how it uses RAM and the best approaches to make it performant, ergo:The last two sessions I’ve picked address MongoDB’s use with other interest areas of mine, namely the Cloud and Big Data. These sessions are:So there you have it – my plans for the event (assuming life doesn’t get in the way too much). If you want to book a meeting slot, feel free to contact Cameron (via email or twitter) or myself. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
199,https://redis.com/blog/scopely-gets-the-high-scores-with-redis-labs/,Scopely Gets the High Scores with Redis Labs,"June 24, 2014",Itamar Haber,"It is with great pleasure that I share Redis’ successes regarding customer care and performance. This case study, in particular, epitomizes the need for clear communication and instant response times, especially in the world of mobile gaming. Scopely, a very special customer of ours, is a successful gaming company whose journey to the top exposes the real challenges encountered by gaming companies across the globe, as well as how Redis aided in turning those challenges into distant memories.Scopely is a rapidly growing mobile gaming company that is trailblazing a new publisher model. In just three years, Scopely has more than quadrupled in size and launched an unprecedented six consecutive Top-5 games in the Apple and Google App Stores. Their platform has been designed to build internal games, and handle in-app purchases, communication flows, monetization, in-app currency, virtual economy, content handling, friend networks, and the infrastructure tools necessary to support a game.>>> download the case study here <<<It is no coincidence that Scopely turned to Redis for all of their Redis needs. Redis’ numerous developments and high level of experience enable them to administer the best possible Redis performance. With their best practices prioritizing responsiveness and support, regardless of the circumstances, Scopely quickly learned that they were in safe hands with Redis. That being said, I welcome you to read the full story, and take a deeper look at what Scopely has accomplished with Redis."
200,https://redis.com/blog/top-redis-headaches-for-devops-replication-buffer/,Top Redis Headaches for Devops – Replication Buffer,"June 26, 2014",Yaron Dolev,"Redis provides a wide variety of tools directed at improving and maintaining efficient in-memory database usage. While its unique data types and commands fine-tune databases to serve application requests without any additional processing at the application level, misconfiguration, or rather, using out-of-the-box configuration, can (and does) lead to operational challenges and performance issues. Despite the setbacks that have been the cause of quite a few headaches, solutions do exist, and may be even simpler than anticipated.This series of installments will highlight some of the most irritating issues that come up when using Redis, along with tips on how to solve them. They are based on our real-life experience of running thousands of Redis database instances.Replication buffers are memory buffers that hold data while a slave Redis server synchronizes with the master server. In a full master-slave synchronization, changes performed to the data during the initial phase of the synchronization are held in the replication buffer by the master server. After the completion of the initial phase, the contents of the buffer are sent to the slave. There is a limitation to the size of the buffer that can be used in this procedure, causing replication to start from the beginning when the maximum is reached, as mentioned in our post on endless Redis replication loops. In order to prevent this from happening, an initial configuration of the buffer needs to take place according to the amount and types of changes expected to be made during the replication process. For example, a low volume of changes and/or smaller data in the changes can get by with a smaller buffer, whereas if there are a lot of changes and/or the changes are big, a large buffer is needed. A more comprehensive solution entails setting the buffers to a very high level to offset the possibility of a lengthy or heavy replication process that will eventually exhaust the buffer (if the latter is too small). Ultimately, this solution requires fine-tuning the specific database at hand.Redis Default Setting:As explained here, this default configuration replication link will be broken (causing the synchronization to start from the beginning) once the 256MB hard limit is reached, or if a soft limit of 67MB is reached and held for a continuous 60 seconds. In many cases, especially with a high ‘write’ load and insufficient bandwidth to the slave server, the replication process will never finish. This can lead to an infinite loop situation where the master Redis is constantly forking and snapshotting the entire dataset to disk, which can cause up to triple the amount of extra memory to be used together with a high rate of I/O operations. Additionally, this infinite loop situation results in the slave never being able to catch up and fully synchronize with the master Redis server.A simple solution that offers an immediate improvement results from increasing the size of the output slave buffer by setting both the hard and soft limits to 512MB:As with many reconfigurations, it is important to understand that:That brings us to the end of our first installment of the top Redis operational headaches. As pointed out above, in terms of replication buffer limits, proper configuration can go a long way. Be sure to keep an eye out for the next post in this compilation covering replication timeouts and how to handle them accordingly."
201,https://redis.com/blog/whatcha-doin-aws-summit-2014-new-york/,Whatcha Doin’? AWS Summit 2014 New York,"June 30, 2014",Itamar Haber,"There’s 104 days of summer vacation and in our industry there’s no shortage of ways to spend them. Last week we were at MongoDB World, and next week it’s going to be all about the Amazon Web Services Summit in NYC for the Redis gang. We’ll be there to meet with you and discuss application development, the cloud and, of course, Redis.In the spirit of what’s quickly becoming a tradition, I’ve compiled a list of the sessions I plan on attending:Of course, the team and myself will also be manning our Redis booth (someone needs to give away all that awesome shwag, right?). If you want to book a meeting slot, feel free to contact Cameron (via email or twitter) or myself. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
202,https://redis.com/blog/red-is-beautiful-a-visualization-of-redis-commands/,Red is Beautiful: A Visualization of Redis Commands,"July 14, 2014",Itamar Haber,"Sometimes inspiration strikes and you feel compeled to follow it. That’s exactly what had happened to me when I was reviewing Redis’ documentation repository – while browsing through the commands.json file, I felt an inexplicable urge to haul it into D3.js. The result is a web toy, an eye candy that serves no practical purpose apart from being that which it is. The code of this experiment was adopted from Mike Bostock’s example.Redis’ 160 or so commands are grouped by topic and zooming in/out is by clicking the mouse. Each command is represented by circle, whose size roughly corresponds to the command’s asymptotic time complexity. Clicking a command will open its documentation page from redis.io. I hope you’ll enjoy it and do feel free to email or tweet me with any feedback – I’m highly available 🙂"
203,https://redis.com/blog/new-in-memtier_benchmark-pseudo-random-data-gaussian-access-pattern-and-range-manipulation/,"New in memtier_benchmark: Pseudo-Random Data, Gaussian Access Pattern and Range Manipulation","July 8, 2014",Oran Agra,"Last year we open-sourced memtier_benchmark, a high-throughput benchmarking tool for Redis and Memcached resources. At Redis, we use this tool on a daily basis, and those of you who gave it a shot came back to us with great feedback. Based on these suggestions, we made several improvements to the project that I wanted to share with you today.The first addition to our benchmarking tool is the generation of randomized data according to known size range. This is achieved by setting the new –data-size-pattern switch to the value ‘S’ (Sequential) and specifying its range with the –data-size-range switch. The following example will yield a benchmark keyspace whose values evenly range from 4 to 204 bytes values:In the example above, we’ve used the -–random-data switch to generate random data as well as the –key-minimum and –key-maximum switches to control the range of key name IDs, yielding a total of 200 keys. The first key, memtier-200, will hold 4 bytes of data, the next will have 5 bytes and so forth until the last key, memtier-400, which will store 204 bytes.The next addition is the ability to use Gaussian (a.k.a normal) distribution to access the data. Before we made this change, you could specify a uniform random or sequential distribution for the benchmark’s access pattern. But to better mimic real-life use cases, this new option lets you conform the memtier_benchmark key access pattern to the familiar Gaussian distribution’s bell curve. When used, you can also control and set the standard deviation and median that the distribution will follow. For example, invoking the tool with the following arguments:will result in most read/write accesses being centered on the 100th (memtier-300) key.Lastly, we’ve added the ability to use the SETRANGE and GETRANGE Redis commands in place of SET and GET. This allows you to construct benchmarks that use significantly less network traffic while still using larger data sizes. For example, you can have key values of 1MB but only read and write the last byte with the following arguments:I hope you’ll find these new additions useful – if you want to share your memtier_benchmark input or war stories, feel free to do so directly in the project’s GitHub repo."
204,https://redis.com/blog/redis-labs-performance-testing-with-live-traffic/,Redis Labs Performance Testing With Live Traffic,"July 16, 2014",Tung Nguyen,"Originally published on bleacherreport.comOur API service uses Redis as it’s caching layer. The Redis set operations are extremely useful for the logic required in our API service. Unfortunately, we have had some issues scaling Redis. Redis is a single-threaded server that reads incoming connections using an event-based paradigm, thus uses only 1 CPU core. So you can scale Redis vertically but it is very challenging to scale Redis horizontally. We see this when our app calls heavier zset and zunionstore operations. Redis hits the CPU limit and it gets pegged at 100%.We’ve also seen issues with the way Redis snapshots the data in memory to disk. Redis forks off another process to save the data and that process doubles the RAM footprint of the Redis. Once Redis runs out of memory on the box it gets painfully slow. This effectively reduces the memory you can use on the box to half of the total actually available. In our case we use Redis as a cache, so we’ve simply disabled saves so we could use all the memory on the box and we’ve been veritically scaling. However, vertically scaling will only get you so far. Our current bottleneck is that we’re approaching the upper bound of available CPU.We wanted try out Redis clustering, but the open source version is a still work in progress. Redis, one of the top contributers to the open source Redis project, already has clustering working on their platform. Redis is engineered to scale up automatically and seamlessly. The Redis db will grow in capacity on a single shard and, when it needs to, automatically start to cluster across multiple shards. All of this happens behind the scenes transparently. Customers need only manage a simple single endpoint.Beyond basic sanity tests and functional testing, we required quantitative measures and performance metrics to legitimize our move.We used em-proxy to blue-green traffic in migrating our servers to a new cluster. This allows us to proxy a portion of traffic to Redis, testing it, and reacting to changes with contained exposure.After a few google searches that night, I ran into it, and then I did some testing that same night with a single instance. Then on the following night, I created an additional test cluster, which took about 5 minutes, and then directed ALL of traffic from live production cluster to the test cluster and let it run through the night and following day. The powerful thing about this technique is that we are not guessing with input benchmark data, instead we’re actually using live production traffic. On top of that, we can compare the New Relic data sets against each other in real time.The results were incredible.The left graph is based on Redis servers that we maintain in-house. On the right is the test cluster connected to Redis. The only difference between the clusters is that one uses open source Redis server and the other one uses Redis. With the standard open source Redis server we see intermittent spikes in the response times up to 700ms and averaged 435ms; it’s all over the place. Redis brings our response times to a consistent and smooth average of 55ms! Whatever voodoo blackmagic Redis is conjuring up, it’s obviously working.During our testing we found that Redis disables some Redis commands, thus were required to make some changes to our API code.For instance, Redis disables the object command.The next error is due to the fact that Redis was sharding, yet our app was not equipped to handle it.To resolve this issue you need to name Redis keys such that Redis can manage the data shards. Keys need to be named with curly braces like so hello{dynamic}world. So the following keys map to separate shards:It is explained in more detailed here: https://redis.com/kb/redis-cloud-cluster. We ended up deploying to a single shard, which is apples-to-apples with our prior environment, and will circle back to sharding at a later time.This method of testing is extremely powerful. We found a lot of issues that we were able to fix before we actually rolled out Redis on production. Had we instead moved all traffic to Redis, we would have been in a fire fighting situation and probably would have rolled back, not realizing the dramatic performance opportunities Redis offers.Redis is fast! If you’ve read closely, you’ve probably realized the kicker. We’re getting this performance increase from Redis without even sharding. We still have more follow up work to do, including sharding, which should improve our performance even more dramatically. If you are using Redis and are having issues scaling it beyond a single CPU or are getting memory bounded, Redis is a great option."
205,https://redis.com/blog/top-redis-headaches-for-devops-replication-timeouts/,Top Redis Headaches for Devops – Replication Timeouts,"July 21, 2014",Yaron Dolev,"Redis provides a wide variety of tools directed at improving and maintaining efficient in-memory database usage. While its unique data types and commands fine-tune databases to serve application requests without any additional processing at the application level, misconfiguration, or rather, using out-of-the-box configuration, can (and does) lead to operational challenges and performance issues. Despite the setbacks that have been the cause of quite a few headaches, solutions do exist, and may be even simpler than anticipated. This series of installments will highlight some of the most irritating issues that come up when using Redis, along with tips on how to solve them. They are based on our real-life experience of running thousands of Redis database instances.Following our previous installment in the series, the Replication Buffer, the next headache on our list will carry on with the topic of master-slave replication. In particular, we will look a bit deeper at the length of time needed to complete the process as well as some configuration issues that can cause major inconveniences.As we’ve previously discussed in the Endless Replication Loop post, Redis’ replication process is made up of two synchronization stages: initial and ongoing. While the ongoing stage is fairly stable (as long as the link between the master and slave is kept), the initial phase is somewhat trickier to complete. Successful completion of the initial synchronization is dependent not only on the amount of memory that’s allocated for the replication buffer (see previous headache) but also on the amount of time that this step takes.You may recall that the initial synchronization step consists of a background save and the transmission of the entire database from the master to the slave. Depending on the dataset’s size and the quality of the network connection, this may prove to be a lengthy process to complete. If that phase takes too long, Redis’ replication timeout setting may be reached, thus causing the initial phase to be repeated over and over, ad nauseam. In such cases, you will find your slave’s Redis log riddled with messages such as:Redis’ replication timeout is set to 60 seconds by default (see the repl-timeout directive in your redis.conf file or do a config get repl-timeout using redis-cli). This period of time may be far too short, especially when you have:You can rectify this by setting the replication timeout to a more appropriate value. Start by working on an acceptable estimate of the time needed to replicate the database. First, check how long it takes Redis to perform a background save by executing the BGSAVE command and examining the log file for the relevant lines (i.e. * Background saving started by pid nnn * indicates that the process started, whereas * Background saving terminated with success * indicates its termination). Next, time how long it takes you to copy the resulting RDB file from the master to the slave’s disk. Lastly, you’ll need to time how long it takes to actually load the data from disk (e.g. by restarting Redis and looking for the * DB loaded from disk line in the log file). The sum of these measurements can serve as a rough estimate of your desired replication timeout value, but you’d probably want to add 10-20% to it for safety.Once you’ve set up the timeout based on the estimate, you can test how long replication actually takes by having the slave do a full synchronization a few times and examining the log file. If possible, try repeating this exercise at different times throughout the day to better gauge the system’s behavior under different loads. Lastly, keep in mind that the timeout setting’s value should be reviewed periodically based on your database’s growth.This concludes our review of Redis’ replication headaches. Replication is a powerful tool for keeping your database available and scaling its read throughput, but mind the default settings and make sure you’ve configured the database to your use case.If you’ve finished reading this article and want to dive into the next common cause for Devops’ headaches then continue reading about the client buffers."
206,https://redis.com/blog/managing-50k-redis-databases-over-4-public-clouds-with-a-tiny-devops-team/,Managing 50K+ Redis Databases Over 4 Public Clouds with a Tiny Devops Team,"July 31, 2014",Yiftach Shoolman,"Modern applications have a general list of needs in order to not only survive, but thrive in today’s fast paced cloud environment. These include low response times (less than 100 milliseconds), limitless scalability, high availability, and optimal performance, to name a few. With a selection of modern database options available, Redis has proven to be one of the most popular. Redis has aided in the creation of over 50,000 databases by over 2,500 paying customers, with more than 100 new databases created daily. Being a major contributor to Redis’ open source project, many of the use cases that we see using Redis include social applications, online advertising companies, and gaming companies. Our experience running Redis services across the four major clouds (AWS, Azure, GCP and SoftLayer) has made us aware of a number of challenges that users encounter, which consequently led to our thoroughly tested solutions, a few of which we have shared below.While Redis is very fast, with the ability to respond to requests in less than 1 millisecond, running it in the cloud can cause performance to degrade significantly. However, with Redis, all operations are performed behind the scenes, incorporating as many Redis instances as possible into a cluster to enable pure multi-tenancy architecture without degraded performance. Redis databases on our platform use master-slave replication, where the master is located on one node while the slave is located on another, with as many instances as possible on every node. Additionally, the cluster is built around an odd number of nodes in order to have a quorum in case of a failure. Our zero-latency proxy hides everything from the user’s perspective, so that users only see a single-end point, with the ability to add proxy to receive more throughput, without visibility of shards, clusters or nodes.When we began our journey with Redis, our primary challenge was understanding which data center would be optimal for each application. It is important that every Redis database be run on the same data center as its respective application so as to avoid network latency. Data centers are selected by users when they create an instance inside a region, however, an issue arises when selecting a zone or data center from AWS, because they are mapped out differently between accounts. For example, Redis’ ‘us-east-1a’ can be signified as ‘us-east-1c’ for other users. While these are completely different data centers, AWS set up this method to ensure stable load balancing for its internal architecture. Otherwise, since most users have no preference regarding a specific data center, most will choose the first one, thus creating an unbalanced level of demand. To deliver Redis’ multi-cloud, multi-region service from the closest location possible to the user’s application, we developed a code that performs mapping between our zones and those of our users. Applying that to our example, we found that the code for ‘us-east-1a’ matched that of ‘us-east-1c’. This tells us that when a user chooses to create a database in ‘us-east-1c’, it should be mapped to our ‘us-east-1a’ to ensure minimal network latency.Deciding which instance to select when creating a node can be confusing. Consequently, we decided that any type of instance can be used in Redis’ clusters. Though each instance type has a predefined set, there is no limitation to the range of sizes that can exist within a cluster, whether it be 30GB or 200GB. This kind of flexibility is key. We want to be able to cope with high memory usage as well as high CPU usage. Additionally, we want to run everything on dedicated infrastructure to avoid ‘noisy neighbors’ and be as cost effective as possible. Using large instances automatically provides dedicated instances by design. These large instances are then used in creating our own controlled multi-tenant infrastructure. In order to create a solid infrastructure for any architecture, it is advised to use specific instances across the clouds: c3 (for performance) and r3 (for memory) in AWS; a4, a5, a6, and a7 in Azure; the standard high memory and high CPU in GCP; base clusters over bare metal servers in SoftLayer with added virtual machines to scale out.With users who run 1 million operations per second, 50% of which being write requests, data persistence is extremely significant with Redis. The question is, how can this be performed over AWS’ EBS infrastructure? First of all, you need to understand the details behind the storage architecture of the cloud: local ephemeral storage is relatively fast and network attached storage, such as EBS, is persistent. The largest EBS volumes on AWS provide dedicated EBS disk storage (the same goes for GCP). Unfortunately, this is not enough to cope with Redis’ performance. As a result, we hybridized the two, using ephemeral for some storage needs, incorporated with EBS for persistence. Accordingly, we have fine tuned Redis to enhance its speed when accessing a disk, and use slaves to perform data persistence activities when using replication, freeing up the master.How is everything monitored? Zabbix is used to monitor nodes, and as far as monitoring database metrics goes, we searched for an open source project to no avail, which led us to build our own monitoring system – Limbic – that is based on Python, RRD and Redis. With this platform we are able to monitor 50,000 databases, each with 100 metrics, and keeping 10,000 time resolutions. In due time, it will be available on open source for everyone to enjoy.We are able to handle these complex infrastructures at the hand of our strong DevOps team that, while humble in size, is backed up by the devs who know the system inside and out, and can be dragged into resolving production issues in real time. We also use a ‘baby steps’ approach when moving to production. As a result, we always begin with manual configuration, then slowly make our way to automation. We’ve found that this practical approach always wins.Overall, Redis has taken the necessary steps to ensure quality Redis performance for our customers. Our solutions to the challenges mentioned above have provided the peace of mind our customers need in order to successfully focus on their own core capabilities. For more information, check out the full video that explores the challenges above as well as the corresponding slide show."
207,https://redis.com/blog/like-a-kid-in-a-candy-store-dataversitys-nosqlnow-2014/,Like a Kid in a Candy Store: Dataversity’s NoSQLNow! 2014,"August 5, 2014",Itamar Haber,"Here’s a shocking confession for you: I’m a data-phile. I am fascinated by data and everything that has to do with it. Even after having spent the last 25 years with, in, under and against all sorts of data management systems – I still can’t get enough of it. I’m thrilled whenever I play with a new database technology and I thrive when I get to dive into a dataset. Yes, I’m a datamaniac and a data junkie.I’m spilling my heart like this here because there’s a good chance that my addiction will get the better of me, and I’ll overdose soon. I’m talking, of course, about the upcoming NoSQLNow! event that’s two weeks away. This event is crammed with top notch sessions (see below for a partial list), with my personal highlight being the one where a customer presents his experience with Redis Cloud (see if you can spot it – and hurry, tickets are running out!).I’ve prepared a list of sessions that I want to attend there, and believe you me it wasn’t an easy task – everything looks so yummy but I’ve had to painfully limit myself to one treat at a time:The volume, variety and velocity (:)) of quality sessions in this conference is really overwhelming – clearly it’s a must-attend event for my kind. I hope to see you there (if you’re still on the fence and trying to justify the cost – use the invitation). Tung and the team will also be hanging by our Redis booth with our usual pledge to dress up Redis users, decorate your laptop with stickers, and raffle some prizes. Or talk shop. If you want to book a meeting slot, feel free to contact Cameron (via email or twitter) or myself. Questions? Feedback? Email or tweet me – I’m highly available 🙂Photo credit: Jixar"
208,https://redis.com/blog/5-tips-for-running-redis-over-aws/,5 Tips for Running Redis over AWS,"August 6, 2014",Itamar Haber,"Cloud computing can help host an application in a way that is both scalable and cost effective. The leading vendor in the infrastructure as a service (IaaS) arena is Amazon Web Services (AWS), which offers scalable, highly available and secure cloud hosting with Redis. If your application uses Redis for caching or data storage, below are a few tips which will help you save time, money, and achieve better performace with redis on aws with AWS Redis.When applicable, hardware virtual machine (HVM) enabled instances (e.g., second generation M3 instances and R3 memory-optimized instances) should be used to reduce latency issues due to fork times and in order to run multiple Redis servers on different cores.Non-HVM EC2 instances use the Xen hypervisor, which is much slower when it comes to forking large processes. Redis’ persistence options, RDB and AOF, both fork the main thread and launch background rewrite or save processes. When forking involves a large amount of memory, it can increase Redis’ latency significantly.Multiple cores on the same machine can also be used to deploy a number of Redis instances. In this set up, since each Redis instance manages a smaller dataset, per-database forking times can be further mitigated.Swapping is an operating system  mechanism that attempts to efficiently use RAM. However, seeing as Redis AWS is an in-memory database, in order to provide top performance, all data must be kept in RAM.With swapping enabled, Redis on aws may attempt to access memory pages that are on disk. This will cause Redis’ process to be blocked by the I/O operation of the disk, which can be a slow process since it involves random I/Os. By chaining Redis to disk performance, your are likely to experience a latency increase. To mitigate this, you can configure the following options in Linux in /etc/sysctl.conf:For Redis AWS, this means that when the memory is almost full, the kernel will try to steal pages from cache instead of swapping out of program to memory. Use these settings, along with Redis’ maxmemory and maxmemory-policy directives, to prevent Redis AWS from denying writes or the kernel’s out of memory manager (OOM) from terminating Redis.EC2 instances are equipped with ephemeral storage, with EBS as the de facto standard storage for persistent data in AWS. Keep in mind that EBS is network-attached storage. Therefore, if you are persisting data to the volumes, EBS competes with Redis over network bandwidth. There is a chance that extra I/O and limited bandwidth may affect Redis’ performance when using EBS. Nonetheless, this, too, can be reduced in a number of ways, including:If your data’s maximum size is less than 4GB per instance, by saving bits, you can also save money. When compiling and running Redis on a 32 bit target, it uses less memory per key, since the pointers are smaller.If you are unsure of the size of your dataset, use the INFO command to get the relevant numbers (e.g. used_memory). You can also use a benchmarking tool, such as redis-benchmark or memtier-benchmark, to generate random datasets for guesstimations.Remember that the size of your dataset is not the only consideration when planning the overall RAM for a Redis AWS server. For example, in case you have enabled persistence or replication, a background memory thread will periodically create a copy of the data, which will be in-memory at that point. Any data then remains in-memory until the operation is complete and therefore, any update operation on the data/key being copied requires extra memory in the form of memory pages and buffers. A “worst case” scenario, where all keys are updated during this period, may require twice or more memory than what would have been required without persistence and/or replication. To provision against running out of memory in such a scenario, use a factor of 2 at the very least on the actual dataset’s size.The final tip comes full circle, picking up from where tip #1 left off. Redis is a (mostly) single-threaded process, which means that each Redis process can use a single core, at the very most. To make better use of EC2 instances with multiple cores, you can start one Redis process for each core of a server and efficiently utilize the extra cores. These extra servers can act as independent Redis servers. For example, one can be utilized to manage user session data while another one can be used to cache user profiles. An alternative approach for harnessing these multiple cores is to shard the data between servers.Sharding is a big topic, in and of itself, and we’ll dedicate more time to it in the upcoming weeks. Prepare yourself for the drama associated with choosing between the application-side, client-assisted, by proxy or via cluster sharding. Learn the horrible truths about sharding for yourself and how to battle them bravely. Understand how to take Redis to the next step. Have questions about Redis on AWS? Feedback on Redis AWS? Email or tweet me – I’m highly available 🙂"
209,https://redis.com/blog/nosqlnow-2014-case-study-bleacher-report-on-redis-labs/,NoSQLNow! 2014 Case Study: Bleacher Report on Redis Labs,"August 6, 2014",Leena Joshi,"Don’t miss Tung Nguyen’s appearance on stage at NoSQL Now! 2014 event, on Tuesday, August 19th, 2014 between 12pm and 12:30pm.In this 30 min session you will learn how Bleacher Report used Redis to build a highly scalable, highly available, and top performing application with sub 60ms response times. Redis is an open source in memory NoSQL database best known for its sub-millisecond latency.In this session you will learn:"
210,https://redis.com/blog/3-more-common-uses-for-redis-in-the-gaming-industry/,3 (More) Common Uses for Redis in the Gaming Industry,"August 14, 2014",Itamar Haber,"At the Game Developers Conference (GDC) four months back, I had great discussions with game developers that use Redis – here’re my takes from that event. And this week, I was pleased to check out GDC Europe and at Gamescom. While GDC’s European version is significantly smaller than its American equivalent, it was still a lot of fun meeting and conversing with developers. Gamescom is, well, gamescom (see photo of yours truly).Interestingly, and similarly to my previous GDC experience, I found out about more common use cases for Redis. Leaderboards, sessions and profiles are naturally still the top functions for which game developers use Redis, but a few more kept popping up insistingly…There are all kinds of games, and some of them use rely on data almost as extensively as they do on their code and media. Online, social and mobile games are perhaps most known for stringent latency and performance requirements when it comes to data management, and Redis is heavily used to fulfill these. One gaming developer I talked with uses Redis to cache Javascript snippets, which make up the entire game. I also heard that a well-known online gaming brand is using Redis to keep every real-time statistic and aggregate that it collects from each of its players in real time. I learned that another gaming service captures real-time in-game events using Redis – its raw event stream is used for dashboards and powering interactive customized campaigns, on top of being consumed by downstream processes for deeper analytics and long-term storage.Whenever your free-but-ad-supported game presents you with a 15-second trailer, you can be almost certain that there’s a Redis server involved (and probably more than one). This “fact” is hardly surprising considering the volume of requests and tight latency budgets ad networks are working with/against. An ad server is all about speed, and Redis fits that bill perfectly. Counting impressions? Check. What about clicks? Of course. Need your campaign metrics snappy? Done. Want to keep your viewers’ segments handy? No problem. And these are just the basics of ad serving. More often than not, once Redis is a part of the ad network’s stack, it is put to extensive use to support the unique focus and strengths of each network.Redis can be extended via Lua scripts or, as an open source project, forked and modified freely. This flexibility and openness appeals to a lot of developers, but perhaps even more so to the creative members of the entertainment industry. Some developers I met had implemented the entire player matching mechanism of their multi-player online game using a clever combination of Lua scripts, sorted sets and lists. I found out another game’s servers are using a modified fork that includes “optimizations to almost all commands to better fit our use cases” (stay tuned for more on that – I’m hoping to get more details). I also got a tip about a persistence to Big Data store patch that is supposed to be open sourced in the near future.It is really amazing to see Redis all over the place, but personally I think the fact it is used so extensively with games is awesome. I really need to get into my Bowser costume now, but feel free to email or tweet me – I’m highly available 🙂"
211,https://redis.com/blog/the-1-2m-opssec-redis-cloud-cluster-single-server-unbenchmark/,The 1.2M Ops/Sec Redis Cloud Cluster Single Server Unbenchmark,"September 4, 2014",Itamar Haber,"While catching up with the world the other day, I read through the High Scalability guest post by  Anshu and Rajkumar’s from Aerospike  (great job btw). I really enjoyed the entire piece and was impressed by the heavy tweaking that they did to their EC2 instance to get to the 1M mark, but I kept wondering – how would Redis do?I could have done a full-blown benchmark. But doing a full-blown benchmark is a time- and resource-consuming ordeal. And that’s without taking into account the initial difficulties of comparing apples, oranges and other sorts of fruits. A real benchmark is a trap, for it is no more than an effort deemed from inception to be backlogged. But I wanted an answer, and I wanted it quick, so I was willing to make a few sacrifices to get it. That meant doing the next best thing – an unbenchmark.An unbenchmark is, by (my very own) definition, nothing like a benchmark (hence the name). In it, you cut every corner and relax every assumption to get a quick ‘n dirty ballpark figure. Leaning heavily on the expertise of the guys in our labs, we measured the performance of our Redis Cloud software without any further optimizations. We ran our unbenchmark with the following setup:We didn’t have the time to set up a VPC and tune the placement groups for optimal performance, so we ran the entire thing in our standard service environment – i.e. on a noisy, crowded EC2 network. We also didn’t tune the CPU behavior or the number of threads or the shards’ configuration for this experiment – we let Redis Cloud use its defaults. We didn’t test multiple network configurations or add additional Elastic Network Interfaces (ENI). We simply took a freshly provisioned Redis Cloud server on HVM and did the unbenchmark against it…You can find the raw output from our run in this gist, but the bottom line is that it scored a little over 1.2 million TPS (1228432 to be exact). Naturally, this amazing result really whetted my appetite and I immediately asked for a full-blown, all-optimizations-included, exhaustingly-exhaustive, fruit-and-vegetable-inclusive benchmark to really see how much we can push the limits with Redis! And guess what? It’s in the backlog 🙂1 Some notes about sharding and Redis Cloud clusters (as promised):By design, the Redis server is (mostly) a single-threaded process. That being the case, sharding is usually employed to scale a Redis database beyond the boundaries of a single CPU core or the RAM capacity of a single server. There are three generally-accepted approaches for implementing sharding: client-side, proxy or clustering. Since client-side and proxy-based solutions for sharding Redis are easier to implement independently of the actual underlying database engine, these (e.g., Redis-rb and nutcracker) have been around for quite some time now. There are, however, only a few Redis cluster solutions today.A sharded Redis cluster means a bunch of Redis servers (processes) deployed over one or more compute nodes across a network. The cluster runs Redis databases, each potentially spanning a number nodes and multiple cores, over an aggregated amount of RAM. A production-grade cluster should also meet challenges such as ensuring the database’s availability, as well as its performance and management of infrastructure and database resources.The best known implementation of a Redis cluster is, naturally, that of the open source’s. Redis cluster (v3) is already in advanced stages of beta and it is expected to be production-ready within months. This upcoming version will provide excellent answers to many of the challenges I mentioned. Among its many new features, the new OSS version also includes the ability to create sharded clusters. Speaking on behalf of the entire Redis community (my apologies if I have offended anyone by presuming :)), we expect Redis version 3 to be a major new version in every respect!Besides the open source v3, there a few other Redis clusters already out there. Some folks went ahead and built their own clusters, each for her or his own reasons. I don’t want to namedrop anyone (likeTwitter, Flickr or Pinterest) but one company that has built a cluster is Redis. Our Redis Cloud service is powered by our own implementation of a Redis cluster and it has been in use in production for the better part of the last two years. During that period, we’ve been building and operating our clusters across multiple clouds and data regions. As a startup company, we’ve had to build our systems to scale both well and economically in order to accommodate the spectacular success that our commercial public service is having.Redis is a contributor to the open source Redis project – most of our staff are amazing developers that live and breath Redis – but our users needed solutions that weren’t ready within the open source’s scope. In order to meet these business challenges, we’ve developed solutions that allow us to scale Redis databases on the fly from megabytes to terabytes. We deploy, scale and manage clusters over four different IaaS providers and across ~20 data centers. Our users have created tens of thousands of databases, and for each database we’ve maintained not only availability and performance, but also taken care of the operational and administrative tasks (all with a tiny devops team).To use our Redis Cloud clusters, just sign up to our service and create a database. You can set up a sharded database in seconds (although sharding isn’t included in our free tier). Redis Cloud instances can be sharded using the standard policy, i.e. per the open source Redis Cluster Specification, or with a powerful custom sharding policy that’s entirely configurable via a list of regular expression rules (see the feature’s help page and future updates for more information).As long as I’m on the subject, here’s one of the less-known facts about Redis’ clusters: you don’t have to change anything in your application to start using them. Yep, you can use your existing code and client library (whether cluster- and sharding-aware or not) and you’ll still get all the scalability, availability and operational benefits that the cluster offers. Our users need only create the database and configure its options (availability, data persistence, sharding, security and what not) and bazinga! They can just use the single Redis URL (hostname and port) that is the gateway to their database in a Redis Cluster. Sure, there are tweaks, best practices, optimizations, tips, tricks and a gazillion other things you could do on top, but (as shown in the unbenchmark), our cluster is a quite a performer even without them.I had meant to keep these notes brief, but clustering is a rich and favorite topic of mine and I could go on and on for days (actually I promise that I will – more related announcements/technical/benchmarks/specs/comparisons to follow). To keep this a medium-sized post, I’d like to end here and invite you to follow us and shout at me – I’m highly available 🙂This post was originally published at HighScalability.com on August 27th, 2014."
212,https://redis.com/blog/aws-summit-tel-aviv-2014/,AWS Summit Tel Aviv 2014,"September 10, 2014",Itamar Haber,"Exactly one week from today, the travelling AWS Summit will reach sunny and humid Tel Aviv for a day. The team and I have already been to a couple of these, and one thing I’ve noticed is that while the Summit’s venue changes, its program remains practically static. Whether in Berlin, New York or Tel Aviv, the AWS Summit delivers lots of relevant information in a repeatable and respected manner. It’s no secret that I (and a lot of other folks) admire Amazon’s ability to scale operations, and that admiration definitely extends to the company’s events.Redis is proud to sponsor yet another AWS Summit, and we welcome you to visit our booth and learn how to supercharge your app with blazing fast Redis! If you’re planning to attend, just make sure that you’ve registered there (for free) to secure your spot.As a veteran of the AWS Summit, I’ve already attended most of the sessions so I’ll can recommend wholeheartedly that you listen to How and When to Use NoSQL Databases as well as Maximizing EC2 and Elastic Block Store Disk Performance (Wednesday, September 17th, 2014 at 1:30pm and 2:20pm, respectively). If this is your first AWS Summit or even if you attended last year, I’m sure you’ll find plenty of new and interesting sessions in the agenda, and of course the keynote by Dr. Vogel is a must.Another must-attend activity AWS is organizing is its “Bar Takeover” days. The idea is simple and effective: rent a comfortable place, bring interesting speakers, provide lubrication in the form of snacks and beverages, add some giveaways and BAM! You’ve instantly got the hottest techie gathering in town 🙂 I like this format even better than the “formal” Summit, and not only because there’s free food and drinks. Informal bar takeovers are much better for networking and unwinding, and from my experience, the connections I make and the information I pick up in these more social events are invaluable.If you feel the same, then I’d like to invite you to a free AWS Partner presentation –  How to Build A Top Mobile App using AWS and Redis  – on Tuesday, September 16th at 4pm at Casa Veranda, 32 Rothschild Blvd. This presentation will be given by one of our Redis Cloud customers, Max Rabin from Glide.me, and I can assure you based on my behind-the-scenes knowledge that the story he has to tell is fascinating! (Just remember to register separately for our bar takeover event).Questions? Feedback? Shout at me – I’m highly available 🙂"
213,https://redis.com/blog/so-youre-looking-for-the-redis-gui/,"So, You’re Looking for the Redis GUI?","September 11, 2014",Itamar Haber,
214,https://redis.com/blog/oracle-openworld-2014/,Oracle OpenWorld 2014,"September 24, 2014",Leena Joshi,"The annual Oracle OpenWorld event is taking place next week in San Francisco. From past experience and judging by the amount of buzz around it, OpenWorld is a definite must-attend event for anyone who’s interested in data management. Traditionally, NoSQL technologies were regarded as distant cousins of mainstream DBMSes, but Oracle’s recent foray into that domain is solid proof that times have changed. Enterprises are hungry for innovation that helps them tackle their business challenges and are embracing the ease of use, speed and scale that NoSQL brings.If you’re looking to power your application with an Enterprise-class Redis, I’d like to invite you to talk with us. The Redis team will be at Oracle OpenWorld and our booth is 204 Moscone South. You’ll meet our CEO, Mr. Ofer Bengal, and our CTO, Mr. Yiftach Shoolman, to discuss how you can start using Redis with your existing database to immediately take it to a new level of performance and scale. To book a meeting now send me an email to: cameron@redis.com."
215,https://redis.com/blog/redis-running-slowly-heres-what-you-can-do-about-it/,Redis Running Slowly? Here’s What You Can Do About it,"October 2, 2014",Itamar Haber,"Click here to get started with Redis Enterprise. Redis Enterprise lets you work with any real-time data, at any scale, anywhere.Redis is blazing fast and can easily handle hundreds of thousands to millions of operations per second (of course, YMMV depending on your setup), but there are cases in which you may feel that it is underperforming. This slowness of operations – or latency – can be caused by a variety of things, but once you’ve ruled out the usual suspects (i.e. the server’s hardware/virtualware, storage and network) you should also examine your Redis settings to see what can be optimized.A good place to start is by verifying that the CPU load of your Redis server is indeed working at full throttle and not blocked somehow. If your Redis process is at a stable 100%, then your issue may be attributed to one or both of two things: your volume of queries and/or slow queries. The optimization of slow running queries (and in most cases the underlying data structures) is an art in itself, but there are still quite a few things that you can try before tryin other measures like upgrading your hardware or clustering Redis.Review your Redis’ SLOWLOG to ensure that there aren’t any particularly slow queries in it – these should be easily identifiable by their high execution times (the third integer in each log entry). Higher execution times mean higher latency, but remember that some of Redis’ commands (such as Sorted Set operations) can take longer to complete, depending on their arguments and the size of data that they process. By using Redis Cloud’s enhanced version of the SLOWLOG or doing the math in your head, you can further drill down to identify the troublemakers.Next, take a look at the output of the INFO ALL command. Use your experience, keen judgment and cold logic to answer this question – does anything look weird? 🙂 When not tracking persistent storage, networking or replication bottlenecks, you should focus on the stats and commandstats sections.When the value of your total_connections_received in the stats section is absurdly high, it usually means that your application is opening and closing a connection for every request it makes. Opening a connection is an expensive operation that adds to both client and server latency. To rectify this, consult your Redis client’s documentation and configure it to use persistent connections.Use the information in the commandstats section to hone in on the commands that occupy most of your server’s resources. Look for easy kills by replacing commands with their variadic counterparts. Some of Redis’ commands (e.g. GET, SET, HGET & HSET) sport a variadic version (i.e. MGET, MSET, HMGET & HMSET, respectively), whereas others have infinite arity built right into them (DEL, HDEL, LPUSH, SADD, ZADD, etc). By replacing multiple calls to the same command with a single call, you’ll be shaving off precious microseconds, while saving on both server and client resources.If you’re managing big data structures in your Redis database and you’re fetching all their content (using HGETALL, SMEMBERS or ZRANGE, for example), consider using the respective SCAN command instead. SCAN iterates through the Redis keys’ namespace and should always be used instead of the “evil” KEYS command. As an added bonus, SCAN’s variants for Hashes, Sets and Sorted Sets (HSCAN, SSCAN and ZSCAN) can also help you free up Redis just enough to reduce overall latencies.Another way to reduce the latency of Redis queries is by using pipelining. When you pipeline a group of operations, they are sent in a single batch that, while bigger and slower to process, requires only a single request-response round trip. This consolidation of trips makes for substantial overall latency reductions. Note, however, that a pipeline operation could block your application if it is waiting for replies without sending the pipeline – Redis will provide all of the replies to the pipelined stream of commands only after the pipeline has been sent. Also bear in mind that when using pipelining, Redis caches all the responses in memory before returning them to the client in bulk, so pipelining thousands of queries (especially those that return a large amount of data) can be taxing to both the server and client. If that is the case, use smaller pipeline sizes.When pipelining isn’t an option, you can still save big time on round-trip time (RTT) latency by taking the “moonlit” scripting road. With Lua scripting in Redis, you can use logic that interacts with the data locally, thus saving many potential client-server round trips. To further reduce bandwidth consumption, latency, and to avoid script recompilations, make sure to use SCRIPT LOAD and EVALSHA.Lua Tips: Refrain from generating dynamic scripts, which can cause your Lua cache to grow and get out of control. If you have to use dynamic scripting, then just use plain EVAL, as there’s no point in loading them first. Also, remember to track your Lua memory consumption and flush the cache periodically with a SCRIPT FLUSH. Also, do not hardcode and/or programmatically generate key names in your Lua scripts, because doing so will render them useless in a clustered Redis setup.These are just some of the ways that you can easily get more out of your Redis database without bringing in the heavy artillery. Adopting even one of these suggestions can make a world of difference and lower your latency substantially. Know more tricks of the trade? Have any questions or feedback? Feel free to shout at me, I’m highly available 🙂"
216,https://redis.com/blog/new-redis-cloud-clusters-with-regular-expression-sharding/,NEW: Redis Cloud Clusters with Regular Expression Sharding,"October 29, 2014",Itamar Haber,"I’m delighted to announce that today we’ve made our clustering technology even more useful with the public availability of RegEx Sharding. This feature allows you to define exactly how Redis Cloud distributes data between a database’s shards, thereby enabling your application to continue performing multi-key operations at top performance on huge datasets. Our standard and new RegEx sharding policies are immediately available to all our Redis Cloud Pay-as-You-Go subscribers.Before diving into the details of this announcement, I’d first like to go over the “why” and “what” of a Redis Cluster. Redis, the fastest data store available today, is an open source, in-memory NoSQL database. Redis’ architecture is such that a single Redis server is bound by the hardware of the host that it is running on — specifically that server’s CPU, RAM and network. Being a (mostly) single-threaded process, Redis utilizes only one of the server’s CPU cores. And because it is an in-memory database, all data that a Redis process manages has to fit into the RAM of the server it’s running on. Lastly, the network interface of the server running Redis may also become a bottleneck once saturated with traffic generated by Redis and the application. While a single Redis server can process tens and hundreds of thousands of operations per second, there are cases in which applications need more.Scaling up a Redis server (vertically) is feasible, to an extent. Sure, you can add more RAM to a server, replace the CPU with a faster model and even use a broader and faster network, but at the end of the day you’ll hit the upper limit of any single server’s hardware. That’s where clustering and sharding can help by allowing you to use multiple CPU cores on each server and beyond.A Redis cluster is made up of one or more servers, with each server running one or more Redis processes. Each process manages a shared-nothing database instance that’s called a shard. The keys in the clustered database are mapped to hash slots, which in turn are mapped to shards, so that each shard manages a mutually-exclusive subset of the database’s namespace. In a sense, shards are the physical databases and hash slots are an additional layer that facilitates administrative operations such as resharding. By running the shards on multiple servers, a cluster essentially allows you to use more CPU cores, more RAM and more network resources for managing your database.Clustering is an effective approach for horizontally scaling your Redis database, as it lets you use a distributed setup. It offers an efficient way to split the memory requirements, processing load and bandwidth that your database requires between multiple servers. There is one catch however – because Redis clusters are implemented with share nothing shards, you can’t execute multi-key operations that span more than one hash slot (e.g. ZUNIONSTORE on several sorted sets). Doing so will trigger an error. In order to execute atomic operations on multiple keys (i.e. single commands that operate on multiple keys, MULTI/EXEC blocks and Lua scripts), you have to ensure that all relevant keys are mapped to the same hash slot (more on that below).Open source Redis v3 will be all about native clustering support. A few weeks ago, Salvatore Sanfilippo released the first v3 Release Candidate, and it is expected to be production-ready within a few months. Once the open source cluster has stabilized, it will provide all the tools needed for anyone to set up and operate a Redis cluster, and effectively address scalability challenges.There are, however, other Redis clusters besides the open source implementation. Over the better part of the last two years, we at Redis have been operating our own independently-developed version of a Redis cluster to provide Redis Cloud’s scalability features. Redis’ production-proven clustering technology lets you dynamically scale your Redis databases well beyond the limits of any single server. Some of our customers use clustering to manage TB-scale datasets, whereas others rely on it to sustain massive throughput with sub-millisecond latencies. Like everything in our service, our clustering is dead simple to use and doesn’t require any special effort. Once employed, it is transparent to the application and does not require any code changes or specialized client libraries – you just continue working via a single database endpoint that masks the architecture’s underlying complexities.Redis Cloud clusters offer a choice between two sharding policies: Standard and RegEx. The standard policy is designed to behave just like the open source Redis Cluster. By using hash tags in your key name (i.e. the ‘{‘ and ‘}’ characters), you can precisely specify the part of the key’s name that will be used for hashing. This standard sharding policy will use the substring of the key’s name that’s surrounded by curly brackets to map that key to a hash slot.While this standard sharding policy is a powerful tool, we’ve taken it to the next level by introducing our RegEx sharding policy. This type of sharding allows you to configure a set of a regular expression rules that are used to extract the hash tag from the names of the keys. Because you can use multiple rules, and because each rule is a fully-fledged regular expression, this custom sharding policy offers a lot of flexibility — effectively allowing you to implement any pattern matching logic on your key names for extracting hash tags. You can embed a part of your business logic directly into the sharding policy to ensure a perfect fit for your application’s requirements.Custom RegEx sharding policies are especially useful when using clustering with an existing application and dataset, because you can skip both the data migration and and code changes for your new implicit database schema. With custom RegEx sharding, you can “teach” Redis Cloud clusters how your dataset’s key names are constructed and ensure that keys which need to be in the same hash slot (for multi-key operations) are identified correctly. You can read more about our sharding policies and how to use them on our documentation page.Clustering Redis is easy and fun with Redis Cloud. Our databases can be clustered and scaled to accommodate growth in data volume, throughput and traffic with a click of a button and without any changes to your existing application. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
217,https://redis.com/blog/take-2-is-amazon-ssd-piops-really-better/,Take #2 – Is Amazon SSD PIOPS Really Better?,"November 3, 2014",Guy Lubovitch,"For many of today’s applications, disk speed is extremely important, which is why some AWS users turn to the wide range of available Elastic Block Store (EBS) options. As an in-memory database, Redis relies mainly on memory, rendering disk speed less important, but – and this is a big ‘but’ – Redis is also persistent and uses disk for replication. So is it worth paying extra bucks to get a faster disk from AWS when you’re already running Redis servers?Our initial assumption was “NO,” primarily because Redis uses sequential writes when creating AOF files, and when you perform replication it just dumps the memory into the disk. Neither of those operations exploit the fast allocation of SSD or the advantages of PIOPS. Two years ago we conducted some tests that proved our hypothesis, and recently we tested it again to check whether the results have changed.I created three benchmarks to compare EBS disk speeds against plain magnetic disk, EBS with SSD and EBS with SSD PIOPS. The benchmark was done on three nodes, each with its own EBS and our Redis Clouds service installed on top of it. I wanted to use the optimal configuration for PIOPS so I chose the c3.4xlarge instance type (more about that here). I configured Redis to write the its append-only file every second (i.e. appendfsync everysec), which is the default setting for AOF and is considered the common use case. Lastly, I did not use replication to avoid unneeded network noise, the focus of this benchmark being disk usage.Our client machine’s instance type was c3.2xlarge and I used our home-grown memtier_benchmark tool to simulate the workload.The benchmark run had performed write operations exclusively . I used 10KB objects and had the benchmark use 50 concurrent connections and a pipeline size of 25. For your reference, here are the command line arguments that I used with memtier_benchmark:–ratio=1:0 –test-time=120 -d 10000 -t 1 -c 50 –pipeline=25The ratio was set for write (SET) operations only and the test’s time was 2 minutes. I ran each test twice, once with a single thread and once with 4 threads, because I wanted to examine the affect (if any) that the number of threads has.To my surprise, these benchmarks showed that running Redis on SSD and SSD PIOPS performs much better than it does with magnetic disk:The logic had remained the same and disk physics have not changed – by itself, magnetic EBS should provide similar throughput and latency to that of SSD EBS. But we found that using SSD EBS will get you better hardware and a faster network, which will double the performance. Note that increasing the number of memtier_benchmark client threads from 1 to 4 had caused performance to degrade, demonstrating that we’ve effectively saturated the EBS device.If your durability requirements are stringent, it pays to pay more for faster disks. Additionally, you can increase the storage’s throughput by attaching multiple EBS volumes but this approach was not tested in this benchmark. Another approach for increasing the transactions per second (TPS) score from your Redis setup is to delegate persistence to a slave instance, thus freeing the master to do actual data processing. Note, however, that replication may add load to your network and may reduce the consistency of your database.I’ve used Linux’s iostats to monitor the write throughput for each storage configuration during the benchmark’s run. Once each run was over, I’ve also triggered a manual snapshot (the SAVE command) and monitored its write throughput as well. Here are the results:Analysis:Although SSD storage is optimized for random access rather append-only write patterns (the latter being Redis’ modus operandi), we were pleasantly surprised to find that it is in fact more performant than magnetic disks for our purposes. Furthermore, AWS’ SSD PIOPS indeed deliver better throughput compared to the regular SSD offer. The AWS team deserves a big kudos for this amazing service offering and our recommendation is: if you’re running Redis on EC2 and need data persistence, your money will be well spent on PIOPS SSD EBS volumes."
218,https://redis.com/blog/our-2014-reinvent-experience-and-3-aws-cloud-announcements-for-developers/,Our 2014 re:Invent Experience and 3 AWS Cloud Announcements for Developers,"November 24, 2014",Leena Joshi,"AWS re:Invent 2014 turned out to be an exciting three days for us at Redis. Amazon revealed a considerable amount of announcements this year that are especially exciting for the developers among us (we’ll discuss the highlights in our summary below). As seasoned AWS re:Invent sponsors and exhibitors, having taken part in all three conferences, we highly enjoyed it, as always, and learned quite a lot from the keynotes, breakout sessions, and interactions with the impressive attendees.We were delighted that Bleacher Reports and Hotel Tonight, both of whom use Redis to support their vast needs for excellent Redis performance and availability, collaborated with us at our booth. We’d like to thank them for sharing their stories with those who visited us. We also gave away various goodies this year, including Redis Geek baby swag, which was catalogued by Carmen Barr, wife of the AWS evangelist, Jeff Barr, in her tour of re:Invent swag. We are happy to see that so many toddlers are going to be future Redis Geeks 🙂Three major announcements, of many, were made by Amazon’s Andy Jassy and Werner Vogels in the keynotes on the last two days of the event.The most prominent announcement of the event was the introduction of Lambda. It is an interesting new AWS service that helps developers code on top of AWS’ infrastructure, which we see as a big step towards Amazon’s pure PaaS (Platform-as-a-Service) model. With Lambda, developers don’t need to manage their underlying compute resources. It runs code written in JavaScript, and is powered by a server-side Node.js, while performing general requests, like image uploading, or custom requests in backend services. Amazon then bills by the amount of computing that was consumed in order to run the code. We recommend that you check out the tests done by @esh that explore the AWS Lambda file system and runtime environment. As a developer, this service can become your best friend, since you won’t need to waste time building a new VM to run your code.Amazon knows that developers love Docker containers, and accordingly introduced the EC2 Container Service. Containers separate applications from their underlying infrastructure, easing IT operations like portability and environment replication, making Docker container support essential to easily run a distributed application. This new Amazon service facilitates container management aspects such as cluster management, authentication, firewalling, and movement between different availability zones. Amazon provides this service for free which makes sense as it facilitates infrastructure provisioning for every application.The third notable announcement was AWS’ launch of Amazon Aurora. In today’s web applications and mobile apps, such as online games, we see users increase from hundreds to millions in a matter of months, which isn’t truly supported by any of the current relational database offerings. According to Jeff Barr, the principles that these databases were built upon are old, all database operations start and end on one single server. However, according to Andy Jassy, Amazon recognized this and came up with Aurora, the second generation of RDS, built by the RDS team over the last 3 years. The team rewrote the MySQL engine, the most popular open source database. Amazon used their cloud capacity and scale in order to support an enhanced read replica mechanism, that, relies on a scalable infrastructure to achieve better performance than in RDS. In addition, with Aurora, a master database is replicated in 3 AZs parallelly, and all of the replicas are directly linked to the master server, allowing for higher availability and durability.It was remarkable to see that the conference tripled itself in all dimensions, including the amount of exhibitors, attendees, and even tweets. We expect this conference to grow even more next year, just like AWS. This year’s announcements have demonstrated Amazon’s amazing innovation, positively disrupting the IT industry time and time again. Needless to say, we’re excited for next year’s AWS re:Invent!"
219,https://redis.com/blog/redis-labs-launches-on-dotcloud/,Redis Labs Launches on dotCloud,"December 8, 2014",Itamar Haber,"It is my pleasure and privilege to announce that as of today our popular Redis Cloud service is available on cloudControl’s dotCloud PaaS. Running on top of the Google Cloud Platform, dotCloud allows combining best-of-breed services for building the ideal stack for your application. Focus on developing your code instead of wasting your time doing ops – dotCloud keeps your application running 24/7*365 with load-balancing, monitoring, failover and scaling.Creating a blazing fast application begins with choosing the right database – add Redis Cloud to your dotCloud stack and immediately benefit infinite scalability, high availability, automatic failover and top stable performance. Questions or Feedback? Email or tweet me – I’m highly available 🙂"
220,https://redis.com/blog/how-hoteltonight-handles-the-demand-for-instant-booking-and-check-in/,How HotelTonight Handles The Demand For Instant Booking And Check-In,"December 12, 2014",Steven Melendez,"Originally published on fastcolabs.comThe app HotelTonight is built around last-minute hotel booking. Users depend on the app to quickly deliver room rates and vacancies, especially when they have a spotty cell connection and it’s a busy holiday weekend.“We can have customers that are standing outside a hotel and booking a room and walking in and expecting to be able to check in,” says HotelTonight cofounder and chief architect Chris Bailey.The app’s backend needs to be able to quickly sync reservations and vacancy information with a range of hotel reservation systems and users’ phones and tablets, so it doesn’t fail to display a convenient deal or, worse, book someone a room that’s already taken, says Bailey. The app’s Black Friday traffic peaked at eight times normal load levels, with a $7 room special selling out in under seven minutes.“Historically in the industry, people aren’t booking so much last minute, but that’s really shifted,” he says, thanks to mobile devices. “They can be in the back of a cab; they can be at a bar; maybe they’re at a party and decide, ‘Hey, I don’t want to drive home tonight.’”To keep data updated in real time even under heavy load, the company uses the speedy Redis, an open source data structure server that is currently sponsored by Pivotal. Bailey says it also helps engineers quickly add and tweak new features and record new stats without painstakingly building traditional database tables.“We didn’t initially start with Redis, but we pretty quickly figured out a need for it there,” he says, explaining the company still uses MySQL for some of its infrastructure. “All these things that you thought well, shoot, I don’t really want to write that to MySQL—it just seems like a lot of overhead, I have to create a table and manage a schema and there’s a lot of overhead—now I just write it to Redis.”The database system also helps with a common issue for HotelTonight: handling network timeouts and other errors in communicating with customers’ devices and third-party services like booking APIs, says Bailey.Redis includes atomic counters—that is, variables that can be incremented in a single operation so they report and store consistent values even when accessed by multiple processes—which help keep track of how often an API call or other operation has failed.Partially to avoid focusing too much on the uptime and scaling of Redis and other core services itself, HotelTonight relies on cloud providers, including Amazon Web Services and hosting from Redis, he says.“It allows us to not have to have as much expertise in house and rely on true experts in those technologies for things like Redis or some of the other services that we use,” he says.That, and the flexibility of the kind of data that can be stored in Redis, make it easy build custom logic to track and handle failures of different types of operations, from rapidly scheduling retries to failing over to alternative providers, says Bailey.“What’s really nice about that is from the customer perspective we avoid giving them an error message,” he says. “There’s no error for the customer; they get what they’re after.”Using Redis also makes database changes more painless, as engineers can effectively tweak what’s stored in the database without rebuilding formally specified SQL tables, which used to require downtime, he says.“Don’t get me wrong—you will have a schema, so to speak, at some point,” he says. “It’s more about how officially defined is it.”"
221,https://redis.com/blog/the-lessons-missing-from-benchmarking-nosql-on-the-aws-cloud-aerospikedb-and-redis/,The Lessons Missing from Benchmarking NoSQL on the AWS Cloud (AerospikeDB and Redis),"January 29, 2015",Itamar Haber,"Delivering on its promise from last week, Aerospike yesterday published the results of a benchmark done by Lynn Langit titled “Lessons Learned – Benchmarking NoSQL on the AWS Cloud (AerospikeDB and Redis).” Salvatore Sanfilippo, Redis’ creator, posted a response at “Why we don’t have benchmarks comparing Redis with other DBs,” in which he describes some of the pitfalls of doing ‘comparative “advertising”‘ and provides several approaches for getting better results from Redis.One thing the benchmark clearly shows is that in some read-write mixes, both Redis and AerospikeDB can be used as “dumb” key-value stores and essentially deliver the same value in terms of throughput per single server. So what is there besides comparable performance? Well, Aerospike rightfully boasts of a nice UI and friendly automations (and I’ve seen the same for Redis :)), but yesterday’s benchmark still leaves some unanswered questions.Why didn’t the benchmark use conventional, recommended Redis practices such as pipelining and multi-key operations? I imagine it’s because Aerospike doesn’t have the equivalent of these and has to resort to simple GET/SET operations, essentially measuring the network (and OS perhaps) rather than the technology’s true capabilities. Another missing piece is a 20%-80% read/write test (-w RU, 20) and a 100% write test (-w RU, 0). I suspect the results of such runs would not align with the benchmark’s message.Regardless, why is it that some of the points simply don’t add up? Take, for example, Lynn’s results for the 100% read pattern against the sharded Redis from Test 1 and Test 2. In the first test, the configuration scored 928K TPS whereas in the 2nd test it only reached 860K – a decrease that is totally unexplained by the fact that she used AOF. Even with AOF enabled for Test 2, Redis only accesses the disk in response to changes to the data (a.k.a writes), so AOF should have exactly 0% impact for read-only traffic. That by itself is puzzling, but here’s another curiosity: her benchmark finds that an EBS-backed Redis actually performs better than a Redis server that’s RAM only – again, compare Test 2 and 1’s never-write scores for single shard Redis: 180K and 132K TPS, respectively.Surprises continue if you compare single-shard Redis in Test 1 and 2 when writes are part of the traffic. In Test 1 (50% and 20% writes), single sharded Redis gets 128K and 129K TPS (respectively), whereas in Test 2 we see the same patterns’ performance improvement (yes, by turning AOF on) with scores of 150K and 164K TPS.So while the multi-sharded Redis set up mysteriously gets hit by AOF activity (which shouldn’t have been happening in a read-only scenario anyway), the same AOF setting actually boosts performance of the single shard configuration, even with writes. That’s either a paradox, some foul witchcraftery or an act of divine intervention – and I don’t buy it.Apropos the use of AOF, a common practice (that’s also employed in our Redis deployments) is to have a slave Redis instance deal with persistency considerations to lessen the master’s load. I can only assume that this best practice for Redis wasn’t used because an SSD-optimized, multi-threaded database does things differently.Comparisons are as hard to do right as they are easy to do wrong, and the effort put into this one is notable. I respect Aerospike’s engineers for uncovering and sharing so many useful network tweaks, and I greatly appreciate the entire team’s attempt at sharding Redis. But in my opinion, the final outcome is somewhat lacking and simply leaves too many loose ends."
222,https://redis.com/blog/milestone-reached-4k-customers/,Milestone Reached: 4k Customers,"February 19, 2015",Itamar Haber,"I like people and I like numbers, so it gives me great pleasure to announce that our customers have carried us across the four thousand mark. Based on the data, in little less than two years we have been experiencing Moore’s Law-like growth as shown by the graph above.4000 is a significant number, and not only in the CS-sense. For starters, it is large enough to indicate scale – it would take a sizable conference hall to hold a party for all our customers! Every one of our 4k customers is part of the bigger Redis users base, which is in turn a part of the global Redis community. 4k customers have at least that many applications, all being used by hundreds of thousands of users (inspired by Matt Stancliff‘s vision).Apart from establishing scale, 4k is also a velocity measurement – Redis ranks among the top NoSQL databases every year and there are no signs that this trend is about to stop. If anything, it’s just the opposite:Personally, I find that the nicest thing about the number 4k is that it is made of out of meat (or is it comprised of?). Each one of you – Redis’ customers, users and casual readers of this blog – is a real person with a story to tell about Redis. And that’s a story that I’d love to hear so feel free to tweet or email me – I’m highly available 🙂"
223,https://redis.com/blog/events-horizon-aws-summit-san-francisco-and-pycon-2015/,Events Horizon: AWS Summit San Francisco and PyCon 2015,"March 31, 2015",Itamar Haber,"The 2nd week of April is going to take a considerable toll on Python developers who are using AWS – two major events will be held back to back but more than 2,500 miles (or 4,000 km) apart. I’m referring, of course, to the upcoming AWS Summit in San Francisco on April 8th and 9th, and PyCon 2015 in Montreal from the 10th to the 12th. If you are going to schlep yourself to these two events, worry not! The Redis team will be at each conference to talk with you about everything Redis, and give away our coveted swag 🙂In my experience, the AWS Summits are the place to be if you’re on that cloud – these free-to-attend events are packed end to end with top-notch sessions on all topics. This spring’s SF summit appears to be just as interesting as ever, with several talks focusing on the newest services that were announced during last year’s re:Invent: Amazon Aurora (the MySQL-compatible database service), AWS Lambda (the event-driven compute service) and the EC2 Container Service.The annual PyCon is by far the largest community gathering of people who use and develop the open-source Python programming language. The entire event is an 8-day long marathon that begins with tutorials, continues with the actual conference, and ends with sprints (including a 5K Charity Fun Run). The conference itself has no less than 5 tracks running in parallel to cater to every Python developer’s whims and fancies, and features an expansive expo hall where you can meet the event’s sponsors (over 130 companies, including Redis of course!).With Redis being the fastest database in the world, it is extremely popular with both communities – AWS cloud and Python users. So, whether you’re new to Redis and want to learn about it, or you’re already using Redis but want to take it to the next level – drop by our booths (#536 at the AWS Summit, #329 at PyCon) or book a meeting by shooting an email to Cameron, our VP of Marketing. Happy conferencing!"
224,https://redis.com/blog/redis-ram-ramifications-part-i/,Redis RAM Ramifications – Part I,"April 16, 2015",Itamar Haber,"If there’s one most often repeated Redis question, it’s this one. It’s also one of the hardest to answer accurately because the answer depends on oh-so-many factors. In this post (and most likely a few upcoming ones as I do tend to ramble while I rumble and mumble), I’ll try mapping out Redis’ RAM consumption. While I can’t guarantee an ultimate answer to everything, I hope it’ll help to:Side note: you can track my progress on this series by subscribing to the RSS feed. Alternatively, sign up to Redis Watch and get weekly updates delivered to your inbox. Lastly, but only if you’re up to it, follow us on Twitter for real-time action.So, Redis is a piece of software and as such it requires RAM to operate. But Redis is not just any software, it is an in-memory database, which means that every piece of data Redis manages is kept in RAM too. Lets call the RAM that Redis needs to operate the Operational RAM, and name the RAM used for data storage the User Data RAM.We’ll begin our journey with a quick peek at Redis’ Operational RAM.It is used for many purposes and tasks that Redis performs, and one way to think of that chunk of RAM is as all the memory that’s used by Redis for everything that isn’t the user’s data (I’ll probably waddle knee-deep into this later on) Redis’ RAM footprint is influenced by a myriad of deployment factors, including:We can easily, however, set a baseline for Redis’ operational RAM requirements by examining it at rest on a typical server. For example, the memory footprint of unladen African swallow v3.0.0 instance on a virtualized Ubuntu 14 64-bit server is 7995392 bytes (or about 7.6MB). You can quickly determine how much total RAM your Redis instance has allocated from the command line with ps‘ RSS column, or with Redis’ INFO command:Side note: the results of from the two methods above and others may not always be the same. Also, note that used_memory_rss differs greatly from Redis’ used_memory as we’ll see.Since this is a freshly initialized Redis instance, we can assume that this figure is a fair representation of our operational baseline. Redis’ operational RAM can grow, even significantly, but lets not worry about that for now.Redis is beautiful, but you don’t keep it around just for its pretty face, do you? No, you make Redis manage your data because you need the fastest NoSQL database on the planet today. You let it carry your coconut and depend on it to keep beating its wings forty-three times (so close!) every second. So how much RAM does user data take? That depends on the coconut 🙂Redis’ schemaless schema* is based on the Key-Value model. Every user datum that Redis manages is primarily a KV pair. It doesn’t take a genius to understand that the longer/bigger your keys and values are, the more RAM Redis will need to store it. But Redis does have a few ingenious tricks designed to keep data compacted and organized.* There is no such as a schemaless database – at most it can have only an implicit one.Let’s jump right into it with an example concerning the simplest Redis data type – consider the following:How much RAM did we just use? Since all data is organized by keys, the key’s name is the first element we’ll examine. We know that Redis key names are binary safe strings that can be up to 512MB long. Cool. String values in Redis are also binary safe and up to 0.5GB, so in our example above we can assume 7 bytes for “swallow” and another 7 bytes for the “coconut”… and we’d be wrong, at least partially. Here, let me show you:As shown above with STRLEN, Redis insists that the length of the value (“coconut”) that’s stored under the “swallow” key is 7 bytes long. Even more so, the “secret” DEBUG SDSLEN command also makes the same claims, but what both don’t account for is the datum’s overhead, and every Redis data structure comes with its own baggage. That means that besides the actual strings (“swallow” and “coconut”), Redis also needs some RAM to manage them.Since every key-value tuple in Redis uses additional RAM for its internal bookkeeping, and because the amount of that RAM depends on the data structure and the data itself, I consider this overhead, while meta, a part of the user data. Put differently, if for every X bytes string Redis requires X + Y bytes, then Y is definitely a KU that I’d like to make into a KK.Strings in Redis are largely implemented by one of Salvatore Sanfilippo @antirez‘s sub-projects, called sds (or Simple Dynamic Strings library for C). While sds strings bring a lot of power and ease of use to Redis internally, they do carry some overhead with them as an sds string is made up of:(diagram courtesy of antirez, https://github.com/antirez/sds/blob/master/README.md)The size of an sds string’s header (at the moment) is 8 bytes and the null character is an additional byte, which give us a total of 9 bytes overhead per string. The 7 byte long “swallow” suddenly takes more than twice that amount – 16 bytes of RAM – to store in Redis!Side note: It actually uses moar. The reference to every key is also stored in Redis’ keyspace hash table, which in turn requires even more RAM… And there’s also the robj “object” that Redis uses to facilitate some stuff like LRU… But let’s just keep the key’s management RAM overhead as a KU and throw it into the operational RAM side of the house for now 🙂Back to our swallows and coconuts – so now we know that Redis will use 36 bytes for storing the two strings that make up the key and value in the example above, but is that all? Let’s take a closer look at our coconut:The plot thickens (or maybe it’s the coconut growing hair?). This cryptic response begs an explanation, but are all coconuts encoded the same? Let’s try carrying a few different shapes of string coconuts and see:Each of our coconuts is different, so Redis uses different encodings. The “int” encoding is used to efficiently store integer values between LONG_MIN and LONG_MAX (as defined in your environment’s limits.h) and also leverages the shared.integers construct to avoid duplicating data. Consequently, these take up less space. On the other hand, strings longer than 39 bytes are stored as “raw”, whereas shorter ones use the “embstr” encoding (the magic number is defined by REDIS_ENCODING_EMBSTR_SIZE_LIMIT in redis.h).What about other coconut structures? Strings are the simplest data structure that Redis offers and they are used internally to make other, more advanced structures. The Hash is made of a bunch of Strings (fields & values) added with a dictionary data structure, in which each entry is a linked list… BUT they could be encoded entirely as a ziplist. And speaking of lists, we have the linked lists and ziplists (and perhaps even Matt Stancliff @mattsta‘s quicklists soon) that are also used by Sets and Sorted Sets…I could go on, indefinitely, into the delicate intricacies of data encoding in Redis and how they affect memory consumption. I fear, however, that it will bore all of us to death pretty soon. Alternatively, you can git checkout the source code and start reading it – I know a few people who have done it, but you’ll need a certain level of programming skills for that.Which still leaves the big KU – How much RAM does Redis need? – and its little brother KU – How much RAM do coconuts need? – pretty much unanswered. But where there’s a will there’s a way. Feel free to interact with me via the usual channels – I’m Highly-Available 🙂Side note: to be continued."
225,https://redis.com/blog/nyc-redis-tech-talk-featuring-lukas-sliwka-vice-president-engineering-grindr/,"NYC Redis Tech Talk Featuring Lukas Sliwka, Vice President Engineering @Grindr","April 17, 2015",Leena Joshi,"Redis has teamed up with Database Month, the New York City based festival for NoSQL, NewSQL, and big data.This year we’re hosting a Tech Talk titled “Using Redis to Create a Blazing Fast Mobile App” and featuring Redis enthusiast Lukas Sliwka. Lukas is the VP of Engineering at Grindr, one of the world’s leading mobile dating apps, used by millions of people in over 200 countries.Network and data access latency can kill mobile app performance. This problem is even more difficult to solve when building a mobile application that requires global availability. Grindr used Redis to overcome a series of performance challenges, which enabled Grindr to bring its service as close to its customers as possible.In this Tech Talk you will learn:We hope you can join us for this session on Tuesday, April 21st, at 6:30PM at 557 Broadway, New York, NY. RSVP here!"
226,https://redis.com/blog/redis-labs-at-mongodb-world/,Redis Labs at MongoDB World!,"May 21, 2015",Leena Joshi,"The Redis team will be in New York next month to sponsor MongoDB World, MongoDB’s largest end user event, which brings together thousands of Mongo developers, enthusiasts, and influencers from around the world.The event will take place on June 1 – 2 at the Manhattan Sheraton in New York City and welcomes developers and ecosystem partners like Redis. MongoDB and Redis offer complimentary uses cases for both startup and enterprise developers. As we’ve shared in the past, we find that many developers who use Redis choose to use a wide variety of databases together, including MongoDB and MySQL, to solve a range of challenges for performance-driven use cases.As usual, the Redis team has a great lineup of activities planned for the event, including:Questions? Feel free to email me or reach out on Twitter!"
227,https://redis.com/blog/rising-nosql-star-aerospike-cassandra-couchbase-or-redis/,"Rising NoSQL Star: Aerospike, Cassandra, Couchbase or Redis?","June 4, 2015",Itamar Haber,"A new NoSQL benchmark was just released by Avalon Consulting, LLC, and I couldn’t be happier to brag that Redis out-performed its competitors by a landslide. With more than double the throughput and half the latency of other NoSQL databases, our Redis Enterprise Cluster dominated in a real-world application scenario. The Avalon benchmark report is freely available here and the results speak for themselves.But before we get into all the fun background on this particular test, let’s acknowledge a few things about benchmarks. There’s no way around it – performing meaningful comparisons between NoSQL solutions is a hard task. That is because of benchmarking’s “original sin” — results from any benchmark are truly relevant only to the specific application that was used for the test (see Haber’s Benchmarking Theorem). This fact is compounded by the diverse capabilities of all the different NoSQL databases. Typical benchmark models tend to generalize a specific use case, and in the process they distance themselves from the underlying data management system and fail to leverage its strengths.This by itself is hardly news and the past is riddled with attempts at comparing apples to oranges. I gave an entire presentation at RedisConf 20Fifteen on this subject ( “Benchmarking Redis By Itself and Versus Other NoSQL Databases”). If you’ve watched it then you already know I believe that the only way to compare apples with oranges is through an applicative benchmark, in which the test application is optimized independently for each DBMS. That RedisConf talk, it turns out, was only the warm-up act for a session by Lahav Savir, CEO of Emind, who presented a real life benchmark using that exact approach (“Real-Time Vote Platform Benchmark”).Emind’s use case is a great example of how Redis is put to use in tackling some of the hairier challenges that real-time analytics presents in the context of Big Data and the IoT. The story behind Emind’s benchmark brings together all my passions: data, people, technology and the cloud. It is a brilliant experiment designed to identify the best-performing NoSQL database for a real-time voting platform. The voting platform supports large events such as televised talent shows (think “American Idol” or “Rising Star”), where the audience is actively involved and directs the course of the show by voting. The volume and velocity of votes that must be tallied as they come in is staggering, so the platform’s performance must have superstar qualities (much like the shows’ participants) to support that kind of traffic.Emind’s team identified several NoSQL technologies that could potential power their platform: Aerospike, Cassandra, Couchbase and Redis. While all candidates seemed promising, Emind needed to be sure that it chose the database that would best meet its requirements. To do that, Emind’s engineers built a mock application (“mockapp”) in Go that simulated the voting process and tailored it to use each of the different candidates.Emind then solicited the services of Avalon Consulting, LLC to ensure the benchmark was executed optimally and impartially. Avalon reviewed and optimized the mockapp’s source code, approached each database vendor (Aerospike, Datastax, Couchbase and Redis) for guidance and certification of its respective solution’s deployment, executed the benchmark and compiled a comprehensive report with the results. Check out the full write-up to find out more about how my favorite performer is a rocking superstar. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
228,https://redis.com/blog/appfabric-coming-apart-5-reasons-to-move-to-redis-labs/,AppFabric Coming Apart? 5 Reasons to Move to Redis Labs,"June 17, 2015",Leena Joshi,"Microsoft recently announced that Microsoft AppFabric 1.1 for Windows Server will be at the end of support on April 2, 2016. Less than a year away! Don’t panic yet, there is another, better solution.Redis is the product of choice for thousands of developers worldwide who want to accelerate their applications. Redis is the fastest growing NoSQL datastore, that runs in-memory and delivers millions of transactions at sub millisecond latencies. Redis provides enterprise class Redis: as a downloadable product Redis Enterprise Cluster (RLEC) or as a seamlessly scalable, highly available service, Redis Cloud.Here are the top 5 reasons to move applications using Microsoft AppFabric to Redis Cloud or Redis Enterprise Cluster :RLEC is free to download and Redis Cloud has a free tier as well – so it is really easy to try out both those products.If you have any questions about migrating from AppFabric to Redis, our solutions consultants are always here to help. Email us at sales@redis.com and we can set up some time to walk you through the migration!"
229,https://redis.com/blog/redis-labs-series-b-funding/,Redis Labs Series B Funding,"June 25, 2015",Itamar Haber,"Today we are announcing Redis’ Series B funding of $15M. Our leadership team secured these funds to continue fueling the incredible growth we’ve been so fortunate to enjoy, both with our ever-popular Redis Cloud service as well as with our newest product, Redis Enterprise Cluster (RLEC), which we unveiled earlier this year.IMO, what we do here at Redis can be summed up in just two words: Technology and Service. I put Technology first because it’s easy to understand. After all, most of us here are geeks who get to mess around with some of the coolest technologies out there. Our engineers eat network splits for breakfast, munch on scalability for lunch, have a bite of performance optimization for dinner, and then start working in earnest.Any technology is only good as long as it operates, and it’s no secret that technologies break. A server will crash, the Cloud can evaporate and software defects are as sure as death and taxes. Anyone who tells you differently is stupid, lying or both. But while you can’t avoid these facts of life hitting you at the least expected of moments, there’s one thing that makes all the difference once they do. Yep, Service is what really sets apart a technology because it’s the one and only thing that can turn an inevitable malfunction into a glitch instead of a total meltdown. Here’s a mushy analogy – if Technology is the mind of Redis, then Service is our soul.Our company’s technological achievements would have been dwarfed had it not been for our passionately dedicated team who is entrusted with providing services to customers. You don’t notice them when everything’s running smoothly – it’s their job to make sure that it does. And when you do get to meet them, it is rarely in a stress-free context. Much like other unsung heros, they need to save the day, and do it with a smile.We believe in providing the Best Service and Technology possible. We have our investors’ confidence (and money ;)) because they know this is but the beginning. The team we’ve assembled and the products we offer are proof of that. We’re scaling, and the sky isn’t the limit, it’s just the first step. Now, onwards."
230,https://redis.com/blog/salvatore-sanfilippo-welcome-to-redis-labs/,Salvatore Sanfilippo – Welcome to Redis Labs,"July 15, 2015",Itamar Haber,"I usually begin this type of post with a variation on the sentence “Today I am delighted to…” Today, however, these words aren’t filler — they truly are the best way to express that I am delighted to welcome Antirez (a.k.a Salvatore Sanfilippo), Redis’ creator, to Redis. You can read more about this development on Antirez’s blog and in our press release, but here’s another perspective – mine.I’m a Redis geek and if you’re reading this then I’m guessing that you are one too. Redis is awesome, but I suppose that each of us finds in it the things that she/he relates to most. As for myself, the Redis community is what I feel strongest about. In fact, if there’s one key value that really sets the Redis project apart to me, it is the people who build it, develop for it and use it with their applications.Redis’ community is big. It comprises college students, veteran developers, garage startups and multi-billion-dollar companies – they’re all a part of it. For the last couple of years, I’ve had the honor and privilege of being a part of it and witnessing its evolution firsthand. I’ve met a fair share of its members online and IRL, and I work with many of them on a daily basis at Redis. I’d like to believe that I’ve given something back to the community as well 🙂Ten months after releasing the first public version, Antirez wrote that he chose to develop Redis because it would probably keep him “interested for the next decade.” Half a decade later, his key tenets for Redis still ring true. Early adopters became the majority. Simplicity remains a central theme. Feature bloat is all but non-existent. His pragmatic roadmapping approach and the ~95% of code that Antirez wrote brought Redis to where it is today.There’s really only one sentence in his 2009 post that’s not true anymore – “Redis can’t be considered a successful project yet, it’s just too early.” Over the past six years, Antirez and the community have multi-handedly made Redis into one of the most successful open source software projects in existence. This would not have been possible had it not been for the project’s sponsorship by VMWare and Pivotal, companies whose commitments to Open Source Software are inspiring. The time is now right for Redis to take on its natural role as the next successor.Everyone else here at Redis are Redis geeks like myself. We eat, breath and live Redis, and we’re absolutely certain that we’re just getting to the interesting parts of the first Redis decade. We want to make sure that Redis keeps defining the meaning of performance and scale delivered with simplicity. We are passionate about this, and we have the resources to help continue making it spectacular. Our company is of the Redis community and for the Redis community.So today, on behalf of the entire Redis team, I am delighted to welcome Salvatore Sanfilippo, Redis’ creator and community member numero uno, as he joins us. Together with Antirez and all of our fellow Redis community members, we’ll make Redis even better."
231,https://redis.com/blog/nosql-now-2015/,NoSQL NOW! 2015,"August 6, 2015",Itamar Haber,"It’s hard to believe that a full year had passed since the last NoSQL Now! event, and yet now we’re only a week away from this year’s conference! If the 2014 agenda made me feel like a kid in a candy store, then 2015 looks like a trip to Willy Wonka’s Chocolate Factory. I mean, look at this (partial) list of topics that I’ve lifted from the conference homepage – there’s something for everyone in there:Redis is proud to be a Gold Sponsor of the event, and you’ll be able to find us in our session or our booth. I find that the best sessions are the ones that mix theory and practice, so that is exactly the type we’re planning to give. In it, Andrew Spencer, Director of Technology at Stance, will present how the company architected its website for optimal user experience using aMEANR stack (MongoDB, Express, AngularJS, Node.js and Redis). Stance’s site is designed to be highly responsive to shoppers even under significant loads, so Redis is a natural choice for powering its unique online retail experience. Joining Andrew in this session will be Redis’ Co-Founder & CTO, Yiftach Shoolman, who’ll not only discuss use cases for in-memory NoSQL databases, but will also surprise you by explaining how these are, in fact, cost-effective.Our booth will be the usual deal – top notch-Redis Geek gear for the taking, and quite a few raffles. But more importantly, we expect to have a full team at the event, so if you want to learn more about Redis and how you can use it to build great applications with top performance, then please come over for a chat!Questions? Feedback? Email or tweet at me – I’m highly available 🙂"
232,https://redis.com/blog/join-redis-labs-in-seattle-for-containerday-linuxcon/,Join Redis Labs in Seattle for ContainerDay & LinuxCon,"August 12, 2015",Redis,"It’s no surprise that Redis has been one of the fastest growing databases over the last few years. But did you know Redis is tied with MySQL as the most popular database deployed in Docker containers? Earlier this summer, devops.com and ClusterHQ published a revealing container report, which called out that “MySQL and Redis virtually tied for first place” among customers using or planning to use containers.This comes at an opportune time. We’re sponsoring LinuxCon (including ContainerCon) and ContainerDay next week in Seattle and you can be sure we will be celebrating this tidbit of news. We will also be reaching out to future partners in addition to our existing partners. If you would like to partner with us to spread Redis in containers and beyond, please visit us in booth #216 on the 2nd floor or reach out to us via @redis.The first event at LinuxCon will be ContainerDay (free to all), which kicks off LinuxCon on Sunday Aug. 16th at 6pm at the Seattle Sheraton. There will be a mixer, lightning talks and even a container workshop led by Boyd Hemphill, one of the creators of the first ContainerDays. Also participating will be Cloudsoft and ClusterHQ, talking about Clocker and Flocker, respectively. If you want to find out what’s new in containers, ContainerDay will be a great way to start-off your week.The full LinuxCon & ContainerCon event will start the following day. It features a jam-packed agenda of over 200 sessions, a developer lounge, and activities such as The New Stack pancake breakfast on Tuesday and the Mars Data Challenge organized by on Thursday.If LinuxCon hasn’t sold out, you can get a 15% discount using code CCCD15 when you register. We hope to see you there!"
233,https://redis.com/blog/redis-wins-at-the-polls/,Redis Wins At the Polls!,"September 15, 2015",Leena Joshi,"Last week, users of NoSQL databases worldwide crowned Redis champion in G2 Crowd’s Grid. The Grid℠ represents the democratic voice of real software users, rather than the subjective opinion of one analyst. G2 Crowd rates NoSQL databases algorithmically based on data sourced from product reviews shared by G2 Crowd users and data aggregated from online sources and social networks.It was no surprise to us – Redis won hands down as the top scoring software in the NoSQL database category and # 4 overall among all databases.Top 5 things that should come as no surprise to any Redis users:Why does this matter?While RDBMS-es are well understood as a category, NoSQL technologies are still being digested by application developers, devops and operations. To make it even more confusing, vendors are adding multiple models to their databases, leaving customers confused about when to use which database technology.Redis makes it simple. Blazingly fast, highly stable and persistent, it is the #1 choice of users for a variety of different applications and use cases. The unique data structures in Redis, 180 or so commands, Lua scripting language all endow it with a versatility and performance that can’t be beaten. Whether you need high speed analytic processing (example: leaderboards in gaming), in-app social functionality (example: who’s following who and who you also follow), online session management (example: what’s a user doing, clicking, purchasing etc), pub-sub functionality (example: inventory notification to thousands of web clients), geo-spatial lookups (example: who’s close by), unique user counts (example: ranking articles by unique page views), high speed transactions (example: price/currency notifications) – Redis not only delivers it, but delivers it with fewer lines of code, lower latencies and higher throughput than anyone else.To quote the most read user review on G2 crowd:“I use Redis for a series of things, specifically a chat system and leaderboards for tracking site activity. I have also used Redis a ton in front of MySQL to serve as a more direct index in scenarios where queries are underperforming and the tables are so larger (50m+ rows) that trying to reindex means a chunk of downtime. I also use Redis for user sessions. I am also starting to migrate away from Memcached for basic key/value stores in front of MySQL.”The robust data structures that Redis offers are by far my favorite feature. It allows you to create hash tables to simulate more Mongo-like features as well as lists and sorted sets that can be used for queues, chat systems and leaderboards (basically anything you want sorted). Also, the option to flush to disk and have persistence is pretty awesome.A recent benchmark outlines how much of a no-contest it is when you compare Redis’ performance to other technologies – get it here!"
234,https://redis.com/blog/redis-keys-in-ram/,Redis Keys in RAM,"October 5, 2015",Itamar Haber,"Adapted from Dr. Seuss’ “Green Eggs and Ham”. Link to text, art copyrighted by Dr. Seuss.I am San.I am San.San I am.That San-I-am!That San-I-am!I do not like that San-I-am!Do you like Redis keys in RAM?I do not like them San-I-amI do not like Redis keys in RAM.Would you like them large1 or small?I would not like them large or small.I would not like them not at all.I do not like Redis keys in RAM.I do not like them San-I-am.Would you like them as a String?Would you serialize everything?I do not like them as a String.I do not like to serialize things.I do not like them large or small.I do not like them not at all.I do not like Redis keys in RAM.I do not like them San-I-am.<a</aWould you like them in a Hash?Would you like a Hash as cache?Not in a Hash. Not as a cache.Not as a String. No serialized, no anything.I do not want them large or small.I do not want them, not at all.I do not want Redis keys in RAM.I do not want them, San-I-am.<a</a<a</aWould you want them as a List instead?Do you want to access tail, body and head?Not as a List. Not as a Hash.Not as a String. Not as a cache.Small or large I will have naught.Goodbye San-I-am and thanks a lot.<a</aWould you? Could you? As a Set?Get the difference! Store a union! Or just intersect…I would not, could not, as a Set.You may like them.You’ll see for sure.You may likeSorted Sets by score?<a</aI would not, could not by a score.No more Sets! I say no more!I do not like them as a ListStop this now – I do insist.I do not like them as String or HashI do not like an in-memory database or cache.I do not want Redis keys in RAM.I do not want them, San-I-am.You do not like them. So you say.http://try.redis.io! Try them! And you may.Try them and you may, I say.San! If you will let me be,I will try them. You will see.<a</aSay! I like Redis keys in RAM!I do! I like them, San-I-am!So I will have them as a String.And as a Hash, a List or anything.And as a Set – both unordered and an ordered one.Say! Data structures are so much FUN!I do so like Redis keys in RAMThank you! Grazie, San-I-amI presented the above as part of my “Use Redis in Odd and Unusual Ways” talk at Percona Live Europe 2015 in Amsterdam. While mainly focused around MySQL, the conference’s program had no less than four sessions solely about Redis.Excluding my own. Being a Redis advocate, that’s probably the best thing I could hope for, but as a presenter it posed a challenge: how do I prepare a talk that is relevant to an audience that’s heterogeneous not only in its experience with Redis, but also with NoSQL at large?So the result is a mix of basic and advanced Redis topics that I hope both newcomers as well as Redis veterans can benefit from. The DR.ediseuss motif merits a little background: like many before me, when I took my first steps red-bricked road I was somewhat befuddled2 with the API’s naming scheme. While soaking it all in, I came up with a little “poem” that was but the first step to all that. For your enjoyment, here’s the first (and only) page from my “Dr. Seuss Reads Redis” book:This is my friendHis name is ZADDZADD’s a ladWho’s always SADDIt’s really badthat ZADD is SADDI don’t know whyAnd that makes me SCARDI hope that ZADDWill be someday gladAnd that he’ll get overThis stupid PFADDLawsuits? Critical acclaim? Email or tweet me – I’m highly available 🙂FN#1 Keys in Redis can be up to 512MB and are binary safe. Simple string values can be up to 512MB and are binary safe as well. Other data structures can hold 232 elements, each up to 512MB.FN#2 Note of encouragement to fellow beginners – it will soon fall perfectlyinto place and you’ll wonder what was so befuddling in the first place 🙂"
235,https://redis.com/blog/rlec-4-2-1-brings-more-granular-controls-to-high-availability-and-performance/,RLEC 4.2.1 Brings More Granular Controls to High Availability and Performance,"November 2, 2015",Itai Raz,"Redis Enterprise Cluster (RLEC) has been very successful since its initial release. It serves many customers’ production Redis deployments, enabling them to use the fastest database in the world with minimal operational overhead.Recently we released our 4.2.1 version of RLEC, which includes some very useful enhancements and features that customers have been asking for, such as:and many more…Below is an overview of some key updates, and you can read the full details in the release notes.RLEC includes a very useful feature called “Replica of” so you can define a database to be a replica (destination) of another database (source). Once defined and loaded with your initial data set, all write commands are synchronized from the source to the destination. This lets you maintain a database (destination) that is an exact replica another database.This configuration can be very useful, for example, if you would like to distribute the read load of your application across multiple databases. In addition, it can be used for a one-time synchronization of a database either within RLEC or from an OSS Redis.Your source and destination databases can even have different deployment architectures. For example, the source database can be a clustered (sharded) database, while the destination database can be a simple one-shard database.Now, the “Replica of” feature has been enhanced to allow cross region / cloud replication over WAN by allowing traffic compression. So now you can deploy two RLEC clusters in different regions or clouds, and enable replication between databases in these clusters. When defining the replication, you can decide whether you would like to use data compression, and control the compression level.In addition, the feature has also been enhanced to allow defining multiple source databases for a single destination database.Here is an architecture example that shows how multiple replicas in the same cluster can help scale read load, and a replica in another cluster on another cloud platform can handle remote reads or serve for disaster recovery purposes.We’ve also added support for multiple IPs per node. You can now define multiple IPs per node, and determine which are used for internal cluster traffic, and which are used for traffic from the client to communicate with database endpoints. By doing so, you can physically separate internal cluster management traffic from client-database traffic, allowing you to achieve better performance from your databases.Another nice feature that can greatly improve security is our support for IPv6 address types. The multiple IP addresses from above can be either traditional IPv4 type addresses, or newer and more secure IPv6 type addresses.Here is an architecture example that shows how multiple clients can connect to the same node using different IPs, or connect to a different node using IPv6. This is a great way to logically separate different client connections to a database, or to distribute the traffic.AOF (Append Only File) is a very useful persistence mechanism for Redis. When you use it, every write command to Redis is accumulated in the persistence file, so you can “replay” all these commands if you ever need to restore the data from persistence. As you can imagine, this file can get big over time, which is why there is an AOF rewrite mechanism, which can be triggered every once in awhile to reduce the AOF file size.It is a great mechanism, but at times can be less than optimal and cause overhead on the storage system — delaying Redis execution and slowing down your database.In this release, we have made various improvements to the AOF rewrite mechanism, and also introduced the concept of AOF SLAs in which you can define AOF file size and load-time thresholds to control when AOF rewrites are triggered.In addition, we added AOF-related alerts that help identify disk space and degraded disk I/O performance issues.We have many customers running RLEC in their own data center over their own network, but we also have many who run RLEC on public cloud platforms, where network stability is far less reliable. In a public cloud environment you would rather wait a bit before declaring a node as down because it might just be unresponsive due to a temporary network glitch that could be resolved after a few seconds. On the other hand, in a stable on-premises or private-cloud environment you’d likely prefer to immediately declare a node as down and trigger auto-failover as soon as the event happens.To allow customers to optimize their deployments no matter what environment they are running on, we have introduced the concept of environment profiles. You can now choose which profile you are running on, and RLEC will adapt itself to provide optimized performance and high-availability assurance in accordance with your specific environment.As indicated above there are many other interesting and noteworthy features and improvements in this release so please make sure to download the new release from our downloads page, and review the release notes.We are now working hard on more great features for the next release — stay tuned.If you have any questions or feedback don’t hesitate to reach out to me at: itai.raz@redis.com."
236,https://redis.com/blog/3-critical-points-about-security/,3 Critical Points about Security,"December 23, 2015",Itamar Haber,"Over the recent weeks there’s been an increase in the number of reports about NoSQL breaches in general, but also specifically in those relating to Redis. The latter is, in all likelihood, the aftermath of Salvatore Sanfilippo’s blog post “A few things about Redis security.” That particular post was only the spark that lit the fire – I’d argue that the seeds for the breaches were laid in the ground long ago. And they’re still there unless you do something about it. But before continuing with the story, there are 3 critical points and 1 important note that I want you to take away from this:A note: if you’re using the Redis Enterprise Cluster or Redis Cloud, then you can rest assured that the servers have not been breached by this attack. While it is possible to create an unprotected Redis database with our solutions, the overwhelming majority of our users’ databases, direct and from PaaS partners alike, are using at least one of the security measures we provide (such as password protection, source IP/Subnet whitelists and SSL). Furthermore, because our solutions provide separate operational and functional interfaces, even an unprotected database is not vulnerable to the full extent of this type of attack. That said, if your database is unprotected then your data is still at risk and you really should do something about it (hint: set a password).Back to the origins of the breaches. In his blog post, Salvatore summarizes Redis’ security model with his typically brutal honesty: “it’s totally insecure to let untrusted clients access the system, please protect it from the outside world yourself.” To demonstrate how insecure “totally insecure” actually is, Salvatore shows how an unprotected Redis database can be used to gain access to the server running it.The important thing here is that if you’re using Redis you must not ignore security. Once Redis isn’t run in a sandboxed environment, it is up to you (or your Redis provider) to take the steps needed to secure it properly. Redis databases can be made appropriately secure – here is a documentation page to get you started on doing just that. The first step to securing your Redis is reading the documentation. Redis has great official docs (it’s not just me saying that – check around) and tons of other materials online.Redis’ default password is set to none. Make sure to set it and set it to something non-trivial. Your security is only as good as the precautions you take.The second step is really paying attention to the defaults. Trusting default values to work for you is like driving your car with your eyes closed and trusting the road to bend to your will. Unless the road is one that you fully control and have memorized, there’s little chance that you’ll get where you wanted to go. You would be wiser to keep your eyes open. When it comes to defaults, the only safe way to use them is know about them, which means – you guessed it – opening your eyes and reading their documentation or hiring a chauffeur.Salvatore’s blog post was received only too well by the internets, initially inspiring “script kiddies” to crack unprotected servers for fun but eventually also being used for profit by professional cyber criminals. There also appears to be a white hacker (artist’s rendition) who sets passwords for unprotected servers. It is a shame that so much damage has been done but unless made public, this “vulnerability” would have stayed unnoticed by most and its consequences unknown. It wasn’t meant as a lesson, but we should learn from it not to take security for granted.Future versions of Redis will include safer defaults (the release candidate for v3.2 that was released earlier today binds to 127.0.0.1 instead of 0.0.0.0 and RC2 will have a new protected-mode enabled by default) and perhaps even improved security mechanisms, but the responsibility for protecting the database will always be with the operator. If you’re operating your own Redis database in an environment that’s open to the outside world, you should review the security measures that you have in place and ensure that you’re protected. If you were using none, I recommended that you treat your server as suspect at the very least (nuke it and start from scratch if possible) – it is not unlikely that it was already breached."
237,https://redis.com/blog/february-8th-outage-post-mortem/,February 8th outage – post-mortem,"February 10, 2016",DevOps Team,"On Monday February 8th 2016, at 17:45 UTC, we experienced a major outage in one of the Google Cloud Platform clusters that we employ in providing the Redis Cloud service. By 19:30 UTC we were able to bring the service back to over 95% of affected users that have the following suffix pattern in their databases endpoints: us-central1-1-1.gce.garantiadata.com, and had it completely recovered by 20:38 UTC.Redis Cloud clusters are built in a Docker container-like fashion – the infrastructure consists of bare-metal or virtual servers, on top of which we deploy our cluster tools. Once deployed, each cluster can host multiple databases, with each such database running in a fully secure and isolated manner. Our clusters are fault tolerant so they are able to withstand a single node’s failure and recover from it without interruption to the service. The day before yesterday, however, regrettably was different as a bug introduced by a software update caused multiple nodes in the same cluster to fail simultaneously. Due to the failure’s nature, we had to resort to manual recovery of the cluster.The following is the outage’s timeline:While words are of little consolation, we sincerely apologize for this outage and are acutely aware of the impact it had on many of our customers. All of us here at Redis are devoted to providing the best Redis-as-a-Service solution and we’ll spare no effort in fulfilling this mission. Due to this outage, despite it being the first of kind for us, we’ve taken immediate steps to ensure that such incidents do not recur. Specifically, here are the action that we’ll be taking in the immediate short term:"
238,https://redis.com/blog/money-money-money-how-to-save-it-good/,Money! Money! Money! – How to Save it Good!,"April 25, 2016",Rod Hamlin,"Redis, as we know, is the world’s most popular in-memory NoSQL and rapidly becoming a default ingredient of most modern application stacks. Modern application architects use it to help their applications scale to humungous volumes of traffic at sub-millisecond latencies –with very little hardware. This blog is about saving even more money with IBM’s RapidBuild Program for POWER8 systems. Today, Avnet announced that it is the first distributor to roll out this program, with pre-installed Redis Enterprise Cluster (RLEC) on IBM Power Systems.So where’s the money savings part? Well, if you’re running Redis to accelerate your application, whether its processing high speed transactions or speeding up reporting and analytics, and you plan to extend it further to other uses, IBM POWER8 offers formidable horsepower, in terms of number of virtual cores per server, that allows you to run higher numbers of Redis instances on a single server. This helps increase throughput by 67% while keeping latencies under 1 ms. In fact in the below comparison of IBM POWER8 with standard x86 servers, you can see that the maximum throughput of RLEC on IBM POWER8 is so much higher, that on a per transaction basis, you end up with 67% higher throughput at 54% lower costs.With RLEC, you also get the capability to run Redis on Flash memory used as a RAM extender. Flash memory is 10 times cheaper than RAM, and with a judicious mix of RAM and Flash, you can still get hundreds of thousands of operations/min with sub-milllisecond latencies. IBM’s innovative CAPI interconnect, unique to POWER8, allows you to bypass I/O latencies by connecting flash memory directly to the main processors, resulting in a gain of 200% in terms of throughput at 70% lower costs. Not only will you gain increased performance and cost savings POWER8 technology, you can reduce your server footprint from 24 to 1 with POWER8 and CAPI.1Given these amazing numbers, of course, we are more than pleased to be included in IBM’s RapidBuild program, which aims to deliver POWER8 systems with pre-installed software that customers are most likely to need. As Avnet rolls this out today, we are delighted to be putting these cost savings and rapid time to value at the fingertips of Redis users worldwide.1 24:1 system consolidation ratio (12:1 rack density improvement) based on a single IBM S824, (24 cores, POWER8 3.5 GHz), 256GB RAM, AIX 7.1 with 40 TB memory based Flash replacing 24 HP DL380p, 24 cores, E5-2697 v2 2.7 GHz), 256GB RAM, SuSE Linux 11SP3 . Inbound network limits performance to 1M IOPs in both scenarios, equal capacity (#user, data) in both cases. x86 cost includes 10k$ for 2x 1U switches"
239,https://redis.com/blog/redis-cloud-integrates-with-databricks-spark/,Redis Cloud Integrates With Databricks Spark,"June 6, 2016",Itamar Haber,"Announcements are usually made about past events, while they are happening or about future ones. Today’s announcement of the integration between Databricks’ Spark service and Redis’ Redis Cloud is sort of a mix of all three types. It should be obvious why it falls into the third category, so I’d like explain what had transpired and is still happening that had led to this. Since you’re reading this blog, I assume that you already have the relevant background in data processing, so I’ll skip the product introductions and jump right into the middle of the matter at hand.The relationship between Spark and Redis may not be immediately apparent, although both have put performance at scale as their key values. The purpose of each seemingly lies at opposite ends of a spectrum – the former is focused on processing data whereas the latter on serving it. But the fact that both are data-centric technologies is that which drew the two together despite, or perhaps because of, this polarity. While each excels in its domain, it is combining them that seemed to yield the greatest rewards – as each solution evolved and matured independently, so did the body of evidence from users of the potential synergies based on real use cases. Which led to the inception of the spark-redis connector.The connector’s purpose is providing a way for moving data between Spark’s and Redis’ native data structures. Its initial release was aimed at providing that connectivity, while conserving a primary design principle shared by both Spark and Redis: the ability to use simple building blocks for composing something much more powerful. Once that bridge which allows transforming data between Spark’s RDDs and any Redis data structure was ready, we were able to use it for showing the effectiveness of that combination.We’ve just released the new version of the connector, v0.3, that brings full support for Spark SQL but even without this new feature, the connector has been steadily gaining traction and is being put to good use (assuming that GitHub’s stars are a solid indicator :)).Nothing, however, had quite prepared us for the amazing demo that Databricks’ Reynold Xin had shared during last month’s RedisConf. This demo is a pure learning experience, and paraphrasing on a quote from Douglas N. Adams, a learning experience is one of those things that says, “You know that thing you just did? I didn’t know you could do that with Spark, Redis and the spark-redis connector!”.The demo notebook starts by showing how a table is stored in Redis using a couple of Hash data structures. Once the data’s in place, the RDDs from Redis are used to create the respective DataFrames, which in turn are registered as tables. That’s basically less than 10 lines of code and that’s all it takes to start doing bona fide SQL queries with joins on the data in Redis. In the demo’s second part, Reynold shows how to perform textual analysis on the data with SparkML learning and ends by storing the models stored in (which can be consequently served from) Redis. That’s short, elegant and to the point – process in Spark, serve from Redis.Which brings us back to the beginning – if you’ll scroll back to the demo notebook’s beginning, you’ll see that it begins by manually setting up the properties for connecting to Redis Cloud. We’ve published this notebook that explains how to setup a Redis Cloud database and obtain these connection properties. Eventually the integration will allow users of both products to easily use their existing accounts from both platforms. In practical terms this means that you’ll be able to spin up a Databricks Spark cluster from the console of your Redis Cloud database to immediately process the data in it, and at the same time you’ll be able to provision and use Redis databases directly from your Spark workspace. Once ready, the integration will make connecting the two services as simple as clicking a button, literally. Questions? Feedback? Email or tweet me – I’m highly available 😉"
240,https://redis.com/blog/first-ever-redis-modules-hackathon/,First-Ever Redis Modules Hackathon,"December 7, 2016",Itamar Haber,"Every day is a special day, but last Friday was a really special day because of the release of the first release candidate for Redis v4. The new version packs a lot of new stuff, although it’s most exciting addition by far is the new modules API – it allows just about anyone to go right ahead and make something more useful with Redis. It literally means endless possibilities and is the reason why the Sunday before was also an extra special day.Last week’s Sunday was the submissions’ deadline for the Redis Modules Hackathon by Redis – a friendly competition for developers around the theme of the new API. The hackathon ran for 42 days (yep), and was open for anyone to participate online. We also hosted physical onsite sprints in Tel Aviv and San Francisco for participants in these regions. The guidelines for submissions were intentionally open to interpretation – basically anything to do with modules was legit. And just in case that the pure intellectual satisfaction of developing modules wasn’t motivating enough, we had also put out substantial monetary prizes to the winning projects 😉The winners were picked from the list of 17 finalists by a panel made up of five judges of the Redis community: Salvatore Sanfilippo, Pedro Melo, Francois-Guillaume Ribreau, Dvir Volk and myself. Every judge independently evaluated and ranked each of the submissions on a scale from 1 to 10. Among the criteria for evaluation were the project’s originality, its potential impact, the design and the overall quality of submission. The scores given by the judges were then summed and the entries possessing the highest scores were chosen as winners.The scores will not be made public, but I can share an interesting observation about them: although the each judge used his own judgment in evaluating the submissions and assigned them with scores based on his personal weighing methodology and preferences, the resulting order imposed by the scores between all judges’ was remarkably similar. Once the scores were tallied it was an extremely close race, where the top-scorers differed by only a fraction of a point.Without further ado, I’d like to congratulate our prizewinners and present you with their projects:The first place is awarded to Brandur Leach for his redis-cell module, which impressively delivers on several fronts. Primarily, redis-cell is a rate limiter that can readily be used to conserve the system’s resources and protect it against unexpected loads. While rate limiting is a common use case for Redis and a seemingly simple problem, the correct implementation of the solution may “take a few tries to get right”. Using the module is dead simple as it exposes a single new command, the output of which can be conveniently used for constructing the appropriate (HTTP) responses. But the module has another aspect that’s probably just as useful as its intended function – it is written using the systems programmers’ shiney new toy, Rust. And while the choice of programming language alone does not merit points by its own accord, Brandur had actually laid solid foundations for what could be a stand-alone library (or crate if you will) that acts as a bridge to Redis modules written in Rust.Second prize is given to RedisModuleTimeSeries submitted by Noam Sagi. The module introduces a new data type that’s used for aggregating time series data, and can be used in either single or streaming modes. Noam was motivated to develop the module to achieve optimal performance in analyzing structured, mostly numerical data. The author claims that other solutions built using state-of-the-art frameworks are less effective when it comes to processing data within predefined numerical ranges, and provides a benchmark showing orders of magnitudes improvements in performance when compared to Elasticsearch.The third place goes to the to Roi Lipman in recognition of his Redis Graph module, which is a graph database that’s stored in Redis’ native Sorted Set data type. This neat trick is done using the approach described in Representing and querying graphs using an hexastore, but what really makes this submission shine is its choice of Neo4j’s Cypher as query language. While still young, the project has considerable potential and we hope that Roi will continue developing it and contributing to the Redis community.The winner in our San Francisco regional challenge is Nick Chadwick who had authored the Redis Zstandard Compressed Values module. Given its comparatively high price and the implications of running out of it, RAM is always a concern for Redis users. Nick’s module attempts to alleviate that by employing Facebook’s new-ish library for compressing the data as it is store. It runs multiple threads for performing the actual work and can be used with a custom dictionary to improve the compression rate of domain-specific data.The Tel Aviv regional prize is given to Doron Sadeh for his Pyrecks module. Pyrecks allows executing Python kernels – a term for describing small computational units – in a Redis sandbox. Each kernel is run encapsulated in its own thread and the kernels can be chained one after the other, the former’s output serving as the latter’s input, to implement even more complex processes. Pyrecks had started as a proof of concept at the Redis TLV Meetup, but was developed since then to full alpha status in order to be submitted. It is an awesome demonstration of modules’ potential to extend Redis by bridging it with other languages and its “Future Development” plans suggest that great things are to come.Although we ran out of prizes to give, there are still a few submissions that deserve special recognition:I’d to sign off this post by thanking everyone who took a part: the developer teams who participated for your hard work and invaluable contributions, the HackerEarth staff for helping to orchestrate everything and, of course, the Redis team. It is because of you that this event came out so successful, and I’m certain that is only the first of many more to come. In every competition there are only a few winners, but in this case the real winner is every user in the Redis community.If you’d like to develop your own Redis module a good place to start is this blog post, and we also have a mailing list for module developers at https://groups.google.com/forum/#!forum/redis-module-devs. As usual, feel free to tweet or email me – I’m highly available 😉"
241,https://redis.com/blog/redis-pack-v4-4-release/,Redis Pack v4.4 Release,"January 20, 2017",Kirk Kirkconnell,"We are very excited to announce the release of Redise Pack (RP) v4.4 (formerly Redis Enterprise Cluster or RLEC), which delivers several new features, improvements and bug fixes. RP 4.4 includes numerous functionality and performance enhancements that customers have been eagerly awaiting!Redise Pack v4.4 features the following major enhancements:Let’s get a quick overview of some of these major enhancements in version 4.4.As a developer, you may want to utilize different consistency and durability levels for each call to the database. For example, for one call you might need to make 100% sure it gets to a replica and down to disk, so you are willing to wait for it. However, for the next call, you may need the speed and cannot wait for the cluster to do its part.With the support of the WAIT command in RP v4.4, you can control consistency and durability guarantees, by operation even, for a replicated and persisted database across an RP cluster.See “Tunable Consistency and Durability” in the RP documentation for more detailed information.Redise Pack v4.4 now supports the popular geo commands, making it easier for developers to use Redis for location-based data processing and analytics. It also supports BITFIELD, which allows for arbitrarily sized counters to be implemented with maximum memory efficiency, and is particularly useful for real-time analytics. These additional commands make it possible to develop richer applications with Redise Pack. RP v4.4 supports Redis 3.2 databases and the commands introduced in 3.2, specifically:We all need more uptime, fewer single points of failure and improved HA capabilities from our databases. Multiple Active Proxies can provide all three benefits by allowing your existing Redis clients to connect and use (on a round-robin basis) multiple RP proxies. This can improve throughput and HA capabilities for your database.See “Multiple Active Proxies” in the RP documentation for more detailed information.For those using Redis for larger datasets, Tunable Consistency and Durability is a cost-effective tool that improves performance by spreading your Redis databases across both RAM and Flash Storage (e.g. SSD, FusionIO). Objects that are used frequently are kept in RAM by RP and objects used less frequently are relegated to Flash storage. You can tune hot-to-warm values in RAM to achieve your ideal application needs without requiring RAM for colder data. Version 2 of Redis on Flash improves performance and reliability and is provided as a preview to customers who want to try the new version of the product in test environments. Production ready version will be available in the next few months.With more and more companies making security and separation of duties a priority, RP 4.4 introduces Role-based Administration, allowing you to assign a role for each administrative user.For example, you can give a user the ability to see and read database logs while prohibiting them from viewing cluster level resources or editing any settings.See “Security Roles” in the RP documentation for more detailed information about this feature and what permissions can be granted.You can download RP 4.4 today from our Downloads page and find more details on this release in the release notes and documentation."
242,https://redis.com/blog/2016-wrap-up-redis-labs-answers-the-demand-for-redis-enterprise/,2016 Wrap-Up: Redis Labs Answers the Demand for Redis Enterprise,"January 23, 2017",Jason Forget,"Since its release in 2009, Redis has blazed a new trail in the database world, allowing users to solve complex problems through optimized data structures and commands, executed in-memory very fast and with great simplicity. As Redis’ reputation and popularity have grown—along with demand for enterprise-class capabilities (which are commercially developed and maintained by Redis)—we’ve begun to see a significantly accelerated adoption of Redis Enterprise among the largest companies in the world.In fact, in 2016 alone, Redis brought Redis Enterprise to five of the top 20 U.S. banks, two of the top three telecommunications companies, and more than 20 Fortune Global 500 corporations. All in all, more than 1300 enterprises adopted Redis as mission-critical components of their operations last year—including Groupon, Intuit, LifeLock, TD Bank, Verizon, Hipchat, DBS, RingCentral, Twitch, and Menards. And let’s not forget (as if we would) the other 6,700 enterprise customers —and 60,000 users—we service around the world.Redis’ dramatic leap into the mainstream is due in no small part to the many strategic partnerships we cultivated in 2016. Global leaders such as AWS, Microsoft Azure, Intel, Samsung, IBM, Accenture, and Wipro—all important contributors in today’s application development space—partnered with Redis to bring the advantages of enterprise Redis to their customers. These partner relationships have expanded not only Redis’ reach, but also its capabilities as we worked to optimize and enhance Redis across new use cases that our entire partner ecosystem could leverage.Redis Enterprise’s exploding popularity in 2016 is also reflected by a string of other achievements and milestones. Attendance at our annual RedisConf increased by an impressive 400%, with high-profile customers and industry experts from Apple, Google, GoPro, Netflix, and Rackspace (to name a few) dropping by to lead discussions on the trends driving database decisions. And at the recent AWS re:Invent conference in Las Vegas, Redis hosted a well-received session that featured panelists from Groupon, Intuit, and LifeLock to illustrate how large enterprises use Redis to solve their big data database challenges.Strengthening our conviction—and that of the marketplace—that 2016 was a breakout year for Redis enterprise offerings, two of the leading independent analyst firms, Gartner and Forrester Research, weighed in on the matter. Each named Redis a leading database provider in their respective 2016 Operational Database Management Systems (ODBMS) Magic Quadrant and The Forrester Wave™: Big Data NoSQL reports.A huge thanks to everyone that helped make 2016 Redis’ best year yet! It’s a long list that includes our innovative team members, inspiring customers, supportive partners, and the highly engaged Redis developer community.Of course, Redis has no intention of resting on the achievements of 2016. We’re energized and raring to go with a brand new London office; new, larger headquarters in Silicon Valley; and plans to fill up both by more than doubling the headcount of our sales and customer success teams so that we can continue providing the best customer experience in the tech industry.We look forward to working with you and your enterprise in 2017! Rest assured, you’ll be in excellent company."
243,https://redis.com/blog/redisday-tlv-2017/,From Tel Aviv w/ <3: Redis Day TLV 2017,"March 6, 2017",Itamar Haber,"Last month, Redis geeks and geekettes received the perfect Valentine’s gift: a day packed with sessions about everyone’s favorite in-memory database. Sponsored by Redis and hosted at the Tel Aviv Talkhouse, the event was a huge success, with over 200 attendees showing up and staying all the way until SHUTDOWN.Footage of the sessions and presentations is available so you can absorb everything that went on at your own leisure. There’s also a Twitter moment and a bunch of photos here. For a recap, read on.The day consisted of no less than thirteen sessions spanning a wide spectrum of topics and presented by Redis users and developers. We kicked off with a keynote by Redis’ creator and developer, Salvatore antirez Sanfilippo, who was visiting our local offices that week. In his session, Salvatore shared some of his philosophy and the near future plans for Redis.Next on stage was Ishay Green, co-founder, and CTO at Spot.IM, a company that uses Redis as its primary database and powers the real-time communication between millions of users across thousands of website. Ishay delivered his entire presentation without the use of slides and live-demoed everything–ballsy move!Following Green was Cihan Biyikoglu, our VP of Product Management, who presented the company’s roadmap. In his talk, titled “Redis for the Enterprise,” Cihan presented Redis’ vision for our product lines and demonstrated how the open-source and commercial offers complement each other and meld together. Then it was time for a coffee break.We resumed the sessions with a brilliant presentation on Multi-Master Replication in Redis from the Redis Architect, Yossi Gottlieb. Over the last year, Yossi and the Redis Labbers have been implementing a new way for replicating Redis databases. This new mechanism leverages Redis modules and is based on Conflict-free Replicated Data Types (CRDTs) for consensus-free and strong eventual consistency replication. Watch his presentation and prepare to have your mind blown to pieces and then put gently back together.Back already? Good, because next on our agenda we had Dvir Volk, Senior Architect at Redis, with a guided tour through the rapid rise of Redis modules: their conception, the modules’ API evolution, early iterations, the full text search module RediSearch, a module for serving machine learning models, and much more.The last talk before lunch was given by Eitan Shapiro of Videocites, a company that offers a SaaS platform that performs a video-by-video search online, enabling content owners to track and monetize their properties. The service relies on RediSearch for indexing and querying millions of fingerprinted videos in real time and at low cost.Our afternoon track began with Datorama‘s Udi Kidron, who recounted the company’s recent migration of its centralized datastore to Redis from Hazelcast. Such migrations are far from trivial, especially given the company’s focus on data analytics and the criticality of its datastores. But with the team’s remarkable ingenuity and the assistance of the Redis community, it appears that it was pulled off expertly.The next presentation was about RedisGraph, a module that implements a graph database and supports a subset of neo4j’s Cypher query language. The developer, Roi Lipman, started the project in his spare time and submitted it to the first ever Redis Modules Hackathon. He won a prize for it and we made him a job offer that he couldn’t (and didn’t) refuse.Eyal Altshuler from Stratoscale presented how the company’s core product, Symphony, can be used for deploying and managing Redis OSS clusters in the Hybrid cloud. The proprietary solution is not only offered to their customers, but the team is actually “dog-fooding” and uses Redis for managing the platform’s object store.Adam Lev of Tamar Labs presented a use case in which Redis was key to scaling a “Big Data” application for stream processing. With 20M concurrent active user events and thousands of active sessions, it had to be fast.Another Redis use case was delivered by Omer Anson of Huawei. In order to develop a distributed cloud network that is both open source and vendor-neutral, Huawei used Redis as the northbound database on each node of Dragonflow. The presentation also included several benchmark tests that demonstrated the configuration’s scalability.The case study about WeAreTV, a startup that connects the audience with TV shows in real time, was presented by Nimrod Ticozner. Deployed on Google’s Cloud Platform, the company operates Redis clusters in multiple environments. These are used for tracking user sessions and maintaining various metrics’ counters.You usually save the best for last, but in this case, all that was left was me. I took the stage and presented WeAreTV, a module we’ve developed that provides a native JSON data type in Redis. I did a stupendous job developing it and delivering the preso, as usual, and upon ending I unlocked the doors and let everybody finally leave ;).It was a marvelous day, and we’d like to thank everyone involved, both the amazing Redis community who attended in unprecedented numbers and the speakers who kept them glued to their seats. We’d also like to apologize to all the people who didn’t make it past the waiting list. We promise that next year it’ll be bigger and better (the event, not the waiting list)! Feel free to reach out to me with anything via email or Twitter – I’m highly-available!"
244,https://redis.com/blog/redis-labs-and-mesosphere-dcos-join-forces/,Redis Labs and Mesosphere DC/OS Join Forces,"March 14, 2017",Cassie Zimmerman,"As infrastructure diversification becomes more relevant, the tools developers need in order to build truly scalable and professional-grade software must evolve. Today marks a big step in that evolution, now that Redis Enterprise is available on the DC/OS (Data Center Operating System) platform.When first developed in 2009, open-source Redis quickly gained popularity amongst users for its incomparable high performance and ease of use. It was, and still remains, the most powerful and unique database on the market. Today, hundreds of thousands of users are building and deploying complex big data applications on top of Redis.Redis Enterprise (Redise) enhances open-source deployments with a technology layer that makes scaling effortless and transparent to the user. Redise also adds unmatched resilience, with high availability that protects against every failure scenario and is benchmarked to recover within seconds without losing data. Performance optimizations within Redise ensure that applications that use it achieve flawless high performance under any load. Today Redis supports over 60,000 customers globally, and is the first choice for enterprise Redis deployments both on-premises and in the cloud.Mesosphere is gaining popularity as the provider of DC/OS, a platform that makes running containers, data services, and microservices easy across your own hardware and cloud instances. DC/OS is based on Apache Mesos, and is trusted by Verizon, Autodesk, Charter, Esri and many other Fortune 1000 companies.Just this week, Mesosphere released DC/OS v1.9, a cutting edge, infrastructure-agnostic platform for building, deploying, and elastically scaling modern applications and big data services. We are thrilled to have been approached by the Mesosphere team, requesting Redis be one of their flagship partners for this launch. By enabling Redise Pack on DC/OS, users gain high performance, high availability and seamless scaling of Redise with Mesosphere’s sophisticated DC/OS technology, providing the premier platform for elastically scaling big data applications. Redis joins other popular services available in the DC/OS ecosystem, including Spark, HDFS and Kafka. DC/OS allows you to easily run containers, Redise Pack instances and micro-services across your hardware and cloud instances.As the adoption of Redis continues to accelerate, users choose Redis to ensure enterprise-grade scaling and availability for their mission-critical applications. Now with the availability of Redis within Mesosphere DC/OS, customers have more choice and flexibility in how they deploy Redise. Together, we are powering digital transformation and solving the complex data issues that modern enterprises must solve.Additional resources:To learn more about Mesosphere DC/OS v.1.9, visit here To learn more about running data services on DC/OS, see here Register now for the Mesosphere and Redis technical webinar on May 9"
245,https://redis.com/blog/high-speed-transactions-demand-world/,High-Speed Transactions for an On-Demand World,"March 30, 2017",Roshan Kumar,"When I was shopping last week, I noticed a significant difference between the checkout times of Walgreens and Safeway. Both were quick to scan and bag my items, but Walgreens was able to process my chip-based credit card in a second, if not less. I pushed my card inside the reader, and immediately heard a beep. At Safeway, validating my card took about 10 seconds. Though 10 seconds doesn’t seem all that long, imagine if you were waiting in line behind eight people, all paying with credit card– just the payment processing alone would add about 80 seconds to your wait time!For retail stores, improving operational efficiency is crucial to their success. A recent article published in the Harvard Business Review validates this claim.  The article quotes a study that  examined the financial data of 37 U.S. retailers, and found that those who focused on operational improvements were the most successful. Kroger, for example, implemented hi-tech solutions that helped them reduce the average customer wait time in the checkout line from four minutes to 26 seconds. Kroger cites this as one of the main factors that contributed to its 50 successive quarters of revenue growth.Real-time, high-speed transactions have become a necessity for most businesses in this on-demand era. When we go down to the database level, the term “transaction” can refer to any system of record (not just financial in nature) where the data or activity is recorded permanently. High-speed transactions are in demand in many industry verticals. Here are a few areas  that require high-speed transactions in specific industries:Retail: Payment processing, inventory management, sourcingFinancial: Trading, cash transfers, disbursement, claim managementE-Commerce: Order processing and fulfillment, payment processingEntertainment: Entitlement, digital asset managementTransportation: Inventory management, dispatchTravel and leisure: Reservations, inventory management, payment processingMany Redis customers leverage the Redise Platform to power high-speed transactions for their applications. To meet the growing demand for more information about managing high-speed transactions in Redise, the Redis team has published a technical white paper on the topic, available here: https://redis.com/docs/managing-transactions-redis/."
246,https://redis.com/blog/redis-flash-double-performance-nvme/,AWS I3 instances are x2.6 faster and 80% cheaper with Redis Flash,"April 5, 2017",Redis,"Recently, AWS announced the availability of I3 instances across 15 different regions. We were happy to be a part of their I3 instances beta program and used our own Redis Enterprise Flash (Redise Flash) technology to extensively test and benchmark the new instances before they were formally launched. The I3 SSD storage is based on NVMe technology, which theoretically should provide significantly higher throughput and much lower latency than the previous generation I2 instances (which are based on SATA SSDs).Before I get into the results of our benchmark, I’ll first cover some quick background about Redis and Redise Flash. Redis is known for its extremely fast performance serving datasets entirely from RAM. However, RAM prices have remained flat during recent years, and deploying large datasets may prove to be expensive and may not fit all business models and economics (not to mention the cases where one or more replicas are needed for high-availability).Redise Flash solves this problem by storing Redis’ keys, dictionary (the main data structure behind the keys), and “hot” values (the most accessed values) in RAM, while “cold” values (the least accessed part of the dataset as identified by the LRU algorithm) are kept on Flash (the technology behind SSDs). Distributing the data this way guarantees that ongoing operations are executed almost as quickly as Redis running entirely on RAM.This architecture is mainly designed for use cases where the working dataset is smaller than the total dataset (which is the most common scenario), as it allows Redise Flash to maintain a similar performance to that of RAM while dramatically reducing the server infrastructure costs. Redise Flash is fully compatible with open-source Redis and incorporates the entire set of Redis commands and features. Flash is treated as a RAM extender, and does not replace the existing data-persistence mechanism of Redis. With all of this in mind, let’s look into our latest performance tests on AWS.What We BenchmarkedWe compared the performance of Redise Flash over three AWS instances:Test ParametersWhat We FoundThe two graphs below show the 100B and 1000B test results:Performance ImprovementThe table below summarizes the average improvement factor of the i3 over i2 instances across all tests:The results above show that the new I3 instances are truly an improvement over I2 as demonstrated by Redis’ Redise Flash technology.Savings Per OperationAWS I3 instances are not only faster than AWS I2 instances, they are also much cheaper. For example, the cost of an on-demand Linux i3.8xlarge instance in US East North Virginia region is $2.496/hr versus $6.820/hr of i2.8xlarge, that’s 73% cheaper!When we calculated the savings per RoF operation on an i3.8xlarge instance, we found the following:Note: We only compared the i3.8xlarge and i2.8xlarge instances as both have similar RAM and SSD capacity.SummaryAs expected, we found that Redise Flash runs up to 2.6 times faster on AWS I3 instance than it does on AWS I2 instance. We also found that due to the relatively attractive cost of I3 instances, each Redise Flash operation is up to 5.4 times cheaper on an I3 instance than it is on an I2 instance.Appendix:Benchmark setupAWS I3 and I2 instances configurations:Detailed pricing can be found on the AWS website."
247,https://redis.com/blog/docker-redis-enterprise-pack-developing-redis-applications-windows-macos-linux-containers/,"Docker and Redis Enterprise Pack – Developing Redis Applications on Windows, MacOS or Linux with Containers","April 12, 2017",Cihan B,"We are excited to announce the preview release of the new docker image for Redis Enterprise Pack.Redis is the most popular database used with Docker containers. Redis Enterprise Pack extends open source Redis and delivers stable high performance, linear scaling and high availability with significant operational savings.The new Redis Enterprise Pack image is available on Docker Hub.Docker brings a great deal of benefits when working with Redis Enterprise Pack. Containers help scale-minimize Redis Enterprise Pack and fit it right into your development environment. You can run a full cluster locally on your Windows, macOS or Linux host.You can use Docker to run Redis Enterprise Pack container in MacOS, various Linux and Windows-based machines. Getting started is simple:Step 1: Run the Redis Enterprise Pack containerdocker run -d –cap-add sys_resource –name rp -p 8443:8443 -p 12000:12000 redis/redisStep 2: Setup Redis Enterprise Pack clusterSimply visit https://localhost:8443 on the host machine and follow the setup instructions.Step 3: Create a Redis databaseCreate a Redis database on port 12000 – Click on advanced options to set the database port.Step 4: Connect to your database using redis-clidocker  exec -it rp bash# sudo /opt/redis/bin/redis-cli -p 12000# 127.0.0.1:16653> set key1 123# OK# 127.0.0.1:16653> get key1# “123”A container image represents a single node of the Redis Enterprise Pack cluster. Each container instance can run multiple open source Redis shards to provide seamless scaling. Redis Enterprise Pack Proxy is a high-speed process that scales all connections from Redis applications to the cluster while improving latency and throughput. The Cluster Manager governs and constantly monitors the cluster of Redis Enterprise Pack nodes, and provides efficient multi-tenancy architecture to reduce effects of noisy-neighbours. Redis Enterprise Pack also comes with a simple visual UI for administration, alerting and monitoring over HTTPS.When deploying Redis Enterprise Pack using Docker, there are a few common topologies:You can find more detailed information in our documentation on Docker."
248,https://redis.com/blog/adding-search-engine-redis-adventures-module-land/,Adding a Search Engine to Redis: Adventures in Module-Land,"October 5, 2016",Dvir Dukhan,"TL;DR: We’ve utilized the powerful Redis Modules API to build a super-fast and feature-rich search engine inside Redis, from scratch — with its own custom data types and algorithms.Earlier this year, Salvatore Sanfilippo, the author of Redis, started developing a way for developers to extend Redis with new capabilities: the Redis Modules API. This API allows users to write code that adds new commands and data types to Redis, in native C. At Redis, we quickly began experimenting with modules to take Redis to new places it’s never gone before.One of the super exciting, but less trivial, cases that we explored was building a search engine. Redis is already used for search by many users via client libraries (we’ve even published a search client library ourselves). However, we knew the power of Redis Modules would enable search features and performance that these libraries cannot offer.So we started working on an interesting search engine implementation – built from scratch in C and using Redis as a framework. The result, RediSearch, is now available as an open source project.Its key features include:Let’s look at some of the key concepts of RediSearch using this example with the redis-cli tool:Our first design choice with RediSearch was to model Inverted Indexes using a custom data structure. This allows for more efficient encoding of the data, and possibly faster traversal and intersection of the index. We chose to simply use Redis string keys as binary encoded indexes, and read them directly from memory while searching, a technique that in the Modules API is called String DMA. We save document content, on the other hand, using plain old redis hash objects.We save the Inverted Indexes in Redis using a technique that combines Delta Encoding and Varint Encoding to encode entries – minimizing space used for indexes, while keeping decompression and traversal efficient.Another important feature for RediSearch is its auto-complete, or suggest, commands. This allows you to create dictionaries of weighted terms, and then query them for completion suggestions to a given user prefix. For example, if a user starts to put the term “lcd tv” into a dictionary, sending the prefix “lc” will return the full term as a result.RediSearch also allows for Fuzzy Suggestions, meaning you can get suggestions to prefixes even if the user makes a typo in their prefix. This is enabled using a Levenshtein Automaton, allowing efficient searching of the dictionary for all terms within a maximal Levenshtein Distance of a term or prefix.Here’s an example of creating and searching an auto-complete dictionary in RediSearch:To assess the performance of RediSearch compared to other open source search engines, we created a set of benchmarks measuring latency and throughput – while indexing and querying the same set of data.Here are some of the results, showing RediSearch outperforming other open source search engines by significant margins (of course, as always, benchmarks results should be taken with a grain of salt. See the linked whitepaper for more results and info on the benchmarks):Benchmark 1: Easy single-word query – helloBenchmark 2: two word query – barack obamaBenchmark 3: Autocomplete – 1100 top 2-3 letter prefixes in Wikipedia"
249,https://redis.com/blog/all-substance-and-plenty-of-deserved-hype-too/,All Substance and Plenty of Deserved Hype Too,"August 28, 2016",Manish Gupta,"In the past 12 months, Redis has celebrated some impressive market leadership milestones—and we have every reason to believe the party’s just getting started.In July, we announced a 350% year over year revenue growth for our Redis Enterprise Cluster (RLEC) and an expansion to more than 55,000 accounts for our Redis Cloud offering—enough to make us the fastest growing NoSQL provider in the marketplace.On August 25th, we announced that Redis’ market leadership position was further strengthened by its debut as a Leader in the The Forrester Wave™: Big Data NoSQL, Q3 2016 report by Forrester Research. The company’s designation as a Leader was driven by the high evaluation marks garnered by Redis Enterprise Cluster (RLEC) across several key categories including installed base, performance, high availability, multimodel, data types, and open source licensing.Forrester Research is the latest independent analyst firm to recognize Redis as a leading NoSQL vendor. Gartner, another highly respected independent analyst firm, also positioned Redis as a Leader in its 2015 Operational Database Management Systems (ODBMS) Magic Quadrant report, which was released last October.It’s clear that our continuous innovation and execution excellence are catching not only the eyes of enterprise architecture professionals looking for high performing, enterprise-class Redis, but also the eyes of analysts and thought leaders serving this growing market segment.And, of course, we’re enjoying the looks—made all the more gratifying by the knowledge that there’s plenty of substance to back up the hype. Redis’ solutions enhance open source Redis and are built on a rock solid foundation of high performance, seamless scalability, true high availability, and best-in-class expertise. And, as the Home of Redis, Redis enjoys the support and innovation of one of the largest and most engaged developer communities.With an impressive streak of big wins, I’m eager to see what the rest of the year has in store for Redis. There are more market reports to come, which means more opportunities for Redis to turn heads and take names!"
250,https://redis.com/blog/writing-redis-modules/,Writing Redis Modules,"August 2, 2016",Dvir Dukhan,"I’ve been using Redis for about six years now, and over the years I’ve exploited its few data types to do some pretty versatile things such as geographical queries, text search, machine learning and more.But over the years, I can’t even count the number of times I’ve found myself (as did many other users) wishing that Redis would have this or other data structure, command or capability it is lacking. Not having them forced user to resort to less elegant or less efficient methods to achieve our goals with Redis. And while Lua was a step in the right direction, it has its limitations.But all that is changing, since Redis now has a (not yet stable) module system that allows developers to write C libraries, which add new capabilities and data structures to Redis – in speeds similar to normal Redis commands.I personally think this is the most exciting new feature to hit Redis in a long time, and it will go beyond an API – dramatically shaping the dynamics and the usage of Redis in the years to come.Over the past couple of months, while Salvatore has been working on the pretty big Modules API, we here at Redis have been busy experimenting with it, creating modules that add things like authentication, probabilistic data structures and full-text search — all using new data structures and adding new commands to Redis.So I wanted to share the lessons I’ve learned as a sort of basic guide to writing modules. The documentation of the API is very good, but also very long. So take this guide as a TL;DR guide to module writing.Let’s start with the basics: Modules are basically C shared libraries (.so files) that Redis can load at runtime or on startup. They can register new commands that Redis doesn’t support, and can access Redis’ data to do their thing. But there are a few key differences that make modules a lot more powerful:What modules basically contain are command handlers. These are C functions with the following signature:int MyCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc)The idea is pretty simple, as can be seen from the signature: The function returns an integer, either OK or ERR. Usually returning OK, even if returning an error to the user, is fine.The command handler accepts a RedisModuleCtx* object. This object is opaque to the module developer, but internally it contains the calling client’s state, and even internal memory management, which we’ll get to later.Next it receives argv and argc which are basically the argument the user has passed to the command being called. The first argument is the name of the call itself, and the rest are simply parsed arguments from the Redis protocol.Notice that they are received as RedisModuleString objects, which again, are opaque. They can be converted to normal C strings with zero copy if manipulation is needed.To activate the module’s commands, the standard entry point for a module is a function called  RedisModule_OnLoad. This function tells Redis what commands are in the module and maps them to their handler.OK, let’s write a Redis Module. We’ll focus on a very simple example of a module that implements a new Redis command – HGETSET <key> <element> <new value>. It’s basically a combination of HGET and HSET, getting the current value in a hash object, and setting a new value instead of it atomically. This is pretty basic, and can be done with a simple transaction or a Lua script, but it has the advantage of being really simple.1. Let’s start with a bare command handler:int HGetSetCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {return REDISMODULE_OK;}Again, this currently does nothing, it just returns the OK code. So let’s give it some substance.2. Validate the argumentsRemember, our command is HGETSET <key> <element> <new value>, meaning it will always have four arguments in argv. So let’s make sure this is indeed what happens:/**HGETSET <key> <element> <new value>*/int HGetSetCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {if (argc != 4) {return RedisModule_WrongArity(ctx);}return REDISMODULE_OK;}RedisModule_WrongArity will return a standard error to the client in the form of:(error) ERR wrong number of arguments for ‘get’ command3. Activate AutoMemoryOne of the great features of the Redis Modules API is automatic resource and memory management. While the module author can allocate and free memory independently, creating Redis resources is managed, and Redis strings, keys and responses allocated during the handler’s lifecycle are freed automatically, if we call RedisModule_AutoMemory.RedisModule_AutoMemory(ctx);4. Performing a Redis callNow we’ll run the first of two Redis calls: HGET. We pass argv[1] and argv[2] which are the key and element as the arguments. We use the generic RedisModule_Call command, which simply allows the module developer to call any existing Redis commands, much like a Lua script.RedisModuleCallReply *rep =RedisModule_Call(ctx, “HGET”, “ss”, argv[1], argv[2]);// And let’s make sure it’s not an errorif (RedisModule_CallReplyType(rep) == REDISMODULE_REPLY_ERROR) {RedisModule_ReplyWithCallReply(ctx, rep);return REDISMODULE_ERR;}Notice that RedisModule_Call’s third argument, “ss”, denotes how Redis should treat the passed variadic arguments to the function. “ss” means “two RedisModuleString objects.” Other specifiers are “c” for a c-string, “d” for double, “l” for long, and “b” for a c-buffer (a string followed by its length).Now let’s perform the second Redis call, HSET:RedisModuleCallReply *srep =RedisModule_Call(ctx, “HSET”, “sss”, argv[1], argv[2], argv[3]);if (RedisModule_CallReplyType(srep) == REDISMODULE_REPLY_ERROR) {RedisModule_ReplyWithCallReply(ctx, srep);return REDISMODULE_ERR;}Which is done much like the HGET command, except we pass three arguments to it.5. Return the resultsIn this simple case, we just need to return the result of HGET, or the value before we changed it. This is done using a simple function – RedisModule_ReplyWithCallReply, which forwards the reply object to the client:RedisModule_ReplyWithCallReply(ctx, rep);return REDISMODULE_OK;And that’s it! Our command handler is ready; we just need to register our module and command handler properly.6. Initialize the moduleThe entry point for all Redis Modules is a function called RedisModule_OnLoad, which the developer has to implement. It registers and initializes the module, and registers its commands with Redis so that they can be called.Initializing our module works like so:int RedisModule_OnLoad(RedisModuleCtx *ctx) {// Register the module itself – it’s called example and has an API version of 1if (RedisModule_Init(ctx, “example”, 1, REDISMODULE_APIVER_1) == REDISMODULE_ERR) {return REDISMODULE_ERR;}// register our command – it is a write command, with one key at argv[1]if (RedisModule_CreateCommand(ctx, “example.hgetset”, HGetSetCommand,“write”, 1, 1, 1) == REDISMODULE_ERR) {return REDISMODULE_ERR;}return REDISMODULE_OK;}And that’s about it! Our module is done.7. A Word on module buildingAll that’s left is to compile our module. I won’t go into the specifics of creating a makefile for it, but what you need to know is that Redis Modules require no special linking. Once you’ve included the “redismodule.h” file in your module’s files, and implemented the entry point function, that’s all Redis needs to load your module; any other linking is up to you.Provided here are the commands needed to compile our basic module with gcc.On Linux:$ gcc -fPIC -std=gnu99 -c -o module.o module.c$ ld -o module.so module.o -shared -Bsymbolic -lcOn OSX:$ gcc -dynamic -fno-common -std=gnu99 -c -o module.o module.c$ ld -o module.so module.o -bundle -undefined dynamic_lookup -lc8. Loading our moduleOnce you’ve built your module, you need to load it. Assuming you’ve downloaded Redis v4 or above, you just run it with the loadmodule command-line argument:$ redis-server --loadmodule /path/to/module.soAnd that’s it! Redis is now running and has loaded our module. We can simply connect with redis-cli and run our commands!The full source code detailed here can be found in the example directory of RedisModuleSDK, which also includes a Module project template, makefile, and a utility library with functions automating some of the more boring stuff around writing modules, that are not included in the original API. You do not have to use it, but feel free to."
251,https://redis.com/blog/how-redis-is-used-in-practice/,How Redis is Used in Practice,"November 23, 2015",Leena Joshi,"In a recent eye opening survey of 116 redis users, we at Redis were pleasantly surprised by the enormous breadth and depth of Redis usage. 71% of respondents were planning to increase their usage of Redis as their data, applications and users scaled up. Some of our other favorite data points were:1 Top applications using Redis spanned Analytics, Mobile, Social, Retail, Financial, Gaming – and in addition to these – there was an additional long tail of 16 other different types of applications. We knew Redis is a favorite, we found out that the love is spread out all across the board!2Almost half the respondents had more than a single use case for Redis. We expected caching would be a popular use case for Redis, what we hadn’t expected was how many people used it to handle message queues, as their primary datastore and for high speed data ingest (total of 70% across all three). Dr Josiah Carlson, author of the extremely popular book ‘Redis in Action’, delivered a webinar on top use cases of Redis – in which he reviews several more including distributed locking, ad targeting and many more. You can watch the full webinar here.3We also asked respondents why they chose Redis – and of course, Redis was chosen for its blazing fast performance – but that isn’t all. Redis is a persistent datastore despite being in-memory and the high availability features of Redis make it quite popular. The versatility of Redis – its highly optimized and high performing data structures are a huge reason for the growing widespread adoption too!The sample companies included in this survey ranged from tiny startups like Fifth Planet Games and Learnbop to really large companies like Nordstrom, Telstra, Fitbit and spanned a range of industries too. For more detail on the survey, do reach out on Twitter to @leena_joshi2015."
252,https://redis.com/blog/redis-labs-named-leader-in-gartners-odbms-mq-2015/,Redis Labs named leader in Gartner’s ODBMs MQ 2015,"October 19, 2015",Leena Joshi,"This is our very first year in Gartner’s ODBMS Magic Quadrant. And we are in the leader’s quadrant! This is a moment of unprecedented recognition for us and we are very proud.This is a position many companies can only dream of achieving after successful maturation and market adoption of their technologies. For Redis Labs and Redis, this has been several years in the making, of course. But to make an appearance in the leaders quadrant the very first time and with such illustrious company, who have been around for dozens of years is quite an achievement for our team and for the entire Redis community!It is rather like Redis, when you think about it. Redis has the leanest footprint in memory and it is rather simple and elegant to use — but the power it delivers is outsized, its performance beats every other technology hands down and its market adoption is groundbreaking. Redis alone has 40k+ customers which for such a young company is rather extraordinary.The $35 billion database market has its dominant players, and the overall landscape changes slowly as data is precious. But with the increasing volume, variety and velocity of data, many alternatives have emerged to help organizations manage and make sense of different types of data. What drives Redis’ adoption is the unparalleled variety of use cases it serves with low complexity and blazing fast performance. As a result, Redis has become an increasingly critical layer of most web, mobile, IoT and API application stacks.Redis Labs extends open source Redis with true high availability, seamless clustering and scaling as well as enterprise-grade monitoring and support. The robustness of our shared-nothing cluster architecture is what makes it possible for customers to get completely hassle-free Redis operation. And this is what we believe is reflected in our Gartner MQ scores.We believe a key component of Redis Labs’ recognition is the technical prowess of the team. We delivered such an experience for our customers that we absolutely topped reference customer scores in overall experience of doing business with a vendor."
253,https://redis.com/blog/redis-labs-at-aws-reinvent-2015/,Redis Labs at AWS re:Invent 2015,"September 28, 2015",Leena Joshi,"Jack-o-lanterns, candy corn and haunted houses are the usual causes of excitement in October, but at Redis, we find October even more exciting because we’ll be out and about meeting Redis geeks — starting with the AWS re:Invent show in Las Vegas next week! This year, things will be sizzling and crackling at our booth (#254 & 253), right by the AWS Developer Lounge. Stop by for some awesome tricks, and treats!We recently announced some great customer momentum with our Redis Enterprise Cluster (RLEC) product. And Redis Cloud continues to be the service of choice for highly available, seamlessly scalable, always-on Redis. Of course, we will be demo-ing both of these in our booth.For those who are new to Redis, we will be giving talks on “Top Use Cases of Redis.” And for folks running Redis on their own infrastructure, we’ll be sharing some outstanding best practices — “How to Scale Redis With Zero Downtime” and “How to Make Redis Highly Available.”One big trend we are noticing among Redis fans and developers worldwide is that they love containers. The other thing they love is Redis in Containers. In a recent ClusterHQ and DevOps survey, Redis was found to be the #1 NoSQL database deployed in containers. And so we will also be talking about “How to Deploy Redis in Containers” and how best to get seamless scale and high availability for Redis in a microservices world.Join us for these interactive sessions with demos and in-depth discussions! You can reserve a spot to attend our talks here.As usual, our booth will be laden with tchotchkes and some big prizes including:We will also have one amazing surprise giveaway, so stop by the Redis booth for your chance to win a one-of-a-kind exclusive prize!In addition to one on one meetings, we will be hosting a VIP Dinner for analysts, customers, and partners. If you are attending AWS re:Invent and interested in speaking with us, please contact us here, or stop by our booth to sign up for a meeting."
254,https://redis.com/blog/database-usage-survey-of-aws-reinvent-2014-developers/,Database Usage Survey of AWS re:Invent 2014 Developers,"December 23, 2014",Leena Joshi,"We’ve spent 2014 getting to know developers who are building a wide range of new era and enterprise applications on cloud infrastructure solutions like Amazon Web Services (AWS). In our effort to better understand the needs of cloud developers, we surveyed 126 developers who attended AWS re:Invent 2014 and asked them about their database needs and existing deploymentsOur summary of the findings includes these major highlights:The goal of this survey was to show how a sample of the developer community engages with various databases. Data is provided as benchmark only. Questions? Let’s discuss over twitter via @cameronperon and @Redis or via email.Image credit: by Gwendolen, https://flic.kr/p/6bVwJu."
255,https://redis.com/blog/redis-labs-enterprise-cluster-in-beta-now/,Redis Labs Enterprise Cluster in Beta Now!,"January 7, 2015",Leena Joshi,"We’ve listened carefully during 2014 and today we are proud to announce the launch of our on-premise Redis offering: Redis Enterprise Cluster (RLEC). RLEC provides you with the same high performance Redis experience that you expect from us with the added benefit of managing your own cluster on any environment you choose, including on-premise, virtual private cloud, IaaS / PaaS, and many others.We are inviting the Redis and developer community to take the RLEC beta for a test drive and give us your feedback. You can sign up and try Redis Enterprise Cluster here.RLEC is based on proven technology that powers tens-of-thousands databases in our successful Redis Cloud service. With RLEC, you can enjoy all the benefits that made Redis Cloud so popular among developers:Blazing fast performance: Capture the full potential of the fastest database on the planet. Reach 1.2 million ops/sec at <1 msec with a single cloud node.Built-in Redis clustering technology: Deploy Redis clusters with multiple nodes in your preferred environment, on or off the cloud. No clustering/sharding experience or knowledge required!Infinite scalability: Scale your Redis dataset to any size over your own resources; add as many nodes as needed. Easily grow or shrink your dataset with via an intuitive UI, a command line interface or RESTful API!True high availability: Sleep well at night with out-of-the-box replication across datacenters and instant auto failover. Easily configure your app with data persistence and backups.Best-in-class Redis expertise: Redis employs some of the world’s most talented Redis experts to provide an exceptional, reliable, and top performing Redis experience. Elite knowhow and experience at your fingertips.Questions? Ping me on Twitter."
256,https://redis.com/blog/redis-labs-dominates-independent-redis-as-a-service-raas-benchmark/,Redis Labs Dominates Independent Redis-as-a-Service (RaaS) Benchmark,"January 27, 2015",Leena Joshi,"Making the right Redis decision isn’t easy. Should I build out my own Redis environment or pay a consultant or service to manage it for me? When is the right time to start using Redis? Is my current solution scalable and available?With the explosive growth of Redis, we asked the Altoros team to conduct the first wide-scale benchmark test of popular Redis-as-a-Service (RaaS) offerings that you can use on IaaS and PaaS solutions like Amazon Web Services and Heroku.Get the full benchmark results for free here.Altoros’ findings were impressive, with Redis achieving the highest throughput and the lowest latency across all the tested workloads (tests were conducted over a single AWS EC2 instance):Our CEO and co-founder, Ofer Bengal had this to say about our success in the benchmark: “Developers love the speed and open source nature of Redis, yet are challenged by many demanding facets of scaling their cloud architecture. Redis Cloud is a perfect fit at this point, because it allows the developer to enjoy the benefits of Redis without having to do any heavy lifting.”“While most Redis benchmarks focus on simple GET/SET operations, we were interested in better utilization of built-in data types and server-side operations,” said Vladimir Starostenkov, Senior R&D Engineer at Altoros. “For this reason, we designed a combined workload, embodying two different types of queries running concurrently and imitating a real-life Redis use case.”Get the full benchmark results for free here.For this benchmark, Altoros used a single AWS EC2 instance in the same region. In addition, it used two AWS EC2 instances to run memtier_benchmark, an open source traffic generator written in C++, and a Java-based stress tool that simulates a more complex workload. The benchmark consisted of three workloads: simple, complex and combined."
257,https://redis.com/blog/highly-available-in-memory-cloud-datastores/,Highly-Available In-Memory Cloud Datastores,"August 13, 2013",Redis,"Everybody knows you should always use protection, or things can get pretty ugly fast. In the Cloud world, this translates to our in-memory datastore’s fault resilience, which effectively determines if and how it will withstand and recover from failure scenarios.While for some applications, the loss of some or all data is an acceptable mode of operation, for most applications data persistence and high availability are hard requirements. We’ve touched on this in previous posts, but given its importance, we want to share more about maintaining the high availability (HA) of in-memory datastores in Cloud environments.With technologies, such as Redis, it is possible to use out-of-the-box tools to support such goals. These ingrained abilities provide a good starting point, but are usually not quite enough, especially when employed in Cloud environments.Redis, for example, comes equipped with built-in replication, but can still experience undesirable stability and performance effects when used over a congested network. Redis’ Sentinal tackles the HA challenge but has yet to stabilize and mature as well as be integrated with most client libraries.Redis’ integrated data persistence mechanisms (namely snapshots and Append Only Files) are very dependable but may carry a serious performance penalty when used with cloud storage. Unlike Redis, Memcached offers no HA functionality by itself. Two approaches prevail when augmenting it to achieve high-availability: repcached or client-side replication. There are, however, downsides to each of these approaches, rendering them inapplicable for certain application requirements.Repcached’s major handicaps are its inability to scale and its exclusive availability for Memcached v1.2.x, and client-side writes’ multiplicity can become a major performance damper. In order to have a real-world production-grade setup deployed on cloud compute resources you need to season it with extra availability auspices.Our Redis & Memcached Cloud services take into account multiple failure scenarios and provide availability mechanisms to handle them all. The simplest form of threat to availability that we eliminate is the failure of a single node (i.e. cloud server instance).The low-level blocks in the overall cloud environment, nodes can and do fail on a regular basis. We protect our databases from such failures by replicating them on two or more nodes, so that when a node dies there is at least one replica available on a different node. Our service automatically detects such failures, transparently promotes a replica for use and provisions a replacement to the failed node.For our regular plans we offer choice between two modes of replication: in-memory and to-disk. When using in-memory replication, replicas of the database are kept loaded and ready for immediate use in the main memory of the standby nodes. Failing over to an in-memory replica is nearly instantaneous, and in most cases the application does not discern any noticeable effects during or after the switch.We recommend using in-memory replication for critical applications, but RAM resources are significantly more expensive than disk space. Therefore, we also support using disk-based replication at no extra cost. We achieve disk-based replication by storing the database on the local storage of the active node and copying it to the local storage of an idle backup node. When failover is triggered by the service, the backup node loads the disk-stored database and takes the place of the now-inactive replica. Of course, both response and access times to storage are inherently higher by orders of magnitude than those required to access the memory, so failover duration is higher as the probability of unsynchronized data loss.Some of the plans we offer can survive an entire availability zone’s failure and still continue normal operation with only a minimal amount of disruption. These instances, created using Multi-AZ plans, have their replicas kept in different availability zones. If an entire zone fails, our automatic fail over mechanism switches to serve the data from the remaining replica. Due to performance considerations, disk-based replication is not offered for our Multi-AZ plans and only in-memory replication is supported.Another, rarer but still probable, failure scenario is one in which both replicas are affected. In such cases, mere replication won’t help because there no survivors left. We deal with such events by using native data persistence mechanisms, namely Append Only Files and snapshots, on top of persistent cloud storage services such as AWS’s EBS. We use a cluster recovery tool to rebuild damaged setups and bootstrap datasets from storage when such spectacular failures occur.Last on the list of possible failures is the doomsday scenario in which a significant portion of the cloud – such as an entire datacenter (a.k.a. zone) or even multiple datacenters (a.k.a. data region) – disintegrates into thin air. Such outages were experienced no less than 4 times last year by the market’s leading cloud provider, AWS.These meltdowns are impossible to countermeasure (at least within the affected region of the cloud), but our service protects your databases from them by maintaining automated daily and on-demand backups to remote storage (i.e. an S3 or FTP server). These enable our cluster recovery tool to restore the contents of your ravaged databases after it completes the recovery and setup of infrastructure resources.Based on their plan and needs, datasets that are hosted in our service are sharded behind the scenes to enable near-infinite scalability.A welcome byproduct of this design is a positive gain in the overall availability of the database:Ensuring the availability of in-memory datastores in the turbulent environment of modern compute clouds is no small task. Setting up a resilient configuration that can recover from failures to maintain the application’s ongoing operation is anything but a once-off effort and requires constant monitoring and maintenance.While the Do-It-Yourself approach may suit some, our service provides (among other benefits) high availability with the click of a button for those who want to use an in-memory datastore rather than manage it."
258,https://redis.com/blog/redis-enterprise-pack-v4-5-0-release/,Redis Enterprise Pack v4.5.0 Release,"May 10, 2017",Kirk Kirkconnell,"We are very excited to announce the release of Redise Pack 4.5.0. Along with many quality improvements and overall performance enhancements, this release ushers in two major capabilities;With Redise Pack 4.5.0, Redise Flash version 2 is production ready. The new version brings performance, reliability, and stability enhancements when building large high performance databases using Flash memory.Redise Flash (RF) offers users of Redise Pack and Redise Cloud Private the unique ability to operate a Redis database that spans both RAM and flash memory (SSD), but remains separate from Redise Pack’s persistence mechanisms. Whilst keys are always stored in RAM, RF intelligently manages the location of their values (RAM vs Flash) in the database via a LRU-based (least-recently-used) mechanism. Hot, frequently used values will be in RAM  while warm values will be ejected to flash memory. This enables you to have much larger datasets with RAM-like latency and performance, but at dramatically lower cost than an all-RAM database.You can get an introduction to building large scale databases with Redis using Redise Flash in this short video or read more about building large scale databases with Redis in Redise Pack documentation.The Discovery Service enables simple IP based connection for Redis applications. It is compliant with the Redis Sentinel API can be queried to discover the database endpoint (IP address). When used in conjunction with Redise Pack’s other high availability features, the Discovery Service assists an application cope with connectivity under cluster topology changes such as node failures or shard migrations.The Discovery Service is a distributed service with a process running on each node in the cluster and can be the authoritative source for cluster discovery. To employ it, your application utilizes a Sentinel enabled Redis client to connect to the Discovery Service and query the endpoint for the given database. The Discovery Service replies with the database’s endpoint, either internal or external, for that database. In case of a node failure, the Discovery Service is updated by the cluster manager with the new endpoint.You can download Redise Pack 4.5.0 today and find more details on this release in the release notes and documentation."
259,https://redis.com/blog/your-cloud-cant-do-that-0-5m-ops-acid-1msec-latency/,Your Cloud Can’t Do That: 0.5M ops + ACID  @<1ms Latency!,"July 6, 2017",Redis,"For those of you familiar with Redis, it should be relatively straightforward to create a configuration that guarantees ACID-ish (Atomicity, Consistency, Isolation, Durability) operations: merely create a single Redis instance with a ‘master’ role and have it configured with AOF every write (‘appendfsync always’) to a persistent storage device. This configuration provides ACID characteristics in the following ways:That said, most Redis users prefer not to run Redis in this configuration as it can dramatically affect the performance. For example, if the persistent storage is currently busy, Redis would wait with the request’s execution until the storage becomes available again.With that in mind, we wanted to determine how fast can Redis Enterprise (Redise) cluster can process ACID transactions. There are several built-in enhancements that we have made to the Redise architecture that enable a better performance in an ACID configuration, including:We deployed the following benchmark configuration inside an AWS VPC:Despite having tested multiple types of loads, including different read/write ratios; different object sizes (from 100B to 6KB); multiple number of connections; with and without pipelining; we couldn’t get less than one millisecond latency for durable ‘write’ operations, where latency was measured from the time the first byte of the request arrived at the cluster until the first byte of the ‘write’ response was sent back to the client. Finally, we tested a single request over a single connection, but still couldn’t get less than 2-3 millisecond latency. We did a deeper analysis and found that there was no way to achieve less than two milliseconds of latency between any instance on the AWS cloud and EBS storage under an ACID configuration.As most of our customers want <1 millisecond latency, we decided to look for alternatives.In a nutshell VMAX is a family of storage arrays built on the strategy of simple, intelligent, modular storage. It incorporates a Dynamic Virtual Matrix interface that connects and shares resources across all VMAX engines, allowing the storage array to seamlessly grow from an entry-level configuration into the world’s largest storage array.Performance-wise, VMAX can scale from one up to eight engines (V-Bricks). Each engine consists of dual directors, each with 2-socket Intel CPUs, front-end and back-end connectivity, hardware compression module, Infiniband internal fabric, and a large mirrored and persistent cache. All writes are acknowledged to the host as soon as they registered with VMAX cache and only later, perhaps after multiple updates, are written to flash. Reads also benefit from the VMAX large cache. When a read is requested for data that is not already in cache, FlashBoost technology delivers the I/O directly from the back-end (flash) to the front-end (host) and is later staged in the cache for possible future access.We set up the following benchmark environment:Below are some more details:As expected, the ‘read’ intensive tests provided the best results; that said, we were very surprised to see over 660K ops/sec on the standard 1:1 read/write use case with 100B item_size, and only slightly lower throughout (i.e. 640K op/sec) on the write-intensive scenario. We were also impressed with the 6000B results, even under a write-intensive scenario such as 80K ops/sec with sub-millisecond latency on a single cluster node.We were surprised (and happy) to discover that with high-end persistent storage devices like Dell-EMC VMAX, a single Redise cluster node can support over 650K ACID ops/sec while keeping sub-millisecond database latency.On the other hand, we were disappointed to see that we cannot run a single durable operation under sub-millisecond latency on the state-of-the-art cloud storage infrastructure (i.e. AWS io1 EBS). With the multitude of advanced technologies and public clouds services available, there is still a ways to go.The full Redis with Dell-EMC VMAX Performance Assessment Tests and Best Practices can be found on our website here.memtier_benchmark parametersEach test was run with the following memtier_benchmark parameters"
260,https://redis.com/blog/redis-4-0-0-released/,Redis 4.0.0 is Released,"July 14, 2017",Tague Griffith,"On July 14,  Salvatore Sanfilippo (@antirez) announced the availability of the much anticipated Redis 4.0.0 GA release.  Redis 4  contains a number of new features including the Modules API, the PSYNC2 (improved  replication mechanism) engine,, new caching policies, asynchronous deletion operations, microcontroller support, Redis Cluster improvements, memory management improvements and a host of additional changes and bug fixes.ModulesOne of the biggest features in Redis 4.0.0 is the long awaited modules system  & Redis memory docker that was announced at RedisConf 2016.  The modules system provides an API for extending Redis with dynamically loaded modules primarily in C.  The modules API provides multiple levels of API for developers to add new features and functionality to Redis.With Redis Modules, developers can add new operations to existing data types, introduce new data types, such as JSON, or extend Redis with new processes like Search or Neural Network.The new API enables developers to build extensions that were either impractical or not performant enough when built with Lua.  As mentioned in the documentation for modules on Redis.io, “Redis modules make [it] possible to extend Redis functionality using external modules, implementing new Redis commands at a speed and with features similar to what can be done inside the core itself.”Caching ImprovementsFor many Redis users, caching is their introduction to working with Redis and the 4.0.0 release introduces new features to improve the performance of Redis when used for caching.  The 4.0.0 release  adds least-frequently-used (LFU) maxmemory policies giving users another algorithm for evicting keys when Redis hits the maxmemory threshold.  LFU caching provides a better hit to miss ratio than least-recently-used (LRU) caching for many applications.The implementation of LFU caching in Redis is done through an approximated algorithm to provide accurate estimation of the frequency of a key access without adding a significant amount of memory overhead to track the access counts.  Like the current LRU caching policies, LFU caching can be applied only to volatile keys (expire set) or to all keys.Details of the Redis caching policies can be found in the Using Redis as an LRU Cache article on Redis.io.Memory FeaturesTwo useful features related to memory consumption were also highlighted in Salvatore’s release announcement.  They are the addition of a new command for memory introspection and the addition of active memory defragmentation.The new MEMORY command gives users information into the memory consumption of a Redis instance.  The MEMORY USAGE command provides users with the precise amount of memory being used by a given key, while MEMORY DOCTOR provides a framework (similar to the LATENCY DOCTOR command) for observing the overall memory consumption of an instance.Congratulations to Salvatore (@antirez) for getting the 4.0.0 release to GA.  Also thank you to all of the contributors involved in the project for their hard work and perseverance."
261,https://redis.com/blog/making-redis-concurrent-with-modules/,Never Stop Serving: Making Redis Concurrent With Modules,"August 2, 2017",Dvir Dukhan,"Redis has been, from its inception, single threaded. This is a reality that Redis apps, and more recently Redis Modules such as RediSearch , have to work with.While keeping things single-threaded makes Redis simple and fast, the downside is that long running commands block the entire server for the duration of query execution. Most Redis commands are fast so that is not a problem, but commands like ZUNIONSTORE, LRANGE, SINTER and of course, the infamous KEYS, can block Redis for seconds or minutes, depending on the size of data they are handling.RediSearch is a new search engine module written at Redis. It leverages Redis’ powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine.While it is extremely fast and uses highly optimized data structures and algorithms, it faces concurrency challenges. Depending on the size of your dataset and the cardinality of search queries, they can take anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens, the entire Redis server that the engine is running on, is blocked.Think, for example, of a full-text query intersecting the terms “hello” and “world”, each with a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, which is impossible with current hardware. The same applies when indexing a 1000 word document. It blocks Redis entirely for that duration.So search queries could behave very differently from your average Redis O(1) command, in that they could block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Enterprise  – but even if we distribute the data across cluster nodes, some queries could be slow.Luckily,  Salvatore Sanfilippo has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API : Thread Safe Contexts and the Global Lock.The idea is simple. While Redis still remains single threaded, a module can run many threads. Any one of these threads can acquire the Global Lock when it needs to access Redis data, operate on it, and release it.We still cannot really query Redis in parallel. Only one thread can acquire the lock, including the Redis main thread, but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time (This limitation applies to this specific use case only – in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy).Up until now, the flow of a search query was simple: the query arrives at a Command Handler callback in the Redis Module, and it would be the only thing running inside Redis. Then it would parse the query, execute it, taking as long as it needs and return the result.To allow concurrency, we adopted the following design:Thus the operating system’s scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently.The same approach is applied to indexing. If a document is so big that tokenizing and indexing it will block Redis for a long time – we break it into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing, there is enough work to be done in parallel using multiple cores, namely tokenizing and normalizing the document. This is especially effective for very big documents.As a side note, this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.While this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here – this is revolutionary in Redis terms. Think about the old problem of running KEYS * in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. Now it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!There is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result – the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was “asleep”.This is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low.However, if needed, this can be overcome quite easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in “safe mode”, making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation.To enable safe mode and disable query concurrency, you can configure RediSearch at load time: redis-server --loadmodule redisearch.so SAFEMODE in command line, or by adding loadmodule redisearch.so SAFEMODE to your redis.conf – depending on how you load the module.I’ve benchmarked both versions of the module – simple single threaded, and concurrent multi threaded, over the same set up.1. The dataset consists of about 1,000,000 Reddit comments.2. Two clients using Redis benchmark were running – first separately, then in parallel:3. One client doing a very intensive query – “i” which has 200,000 results with 5 concurrent connections.4. One client is doing a very light query – “Obama”, which has about 500 results – with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).5. Both clients and the server running on my personal laptop – MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.This little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.For RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that’s a story for another post), it will make RediSearch a very powerful search and indexing engine."
262,https://redis.com/blog/indexing-with-redis/,Indexing with Redis,"August 10, 2017",David Maier,"This article was originally posted on nosqlgeek.orgIf you follow my news on Twitter then you might have realized that I just started to work more with Redis.  Redis (=Remote Dictionary Server) is known as a Data Structure Store. This means that we can not just deal with Key-Value Pairs (called Strings in Redis) but in addition with data structures as Hashes (Hash-Maps), Lists, Sets or Sorted Sets. Further details about data structures can be found here:With a pure Key-Value Store, you would typically maintain your index structures manually by applying some KV-Store patterns. Here some examplesSo how is Redis addressing these examples? We are leveraging the power of data structures as Hashes and Sorted Sets.A Get operation already has a complexity of O(1). This is the same for Redis.Hashes (as the name already indicates) can be directly used to build a hash index in order to support exact match ‘queries’. The complexity of accessing an entry in a Redis Hash is indeed O(1). Here an example:HMSET idx_email david.maier@redis.com user::dmaier
HMSET idx_email max.mustermann@example.com user::mustermann
HGET idx_email david.maier@redis.com
""user::dmaier""In addition Redis Hashes are supporting operations as HSCAN. This provides you a cursor based approach to scan hashes. Further information can be found here:Here an example:HSCAN idx_email 0 match d*
1) ""0""
2) 1) ""david.maier@redis.com""
2) ""user::dmaier""Sorted Sets can be used to support range ‘queries’.  The way how this works is that we use the value for which we are searching  as the score (order number). To scan such a Sorted Set has then a complexity of O(log(n)+m) whereby n is the number of elements in the set and m is the result set size. Here an example:ZADD idx_age 37 user::david
ZADD idx_age 9 user::philip
ZADD idx_age 36 user::katrin
ZADD idx_age 13 user::robin
ZADD idx_age 0 user::samuel
ZRANGEBYSCORE idx_age 0 10
1) ""user::samuel""
2) ""user::philip""If you add 2 elements with the same score then they are sorted lexicographically. This is interesting for non-numeric values. The command ZRANGEBYLEX allows you to perform range ‘queries’ by taking the lexicographic order into account.Redis supports now Modules (since v4.0). Modules are allowing you to extend Redis’ functionality. One module which perfectly matches the topic of this blog post is RediSearch. RediSearch is basically providing Full Text Indexing and Searching capabilities to Redis. It uses an Inverted Index behind the scenes. Further details about RediSearch can be found here:Here a very basic example from the RediSearch documentation:FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT
FT.ADD myIdx doc1 1.0 FIELDS title ""hello world"" body ""lorem ipsum"" url ""http://redis.io""
FT.SEARCH myIdx ""hello world"" LIMIT 0 10
1) (integer) 1
2) ""doc1""
3) 1) ""title""
2) ""hello world""
3) ""body""
4) ""lorem ipsum""
5) ""url""
6) ""http://redis.io""As usual, I hope that you found this article useful and informative. Feedback is very welcome!"
263,https://redis.com/blog/getting-started-active-active-geo-distribution-redis-applications-crdt-conflict-free-replicated-data-types/,Getting Started with Active-Active Geo-Distribution for Redis Applications with CRDTs (conflict free replicated data types),"October 4, 2017",Cihan B,"Today, Redis applications can take advantage of a few types of replication –In the upcoming version of Redis Enterprise 5.0, we are delivering a new flexible, multi master replication technology built for WAN. The new capability allows active-active geo-distributed Redis deployments using the magic of  CRDTs (conflict free replicated data types). CRDTs simplify development of active-active systems and automatically resolve conflicting writes. Combined with Redis datatypes, CRDTs provide a mechanism that can easily help you develop active-active geo-distributed systems that can intelligently handle conflicting writes.If you’d like to know more about Redis CRDTs and visit “Bending CAP Theorem in Geo-Distributed Deployments With CRDTs“We’ll focus on experimenting with CRDTs in this walkthrough but if you want to dig deeper into CRDTs start with this article by Eric Brewer: 12 years after the original CAP theorem, Eric Brewer explains how CRDTs changes the CAP balance in this great article. To get hands on with CRDTs and try them out, you can sign up for the preview program for Redis Enterprise 5.0. Find the instructions here.We’ll setup a scale-minimized geo-distributed deployment and demonstrate how active-active access works under Redis Enterprise. Here are the four steps:Run 2 containers. We’ll use each one to simulate a Redis Enterprise cluster.Note: Before you run the containers, go to docker settings and adjust your RAM per container to 6GB. Under some operating systems, you may not be able to start the Redis Enterprise Pack containers unless the RAM per container is adjusted.It is important to note the -p options: Each container maps its web UI port (8443), REST API port (8080), and database access port (12000) to a unique host port to ensure all containers can be accessed from the host OS that’s running the containers. This will help you connect to each cluster from host as well as from the containers themselves.Lets setup both clusters.For cluster 1, direct your browser to https://localhost:8443 on the host machine to see the Redise Pack admin console. Simply click the Setup button on the page to get started.Note: Depending on your browser, you may see a certificate error. Simply choose continue to the website to get to the setup screen.On the node configuration page, select your default settings and provide a cluster FQDN: cluster1.local. Then simply click the Next button.If you don’t have a license key, click the Next button to try the trial version of the product.On the next screen, set up a Cluster Administrator account using an email for the login and a password.You are done on cluster1.local.Repeat the same operations for cluster 2. First, direct the browser at https://localhost:8444. The steps are identical except in this pass, specify FQDN as cluster2.local.Once done, We have two Redise Pack clusters with FQDNs cluster1.local and cluster2.local.We’ll create the database using the REST API. The following will create a Redis database of type CRDB (conflict free replicated database). There are a few things special about CRDBs:The REST API call below creates a CRDB Instance on cluster1.local and a CRDB Instance on cluster2.local. On each cluster, the CRDB Instances has an endpoint of port 12000 and both databases are named “sample-crdb”.Before you issue the call below, place the <admin-email> and <admin-password> you specified during setup above.Under the databases tab, choose the Redis database with deployment type set to Geo-Distributed.On the create database page, click the show advanced option link and enter database1 for the database name and 12000 for the endpoint port number. Make sure to add both http://cluster1.local:8080 and http://cluster2.local:8080 to the participating cluster list.Once you activate the database, you will have CRDB instances on each participating cluster that you can connect to.With the Redis database (CRDB) created, you are ready to connect to your database. You can use one of the following ways to test connectivity to your databaseRemember we have two CRDB Intances that are available for connections and concurrent reads and writes. The CRDB Instances are using bi-directional replication to for the global CRDB.Connecting Using redis-cliredis-cli is a simple command-line tool to interact with redis database. In this instance we’ll use redis-cli under each container using “docker exec”. Use “docker exec” to switch your context into the Redise Pack container of node in cluster1.local under the container named rp1Run redis-cli, located in “/opt/redis/bin” directory, to connect to port 12000 and store and retrieve a key1 in the database.Lets see the write to key1 replicated to cluster 2. On a different terminal window, use “docker exec” to switch your context into the Redise Pack container of node in cluster 2.You now have a working CRDB deployment. Lets see how CRDBs simplify development when you have concurrent distributed writes to data.Here is a simple test. Lets see how INCR on k1 across 2 CRDB Instances on cluster1 and cluster2 sync to ensure an accurate final value. t1 to t5 represents the order of events. operations under cluster1.local is performed on rp1 container and operations under cluster2.local are performed on rp2 container.Simulating network failures: Sync between the clusters happen fast. For some of the advanced testing, you will also find simulating network failures between cluster1 and cluster2 so you can observe how CRDTs in each data type work.It is easy to do simulate network partitioning in docker. To create a network partition find the IP address on cluster1. I get 10.0.0.210.0.0.2To break networking between the 2 clusters cluster1.local and cluster2.local, run the following on cluster2.local (rp2 container).At this point cluster1 and cluster2 cannot communicate with active-active replication. At some point you will want to restore the network back. Once you restore network communication between clusters, CRDBs will automatically start syncing again. Here is how you do it;Here is another one to try. This time we will simulate a network failure in between operations to observe the issues. In this case, we’ll see how a Redis SET works with CRDTs. we’ll create the set and let it sync across clusters. We’ll break the network and privately add a new distinct member to the SET in each cluster. Once the communication is restored, you will see how CRDTs resolve the conflicting write and union both sets.We just scratched the surface of the CRDTs in Redis. You can sign up for the private preview to get more details and documentation on the capabilities. Simply follow the instructions here."
264,https://redis.com/blog/bee-queue-redis-based-distributed-queue/,Bee-Queue: a Redis-based distributed queue,"October 18, 2017",Eli Skeggs,"Original post by Eli Skeggs (eli@mixmax.com)We’re excited to announce the v1.0 release of Bee-Queue: a fast, lightweight, robust Redis-backed job queue for Node.js. With the help of Bee-Queue’s original author, Lewis Ellis, we revived the project to make it the fastest and most robust Redis-based distributed queue in the Node ecosystem. Mixmax is using Bee-Queue in production to process tens of millions of jobs each day.Bee-Queue is meant to power a distributed worker pool and was built with short, real-time jobs in mind. Scaling is as simple as running more workers, and Bee-Queue even has a useful interactive dashboard that we use to visually monitor job processing.Here is how Bee-Queue v1.0 compares to other Redis-based job queues in the Node ecosystem (including its own prior v0.x release):So, why another job queue in the Node.js ecosystem? Up until now, Mixmax had been using a similar queue, Bull. Bull served us well for a year, but there was a race condition that resulted in some jobs being double processed, causing major problems at scale. That race condition was fixed in Bull v3, but unfortunately v3 also experienced a significant performance regression over v2. We were faced with the decision to move entirely to a different queue such as RabbitMQ or start over and write our own simple high-performance Redis-based queue.We decided to stick with Redis over other messaging frameworks (such as RabbitMQ) because of how easy it is to work with and how much performance we could get for a low cost. With Redis, we can easily scale our Redis cluster to meet our ever-growing demand, now at a tens of million of jobs an hour. Redis also gives us the benefit of keeping our traffic inside our network VPC – a core security requirement for all our sensitive data. We’re big fans of Redis at Mixmax so everyone on the team has experience working with it.With this in mind, we decided to pursue building our own lightweight queue based on Redis that did only what we needed it to, in order to make performance a priority. Before starting the project from scratch, we rediscovered Bee-Queue, having first seen it a couple years ago. We evaluated its codebase again and quickly realized that it’d be a great platform to build upon. Building on an existing queue saved us weeks of implementation time; we refreshed the codebase, identified the missing features we needed, and added those with the help of the original author, Lewis.We released Bee-Queue v1.0 this week and have switched over to using it entirely instead of Bull. All our production traffic is now flowing over Bee-Queue and we haven’t seen any problems. Resource usage has declined dramatically, measured by lower Node.js and requiring few Redis resources. We were also able to repurpose existing tooling from our Bull days to work with Bee-Queue. Building up Bee-Queue to serve our needs ended up taking the least amount of time, since we didn’t need to rewrite any application code or even change our existing monitoring infrastructure, which was already built to sit on top of Redis.Bee-Queue is now hosted in a new Github organization that we co-maintain with Lewis. We encourage you to check it out and contribute, or if you’re looking for a more fully-featured queue, check out Bull. Our goal with Bee-Queue is to keep it small with performance and stability being the top priorities.If you’re a fan of Redis and enjoy working on problems like these, come work with us! mixmax.com/careers"
265,https://redis.com/blog/cache-vs-session-store/,Cache vs. Session Store,"November 15, 2017",Roshan Kumar,"Scale globally while maintaining low latency and cache more efficiently to lower costs: Click here to talk with the Redis Enterprise team.There’s a saying, “In Silicon Valley you can throw a rock in any direction and you’ll probably hit a software engineer.” The other day I was traveling by train from San Jose to San Francisco when I overheard two software engineers comparing the speed of the products they were developing. When I heard one developer say, “Our app runs fast because we cache all the session data,” I realized that even a Silicon Valley engineer may be unaware of the subtle differences between a cache and a session store. Redis, as an in-memory database, is used for both caching and session store scenarios. Let’s look at how the use cases differ.An application stores data in the cache to serve future requests faster. Typically, the cache storage is located in the RAM and has sub millisecond latency.In the data fetch lifecycle, the application first looks for the data in the cache. If there’s a hit (i.e. the data is in the cache), it serves the data instantaneously. Conversely, if there’s a miss it fetches the data from a permanent store, stores a copy in the cache, and serves the data to the consumer. For all future requests, the data is already there in the cache and is served faster. When an application updates the data, it updates both the cache and the permanent store.This lifecycle works well for scenarios where different consumers request the same data over a period of time. One should also note that the data is stored at the application level, and not at the user level. So the data that’s stored in the cache is shared among users. Images, videos, static HTML pages, JavaScript libraries and style sheets are examples of data that are often stored in cache.A session-oriented application (a web application, for example) starts a session when a user logs in, and is active until the user logs out or the session times out. During this period, the application stores all session-related data either in the main memory or in a session store—a database that doesn’t lose the data when the application goes down. Session data may include user profile information, messages, personalized data and themes, recommendations, targeted promotions and discounts, etc.Figure 1. Cache vs Session StoreThe following points contrast session store from a cache:Redis Enterprise is a popular database ideal for both cache and session store use cases, delivering both the high-availability required for caching and session store scenarios as well as  the durability needed for session store with in-memory replication. It’s possible to use Redis Enterprise as both a cache and a session store in a single setup, as shown in the picture below.Figure 2. Designing Cache and Session Store with Redis EnterpriseIn this design, the application layer accesses and maintains the data in the cache. Therefore, all the sessions running inside the application access the same data stored in the cache.In addition to the data in cache, each session holds the session-related data inside Redis Enterprise. How do you ensure that the sessions don’t access each other’s data? Here’s a solution: First, each session must acquire a random session id that’s not shared with other sessions. Second, the session must append the session id to the keys. In the example below, a session stores the personal information in a Hash data structure, and the user recommendations in a Set data structure. As you may note, the keys have session ID in them. This prevents sessions from accessing the data owned by other sessions.session_data:(session_id):personal_info
name - John Smith
email - john@smith.com
phone - 987-111-1111session_data:(session_id):recommendations
{“product a”, “product b”, “product c”,.....“product n”}In conclusion, cache and session store are two different use cases. While high availability is important to a cache to prevent situations such as cache stampede, a session store requires high availability and durability to support transactional data and uninterrupted user engagement. With proper architecture and design, a single database instance of Redis Enterprise can be used for both caching and session data use cases while ensuring data separation between the sessions."
266,https://redis.com/blog/annoucing-redis-enterprise-pack-5-0-ga/,Announcing Redis Enterprise Pack 5.0 GA,"November 27, 2017",Cihan B,"We are super excited to announce the general availability of Redis Enterprise 5.0 today. This latest software pack is now available with new modules on our downloads page.With the 5.0 release, Redis Enterprise provides higher availability, scaling, performance and unmatched cost efficiency over open source Redis.Here’s what’s new:Active-Active Geo-Distributed Deployments Based on CRDTsDeveloping globally distributed applications can be challenging, as developers have to think about race conditions on concurrent writes across regions and complex combinations of events under geo-failovers. New Redis CRDTs (conflict-free replicated data types) simplify this task by using built-in smarts that handle conflicting writes based on the data type in use. Instead of depending on simplistic “last-writer-wins” conflict resolution, geo-distributed Redis Enterprise combines techniques defined in CRDT research with Redis data types to automatically resolve conflicts based on each data type’s intent.Visit Getting Started in our Redis Enterprise Pack documentation to learn more. And for information on how to develop applications with Redis CRDTs, check out our Developing with Redis CRDTs page.Fulltext Search with RediSearchLow latency search results are critical for engaging users. The RediSearch module extends Redis with a fast, distributed, in-memory index for Text, Numeric and Geo search queries. RediSearch with Redis Enterprise scales easily to billions of documents and can instantly index many tens of thousands of updates per second while providing search results at the speed of Redis.Visit “Getting Started with RediSearch” in our Redis Enterprise documentation for more information. To learn about developing applications with RediSearch, visit Developing with RediSearch.Enhanced JSON Data Handling with ReJSONReJSON is another module that simply makes JSON processing in Redis faster. Native JSON data handling reads and writes subdocument elements with Redis Commands. This eliminates the need to transfer an entire JSON document when doing sub-document updates.Visit Getting Started with ReJSON in our Redis Enterprise Pack documentation for the basics. And for more information on how to develop applications with ReJSON, go to Developing with ReJSON.Bloom Filters with ReBloomReBloom is a command that enables a scalable bloom filter as a data type. Similar to hyperloglog, bloom filters are probabilistic data structures. They power “existence search” or “membership search” with a super efficient index that uses just a fraction of the space of a structured or text index.For more information on how to develop applications with ReBloom, visit Developing with ReBloom.Custom Redis ModulesIn addition, Redis Enterprise 5.0 allows you to install custom modules, enabling you to further extend the functionality of Redis. For example, you can add new data types, capabilities, etc. to tailor the cluster to a specific use case or needs. Once installed, these modules benefit from the high performance, scalability, and availability that Redis Enterprise is known for.The list of Redis modules is growing every day. You can build your own or explore modules built by the Redis community by visiting Redismodules.com.Redis 4.0 SupportRedis Enterprise databases use Redis instances as the shard and each Redis Enterprise database can contain one or many shards for scale throughput and data size for Redis applications. Redis Enterprise 5.0 includes the Redis 4.0 engine as the default version for Redis Enterprise databases. Redis 4.0 improves memory management and powers the modules discussed above.Redis Cluster API SupportRedis Enterprise 5.0 supports the Redis Cluster API for lower latency operations. This API, along with the zero latency proxy in Redis Enterprise, together deliver efficient connection management and lower latency at higher throughputs.Docker in ProductionYour Redis Enterprise Pack cluster can now officially be deployed and run on Docker containers in production. This means you can easily and quickly deploy several containers to start running the scalable and highly available clusters Redis Enterprise is famous for.To get started with Redis Enterprise 5.0 on Windows, macOS or Linux using Docker, please visit Quick Start with Redis Enterprise Pack on Docker.LDAP IntegrationWith Redis Enterprise 5.0, administrator accounts can now use either built-in authentication or authenticate externally via an LDAP store. These accounts can be used to manage resources on the cluster via command line, Rest API or web interfaces.For more information, visit LDAP Integration.Of course, if you’re interested in a more detailed view of what’s new in Redis Enterprise Pack 5.0, please visit our technical documentation. You can also check out the release notes for full details on the improvements we’ve made over our previous 4.5 version of Redis Enterprise."
267,https://redis.com/blog/10m-opssec-1msec-latency-6-ec2-nodes/,10M Ops/sec @ 1msec latency with only 6 EC2 nodes,"December 4, 2017",David Maier,"Our newly launched Redis Enterprise 5.0 introduced support for the Open Source (OSS) cluster API, which allows a Redis Enterprise cluster to scale infinitely and in a linear manner by just adding shards and nodes. The OSS cluster API relies on the intelligence of the clients to decide to which shard/node to send the request to based on the key part of the key/value item and a hashing function shared across the clients and the cluster. This post explains how Redis Enterprise works with the OSS cluster API and validates the infinite linear performance scalability.For those of you who are not familiar with the Redis Enterprise architecture, let’s start with some short background:A cluster, in Redis Enterprise terms, is a set of cloud instances, virtual machine/container nodes or bare-metal servers that allows you to create any number of Redis databases (A database in Redis Enterprise terminology is the entity that manages your entire dataset across multiple Redis shards/instances. Don’t confuse this with the databases inside every Redis instance that you can leverage to do some segmentation in your keyspace using the Redis SELECT command.) in the memory pool that is shared across the set. The cluster has a symmetric shared-nothing architecture, complete separation between the data-path and control & management-path, and it includes the following main components:A database in the Redis Enterprise cluster can be created in any one of the configurations below:Based on its strong multi-tenant technology, a Redis Enterprise cluster can manage multiple databases of different types on the same cluster resources in a completely isolated manner.In order to utilize the new OSS cluster API, you should use the following properties when creating a database with the Redis Enterprise API:This will create a clustered database with properties similar to those shown below:As you can see:With that in mind, we decided to build a test environment on AWS to validate whether Redis Enterprise can really scale infinitely and in a linear manner. Since we planned to have some serious load, we decided to use EC2 m4.16xlarge instances (64 cores, 256GB RAM) for the cluster nodes and c4.8xlarge instances (36 cores, 60GB RAM) for running memtier_benchmark, an open source multi-threaded load generation tool.Using multiple instances of memtier_benchmark is a must, since in many cases a single Redis Enterprise node can deal with more traffic than the volume a single memtier_benchmark instance can generate. This approach also allows us to avoid the network bandwidth and packet per second limitations of a single NIC and makes it easy to increase the traffic load in a step by step (instance by instance) manner.This is what our final setup looks like:We created a 192-shard clustered Redis database using the Redis Enterprise API with the following parameters:{
""name"": ""api-benchmark"",
""replication"": false,
""port"" : 12345,
""sharding"": true,
""shards_count"" : 192,
""version"": ""4.0"",
""memory_size"": 100000000000,
""type"": ""redis"",
""oss_cluster"":true,
""proxy_policy"": ""all-master-shards"",>
""shards_placement"": ""sparse"",
""shard_key_regex"": [{""regex"": "".*{(?.*)}.*""}, {""regex"": ""(?.*)""}]
}> curl -k -X POST -u "":"" -H ""Content-Type: application/json"" -d @create_db.json https://localhost:9443/v1/bdbsWe tuned the proxy on each node to cope with the expected load by setting the number of proxy threads to 24:> rladmin tune proxy all max_threads 24
> rladmin tune proxy all threads 24We used the new version of memtier_benchmark that supports the OSS cluster API to first populate the database with around 10 million items, and then run the tests.Here are the memtier_benchmark parameters we used during our population and benchmarking stages:Here is a memtier_benchmark command line example:> memtier_benchmark -s $DB_SERVER -p $DB_PORT
--pipeline=$PIPELINE_SIZE -c $NUM_CLIENTS -t $NUM_THREADS
-d $DATA_SIZE --key-minimum=$KEY_MIN
--key-maximum=$MAX_KEYS_PER_SLOT --key-pattern=R:R
--ratio=$WR_RATIO --run-count=1 --requests=$NUM_REQ
--out-file=$OUT_FILE --oss-clusterOur final setup ran a 192-shard database over only 6 nodes on the Redis Enterprise cluster and demonstrated outstanding results: over 10 million ops/sec at a slightly higher than 1msec latency! Here is a screenshot taken from the Redis Enterprise UI:We conducted this experiment in order to validate that the shared-nothing architecture of Redis Enterprise can scale linearly thanks to the new OSS cluster API that was introduced in Redis Enterprise 5.0. Our experiment included:We found linear performance scalability when scaling from a 1-node cluster to a 6-node cluster, as shown in the following graph:A deeper analysis of these results indicates that the throughput per node did not change by more than 10% when scaling from a single node cluster to a two-node cluster and then to a 6-node cluster. We believe that these changes in performance between the tests might be related to different resource conditions (network, VM, etc.) on each test iteration.With the introduction of the OSS cluster API, Redis Enterprise 5.0 can easily scale linearly by just adding shards to the database and nodes to the cluster, as proven by this benchmark. We plan to continue breaking performance records in the database space, but we wanted to share this for re:invent 2017, so we stopped here. Please stay tuned!"
268,https://redis.com/blog/local-kubernetes-development-using-minikube-redis-enterprise/,Local Kubernetes Development Using Minikube and Redis Enterprise,"January 5, 2018",Vick Kelkar,"Kubernetes is an open-source container orchestration system used to deploy, scale and manage containerized applications. Kubernetes is a project hosted by the Cloud Native Computing Foundation (CNCF).  At a very high level, it contains two types of resources: a master node (which is the cluster coordinator) and nodes, which are the workers that run containerized applications.Minikube is a tool used to run a Kubernetes cluster on a local machine. Minikube is a single-node Kubernetes cluster inside a VM on your laptop. Minikube can be used to try out Kubernetes and or develop with it day-to-day.The Redis Enterprise offering extends Redis, the most popular database used with Docker containers. Redis Enterprise delivers high performance, low latency and high availability to organizations. This blog post will show you the basics steps needed to setup Minikube and run a 3-node Redis Enterprise cluster on your local laptop.We are going to use  the homebrew package manager to install minikube and kubernetes command line tool on your local laptop.Minikube offers the ability to change the Virtual Machine (VM) driver. For this blog post, we used the vmwarefusion driver.The output of the command should look like:We will use the Kubernetes command-line tool, kubectl, to verify the Minikube installation. You can verify the Minikube install using:The output should look like:Minikube is now running on your laptop and kubectl cli was able to successfully query the master node to get the status of the cluster. We will now deploy the Redis Enterprise service in Minikube with three replica sets. The yaml configuration for the Redis Enterprise deployment and service can be found here.We will use the yaml to create the deployment and service in the Kubernetes cluster.We can verify that three pods were created by issuing the commandWe need to change the binding of the CCS from local loopback address 127.0.0.1 to 0.0.0.0.We need to setup the Redis Enterprise cluster as a master node by logging into the pod and issuing a `create cluster` command.We will use the rladmin utility to create the new cluster:We will use the rladmin utility to join the 2 nodes to clusterandWe will use the Rest API of Redis Enterprise to create a database on the master node:We can look at the status of the cluster and database using the rladmin utility included in Redis Enterprise download:You can access the Redis Enterprise dashboard, which is running on port 8443, by setting up a secure tunnel between local port and the pod port. Once the tunnel is established, you can reach the Redis Enterprise dashboard at https://127.0.0.1:8443:kubectl port-forward <my-pod-name> <localport>:<pod-port>Example: kubectl port-forward redis5-58dc568c56-7qk22 8443:8443You can connect to your database using the IP of the node and the port specified during database creation:We are working on a Kubernetes native Redis Enterprise container that will take advantage of the new primitives introduced in Kubernetes 1.8 and above. We are working on releasing a new version of the Redis Enterprise container image that will leverage both the new Persistent Sets and the Storage class primitives while providing a better cluster bootstrapping experience. In the meantime, learn more about the Redis Enterprise offering."
269,https://redis.com/blog/securing-redis-enterprise-meltdown-spectre-vulnerabilities/,Securing Redis Enterprise from Meltdown and Spectre Vulnerabilities,"January 8, 2018",Redis,"With the recent security vulnerabilities discovered — Meltdown (CVE-2017-5754) and Spectre (CVE-2017-5753 and CVE-2017-5715) — Redis’ engineering, devops and support teams have been working hard to make sure our cloud services, Redis Enterprise Cloud (REC) and Redis Enterprise VPC (REV), are protected.As of now, all our REC and REV clusters on AWS, Azure, GCP and IBM Cloud have been patched by our cloud partners against Meltdown. In addition, some cloud vendors have already managed to mitigate the Spectre’s branch target injection (CVE-2017-5715).Redis Enterprise Software (RES) customers:Performance implications:Redis’ engineering team has done a series of tests to validate the effect on the performance of our cloud services. We found that the patch has a negligible impact on our Redis Enterprise VPC service, between 2.5% – 5%, whereas the impact on our Redis Enterprise Cloud service is in the range of 5%-30%, with minor outliers, depending on the cluster instance types and cloud infrastructure. Our initial tests were performed on our Redis on RAM product, and we plan to extend these to our Redis on Flash product in the coming days and weeks.Customers who feel affected by these patches can contact our support team (support@redis.com) for further help. We have successfully mitigated performance issues for several customers during the last few days.Snippets from our REV tests:We tested a 3-node REV cluster on AWS, here is what we found:Before the Meltdown fix:After the Meltdown fix:We observed a negligible impact of throughput (2.5%-5%) and almost no effect on latency.Test parameters===========The Redis Team"
270,https://redis.com/blog/securing-redis-with-redis-enterprise-for-compliance-requirements/,Securing Redis with Redis Enterprise for Compliance Requirements,"January 10, 2018",Cihan B,"Securing your data is increasingly critical as cyber attacks, security breaches and unauthorized access to data continue to escalate. Redis Enterprise provides advanced security controls to help simplify compliance with self imposed or regulatory compliance requirements such as HIPAA, FISMA, PCI, GDPR and so on.events-and-webinarsWhat is Redis Enterprise?First, for those of you who are not familiar, Redis Enterprise is a distributed NoSQL database engine that adds to the power of open-source Redis with an enhanced deployment architecture that delivers increased scalability, availability, security and lower total cost of ownership.Redis Enterprise fully supports open-source Redis and Redis commands, data structures and modules. To start working with Redis Enterprise, existing applications need only change their connection string to point at Redis Enterprise databases.Redis Enterprise is available for use in the cloud as a DBaaS platform under Redis Enterprise Cloud or Redis Enterprise VPC or it can be managed under your full control as downloadable software with Redis Enterprise Software.Redis Enterprise architecture is built to provide a great deal of control with which to meet security regulations and standards.One essential improvement that Redis Enterprise brings is the separation of paths for administrative access and data access. This separation equips Redis Enterprise with finer grain security controls, simplifying compliance and providing applications and developers with full access to one of the databases in the cluster to read and write data without requiring cluster administration privileges that may impact other databases/applications utilizing the same cluster (and vice versa).Figure: Redis Enterprise architecture showing the separate administration and data access paths.With Redis Enterprise, security controls can be divided up into a few main areas to set up defenses: Authentication and Authorization; Encryption for “data in motion” and “data at rest”; Forensics and Logging. However, it should be said that despite all the security controls, if one does not secure the underlying infrastructure that Redis Enterprise utilizes then it will be hard to protect against any attack. It is important to follow security best practices to ensure a “defense in depth” strategy. Let’s explore how Redis Enterprise infrastructure can be secured first.Defense in DepthEven though Redis Enterprise provides a great deal of security controls, a fully secured deployment requires considering all attack vectors. To have a full defense against attacks, it is important to go beyond configuring Redis Enterprise security controls and secure the infrastructure itself.Figure: Defensive firewalls at each border between public internet and the cluster internal network create the layers of defense.In cyber security this is known as “defense in depth” and refers to placing layers of barriers between the attacker and the target. In this case, you are placing barriers at the edge of the data center, in the perimeter network and within the cluster internal network.Authentication and Authorization with Redis EnterpriseIn the Redis Enterprise security model, administrative access and data access paths are separated into two independent channels. This means that when it comes to authorization and authentication there are distinct paths.Administrative Authentication and AuthorizationAdministrators authenticate to the UI, REST API or to the CLI using administrative accounts. Administrative identities can also be tied to central account management systems like LDAP stores. You can read more about how to configure LDAP-based accounts in Redis Enterprise here.Redis Enterprise limits the administrative access to a set of ports that can be secured using additional OS-level mechanisms like IPTables, firewalls, etc.Redis Enterprise provides role-based access control (RBAC). Each administrative identity in the system is also assigned to one of the built-in roles. You can find the list of available Redis Enterprise roles here.Data Access Authentication and AuthorizationApplications accessing data authenticate to the database endpoint using application passwords and certificates. When both certificate-based authentication and password is enabled, applications are required to present both factors of authentication to connect to the database.For multi-tenant systems, each tenant can be isolated to access a subset of the databases in the system by assigning unique passwords and certificates to each database.Source IP Filtering is another way data access can be controlled. With Redis Enterprise Cloud and Redis Enterprise VPC, administrators can limit the application server nodes that are allowed to connect to a database using IP filtering. This ensures that only application servers from a given source of IPs are eligible to authenticate to the database.Encryption with Redis EnterpriseRedis Enterprise provides built-in encryption for data on the wire (data in motion) and data on disk (data at rest).Encryption of Data in MotionTLS/SSL-based encryption can be enabled for data in motion.Encryption of Data at RestEncryption at rest is also available in Redis Enterprise VPC with a simple checkbox. In Redis Enterprise Software, admins can encrypt data at rest using transparent filesystem encryption capabilities available on linux OS.Forensics and Event Logging with Redis EnterpriseRedis Enterprise provides a detailed set of logs, alerts and tracing facilities to help track administrative actions, from topology changes to modifications to cluster settings.Redis Enterprise provides both alerts to track events in real time and stats to track real-time and historical trends, either rolled up to the cluster and database levels or at node and shard levels. Administrators in the system can view data up to a twelve-month window.Security Compliance and Controls with Redis Enterprise DBaaS optionsRedis provides the Redis Enterprise platform as a managed service on public cloud platforms. Redis Enterprise DBaaS provides topologies that can help simplify compliance with security standards in the public cloud. Customers get two types of deployment models:Figure: Redis Cloud deployment model. Redis Enterprise is running under Redis’ cloud account.Figure: Redis Enterprise VPC deployment model. Redis Enterprise is running under the customer’s cloud account.We just scratched the surface of the many controls that Redis Enterprise provides for securing Redis. If you’d like to get a deeper dive into security best practices, you can join the webcast here.Looking for details on Specter & Meltdown flaws? Visit our post on the topic here."
271,https://redis.com/blog/the-little-known-feature-of-redis-4-0-that-will-speed-up-your-applications/,The little-known feature of Redis 4.0 that will speed up your applications,"February 22, 2018",Redis,"Redis 4.0 brought an amazing feature to the Redis ecosystem: Modules. Modules are a big shift in Redis — suddenly, it is an open landscape of custom data types and full-speed computation right inside Redis. But while most of the fanfare over this release focused on Modules, the new version also introduced a super important command that is a game changer in its own right: UNLINK.To find out if you can use the UNLINK command, run INFO from redis-cli. The response will tell you all about your server. In the first section (#Server) there should be a line called redis_version. If this value is greater than 4.0, you’re ready to use the UNLINK command. All versions of Redis Enterprise 5.0+ and all new subscriptions of Redis Enterprise Cloud should be able to use the UNLINK command. Not all providers of Redis keep up-to-date, so it’s good to check the version before you change any code.Let’s review one of the key architectural features of Redis:  single threadedness. Redis is, for the most part, a single-threaded application. It does one thing at a time and it does those things super fast. Multi-threading is complicated and introduces locking and other gremlins that, counterintuitively, can slow down an application. While Redis (up to 4.0) did a small number of things multi-threadedly, it generally completes one command before starting another.Deleting a key (with DEL) is normally a command that you probably don’t think much about. High speed writing and reading are things to brag about, but in many cases, removing your data is just as important. Like most other commands in Redis, the DEL command operates in a single thread. This is no big deal if you’ve got a key with a value that is a few kilobytes — it will probably take far less than a millisecond. What happens when your key has a value that is a megabyte ? 100 megabytes? 500 megabytes? Hashes, Sorted Sets, Lists, or Sets are often built  by adding items over time, which can result in a multi-gigabyte key. What happens when you delete one of these large keys with DEL? Since Redis is single threaded, your whole server is tied up for… well, a while. Compounding this situation, data held in these keys may have been built over thousands or millions of tiny requests, so the application or operator may have no real understanding of how long it will take for the data to be deleted.Sanity would tell us not to run a command like this on a Sorted Set with a million members:However, DEL on some-zset will take a similar amount of time — there is no transmission overhead, but there is memory de-allocation that really adds up and all the while you’re dead in the water with your CPU pegged. Prior to UNLINK, you might have resorted to the un-atomic method of doing little deletions in conjunction with SCAN to avoid this de-allocation nightmare. Either way, it’s no fun!As you might have guessed, it’s UNLINK to the rescue! UNLINK is syntactically the same as DEL but provides a much more ideal solution. First it removes the key from the overall keyspace. Then, in a different thread it starts reclaiming the memory. This is a safe operation from a multithreaded perspective since it (in the main thread) removes the item from the keyspace and thus makes it inaccessible from any Redis command.If you have big values the speed increase is dramatic — UNLINK is a O(1) operation (per key; in the main thread) regardless of the size of the value held at the key. Whereas a big value could take a few hundred milliseconds or more to delete with DEL, UNLINK will finish in less than a millisecond (including the network round trip). Of course, your server will still need to spend cycles reallocating the value’s memory in another thread (in which the work is O(N), where N is the number of allocations of the deleted value), but your main thread performance is unlikely to be heavily impacted by the operations going on in the other thread.So, should you just replace all the DELs in your code with UNLINKs? Probably. There are a few small edge cases where DEL is exactly what you want. Here are two I can come up with:In a green field environment without extreme memory constraints, it’s really hard to fathom situations where you wouldn’t want UNLINK. UNLINK will provide more consistent behavior and overall better performance and it’s a very small code change (or no-change, if you have the ability to rename commands in your client). If UNLINK is right for your application, go ahead and change your DELs over to UNLINKs and see the improvement."
272,https://redis.com/blog/redisconf18-is-on-the-horizon/,RedisConf18 is on the Horizon,"March 1, 2018",Tague Griffith,"This year’s RedisConf is just a couple of months away—April 24th through the 26th in San Francisco. We’re working on putting together a fantastic program of community speakers, industry leaders, hands-on Redis training and a few fun surprises. If you haven’t registered for the conference already, stop and sign up before moving on to the next paragraph.The conference has moved from downtown to San Francisco’s beautiful Pier 27. The new Pier 27, built in part for the America’s Cup, is right on the Embarcadero waterfront with large glass windows providing panoramic views of the San Francisco Bay, Alcatraz and the infamous Coit Tower. We’re sure you’ll love the pier and we hope it will serve as the home of RedisConf for many years to come (Insiders tip: next door is San Francisco’s beloved Pier 23 Cafe which will play host to some official and less official after hours events).The conference starts out once again with a Day 0 training day. Our Developer Relations team recruited several of our top engineers to deliver two training tracks, Introductory and Intermediate, to help you grow as a Redis developer. If you’ve attended a prior RedisConf training or one of our Redis Enterprise Developer (RED) Workshops, you will want to register for the Intermediate track. If you have little to no prior experience with Redis, then the Introductory track is the track for you.The next two days of the conference are packed full of keynotes, sessions and perhaps a celebrity guest or two. In addition to the founders of Redis, the keynotes this year will feature Scott McNealy (co-founder of Sun Microsystems), Joel Spolsky (CEO of StackOverflow), and Abby Kearns, the Executive Director of the Cloud Foundry Foundation. Several speakers from last year, including Jim Nelson of the Internet Archive and Daniel Hochman of Lyft are joining us again this year to share their knowledge of Redis. We also have many people speaking at their first RedisConf this year, including Gonzalo Garcia of Etermax and Jiaqi Wang of Redfin. In fact, so many great speakers submitted proposals to talk that we’ve expanded the conference to include two special purpose tracks. On Day One (April 25th), we’re putting together a Cloud Native track to help you understand how to use and deploy Redis in environments where on-premise is never an option. For Day Two (April 26th), we’ve put together a DevOps track to highlight operational issues with Redis and how Redis fits into deployment frameworks like Kubernetes.Of course, no RedisConf would be complete without antirez, aka Salvatore Sanfilippo, the creator of Redis. This year Salvatore will be involved in all parts of the conference. During Day 0 training, Salvatore will cover the features and design choices behind  Redis Streams, the newest data type., Salvatore will also be a part of the keynote sessions on the 25th and 26th.We’re looking forward to seeing you in San Francisco in April and if you didn’t register after the first paragraph, do it now so you don’t miss out!PS. If you happen to be in the Tel Aviv area and can’t make it to RedisConf18 (or even if you can), be sure to get your ticket for Redis Day TLV on March 7th—before they run out! Redis Day is a full day conference in Tel Aviv that is open to the public, where Redis users can meet their peers, learn lots, and share stories."
273,https://redis.com/blog/redis-go-designed-improve-performance/,Redis and Golang: Designed to Improve Performance,"March 5, 2018",Miguel Allende,"Golang or Go (https://golang.org/) and Redis have a lot in common. Go is fast and simple. It’s a great tool for prototyping things, and has the added benefit of really fast execution with little memory. Similarly, Redis is simple, persistent and benchmarked as the fastest in-memory database. Developers are always looking to improve performance, but often must increase complexity to achieve it. This is not the case with Redis and Go,  and that’s why both are quickly becoming the most popular open source languages and databases respectively. Redis was named the most loved database by developers for 2017 and Go was named one of the top 5 most loved programming languages by developers. Though they perform different tasks, their value proposition is the same: improved performance without sacrificing simplicity.Interested in getting started with Redis and Go? This blog post  (https://golangme.com/blog/how-to-use-redis-with-golang/) has you covered. It explains how to develop application cache, session store, counters, real-time analytics, publish/subscribe and job queue management. The blog post also highlights a few code examples of how to use the popular Redigo client to:Popular Golang clients for RedisRedigo (https://github.com/garyburd/redigo) provides a print-like API for all Redis commands. It also supports pipelining, publish/subscribe, connection pooling and scripting. Redigo is easy to get started—you can access the complete API reference here: https://godoc.org/github.com/garyburd/redigo/redis.Radix (https://github.com/mediocregopher/radix.v2) provides single purpose, easy-to-get-started packages for most Redis commands including pipelining, connection pooling, publish/subscribe, clustering and scripting.Redis and Golang make a magical combination for programmers. Redis Cloud is a great way to get started with a Redis database in just minutes!"
274,https://redis.com/blog/redis-day-tel-aviv-2018/,Redis Day Tel Aviv 2018,"March 12, 2018",Itamar Haber,"On March 7th 2018, only ten days after Redis’ ninth birthday and for the third year in a row, the Israeli Redis community got together to share use cases and learn about the newest developments. The fast-paced, 16-sessions-long whole-day event was attended by nearly 300 people packed into the urban circus, Basucla. The experience was positively overwhelming and we thank everybody who helped and attended.Our eleven guest speakers represented a diversified cross section of the local high-tech industry, providing unique perspectives from their individual Redis journeys: Gur Dotan from Datorama; Shani Einav from Alooma; Sasha Popov and Etan Grundstein from Dynamic Yield; Dima Vizelman from Kenshoo; Eyal Yavor from Meekan by Doodle; Shahar Mor from Peer5; Omer Levi Hevroni from Soluto; Dekel Shavit from BioCatch; Yaron Wittenstein from Spot.IM; and Oded Shopen from Amdocs.The day also consisted of sessions conducted by laboratorists from Redis who spoke about their latest inventions. Redis’ creator Salvatore Sanfilippo performed a live command-line demo of the Redis Stream data structure. Fabio Nicotra, developer of Open Source Redis, reviewed the porting of redis-trib’s functionality into redis-cli. Yiftach Shoolman, Redis’ Co-founder & CTO, presented the latest innovations that drive adoption of Redis as a primary database. Roi Lipman, who’s currently working on a big unveiling for RedisConf 2018, shared some of his work on distributed graph databases. Señor System Architect Dvir Volk was enthusiastically applauded for presenting RediSearch’s new aggregation engine. Last but not least was Danni Moiseyev’s inaugural presentation about the time series store that he’s been working on.A summary seems to be called for: I’ve been around for a while now, and this isn’t the first “performance” I’ve helped putting together. If anything, I’m an extreme realist (if not a downright pessimist), which may explain why I was totally unprepared for the amount of interest, support and commitment that you, our community showed. I’d like to say that this was a once-in-a-lifetime experience, but I’m already thinking about next year…So, the photos are at this link and the sessions will be online soon™. As usual, feel free to reach me by dropping a line to itamar@redis.com or shouting at @itamarhaber – I’m highly available 🙂"
275,https://redis.com/blog/running-redis-enterprise-kubernetes-service/,Running Redis Enterprise Kubernetes Service,"March 20, 2018",Vick Kelkar,"Background on KubernetesContainerization enables development teams to move fast, deploy software efficiently and operate at scale. Kubernetes provides automated container orchestration and the management of containers in a highly available, distributed environment. The platform handles scaling, configuration and deployment of new versions of images and simplifies application deployment and management. At Redis, we are actively developing a Kubernetes-based deployment built on Redis Enterprise 5.x.What to expect from this articleIn previous blog posts, I talked about Kubernetes primitives and local Kubernetes development. In this post I’ll walk you through the simple 4-step process of deploying cloud-native Redis Enterprise on Kubernetes:Detailed Walk-throughFor this demo, we will use a managed Kubernetes cluster running on Google Cloud.Once a cluster is created, store the license information using Kubernetes secrets primitives. In this example, we will set up a Kubernetes secret object called rp-secret. The command to create the secret object is shown below:The command should output the following:Next, Deploy the headless service, controller and Redis statefulset manifests using yaml files* as shown in the following screenshots:* We will publish the yaml files when Redis Enterprise for Kubernetes is made generally available.A Redis Enterprise three-node cluster with all running Kubernetes resources will look like this:Upon successfully deploying Redis Enterprise on the Kubernetes cluster, create a database using the web interface.Once the database is created using the web interface, the Redis Enterprise service controller will publish the endpoint of the database in the Kubernetes service catalog, as shown in the screenshot below for the database db-k8demo:Using the published endpoint of the database in the Kubernetes service catalog, we perform a load test on our Redis database using a benchmarking tool called memtier_benchmark. The memtier tool is a command-line utility useful for generating and benchmarking NoSQL key-value databases. The load generated by the benchmark tool can be seen in the metrics section of the web interface of Redis Enterprise. The screenshot below shows the load generated by the memtier_benchmark tool on db-k8demo database.Summing upAs organizations begin to use containers at large scale, orchestration frameworks become necessary to manage the increased complexity. While Kubernetes was initially developed to run stateless services, it’s also made running a stateful service such as the Redis Enterprise database much easier. With the rapid migration to cloud-native architectures and microservices, the combination of kubernetes and a super performant and highly available Redis Enterprise database will help organization innovate faster in the new era of application development and delivery. We’d love to hear more about how you would like to see Redis Enterprise in Kubernetes—please feel free to contact me directly for feedback.For more information about Redis Enterprise, please visit our technical documentation and or release notes page."
276,https://redis.com/blog/clock-ticking-ready-gdpr/,The clock is ticking – are you ready for GDPR?,"March 28, 2018",Priya Balakrishnan,"You’ve likely heard about the General Data Protection Regulation (GDPR) that begins on May 25, 2018. These requirements put forward by the European Commission, harmonize data privacy laws across the EU with the overall intent of protecting private information for European citizens. GDPR’s key principles revolve around data privacy and subject rights in our increasingly data-driven world.Under this regulation, when there is a data breach in an organization that holds EU citizens’ information, there are strict procedures for how the company must respond. Given this data privacy overhaul, all of us need to become more diligent about how we collect and store data, and what we do with it. In this blog, let’s explore some of GDPR’s critical new data protection frameworks, and how you should adapt your environment in order to meet them.1It’s not just an EU thing!We’re all impacted by privacy laws and non-compliance penalties are big. This new regulation broadly affects all organizations, government agencies and companies throughout the world that collect or use personal data tied to EU residents (irrespective of any physical operating presence in the EU). Any organization failing to satisfy the new regulations will face maximum penalties of 4% of global revenues or €20M, whichever is higher, as well as the potential suspension of further data processing.Lots of organizations are preparing for it, but many are unlikely to be fully compliant by May 2018. Gartner predicts that more than 50% of organizations will still not be in full compliance by the end of this year — seven months after the regulation takes effect. Yet while complying with GDPR is a HUGE initiative, organizations that make the effort will gain the faith and trust of their customers.These policies completely change the way data has to be handled and are going to change how you approach your data. So, how do you prepare?2Data-related policies:GDPR has many regulations directly related to how data is accessed, stored and protected in the database layer. Here are a few to consider surrounding data design and storage:3Database compliance:It is important to note that full compliance with GDPR cannot be enforced with infrastructure changes alone. It is a heavy process involving policy definition and enforcement, evaluation of the complex application and IT landscape, automation (where possible) to enforce governance and modernization of infrastructure if necessary. One such layer that definitely needs evaluation and compliance is your database. Let’s look at some of the specific database implications below:While you’re reviewing your processes and changing your organization’s data policies to meet regulations, your business, of course, cannot be interrupted. So, how do you ensure that it continues to meet business demands while also preparing for these imminent compliance requirements? It’s a good idea to simplify your assessments and secure your environment by using solutions and tools that inherently meet the standards of compliance.4Redis Enterprise meets GDPR compliance standards!For the database layer, we at Redis have spent a LOT of time making sure your deployments are secure. With Redis Enterprise, you can simplify compliance and protect your data to meet any self-imposed or industry standard security needs. We understand that data is the most valuable asset organizations have today. How it is captured, used and stored is the key to capitalizing on new technology and developing new revenue streams. Since the announcement from the European Commission, we’ve been working diligently to ensure our database software meets all GDPR regulations — demonstrating our commitment to data protection.Redis Enterprise is a secure database that provides a great deal of controls to help you meet security standards. Each database in the system can be isolated using distinct credentials, limiting access to data. It offers multi-layer security configuration for access control, authentication, encryption, forensics, availability and more. Redis Enterprise’s capabilities include data encryption both at rest and in transit. For more on this, check out our recent webinar about how Redis Enterprise helps deliver advanced data security and encryption or the cross-links in this post that describe our security architecture in great detail.Lastly, another great opportunity to learn more about Redis Enterprise is at our annual user conference, RedisConf. It is just over a month away—April 24th through the 26th in San Francisco—and will include training programs that deep dive into the inner workings of Redis, as well as a slew of keynotes, sessions, and topics delivered by industry leaders, community speakers and Redis experts. We hope you will join us there to learn how Redis Enterprise takes you one step closer towards data compliance, as you prepare for D-Day 🙂If you have any questions, please do not hesitate to reach out to us."
277,https://redis.com/blog/redis-enterprise-5-0-2/,Redis Enterprise 5.0.2,"April 3, 2018",Paz Yanover,"We, at Redis, are happy to announce the general availability of Redis Enterprise 5.0.2, the latest version of our high performance in-memory database platform. This new version is now available with each of our deployment options:The major enhancements of version 5.0.2 include:Redis Enterprise Software (RS)Additional Active-Active (a.k.a. Redis-CRDT or CRDB – Conflict-Free Replicated Database) Capabilities– Import a standard RDB file to a CRDB.– Export a CRDB dataset from one of the CRDB instances (i.e. replicas) to a standard RDB.– Dynamically and seamlessly add or remove a CRDB instance (replica) without affecting performance.– Track new performance metrics on the bidirectional replication for CRDB.Improved Encryption Options for Data in TransitRS 5.0.2 provides multiple encrypted communication options for various data flows, which help comply with regulatory requirements and can be easily and intuitively deployed. For example:Encrypted Communication Across Clusters Over the WANFor active-active (CRDB) and active-passive (‘replica-of’) deployments, RS 5.0.2 adds the ability to encrypt data on-the-fly if an SSL handshake was established during the connection setup phase. This allows customers to run multiple types of connections between clusters (i.e. encrypted and unencrypted) without changing the configuration of database endpoints.For more information about how to enable SSL, follow these links for active-passive and active-active.Encrypted Communication for the Discovery Service (Sentinel API)The Discovery Service enables dynamic IP-based communication between your application and Redis using the Sentinel API. Starting with RS 5.0.2, the Discovery Service using Redis’ Sentinel API can require encryption (TLS/SSL-based) when the chosen Redis client library supports it.Redis Enterprise Modules EnhancementsIn RS 5.0.2, Redis Enterprise Modules were improved as follows:– The ReBloom module was enhanced to support Cuckoo Indexes, along with Bloom Filters. Cuckoo Indexes are used for high-speed set membership tests, while supporting adding and removing items dynamically with even higher performance than Bloom Filters.Preview Release of Kubernetes SupportThe initial integration of Redis Enterprise with Kubernetes is reflected in a native Redis Enterprise container that will take advantage of the new primitives introduced in Kubernetes (e.g. Kubernetes secrets and stateful sets). This allows Kubernetes customers to enjoy highly available, durable Redis Enterprise with high-throughput, low-latency, in-memory transactions.Redis Enterprise VPC (RV)Integrated Redis ModulesOur zero-touch Redis Enterprise VPC service now supports the following Redis Modules:RediSearch – An extremely fast search and secondary index engine over RedisReJSON – A JSON data type implementation for RedisReBloom – A scalable bloom filters as a new data type for RedisModules. This can be enabled when creating a new database, as illustrated below:Use this table for a full comparison between various Redis Enterprise deployments.Of course, if you’re interested in a more detailed view of what’s new in Redis Enterprise 5.0.2, please visit our technical documentation or check out the release notes.If you have any questions, drop us a line at: pm.group@redis.com"
278,https://redis.com/blog/redis-enterprise-service-kops-managed-kubernetes-cluster/,Redis Enterprise Service on Kops managed Kubernetes Cluster,"April 16, 2018",Vick Kelkar,"This tutorial will show you how to easily set up a Kubernetes cluster on a public cloud using a tool called “Kops.” This post is a complement to our Kubernetes webinar, in which we explained the basic Kubernetes primitives, and our previous blog posts about Redis Enterprise Service and local Kubernetes development. For this tutorial, we will use the latest publicly available container image of Redis Enterprise Software. You can read about the high performance, in-memory Redis Enterprise 5.0.2 software release here.What is Kops?Kops stands for “Kubernetes Operations” and is an official Kubernetes project. The purpose of the Kops tool is to set up a production-grade Kubernetes cluster on a public cloud. We will now walk through the steps needed to set up Kubernetes cluster and install Redis Enterprise Service on the Kops-managed cluster.STEP 1: Prepare your local machineYou will need to prepare your local machine by installing a few command-line tools (Note: the following instructions are for Mac OS):Install Kubernetes command-line tool, Kubectlbrew install kubectlInstall Kops command-line toolbrew install kopsInstall AWS command-line toolbrew install awscliSTEP 2: Configure DomainKops requires a valid domain name for your Kubernetes cluster. Kops will also create DNS entries for the API and bastion host. For this tutorial, I will use Route 53 Hosted Zones k8.vkelkar.com:We will use ”demo” as the name of the Kubernetes cluster. Once the cluster is spun up, Kops will add additional DNS entries to the delegated subdomain of k8.vkelkar.com:STEP 3: Configure bucketKops stores the state of your Kubernetes cluster in an S3 bucket. You have to create the S3 bucket using the aws command line:aws s3 mb s3://<name-of-your-private-S3-bucket>Once you create your S3 bucket to store the state of your cluster configuration, you need to either export it as an environment variable or pass it as an argument in the kops command. For this tutorial, I set the environment variable as follows:export KOPS_STATE_STORE=s3://<name-of-your-private-S3-bucket>STEP 4: Create Kubernetes ClusterNow that we have configured the DNS and S3 bucket, it is time to create a production-grade Kubernetes cluster. You can specify the name of the cluster as a command-line option or pass it as a variable. This tutorial used the command-line option, but you can set the variable as follows:export NAME=demo.k8.vkelkar.comThe Kops command below will set up a private topology cluster and provision a bastion host for that cluster. We have added labels (in the format of “key=value pairs”) to our nodes to track cluster ownership; you can add additional labels to track resource consumption.kops create cluster --cloud=aws
--networking=weave
--master-size=c3.large
--master-zones=us-west-2a
--node-size=c3.4xlarge
--node-count=4
--topology=private
--bastion
--ssh-public-key=/Users/vkelkar/.ssh/Vick_Kelkar_aws.pub
--zones=us-west-2a,us-west-2b
--name=demo.k8.vkelkar.com
--cloud-labels ""ClusterOwner=Vick,Team=PM-Team,Org=Americas""
--image=ami-5c97f024
--yesThe resulting set of Kubernetes cluster and nodes will look like:kubectl get nodesHere are the additional labels, as shown in the console:You can find out about the AMI used in this tutorial by running the command:aws ec2 describe-images –image-id ami-5c97f024STEP 5: Deploy Redis Enterprise Service on Kubernetes ClusterFor this tutorial, we will deploy a three-node Redis Enterprise cluster in the Kubernetes cluster:Using the `rladmin` command-line utility included with Redis Enterprise service, we can take a look at the three-node Redis Enterprise cluster and the “awsdb” created on the cluster:The resource consumption of Redis Enterprise in the Kubernetes cluster will look like:STEP 6:  Delete Kops Kubernetes ClusterNow that we have successfully deployed the Redis Enterprise cluster on the Kubernetes and were able to create a database called “awsdb” (pictured above), we can delete the Kubernetes cluster by issuing the command:kops delete cluster demo.k8.vkelkar.com –yesConclusionKops is a great tool with which to quickly spin up a Kubernetes cluster for your needs. This could be a cloud-independent way to test your Kubernetes release or to reduce the setup time of a production-grade Kubernetes cluster. If you would like to learn more about the Redis Enterprise Kubernetes release, please contact us at sales@redis.com."
279,https://redis.com/blog/redis-manage-storage-replication/,Redis to Manage Storage Replication,"April 18, 2018",Rahul,"Redis is a simple, yet powerful in-memory database platform with use cases ranging from session management, queues and pub/sub to general-purpose cache. With its persistence and in-memory replication capabilities, Redis Enterprise is also used as a primary datastore.As a freelancer, I frequently use Redis to overcome unique problems. In one project, our use case was simple: I wanted to replicate the contents of a file system partition with a fixed, well-defined structure: at the root of the file system we had a fixed set of directories, each with more than a million files. Our previous solution ran two processes in parallel, 24/7 to identify the modified files. The first process scanned all the file contents and identified the changed content since the previous replication. A job ran once every 24 hours to replicate the changed files. The second process indexed all the files that were successfully replicated.In our previous solution, we used an SQL database to store the file metadata (such as name, size, permissions, path, etc.) and all the information related to modified files. The scheduled replication job queried the database to pull the list of files that were modified, then replicated the content onto a remote server. After replication, it updated the SQL database, marking the files as ‘copied,’ after which the indexing process picked up the marked list to index the file content.Our previous design had major disadvantages: we had to write a lot of code to save/retrieve/modify data in the SQL database, and as the database grew, we had to build a mechanism to prune the data. As the operational overhead became extensive, we started looking at ways to break free of this architecture.That’s when we found Redis. Redis instantly solved many of our problems. In our new solution, we used the Redis pub/sub feature to notify our various processes of new detections. The scan process published the details of the changed file(s) to a Redis channel. The replication process subscribed to that channel and replicated the file as soon as it was notified. Then the replication process notified the indexing process (via another Redis pub/sub channel). This system eliminated the need to run a job once a day and instead ran replication as an ongoing process. We also saved ourselves the hassle of cleaning up the SQL database!Before considering the Redis pub/sub model, we also considered RabbitMQ, Kafka, etc. Each was good but required a lot of work on our end, with a bit of a learning curve. None were as versatile and easy to use as Redis.As we adopted Redis, we discovered many of its other advantages. We started using built-in data structures such as Lists, Hashes and Sets, to perform analytics. For example, we wanted to know the frequency of changes to the files and directories, and which applications made those changes. We used Redis and its data structures to gather this information. We then passed it to the indexing process and enriched the indexed meta data with analytics data.Getting started with Redis is extremely easy. You can sign up for Redis Cloud for free at: https://redis.com/redis-enterprise-cloud-free-30-mb-planThis is a guest post by Rahul, an independent consultant and user of Redis Enterprise as part of his contract work."
280,https://redis.com/blog/persistent-memory-and-redis-enterprise/,Bringing The Latest Persistent Memory Technology to Redis Enterprise,"April 25, 2018",Priya Balakrishnan,"It’s amazing to think that in the last 50-60 years, the tech industry has only produced around 6-7 classes of memory technology. Even so, each of those memory classes brought different attributes to the way we store and retrieve data. From RAM in the early days to SRAM, DRAM, NAND Flash, and now to solid-state drives (SSD) and NVMe (Non-Volatile Memory Express), we’ve seen significant improvements to the speed, density, and stability of our data storage options.Redis on Flash (RoF) takes advantage of the increasing availability of high-speed storage in the form of Flash SSDs, resetting existing price/performance expectations. By extending Redis from RAM to Flash, and using intelligent tiering to always keep hot values in RAM, RoF reached new levels of throughput while still retaining sub-millisecond latencies. With this innovation, the economics of data stored in Redis changes completely.We’ve seen many of our customers benefit from this approach. For instance, Whitepages uses our Redis on Flash solution to store and query several terabytes of data – with hot values and keys in RAM, and cold values in cost-effective, flash-based SSDs. This architecture lets them keep only 30% of their dataset in RAM and still achieve less than sub-millisecond latency from Redis (on Flash). In fact, they’re maintaining end-to-end application latencies of <100ms. This substantial reduction in RAM consumption saved Whitepages hundreds of thousands of dollars in infrastructure costs each year.In the last few years, Intel and Micron have been working on a completely new class of storage and memory technology that is faster, denser and non-volatile, based on 3D XPoint™. Their goal is to improve overall system performance and lower latencies by putting more data closer to the processor on nonvolatile media. At Redis, we’ve been working very closely with Intel to ensure our solutions run optimally with this new technology. We started by benchmarking RoF over Intel Optane, an NVMe SSD card based on the 3D XPoint™ technology, and saw significant performance increases over the standard NVMe-based SSD solution. In the past few months, we have been tuning and benchmarking RoF to work with the new form factor of 3D XPoint technology based on NVDIMM.Intel’s persistent memory based on 3D XPoint technology delivers a new tier between DRAM and SSD that can provide up to 6TB of capacity in a two-socket server at performance comparable to traditional DRAM memory. In addition, Intel has been working with the industry to create a new programming language model for Linux and Windows environments, which would allow applications to directly and persistently engage with data in memory. This means that applications like RoF can decide which part of the dataset will remain purely in DRAM and which part can be hosted on both DRAM and 3D XPoint™. As part of this effort, we’ve redesigned our RoF code path to maximize performance gains from the new technology. We use the embedded storage engine in the main Redis thread and Intel’s DAX to access the NVDIMM. This helps reduce internal bottlenecks and eliminates the overhead associated with context switching between the Redis main thread and the I/O threads that are used to run the storage engine.The end result – Redis on Flash users may see performance comparable to DRAM, even if over 80% of the dataset is stored on NVDIMM. This translates to a significantly lower total cost of ownership.Why is this important to users of Redis?Persistent memory allows you to think of memory as the main storage tier for your data. Operating at the speeds promised by the new Intel NVDIMM technology will be a game changer for Redis users. When this starts shipping with servers, customers won’t just gain a fast, persistent data store that’s closer to the CPU and memory. With RoF, they’ll also gain the ability to extend their “memory.”Additionally, if Redis users have limited themselves because of the cost of memory, that thinking is about to change. Keeping data in memory is about to get even cheaper. The new persistent memory tier allows you to keep more data per node, delivering a significant reduction in infrastructure costs while maintaining performance.Does this mean that not only Redis but every other DBMS can now enjoy the new speed introduced by 3D XPoint™ technology?Actually, this is a misconception, because your DBMS must be optimized to work with the new technology. In fact, when we first started to work with 3D XPoint™, we saw zero impact on performance versus the tests we’d done over Intel Optane™ (NVMe-based SSD card based on 3D XPoint™). That was due to the fact that our entire software stack and storage engine were not designed to work with this level of speed of persistent memory. That said, since Redis is based on an in-memory engine (all its data structures are byte-addressable, with no special serialization/deserialization processes), it was relatively easy to adapt the RoF stack to work with NVDIMM (which is also byte-addressable by design).Existing disk-based databases built their entire engines on storage engines that were not designed to be byte-addressable. Their serialization/deserialization overheads and the long access time to their internal disk-based data-structures will prevent them from achieving the expected performance boosts when running on this revolutionary technology.Moving the majority of processing infrastructures to 3D Xpoint™ technology won’t happen overnight, but it is an event you need to be prepared for. We believe that once 3D Xpoint™ technology becomes mainstream, the majority of the database market will have to move to being in-memory. There will simply be no more reason to continue using disk-based databases. As a developer or software architect, you should start thinking about moving your application code to work natively with in-memory databases, like Redis. The sooner you do, the better you’ll position yourself against your competitors."
281,https://redis.com/blog/redisconf18-in-review/,RedisConf18 in Review,"May 23, 2018",Tague Griffith,"Over 1,200 Redis enthusiasts took over Pier 27 on the San Francisco waterfront for three days of training, talks and fun at RedisConf18.  The theme of this year’s conference was “Everywhere” and with over 60 breakout sessions across six concurrent tracks, Redis really was everywhere.This year RedisConf moved to the beautiful Pier 27 with panoramic views of the San Francisco Bay, both bridges and several iconic San Francisco landmarks. Pier 27 is the San Francisco Cruise Terminal originally built as a staging site for the 2013 America’s Cup. The pier serves as a cruise terminal and a conference venue during the off-season.Moving RedisConf to the waterfront allowed the conference to accommodate the growing number of participants, hold more sessions and stage some really fun things to do in-between sessions. This year we had a DJ, a table full of stickers, a geek lounge and a giant Light Bright wall which featured some amazing pictures built by attendees.A host of food trucks rolled up for lunch, providing attendees with everything from BBQ to Korean Fried Chicken (FYI: cheesesteak and old fashioned donuts are now the Official Lunch™ of Redis Geeks everywhere).Training DaySalvatore Sanfillipo, the creator of Redis, kicked off the all-day Redis training with his explanation of the primary data structures of Redis, followed by a talk on the new Streams data structure. This year’s training day featured two tracks: an introductory track for newRedis users and an advanced track for power users. Developers who attended either track had a chance to learn about Redis directly from folks like Salvatore, Dvir Volk, Itamar Haber and several other contributors to the Redis project.The training session covered a wide range of topics, from the Redis version of “Hello World” to improving performance with the Redis Cluster API. Attendees spent the day listening to Redis experts and working through a number of coding exercises to deepen their understanding of Redis.This is the second year we’ve hosted a training event on the day before RedisConf, and it’s been a great way for folks to really sharpen their Redis skills and get the most out of the conference.Redis is EverywhereRedisConf18 featured six concurrent speaking sessions and over 75 speakers sharing their knowledge and experience with the community; Redis truly was everywhere. This year’s talks covered everything, from using Redis on an avocado farm to deploying Redis applications with Google Skaffold, as well as a practical tutorial on Redis memory optimization.Following Salvatore’s keynote, Ofer Bengal and Yiftach Shoolman of Redis made a series of announcements around Redis modules, Active-Active geo distribution using CRDTs for all major Redis data types, a new University for Redis, and upcoming innovations with Persistent Memory. The Redis modules announcement included the new RediSearch aggregations functionality and changes to the architecture of the Redis Graph module that utilize GraphBLAS technology for even greater efficiency in execution. Prof. Tim Davis of Texas A&M University, the creator of GraphBlas, explained the sparse matrix multiplication algorithms behind GraphBLAS while Roi Lipman from Redis laid out the usage of GraphBlas supercharges Redis Graph to outperform other graph databases by up to 100x. Prof. Carlos Baquero of the Universidade do Minho (one of the lead CRDT researchers) explained the history and research behind CRDT, the technology used to add active-active functionality to Redis CRDTs. Ken Gibson and Andy Rudoff from Intel explained how Redis on Flash can benefit from utilizing Intel’s new Persistent Memory technology.Over 75 speakers from all over the world joined us at the conference to share their unique experiences with Redis. First-time speaker Glenn Edgar of LaCima Ranch  had one of the most unique talks in RedisConf history, detailing how he combines Redis with IoT sensors to run LaCima Avocado Ranch. Using Redis, Linux and ARM devices, Glenn walked us through a history of building a custom, open-source system to manage irrigation and other tasks on the ranch.Many other first-time speakers like Jiaqi Wang of Redfin (“Serving Automated Home Values with Redis and Kafka”), Aditya Vaidya of Oath (“Video Experience Operational Insights in Real Time”), and Darren Chinen of Malwarebytes (“Transforming Vulnerability Telemetry with Redis Enterprise”) joined Glenn in making their debut appearances this year.Several alumni speakers and community leaders like Dmitry Polyakovsky (“Integrating Redis with ElasticSearch”), Stefano Fratini (“Amazing User Experiences with Redis & RedisSearch”), Sripathi Krishnan (“Redis Memory Optimization”), and Dvir Volk (“Introducing Real-Time Insights with the RediSearch Aggregations”) made return appearances to share more of their experience with the Redis community.And of course, no RedisConf is complete without a glimpse into the future of Redis from Salvatore.  Salvatore spent the keynote talking about the future of Redis looking forward to some of the possible work for Redis 6 and 7. Some of that work, including the new version of the RESP protocol, is already starting to emerge. Salvatore held a breakout session to explain the design principles behind the Streams data type.This year saw the conference expand into two focused tracks—one on Microservices and one on DevOps—to bring information about technologies often used in conjunction with Redis. Attendees in these sessions had an opportunity to learn about deploying Redis via Kubernetes and how to manage development, testing and production Redis resources using OpenShift.If you missed out on this year’s conference (or you came but didn’t get to go to every session), never fear—all the breakout sessions and keynote talks were recorded and will be published to the Redis YouTube channel once they are edited. While you’re there, check out some of the talks from previous conferences!Looking ahead to RedisConf19Even though we just finished this year’s conference, the team is already thinking about RedisConf19. The growth of the conference over the years has been astounding and next year we want to bring you another great lineup of speakers and fun activities to make for a memorable experience. Keep an eye out for announcements about 2019 and especially the open call for speakers—if you haven’t spoken at RedisConf before, why not make 2019 your debut?We want to thank all of our sponsors, speakers and attendees for contributing to a fantastic conference this year! We hope to see you next year for RedisConf 2019 for what should be, in the words of our attendees, another:"
282,https://redis.com/blog/accelerating-avatars-redis-enterprise/,Accelerating Avatars with Redis Enterprise,"March 29, 2018",Baptiste Leterrier,"First a little bit about us: created in 2011, Silkke strives to create animated 3D avatars of unsurpassed quality that are usable in various applications, virtual universes and compatible internet websites. Our mission is to create a human element in digital platforms, integrating with various brands to bring them closer to their customers by offering them a unique, personalized experience.Creating an avatar requires several tasks, from the initial scan of the person to the 3D life-like clone. There are four separate steps involved in the scanning process, and even though that doesn’t seem like much, this places major strain on the total capacity of one booth performing scans. Each booth can scan up to one person each minute! Now multiply this by the number of booths around the world.That’s a lot of work.As this process occurs, scalability is more than necessary, as we don’t know how many avatars we will have to process. So we use RabbitMQ to create queues that assign work to every process that renders part of an avatar. Doing so permits us to scale the processing power as the queue fills up.This presents another challenge: how best to organize the content that’s in the queue?As it fills up, we can’t know the order of a set of instructions in the queue. And what if we need to modify an instruction on the go—we can’t edit something without removing all the initial elements and then requeuing them, thus changing the global order and potentially risking a slow down of production.First, we needed a system that could process a lot—and fast!— so we used RabbitMQ. But we also needed a simple system that supports designing and querying on the software side (query all then parse and show), as well as that ability to natively propose FIFO. Rather than develop a new queuing solution from scratch, one that can address all of our needs, why not take the practical approach and integrate an existing technology that works well with RabbitMQ? , That’s why we chose Redis!We introduced a new process that duplicated all instructions: one in RabbitMQ for processing and one in Redis for monitoring/editing:As one message goes into RabbitMQ, the same is put in a Redis Set FIFO style. As the workers process the queue, they also remove/put the messages in Redis, keeping our Set in sync with the RabbitMQ.Next we developed a web interface to control and show what is in Redis. It also controls the state of the queue as a Redis value with expiration that tells the worker whether or not it should consume the queue (we can stop the consumption of a queue temporarily without stopping the workers).For record purposes, a global event can have up to 10-20 messages per second (for 1 booth), and we have booths all over the world – but thanks to Redis Enterprise Cloud’s awesomeness, it can be scaled!Unlike some applications that just display the avatar of a person, Silkke faced the challenge of exhibiting dynamic character movement. For example, avatars needed to wander and interact by chatting and sending messages. Though there were already some solutions for multiplayer support on the market, none were able to fit our needs—especially on the server side. The main problem we faced was how to synchronize the display of one avatar in the game with others.The Answer: rooms.Like chat room for text, we needed a solution to register which avatar is currently present. As for the previous problem, we could have three avatars to several thousands at the same time, but only with random connection spikes.Luckily Redis came to save the day again! We can create rooms where we store the ID of avatars for each person that connects to the app. The architecture is simple: a set for each person. But by benchmarking it, we thought of a new way to implement Redis. How about using it for the chat system, using the pub sub system and a socket system? Then things got pretty good. We could handle 5,000 people in one room. And rather than only using it for text messages, it doubles as a relay for commands.Say you want your avatar to go take a drink at the bar. From your phone you click “grab a drink,” and that’s it. But under the hood, Redis demonstrates its power. In under one second, it checks your Auth key, checks if your avatar is in the scene and connected, sends the instruction to go to the bar, checks whether the instruction started correctly and notifies you. All with pub-sub, expiration and hash-maps. Then we can scale it to every avatar you see in the picture. Redis is once again the perfect fit for our needs.Several more projects are in the think tank for now, and Redis is vital when solving problems of speed, reliability and scalability.To get started quickly with Redis, you can sign up for Redis Cloud for free at: https://redis.com/redis-enterprise-cloud-free-30-mb-plan"
283,https://redis.com/blog/redis-smart-cache/,Redis Cache-Aside Simplified,"May 23, 2023",Allen Terleto and Kyle Banker,"Redis Smart Cache is an open source library that seamlessly adds caching to any JDBC-compliant platform, application, or microservice.Cache-aside is the most common caching pattern implemented by Redis developers for optimizing application performance. And while cache-aside is conceptually simple, correctly implementing it can be more difficult, outage-prone, and time-consuming than it appears.In keeping with a primary pillar of the Redis manifesto – we’re against complexity – we developed a new solution called Redis Smart Cache, an open-source library that seamlessly adds caching to any platform, application, or microservice that uses a JDBC-compliant driver for connectivity with its system-of-record.Redis Smart Cache allows developers to identify their worst-performing queries, dynamically enable query caching, and observe ongoing query performance. All without changing any code. As a result, you can optimize an application’s performance and take advantage of Redis’ speed and reliability simply through configuration.Redis Smart Cache is useful for optimizing demanding online transaction processing (OLTP) applications, data warehouses, and analytical workloads. For example, you can integrate Smart Cache with Tableau, instantly making live dashboards more responsive.Since the proof is in the pudding, we recommend going straight to the Redis Smart Cache demo. You’ll have Smart Cache up and running in minutes, and you’ll see first-hand how quickly you can optimize an application with cache-aside at Redis speed.Typically, adding Redis to an application, for query caching, requires that you change the source code. In the most naive implementation, these modifications take the form of a simple if statement. Consider this pseudo-code implementation of the cache-aside pattern:However, it’s considerably more complex in practice, especially for mission-critical applications and microservices.First, you will need to discover which queries are slow by examining the database’s query log or consulting your application performance monitoring system. Assuming these queries can be effectively cached, you will then need to associate them with the application code from which they are produced. Once you find each query’s origin, you can implement the cache-aside pattern.In addition, you’ll also need to design a key schema for the Redis cache and properly configure each cached query’s time-to-live (TTL). Don’t forget to add error handling where needed. It’s also worthwhile to ensure that serializing the result set is reasonably compact and performant. If you’re using a high-level application framework, such as Spring Boot, you might enable caching through a series of configurations and annotations instead.For business cases with spiky behavior, you will need to account for query performance during seasonal peak usage, changes in user behavior, and maintenance changes to the queries themselves.Finally, you need to test. Even with caching annotations, you have to ensure that the intended database queries are indeed cached as expected.Using the Redis manifesto as our guiding principle, we set out to reduce application complexity and provide insights about the queries flowing through SQL-based drivers. We also wanted to empower developers to easily update caching rules as their data access patterns change.We began with the basic requirements for implementing SQL query-caching within an application, microservice, or third-party platform.Next, we challenged ourselves to implement these requirements in a way that allows existing applications to add Redis without refactoring their code. In other words, a no-code solution that could be managed solely via dynamic configuration.Since most of our enterprise customers leverage Java, we began by extending the JDBC API as it acts as the bridge between most JVM-based applications, microservices, and third-party frameworks for SQL-compliant database connectivity. The databases supported include household names such as Oracle, IBM DB2, Microsoft SQL Server, PostgreSQL, and MySQL, as well as Snowflake, BigQuery, Tableau, and other platforms that support JDBC-compliant drivers.Let’s imagine a real-world application. Suppose you work for an investment brokerage firm that deploys a portfolio management application.The application has two components:If this application isn’t performing within acceptable service level agreements (SLAs), and data access patterns make caching a feasible solution, the engineering team could write the necessary code to address the problems.Doing so begins with the developers analyzing the existing code. Research on how to use Redis for query-caching and performing the actual programming. The QA team would run their load tests, and finally, the application is ready for production.Depending on the amount of technical debt in your application, the effort to cache-aside a few queries might not be so bad. However, if the previous developer didn’t maintain an abstracted layer for each query’s prepared statements or this was just one of many microservices, then it could be a tougher project than it originally seemed before analysis.The accumulated refactoring costs and the longer than expected project timeline are typically where we see these initiatives fall apart. Until the user experience becomes unbearable that is.Here’s the alternative. With Redis Smart Cache you can deliver the same outcome with a few simple steps:(For a production deployment, we recommend consulting the Redis Smart Cache installation guide for more details and configuration options.)That’s it. With Redis Smart Cache, there is no need for code analysis, no need to learn much about Redis (although we do recommend it as a rewarding activity), and you can avoid technical debt which improves your time-to-market. You don’t even have to learn the best ways to handle failover and thundering herds. It’s all baked into the cake. Simplicity!The Redis Smart Cache CLI is a command-line interface (CLI) for managing Redis Smart Cache. While you can configure Smart Cache entirely using JDBC properties, the CLI lets you construct query rules interactively and apply new configurations dynamically.With the CLI, you can also view your application’s parameterized queries, or prepared statements, and the duration of each query. Redis Smart Cache captures access frequency, mean query time, query metadata, and additional metrics. These metrics are exposed via pre-built grafana dashboards; the included visualizations help you decide which query caching rules to apply.Once you identify the ideal queries to be cached, you can use the CLI to stage and then commit a new caching configuration.For example, suppose you want to cache all queries initiated against the TRANSACTION_HISTORY table for up to five minutes. First, you would select create query caching rule from the CLI, then create a match-any table rule, supply the table name (TRANSACTION_HISTORY), and add a TTL of 5m.This new rule change is marked as pending until you commit it. Once committed, every application instance using the Smart Cache library consumes the new configuration and starts caching queries that match this rule.When you first install Smart Cache, you won’t see any changes to your application’s behavior. This is by design. Smart Cache starts capturing the system of record’s query performance metrics but doesn’t cache any queries until you explicitly decide which queries to cache.This is because Smart Cache effectively operates as a rules engine. The configuration that you create is a collection of caching rules. You can build per-query rules that match an application’s exact parameterized queries or create more general rules.As illustrated in the example above, Smart Cache can even match all queries that contain a particular table or set of tables. If the query-matching and table-matching rules still aren’t granular enough, you can also create your own regular expressions for query matching.To get started with Redis Smart Cache, follow the installation guide. The Redis Smart Cache core library is open source and can be used with any Redis deployment.To use the CLI, analytics, or dynamic configuration features, you need a Redis database that includes the capabilities of Redis Stack. In particular, Smart Cache CLI requires search and time series. If you’re unable to deploy Redis Stack on your own or in production, we recommend Redis Cloud and Redis Enterprise.Don’t hesitate to file an issue to suggest improvements, and feel free to contact us for assistance. We’re eager to help and are just getting started! Already on the docket are a number of exciting capabilities, including enhanced Grafana dashboards for monitoring Smart Cache’s performance, prescriptive TTLs, and a cache invalidation protocol to minimize the time window for stale data.To learn more about Query Caching with Redis Enterprise, see our caching solutions overview.Huge thanks to our dedicated field engineers Julien Ruaux for building the core Redis Smart Cache JDBC driver and Steve Lorello for creating the Smart Cache CLI."
284,https://redis.com/blog/multi-model-redis-database-minikube-developers/,Multi-Model Redis Database on Minikube for Developers,"June 11, 2018",Itamar Haber,"Setting up your development environment is rarely straightforward and hassle-free. Whatsmore, given modern applications’ appetite for polyglot persistence and containerized deployment, it becomes quite challenging to bootstrap your laptop with all the infrastructural goodness needed for a simple “Hello, World!”. In this post, I’m going to show you how to quickly get started developing your application with a multi-model Redis database on Kubernetes.A multi-model database is one that supports multiple data models against a single backend. While Redis is, at its core, a key-value-like data structures store, modules can extend it in almost any conceivable way. Presently, Redis’ open source modules add the following capabilities to Redis:Redis Enterprise already includes all these modules and can readily be deployed and run on Kubernetes. However, up until recently there was no ready-made open source Redis container image that delivered the same functionality. So I made one, automated its build and put it on Docker Hub: https://hub.docker.com/r/redis/redismod/The redismod container provides a default installation (i.e. not production-hardened) of a single-instance Redis server. It is also configured to load all five modules upon startup, but you’re more than welcome to override this behavior. Running the container is just a matter of executing the following command at your terminal prompt:docker run -p 6379:6379 redis/redismodTo use the redismod image (alongside your application’s) on Kubernetes, assuming you don’t have access to Kubernetes deployment, you can use minikube. As stated by minikube’s documentation:It takes just five steps to get your minikube “cluster” up and running the redismod container:Once that’s done, you can connect to the redismod service like so:That’s basically all there is to it – all you have to do now is connect to redismod from your application to start modeling your data with multiple modules on Redis. Questions? Feedback? Email or tweet at me – I’m highly available 🙂"
285,https://redis.com/blog/redis-labs-soc-2-compliant/,Redis Labs is SOC 2 Compliant,"June 20, 2018",Aviad Abutbul,"We are excited to announce that Redis completed the SOC 2 Type II compliance audit.Redis attaches a lot of importance to data protection and security. The SOC 2 compliance audit further fulfills the commitment Redis has towards delivering a high degree of trust and security to its customers.What is SOC 2 compliance?Service Organization Control (SOC) 2, set by The American Institute of CPAs (AICPA), verifies service organizations for assurances about security, availability, processing integrity, confidentiality and privacy.How does it benefit you?Redis offers Redis Enterprise databases both as software and a service. The Redis Enterprise Database as a Service(DBaaS) is available in two models: (1) Cloud (Hosted) service where Redis manages your database services and data, and (2) VPC (Managed) service where Redis manages the database service in your virtual private cloud.Both DBaaS models are included in the SOC 2 compliance, which assures:Redis’ SOC 2 compliance takes you closer to meeting the other compliance requirements that are relevant to your industry or territory.Should you have any questions regarding our SOC 2 audit, please contact support@redis.com."
286,https://redis.com/blog/active-active-redis-now-sorted-sets-lists/,Active-Active Redis – Now with Sorted Sets and Lists,"June 26, 2018",Paz Yanover,"We’re delighted to announce the availability of Redis Enterprise v5.2, with much-anticipated features such as:These features simplify application development and allow greater security for your Redis Enterprise deployments. Read on for more details below.With this release, all major data types of Redis are now supported with CRDTs. This makes Redis Enterprise the only database to support complex data types with seamless automated conflict resolution. All major Redis use cases are now addressed by Redis Enterprise in an active-active manner.Several of our customers have remarked that CRDTs save them many person-years of application development time. Given the popularity of Sorted Sets and Lists, we expect many will benefit from these new features. Click here for more information about how to develop active-active applications with CRDTs.Causal Consistency in active-active Redis CRDTs delivers a strong consistency model that captures causal relationships between operations across replicas. With Causal Consistency, all Redis Enterprise Conflict-free Replicated Database (CRDB) instances agree upon and maintain the order of causally related operations. This is an important capability for applications like e-commerce transactions and chat (so the order of messages doesn’t get mixed up). Click here for more information about Causal Consistency in active-active Redis CRDTs.Redis Enterprise’s new  admin action audit trail accomplishes two major objectives. It ensures that system management tasks are appropriately performed and monitored by the Administrator(s), and facilitates compliance with customers’ regulatory standards, such as HIPAA, SOC 2 and PCI. Redis Enterprise audit records now contain the following information about all management actions:Another new security feature of Redis Enterprise is the ability to set a minimum TLS version. This lets you require the TLS version that can be used for encrypting both data and control paths, using the REST API or rladmin command. In addition, with HTTPS enforcement, you can disable users from accessing the REST API via HTTP, ensuring they access it via HTTPS only.Finally, we added support for Redis 4.0.9, which contains numerous bug fixes, and maintains our policy of strong sync with open source versions of Redis. Redis Enterprise 5.2 also contains the LUA vulnerability fix, which is available in open source Redis version 4.0.10, following massive tests to verify the fix.If you’re interested in a more detailed view of what’s new in Redis Enterprise 5.2, please visit our technical documentation or check out the release notes. For  questions/comments on our newest release or feedback for Redis in general, please email us at pm.group@redis.com"
287,https://redis.com/blog/ideal-iot-edge-database-redis-enterprise/,The Ideal IoT Edge Database – Redis Enterprise,"June 27, 2018",Rob Schauble,"Internet of Things (IoT) solutions present a unique challenge for any database. There’s increasingly large and fast data coming from a very broad spectrum of IoT devices, coupled with critical latency requirements. Given this, the data’s processing and analysis must increasingly be handled at the network edge, close to the sensors, actuators and other IoT devices. We no longer have the luxury of being able to crunch IoT data in a cloud environment, where there is seemingly limitless compute and storage resources, because the latency would be unacceptable. Thankfully, there are powerful database and platform solutions tackling this challenge head-on, which we’ll explore below. But first, let’s review some of the data requirements unique to IoT environments.Edge computing has increasingly become table stakes, given the volume, velocity, variety and veracity (the “four V’s”) requirements of IoT and big data, and the distributed nature of many of these use cases. So is there a way to maintain the capabilities we have long enjoyed (and the very reason we’ve had a massive trend to cloud computing over the past 10 years) if we move back to distributed computing at the edge? Can we have our cake and eat it too?Bringing Cloud Principals to the Edge with Fog ComputingFortunately, in this case – with the popularization and growing trend of fog computing – the answer is yes. The solution is to bring cloud principals to the edge, where the fog and cloud environments operate in tandem to handle complex IoT use cases. When you have critical latency requirements, for example with smart city IoT use cases like gunshot detection or criminal face recognition, your data must be handled by ruggedized fog nodes close to IP cameras and other sensors. Non-latency critical data can still be synchronized to the core or cloud. In this way, data from all edge devices and fog nodes in your IoT solution can be aggregated at a core level (for example, a city block in the smart city use case), and ultimately to the cloud or data center environment for business intelligence and other analytics.Image courtesy of the OpenFog ConsortiumWith fog computing, we refer to data communication between IoT devices, edge devices, fog nodes, and the cloud as “north-south” communication, and data communication between the edge/fog nodes across the system as “east-west” communication. For this to be effective, we must have common cloud or data center environment capabilities at the edge, such as machine learning, deep learning and other artificial intelligence. This presents the next challenge: how do we handle these needs, given the distributed nature and modest storage and compute capabilities in edge devices and fog nodes? Having fog and cloud environments operating in tandem is critical. For example, with machine learning, we need to train models in the cloud where we have vast compute and storage resources, and deploy those trained models to the fog nodes and/or edge devices so they can be served close to IoT devices to minimize latency.A Database for the “Intelligent Edge”The “intelligent edge” has arrived and rescued us from these seemingly unsolvable problems (or Kobayashi Maru for you Star Trek fans). The edge has become the battleground for IoT today, but is there a database on the planet that can handle this massively large and high-velocity data from potentially thousands of sensors, cameras and other devices? One that can process that data in real-time, with many different database models, and with a small footprint?Fortunately, there is one such database! Redis Enterprise has blazing fast performance with the ability to ingest millions of writes per sec with <1ms latency at the IoT edge. It can do this with a small hardware and software footprint, so it is perfectly suited to live on fog nodes, edge gateway devices, and even IoT devices in some cases. Redis Enterprise has many native data structures (sets, sorted sets, lists, hashes, streams, etc.), providing ultimate flexibility for IoT application developers. Furthermore, with the many modules that already exist to extend it, Redis Enterprise is a multi-model database that can handle very diverse workloads required at the IoT edge: time-series, graph, machine learning, search, etc. Rather than deploying six different databases to support these needs, Redis Enterprise can manage them all, tremendously simplifying your architecture. Many mission critical IoT use cases are distributed geographically across many regions – another use case Redis Enterprise handles gracefully with high availability, active-active (with CRDTs), disaster recovery, and auto scaling capabilities.Microsoft Azure IoT Edge + Redis Enterprise = Better TogetherNow that we’ve shown there is a database worthy to take on the intelligent edge, you might wonder which platform is best-suited for running Redis Enterprise at the IoT edge? Fortunately, the new Azure IoT Edge from Microsoft is purpose-built to take on this challenge! Redis is delighted to partner with Microsoft Azure on IoT edge solutions, since both companies are putting significant focus and investment in accelerating intelligence and computing at the edge. With Redis Enterprise integrating into the Azure IoT Edge runtime environment over the coming weeks, joint customers and partners will enjoy a fast general data store, a message broker between Azure Edge modules, streams processing, a time-series database, and in-memory processing (machine learning model serving, graph processing, etc.) for the best possible performance.Stay tuned over the coming weeks as we share more details on how the IoT community will benefit from our joint IoT edge solutions. But if you’re looking for a quick read on the criteria to choose the right database for your IoT solution, here is a quick read."
288,https://redis.com/blog/dont-worry-happy-redis-labs-ready-gdpr/,"Don’t Worry, Be Happy. Redis Labs Has You Ready for GDPR!","July 23, 2018",Tal Dagan,"There is hardly anyone on the planet that hasn’t heard the acronym “GDPR” in the last couple of months. But just in case, we’ll explain it briefly, and then outline what Redis has done to support GDPR, and what it means to our customers.GDPR (General Data Protection Regulation) is a European Union (EU) regulation regarding data protection and privacy for individuals. GDPR primarily aims to give EU citizens and residents control over their personal data, but it also addresses the export of personal data outside the EU.The regulation applies to three different types of entities with varying levels of liability:What does this mean for you?First of all, if your applications do not handle personal information, or you do not collect personal information from EU residents, GDPR might not apply to you (that said, if you’re not sure, we highly recommend you consult a GDPR legal expert). If you use one of Redis’ DBaaS services (either the Redis Enterprise Cloud or the Redis Enterprise VPC service), then Redis is considered your Data Processor.The Redis service is built on top of our software. And we’ve taken steps to govern data and achieved SOC 2 compliance. This means Redis puts you on a fast track towards GDPR compliance.On the other hand, if you use Redis’ on-premises solution (Redis Enterprise Software), then Redis is not considered a Data Processor, and the GDPR does not specify any special actions you need to take with us. We do, however, recommend that you gain a deep understanding of how data is accessed, stored and protected and whether or not your applications and data storage policies comply with GDPR regulations. We discussed some of the data considerations for your database in a previous blog post.GDPR Flow Chart:Resources for Redis Customers:Redis has DPA agreements in place with our own sub-processors.At Redis we are committed to the highest level of trust, transparency, standards and regulatory compliance. We strive to deliver the best customer experience while earning the trust of thousands of Redis customers globally.If you have any questions, you can contact us at privacy@redis.com."
289,https://redis.com/blog/recent-enhancements-redis-enterprise-vpc/,Recent Enhancements to Redis Enterprise VPC,"July 25, 2018",Aviad Abutbul,"In this blog post, we’d like to share some of the recent enhancements to Redis Enterprise VPC (also known as RV), one of Redis’ database-as-a-service solutions. RV provides a fully managed Redis Enterprise on your virtual private cloud within major public clouds. It offers highly available, linearly scalable, high-performance, multi-model Redis, with intelligent tiered access to memory (both RAM and Flash).Three of our latest major improvements to Redis Enterprise VPC provide:Redis modules are add-ons to Redis that extend it to cover most of the popular use cases for any industry. They seamlessly plug into Redis, are processed in-memory and benefit from Redis’ simplicity, super high-performance, scalability and high availability. New modules can be created by anyone, and we, at Redis, encourage the Redis ecosystem to extend Redis by developing new modules.To set an example, we’ve developed several interesting modules ourselves, which we share with the community. The modules we now offer in RV are:RediSearch is a powerful text search and secondary indexing engine. Unlike Redis search libraries, it does not use Redis’ internal data structures. Using its own highly optimized data structures and algorithms, the RediSearch module delivers advanced search features with high performance and a low memory footprint. It can perform simple text searches as well as complex structured queries, such as filtering by numeric properties and geographical distances.RediSearch supports continuous indexing with no performance degradation, maintaining concurrent loads of both querying and indexing. This makes it ideal for searching frequently updated databases without batch indexing or service interruptions. The Enterprise version of RediSearch can scale across many servers, easily growing to billions of documents on hundreds of servers.ReJSON is a Redis module that implements ECMA-404 (the JSON Data Interchange Standard) as a native data type. It can store, update and fetch JSON values from Redis keys (documents). ReJSON’s primary features are:ReBloom extends Redis’ native data types and adds two new probabilistic data structures – a scalable bloom filter and a cuckoo filter. These data types are used to determine with a given degree of certainty whether an item is present in (or absent from) a collection.Bloom/cuckoo filters are especially useful because they occupy very little space per element — typically counted in bits not bytes! Although there’s a controllable percentage of false positives, ReBloom provides excellent speed and (most importantly) excellent space efficiency for initial tests of whether a key exists in a set. More information is available in this blog post.To use one of these modules, simply select it when you create your subscription/database:By default, whenever a new subscription is created, we’ve created a dedicated VPC and deployed Redis Enterprise within it. With this new feature, users now get to choose to have their subscription deployed inside an already existing VPC. This removes the need for peering between VPCs, saving you traffic charges from AWS and cutting some latency from your database.Linear scaling of database performance is critical for any application that needs to scale easily and cost-efficiently. Many cloud or on-premises databases claim to scale linearly but can rarely prove it in the manner Redis Enterprise has demonstrated. To achieve this, Redis Enterprise leverages the Redis open source (OSS) cluster API, which allows it to scale infinitely and in a linear manner by simply adding shards and nodes.The OSS cluster API allows Redis clients to directly access the shard that holds a key/value object with no additional network hop. This, combined with the shared-nothing symmetric architecture of Redis Enterprise, ensures that data and control paths are separate, and that the control path does not impose non-linear overheads in a scaled-out environment.Redis Enterprise has set a new industry performance record: delivering over 50 million ops/second under 1 millisecond, in as little as 26 EC2 nodes. You can read more about this in our benchmark report.Try these new features out for free with our 14-day unlimited free trial (no credit card required). Sign up now.For further information, feedback or suggestions, drop us a line at pm.group@redis.com."
290,https://redis.com/blog/release-redisgraph-v1-0-preview/,On the release of RedisGraph v1.0 Preview,"July 31, 2018",Jeffrey Lovitz,"TLDR: We’ve been developing a graph database that transforms queries into linear algebra problems, and the result is an architecture that can perform many standard operations on millions of elements in sub-second time.Nearly two years after starting development, we reached a major milestone with the release of RedisGraph v1.0 today. RedisGraph is a graph database architecture implemented as a Redis module, using GraphBLAS sparse matrices for internal data representation and linear algebra for query execution. It is implemented purely in C, and has been tested on Linux and OS X distros. We are using Cypher as our query language, and look forward to working toward GQL with the graph database community!In this blog, we’ll provide a quick overview of how RedisGraph uses matrix representations to solve traditional graphing problems, share some performance benchmarks, and discuss some next steps toward making our architecture as efficient and versatile as possible.Matrices are a classic encoding format for graph data, starting with the pleasantly intuitive adjacency matrix: a square table with all nodes along each axis, and a value for each connected pair. Naively implemented, this approach scales terribly for graphs that model real-world problems, which tend to be very sparse. Space and time complexity for a matrix are governed by its dimensions (O(n²) for square matrices), making alternatives like adjacency lists more appealing for most practical applications with scaling requirements.For RedisGraph, we’ve employed the GraphBLAS library to gain the benefits of matrix representations while optimizing for sparse data sets. (Special thanks to Tim Davis, who has worked closely with us to integrate his work as effectively as possible!)GraphBLAS encodes matrices in compressed sparse column (CSC) form, which has a cost determined by the number of non-zero elements contained. In total, the space complexity of a matrix is:(# of columns + 1) + (2 * # of nonzero elements)For a dense graph, this is still bounded by O(n2), but for a complete diagonal (the identity matrix) is only 3n + 1.This encoding is highly space-efficient, and also allows us to treat graph matrices as mathematical operands. As a result, database operations can be executed as algebraic expressions without needing to first translate data out of a form (like a series of adjacency lists).The GraphBLAS interface allows the majority of our library invocations to be direct mathematical operations, such as matrix multiplications and transposes. These calls are evaluated lazily and optimized behind the scenes.As an example, let’s take a simple graph that has 6 nodes, 4 of which have the label Person and 2 of which have the label Country:The Cypher query we’ll run is:MATCH (:person {name: ‘Jeff’})-[:friend]->(:person)-[:visited]->(:country)This will find out which countries have been visited by my friends. The query will have a filter operation to only select `person` nodes with a matching `name` property, and we will have to traverse two adjacency matrices, friend and visited.Thanks to the minimal impact of values in the CSC encoding, our adjacency matrices always represent all node IDs. This is in contrast to representing a matrix like visited with people as rows and countries as columns. The same relationships are shown in either case, but the CSC approach allows all matrices to have the same dimensions, with row/column indices that describe the same entities., As such, we can combine any of these matrices in algebraic expressions without needing to first convert them into comparable forms.Our starting point for this query will be the `friend` adjacency matrix:I have Node ID 0, so entries in the first row of this matrix denote the IDs of individuals I am friends with. At the moment, filters are applied node-by-node, but an optimization we’re currently developing is applying them to the matrix at this stage. This will result in a modified adjacency matrix where only my friends are represented:Now we take the `visited` matrix, which has the same dimensions and describes all nodes in the same order:By multiplying the filtered `friend` matrix against `visited`, we obtain a matrix which connects my node (as a row) to the countries that my friends have visited (as columns):(For space optimization, we actually use boolean matrices here, so both entries would be 1 in practice.)Testing the database’s performance on real data sets and more complex problems has yielded promising results.All of the below values were built with the Wikipedia top categories data set on a 2016 Macbook Pro (2 GHz i5, 16gb RAM). The resulting graph contains 1,791,489 nodes and 28,511,807 edges.Here are the performance results we achieved:A few notes:Our development on RedisGraph is still highly active, and the most significant tasks ahead of us are:This is very much a wishlist, incidentally! In the spirit of Redis, RedisGraph is an open source project and contributors are always welcome. Feel free to reach out to us on GitHub if you have any comments, or if you’re interested in becoming involved."
291,https://redis.com/blog/redis-enterprise-delivers-linear-scale-proven-time/,"Redis Enterprise Delivers Linear Scale, Proven Time and Again!","August 3, 2018",David Maier,"In Redis Enterprise 5.0, we introduced support for the Open Source (OSS) cluster API, which allows a Redis Enterprise cluster to scale infinitely and linearly by adding shards and nodes. This post describes the first of our linear scaling benchmark tests and how Redis Enterprise works with the OSS cluster API and demonstrates infinite linear performance scalability.Over the course of the last few months, we conducted tests that included additional benchmarking—n-shard database on a k-node Redis Enterprise cluster, as noted below:"
292,https://redis.com/blog/redis-day-london-2018/,Redis Day London 2018,"August 13, 2018",Tague Griffith,"For three years running we’ve hosted Redis Day Tel Aviv, an opportunity for the developer community to take the stage and share their stories about using Redis. This year we’re expanding Redis Day to include Redis Day London.Our lineup for Redis Day London includes a number of great speakers from the community— both end users and contributors to Redis will take the stage to share their Redis stories. To give you a taste of the event, today we announced a few speakers from the program:Tim will tell the story of how Armakuni relies on Redis to process over 400 payments per second for Comic Relief’s wildly popular Red Nose Day. James will be talking in-depth about the way Pusher uses Redis’ Pub/Sub functionality and Dekel will explore the role Redis plays at BioCatch. These are just a few of the speakers for the day, so check back on the Redis Day London page at the end of September for the complete program.No large Redis gathering would be complete without Salvatore Sanfillipo, the creator of Redis.  Salvatore will open up Redis Day London with a glimpse at the future of Redis—how will the features of Redis 6 shape up?  Salvatore plans to talk about the new RESP3 protocol, client-side caching and improvements to Redis Cluster.We’re talking everything and anything Redis on November 15th—don’t miss out on the inaugural Redis Day London!"
293,https://redis.com/blog/benchmarking-redis-enterprise-5-2-0-vs-hazelcast-3-9/,Benchmarking  Redis Enterprise 5.2.0  vs. Hazelcast 3.9,"August 30, 2018",Redis,"Hazelcast has published two benchmarks comparing its system against the Redis open source database:Recently, multiple prospects and customers have asked us to run a similar benchmark (over the network) between Redis Enterprise and Hazelcast. Given that Redis Enterprise clusters are based on a different architecture than open source clusters (as further explained here), we wanted to see if there would be any differences in these results.At first, we aimed to reproduce the exact same benchmark setup used by Hazelcast, but they apparently used proprietary hardware. As a company born in the cloud, here at Redis we do not host a single server in-house, and thus decided to look for a similar server configuration on AWS. Here is the setup we used:In its benchmarks, Hazelcast used RadarGun to orchestrate the load generated against both Hazelcast and Redis. In the Redis case, RadarGun launched a Jedis cluster and used a configuration file similar to this (although we couldn’t find the exact setup in this fork that Hazelcast listed).Using the right tool to test a product is crucial for running a successful benchmark. For instance, most experts would prefer to test Redis with a pipelining technique, as this speeds up Redis and is used by a large portion of Redis users. However, we didn’t find a pipeline configuration in the Hazelcast benchmarks.We therefore opted for the following approach:The results of our benchmark are presented below:We re-ran the benchmark comparing Hazelcast against Redis over the network with two major changes:We found the results for Hazelcast’s performance to be very similar to (or better than) what the company published here. Therefore, we tend to believe the Hazelcast cluster and the RadarGun orchestrator were configured correctly.On the other hand, our findings reached a much better throughput (over 3.5X) and latency (~3X) for the Redis Enterprise cluster than those from Hazelcast. We think these differences were related to:If you have questions related to this benchmark, please feel free to email me: keren at redis.com. If you would like to find out more about Redis Enterprise, visit here or email product at redis.com."
294,https://redis.com/blog/install-redis-enterprise-clusters-using-operators-openshift/,How to Install Redis Enterprise Clusters Using Operators on OpenShift,"September 10, 2018",Amiram Mizne,"The world of microservices is an exciting place where we are witnessing fast-paced, frequent, meaningful advances. One of the most significant recent steps forward in the OpenShift/Kubernetes ecosystem was the introduction of Operator-based deployments, which we touched on in a recent blog post.In this blog we describe how to simplify the installation process for more complex applications, use cases and stateful apps using Redis Enterprise Operator packaging and deployment.Here is a step-by-step overview of the process.Requirements:Before you get started, make sure you have:Step 1 – LoginThis will shift to your project rather than the default project (you can verify the project you’re currently using with the oc project command).Step 2 – Get deployment filesSpecifically for the redis-enterprise-cluster yaml file, you may also download and edit one of the following examples: simple, persistent, service broker or use the one provided in the repository.Step 3 – Prepare your yaml filesLet’s look at each yaml to see what requires editing:The scc (Security Context Constraint) yaml defines the cluster’s security context constraints, which we will apply to our project later on. We strongly recommend not changing anything in this yaml file. However, you do need to apply it by typing: oc apply -f scc.yamlYou should receive the following response:securitycontextconstraints.security.openshift.io “redis-enterprise-scc” configuredNow you need to bind the scc to your project by typing:oc adm policy add-scc-to-group redis-enterprise-scc system:serviceaccounts:your_project_name(If you do not remember your project name, type oc project)The rbac (Role-Based Access Control) yaml defines who can access which resources. We need this to allow our Operator application to deploy and manage the entire Redis Enterprise deployment (all clusters). Therefore, we strongly recommend not changing anything in this yaml file. To apply it, type:kubectl apply -f rbac.yamlYou should receive the following response:role.rbac.authorization.k8s.io/redis-enterprise-operator createdserviceaccount/redis-enterprise-operator createdrolebinding.rbac.authorization.k8s.io/redis-enterprise-operator createdIf you’re deploying a service broker, also apply the sb_rbac.yaml file. First, edit the sb_rbac.yaml namespace field to reflect the namespace you’ve created or switched to during the previous steps. The sb_rbac (Service Broker Role-Based Access Control) yaml defines the access permissions of the Redis Enterprise Service Broker. We need this to allow our Service Broker application to expose and manage database plans.As a first step, edit the file and change the following:namespace: your_project_nameWe strongly recommend not changing anything else in this yaml file.To apply it, simply type:kubectl apply -f sb_rbac.yamlYou should receive the following response:clusterrole.rbac.authorization.k8s.io/redis-enterprise-operator-sb configuredclusterrolebinding.rbac.authorization.k8s.io/redis-enterprise-operator configuredThe next step applies crd.yaml, creating a CustomResourceDefinition for your Redis Enterprise cluster resource. This provides another API resource to be handled by the k8s API server and managed by the operator we will deploy next. We strongly recommend not changing anything in this yaml file.To apply it, type:kubectl apply -f crd.yamlYou should receive the following response:customresourcedefinition.apiextensions.k8s.io/redisenterpriseclusters.app.redis.com configuredApplying this yaml creates the operator deployment, which is responsible for managing the k8s deployment and lifecycle of a Redis Enterprise cluster. Among many other responsibilities, it creates a stateful set that runs the Redis Enterprise nodes (as pods).Always make sure you have the latest operator.yaml. Alternatively, you can edit the following tag: image:redis/operator:tagTo apply the operator.yaml, type:kubectl apply -f operator.yamlYou should receive the following response:deployment.apps/redis-enterprise-operator createdNow, run kubectl get Deployment and verify that your redis-enterprise-operator deployment is running. A Typical response will look like this:The mycluster yaml defines the configuration of the newly created resource: Redis Enterprise cluster. This yaml could be renamed your_cluster_name.yaml to keep things tidy, but this isn’t a mandatory step.This yaml must be edited, however, to reflect the specific configurations of your cluster. Here are the main fields you should review and edit:Service type value can be either ClusterIP or LoadBalancer. This is an optional configuration based on k8s service types. The default is ClusterIP.persistentSpec:enabled: <false/true>Check your Redis Software nodes’ enabled/disabled flag for persistency. The default is false.This specifies the StorageClass used for your nodes’ persistent disks. This is mandatory when persistency is enabled (for example, AWS uses “gp2” as a default and GKE uses “pd-standard” as a default).For example:limitscpu: “2000m”memory: 4Girequestscpu: “2000m”memory: 4GiThe default (if unspecified) is 2 cores (2000m) and 4GB (4Gi).This specifies persistency for the Service Broker with an enabled/disabled flag. The default is false.persistentSpec:storageClassName: “gp2“imagePullPolicy: IfNotPresentRepository: redis/redisversionTag: 5.2.0-14 –The version tag, as it appears on your repository (e.g. on DockerHub).This is an optional configuration. If omitted, it will default to the latest version.Step 4 – Create your clusterOnce you have your_cluster_name yaml set, you need to apply it to create your Redis Enterprise cluster:kubectl apply -f your_cluster_name.yamlRun kubectl get rec and verify that creation was successful (rec is a shortcut for “RedisEnterpriseClusters”).You should receive a response similar to the following:NAME AGEyour_cluster_name 17sYour cluster will be ready shortly, typically within a few minutes.To check the cluster status, type the following:kubectl get podYou should receive a response similar to the following:All you have left to do now is create your databases and start using them.Step 5 – Create a databaseIn order to create your database, we will log in to the Redis Enterprise UI.Note: your_cluster_name-0 is one of your cluster pods. You may consider running the port-forward command in the background.Note: The Openshift UI provides tools for creating additional routing options, including external routes. Thess are covered in RedHat Openshift documentation.Next, create your database.Note: In order to conduct the Ping test through Telnet, you can create a new route to the newly created database port in the same way as described above for the UI port. After you create your database, to go the Openshift management console, select your project name and go toApplications->Services. You will see 2 newly created services representing the database along with their IP and port information, similar to the screenshot below.ConclusionOpenShift simplifies the use of Kubernetes and provides considerable added value. The support for Operator-based installation now makes it much easier to deploy Redis Enterprise clusters. If you would like to learn more about the Redis Enterprise OpenShift release, please contact us."
295,https://redis.com/blog/3-ways-redis-enterprise-powers-enkidoos-ai/,3 Ways Redis Powers Enkidoo’s AI,"September 27, 2018",Alexandre Vincart-Émard,"Supply chain management is at the heart of every retail company’s operations, whether a “mom and pop” shop or an international manufacturer. With the ever-increasing amount of data generated by modern supply chains, new digital technologies such as artificial intelligence and blockchain are poised to disrupt the old ways of managing inventory, suppliers, etc.Our mission at ENKIDOO.ai is to help small and medium businesses increase their operational efficiency by being part of this smart supply chain revolution. Our optimization-as-a-service platform, which leverages the latest advances in cloud technologies and machine learning, is designed to help companies bolster their competitive advantage with simple, out-of-the-box solutions for demand planning, inventory optimization and network optimization.Of course, all of this requires a primary database that’s fast, reliable and flexible — and that’s where Redis comes in. Below is the first in a series of blog posts wherein we’ll discuss a few of the ways we leverage Redis in our application.We have found Redis’ persistence to be useful in a variety of ways. For instance, we’ve developed multiple forecasting models that use machine learning to predict demand for various products and product categories. The Redis-ML Module streamlined the process of building and deploying our predictive engines, including those we designed to estimate future demand based on historical data, as well as those that predict the sales of newly introduced items based on their features. Since we already did our machine learning development in Python, loading the models into Redis-ML required very little additional work for our team (thanks to the redis-py client).We also use Redis in our conversational analytics platform. At ENKIDOO, we envision a future in which our smart supply chain assistant can answer useful open-ended questions, such as “What are the top three promotions I should run to address declining sales in Quebec?” We believe such insights should be accessible from anywhere, not just from within our app. As such, we have built custom integrations with various communication channels like Slack and Facebook Messenger to empower decision makers on the go.Redis made managing user sessions for these different integrations quite easy. Our users can authenticate themselves on our platform, activate the channels they want, and Redis will take care of the rest (e.g. identifying these external requests to our servers with appropriate user token validation). Compared to alternative databases, Redis allows for larger usage concurrency at a much lower cost, bypassing the pains of sharding to satisfy large throughputs.Additionally, we knew that this user information might be highly confidential, and needed preventive measures to protect it from prying eyes, while still allowing it to be shared with trusted contacts. This is where we found Redis’ “key timeout” functionality to be instrumental, as it allowed us to render sensitive information efficiently for a limited amount of time.Overall, our developers love using Redis because it is lightweight, well-documented and has a great community contributing many useful open source libraries. In our mission to help users optimize their supply chains, Redis’ capacity to support different scenarios is critical. From serving as a simple session store, to allowing us to run predictive models, Redis powers multiple core facets of our service."
296,https://redis.com/blog/redis-labs-red-hat-partner-bring-open-source-enterprise/,Redis Labs and Red Hat Join Forces to Run and Manage Redis Enterprise on the OpenShift platform,"October 16, 2018",Rachel McCafferty,"Digital transformation is now a strategic imperative for companies across all industries. As a result, organizations are modernizing their core infrastructures, exploiting cloud capabilities and adopting next-gen architectures to drive new ways of developing, delivering and integrating applications. In this era of transformation, applications have to be developed, tested and released more quickly and methodologies such as agile and DevOps break down the size of the code delivered into manageable chunks. Consequently, microservices designs, container technologies, and orchestration (i.e. Kubernetes) are leading the charge.With that in mind, Redis and Red Hat announced a new partnership earlier this week and launched the general availability of a Redis Enterprise Operator on the Red Hat Openshift platform. We’re bringing this solution to the market to help organizations rapidly develop and deploy modern cloud-native applications.The Red Hat OpenShift Container Platform, built on Kubernetes and Red Hat Enterprise Linux, provides a powerful auto-scaling, multi and hybrid cloud application platform for organizations to rapidly develop and deploy modern cloud-native applications. Recently, Red Hat also unveiled the Kubernetes Operator framework to remove operational barriers in creating, configuring and managing stateful services (think databases).Over the last few months, we’ve worked very closely with Red Hat to simplify and automate the creation and management of highly available Redis Enterprise on the OpenShift Container Platform with built-in support for Redis Enterprise Operator for Kubernetes.The result of that collaboration is our new joint solution, which will help you build and deploy containerized applications on a stable, high-performing database. OpenShift-based applications can now access the Redis Enterprise Operator and Service Broker directly from the Red Hat Container Catalog.Our partnership with Red Hat demonstrates a significant trend in the industry. Enterprises like you are adopting a microservice strategy to meet the pressure to develop and deploy applications quickly and securely. Redis is often chosen for its superior performance and versatile data structures, and our recent advancements have further extended the multi-model data platform to custom data models like a document store, graph database, search and others — making it a platform ideally suited for microservices architectures.The Redis and Red Hat collaboration is exciting since both organizations share a common vision and passion around our open source ecosystem. We believe in community collaboration and its ability to change and reshape the foundational structure of how businesses operate in today’s highly competitive environments. We also realize the importance of helping enterprises accelerate application delivery, reduce risk, ensure business continuity and future-proof their investments. This partnership and joint certified solution, allows you to take advantage of the best open source platforms without taking on the risk of deploying unsupported/uncertified technologies for your mission-critical applications. Finally, by pairing the multi-tenancy features of Red Hat OpenShift with the efficient resource utilization of Redis Enterprise, we are reducing your cost of ownership and overall risk so your teams can focus on application innovation rather than restoration.Check out our webpage for links to key resources. If you’re convinced and want to jump right to deployment, download the certified solution and read our documentation on getting started with Kubernetes and OpenShift. If you have questions, feel free to send us a quick note at openshift@redis.com."
297,https://redis.com/blog/redis-5-0-is-here/,Redis 5.0 is here!,"October 22, 2018",Redis,"Last week, Redis reached a major milestone with the release of 5.0, which includes a variety of advancements and improvements. The big story here is the introduction of Streams as part of the release. Streams is the first entirely new data structure in Redis since HyperLogLog was introduced as part of 2.8.9 back in April 2014 (over four years ago)!So what is Redis Streams, you may ask? A Redis Stream is a log-like data structure that allows you to store multiple fields and string values with an automatic, time-based sequence at a single key. In many ways, Streams resembles other Redis data structures — it orders data in a manner reminiscent of Lists, it stores fields and values similar to Hashes, it enables you to read ranges of values like you can with Sorted Sets, and it can behave somewhat like Pub/Sub (or Lists) with blocking behavior that waits for items to arrive, allowing for real-time reactions to the stream.All that said, a Redis Stream is quite distinctly its own thing. It’s not exactly fair to say they are like structure foo but with feature bar. There is a unique capability of Redis Streams that sets it apart from any other existing data structure: consumer groups that allow various clients to consume a stream with their own position. This enables a whole new collection of uses for Redis; tasks like event sourcing or unified log architecture are now not only possible but also optimal. As with any Redis data structure, there are also numerous commands (13, in fact) that allow you to interact with the structure — you can find a list of these commands at redis.io.With the latest release, sorted sets have now gained a few new commands that allow you to remove the highest- (ZPOPMAX) or lowest- (ZPOPMIN) scoring member of a sorted set. An oft-requested feature, this enables some new patterns that were previously only accessible with Lua scripting.Accompanying ZPOPMIN and ZPOPMAX are the blocking variants (BZPOPMIN/BZPOPMAX) that wait for a value to arrive, similar to the blocking behavior of lists (BLPOP, as an example). So not only can you now remove the highest or lowest values, but also you can wait for members to arrive.Aside from new commands and data structures, the 5.0 release includes many refinements to existing internals including:For a bit of fun, we have also added the useless yet entertaining LOLWUT command, which generates some computer art using random elements and command arguments. It doesn’t have a significant technical purpose, but it might be a nice thing to test to see if Redis 5.0 is running properly when connecting to an instance of an unknown version.(From LOLWUT: a piece of art inside a database command)For more details or background on the improvements, you are welcome to take a look at the release notes.If you’re itching to try Redis 5.0 in Enterprise — especially Streams — you can download Redis Enterprise Software 5.4, which includes Redis 5.0. And you want to run it on our fully managed VPC offering, just let us know at support@redis.com.Happy Streaming!"
298,https://redis.com/blog/increased-garbage-collection-performance-redisearch-1-4-1/,How We Increased Garbage Collection Performance with RediSearch 1.4.1,"October 29, 2018",Meir Shpilraien,"With version 4.0, Redis exposed a new modules API that allows programmers to extend the database’s capabilities via new commands and data types. RediSearch uses this to allow full-text search of data stored in Redis. The data is saved inside Redis as a hash (a core Redis data type), and each data record is called a document. The document’s data is also indexed inside RediSearch’s internal data structures. It is possible to tell RediSearch not to store the data inside a hash, in which case the document will only be indexed inside RediSearch’s internal data structures (this mode is called NOSAVE).One main challenge search engines have to deal with is deleting and updating documents. In order to understand the problem, we first need to understand how RediSearch indexes the data. Let’s take, for example, the following document:Doc1:Name: “test”Body: “this is an example”RediSearch will create the following inverted index:When a user searches for the term ‘example’, RediSearch immediately finds that term in the inverted index and returns Doc1 as the query’s result.Now imagine that we need to delete Doc1 from the index by scanning all the terms in Doc1 and deleting its record from each term. The problem is that when running in NOSAVE mode, we do not have the data inside Doc1 and we do not know which terms need to be cleaned. It would be possible to keep track of each term in the document, but this can significantly increase the overall memory footprint.The only other solution is to create a “Garbage Collection” (GC) mechanism that scans the entire index in the background and removes the deleted document from the inverted index. RediSearch previously used this approach. However, as the product evolved, we found that it could be extremely slow because it required scanning the entire index. In addition, since Redis is single-threaded, during the scan we had to acquire a global (single) lock, which made it impossible to perform queries or updates to the entire data set.To solve these problems, we came up with a new GC approach that leverages the advantages of the Linux fork process. Our goal was to acquire the global lock in as little time as possible, i.e. by not acquiring it during the scan but rather during actual memory releases only. Each time the GC starts, it creates a fork process, which scans the index in the background while the main process continues performing queries and updates. The fork process notifies the main process about what needs to be deleted, and then the main process acquires the lock for a very short time (that’s sufficient for deleting the relevant documents from the inverted index).We compared this new GC implementation with our prior approach by inserting 5 million records into the database, and then continuously updating all of them.Note: This comparison was done over a simple laptop configuration using the following specs: MacBook Pro (Retina, 15-inch, Mid 2015), OS 10.11.6, CPU 2.2 GHz Intel Core i7, 16 GB memory at 1600 MHz DDR3. We could have run it over a stronger server configuration and probably gotten more impressive results, but decided these specs were good enough to demonstrate the advantages of our new approach.As you can see, the main advantage of our new GC approach is that RediSearch can now collect 80,727 KB as opposed to 908 KB using the old GC mechanism, which is close to a 100X improvement. The old GC’s probabilistic approach of choosing an inverted index randomly and cleaning it took significantly more time. Our new GC approach runs on the entire index in each iteration, so it can clean all the garbage by the time we finish an update. Furthermore, the new approach doesn’t need to acquire the global lock while scanning the index.The new GC mechanism is available today in our latest RediSearch release – version 1.4.1. It’s turned off by default, but can be activated by sending “GC_POLICY FORK” in the command line arguments when loading the module. Keep in mind that this is an experimental GC version, so we do not recommend using it in production just yet. However, if your use case requires lots of updates, you are welcome to try this new GC and give us your feedback. We intend to make this it the default configuration for RediSearch in a future release.We plan to extend our GC mechanism over time to increase GC performance even more. One approach we’re considering is using a heuristics mechanism to indicate which terms are more likely to have garbage, and clean those first."
299,https://redis.com/blog/announcing-redis-enterprise-pivotal-container-service-pks/,Announcing Redis Enterprise on Pivotal Container Service (PKS),"November 5, 2018",Cassie Zimmerman,"We are incredibly excited to announce the preview of a jointly developed solution between Redis and Pivotal, which will deliver easy integration of Redis Enterprise as a native service on PKS. Together, this solution gives developers instant, self-service access to a cloud-native, stateful data service that is portable, operationally simple, and brings speed and simplicity to accelerate application development. But first, let’s take a look back at how we got here, and why we’re thrilled to launch this new solution.For years, Redis has been one of the most vibrant and well respected open source technologies amongst the developer community. Since it was designed as a flexible key-value store that supports various data structures, Redis has grown quickly in popularity as a lightweight, extremely fast and simple-to-develop-on database. In fact, Redis is the world’s most loved database, and by far the most downloaded container on Docker Hub outside of Linux (at over 1 Billion pulls!).Getting started with Redis in containers is easy work for a developer, taking only a few minutes to set up and get working with an application. A small investment of time and effort can have an immediate, dramatic impact on performance, usually by orders of magnitude.If you’re adopting microservices, you’ll want to run multiple containers across multiple machines. You’ll need to manage your containers in a way that lets them communicate, auto-heal (in case of a failed machine) and handle storage considerations, among other things. Doing this manually is an operational nightmare, hence the need for Kubernetes! Kubernetes is an open source container orchestration platform that helps large numbers of containers work together in harmony.The Pivotal Container Service (PKS) takes things a step further, handling the heavy operational burdens required to deliver production-ready environments. PKS acts as the enterprise orchestrator for Kubernetes deployments, allowing users to reliably deploy and run containerized workloads across private and public clouds. It eases the common challenges of container orchestration with built-in high availability, monitoring, automated health checks and much more.At Redis, we follow similar principles. Redis Enterprise expands upon the core principles of Redis: simplicity, transparency and the belief that making software should be fun. Redis Enterprise is a “container-like” enterprise deployment framework for open source Redis. It extends the power of Redis by adding capabilities you need in production scenarios — including true high availability (HA), automatic failover, replication across data centers, data persistence and more. Our enterprise layer makes scaling completely seamless to the developer. At the same time, the scale is completely transparent at the operations layer to give operators complete control over their deployments.Given all of these developments, we feel Redis Enterprise + PKS is a perfect combination. As we work to release a generally available version of Redis Enterprise on PKS in the coming months, we will also add support for active-active replication using CRDTs for globally distributed applications.If you would like to start experimenting with our Redis Enterprise release for PKS, please reach out to channel@redis.com, so we can help you with any Redis questions. We will also have a Redis booth (booth #XYZ?) at VMworld Europe this week in Barcelona, so drop by to learn more.Blog: Why Use Redis Enterprise Kubernetes Release on Pivotal Container Service?Video: Redis Enterprise on Cloud Native PlatformsBlog: Redis Enterprise Operator for Kubernetes"
300,https://redis.com/blog/moving-agile-open-source-docs/,"Moving to agile, open source docs","November 14, 2018",Ben Mansheim,"“Open source” carries a lot of meaning in the world of technology these days. Twenty years after the term was coined, open source projects harness the power of voluntary contributors to produce and support software that is quickly growing in its impact on every industry, in every location.At its core, open source merely means that a product ships with the code that was written to produce it. The ability to access this source code creates the potential for modification and repackaging that is governed by a variety of open source licenses. It also can allow people to contribute modifications back into the core project, so that everyone benefits from those modifications.As a software company built on the principles of providing superior service while also promoting the common good, we at Redis decided to bring this open source agility to a less known, but highly impactful, area — Product Documentation.We feel that joining open source with documentation is a great combination to provide our community with:To find out how you can contribute, check out our contribution guide.So why did we decide to take the open source route for documentation? To understand, let’s look at the evolution of technical documentation over the last few years.It used to be that documentation was a stumbling block in software development deadlines because all supported explanations surrounding the use of the software had to be delivered at the same time as the software itself. Worse, it started off as printed material (think user guides!) that were not only slow to create but even harder to update.Fortunately, as technology progressed, we were able to update user documentation in digital formats, saving on the “heavy” costs of printing and shipping. Writers could provide corrections, rewrites and updates to customers just as easily as engineers could provide patches to software.This revolution closed the delivery gap to enable companies to improve documentation during the lifetime of the product. However, there were still significant bottlenecks preventing customers from getting the best information possible.So, what traditionally stood in the way of getting the most accurate product information out there? The WRITER!The technical writer is the sole content contributor who stands between technical experts and customers to provide technical information that customers can use reliably and efficiently. Granted, a product without technical writers may be difficult to use, but the time and effort that it takes for a writer to understand the information well enough to write is another bottleneck.Since technical writers are frequently the only ones with access to the source files for published documentation, any correction, rewrite or update contribution has to be entered and produced into its final format by them.Open source gives everyone some level of access to the source files of the product. In the case of Redis docs, that means putting the source files of our documentation in a publicly accessible repository. Anyone can view those source files and edit a copy of the files to suggest contributions, changing the writer from the content owner to the content editor and curator. You can even get there straight from the “Edit on GitHub” link on each page in our docs.This simple change in methodology opens ownership of the documentation’s accuracy to the technical experts within Redis, as well as our customers and partners who are experiencing the product’s behavior every single day. Add that to the immediate delivery of approved contributions at docs.redis.com, and we have dramatically improved our ability to provide up-to-the-moment and accurate documentation for our customers.Of course, this is only the beginning of the improvements we are bringing to our product documentation, but we feel that this move will be the basis for a lot of good stuff to come.So if you want to be a part of helping everyone get the most out of the fastest database in the world, don’t hesitate — Contribute!"
301,https://redis.com/blog/new-redisgraph-1-0-achieves-600x-faster-performance-graph-databases/,Benchmarking RedisGraph 1.0,"November 15, 2018",Pieter Cailliau,"Today we are happy to announce the general availability of RedisGraph v1.0. RedisGraph is a Redis module developed by Redis to add graph database functionality to Redis. We released RedisGraph in preview/beta mode over six months ago, and appreciate all of the great feedback and suggestions we’ve received from the community and our customers as we worked together on our first GA version.Unlike existing graph database implementations, RedisGraph represents connected data as adjacency matrices instead of adjacency lists per data point. By representing the data as sparse matrices and employing the power of GraphBLAS (a highly optimized library for sparse matrix operations), RedisGraph delivers a fast and efficient way to store, manage and process graphs. In fact, our initial benchmarks are already finding that RedisGraph is six to 600 times faster than existing graph databases! Below, I’ll share how we’ve benchmarked RedisGraph v1.0, but if you’d like more information on how we use sparse matrices, check out these links in the meantime:Before getting into our benchmark, I should point out that Redis is a single-threaded process by default. In RedisGraph 1.0, we didn’t release functionality to partition graphs over multiple shards, because having all data within a single shard allows us to execute faster queries while avoiding network overhead between shards. RedisGraph is bound to the single thread of Redis to support all incoming queries and includes a threadpool that takes a configurable number of threads at the module’s loading time to handle higher throughput. Each graph query is received by the main Redis thread, but calculated in one of the threads of the threadpool. This allows reads to scale and handle large throughput easily. Each query, at any given moment, only runs in one thread.This differs from other graph database implementations, which typically execute each query on all available cores of the machine. We believe our approach is more suitable for real-time real-world use cases where high throughput and low latency under concurrent operations are more important than processing a single serialized request at a time.While RedisGraph can execute multiple read queries concurrently, a write query that modifies the graph in any way (e.g. introducing new nodes or creating relationships and updating attributes) must be executed in complete isolation. RedisGraph enforces write/readers separation by using a read/write (R/W) lock so that either multiple readers can acquire the lock or just a single writer. As long as a writer is executing, no one can acquire the lock, and as long as there’s a reader executing, no writer can obtain the lock.Now that we’ve set the stage with that important background, let’s get down to the details of our latest benchmark. In the graph database space, there are multiple benchmarking tools available. The most comprehensive one is LDBC graphalytics, but, for this release, we opted for a simpler benchmark released by TigerGraph in September 2018. It evaluated leading graph databases like TigerGraph, Neo4J, Amazon Neptune, JanusGraph and ArangoDB, and published the average execution time and overall running time of all queries on all platforms. The TigerGraph benchmark covers the following:The TigerGraph benchmark reported TigerGraph to be 2-8000 times faster than any other graph database, so we decided to challenge this (well-documented) experiment and compare RedisGraph using the exact same setup. Since TigerGraph compared all other graph databases, we used the results published by their benchmark rather than repeating those tests.Given that RedisGraph is v1.0 and we plan to add more features and functionalities in future release, for our current benchmark we decided to focus mainly on the k-hop neighborhood count query. Of course, we’ll publish results from other queries in the near future.The k-hop neighborhood query is a local type of graph query. It counts the number of nodes a single start node (seed) is connected to at a certain depth, and only counts nodes that are k-hops away.Below is the Cypher code:MATCH (n:Node)-[*$k]->(m) where n.id=$root return count(m)Here, $root is the ID of the seed node from which we start exploring the graph, and $k represents the depth at which we count neighbors. To speed up execution, we used an index on the root node ID.Although we followed the exact same benchmark as TigerGraph, we were surprised to see that they compared only a single request query response time. The benchmark failed to test throughput and latency under concurrent parallel load, something that is representative of almost any real-time real-world scenario. As I mentioned earlier, RedisGraph was built from the ground up for extremely high parallelism, with each query processed by a single thread that utilizes the GraphBLAS library for processing matrix operations with linear algebra. RedisGraph can than add threads and scale throughput in a close to linear manner.To test the effect of these concurrent operations, we added parallel requests to the TigerGraph benchmark. Even though RedisGraph was utilizing just a single core, while other graph databases were using up to 32 cores, it achieved faster (and sometimes significantly faster) response times than any other graph database (with the exception of TigerGraph in the single request k-hop queries tests on the Twitter dataset).The single request benchmark tests are based on 300 seeds for the one and two-hop queries, and on 10 seeds for the three and six-hop queries. These seeds are executed sequentially on all graph databases. Row `time (msec)` represents the average response times of all the seeds per database for a given data set. Row ‘normalized’ per data set represents these average response times normalized to RedisGraph.It is important to note that TigerGraph applied a timeout of three minutes for the one and two-hop queries and 2.5 hours for the three and six-hop queries for all requests on all databases (see TigerGraphs’s benchmark report for details on how many requests timed out for each database). If all requests for a given data set and given database timed out, we marked the result as ‘N/A’. When there is an average time presented, this only applies to successfully executed requests (seeds), meaning that the query didn’t time out or generate an out of memory exception. This sometimes skews the results, since certain databases were not able to respond to the harder queries, resulting in a better average single request time and giving a wrong impression of the database’s performance.In all the tests, RedisGraph never timed out or generated out of memory exceptions.For the parallel requests test, we only compared RedisGraph and TigerGraph. This setup included 22 client threads running on the same test machine, which generated 300 requests in total. The results below show how long (in msec) it took for each graph database to answer all the requests combined, at each depth (one, two, three and six-hops).For TigerGraph, we extrapolated results by multiplying the average response times of the single request times by 300, for each depth. We believe this is the best case scenario, since TigerGraph was already fully consuming all 32 cores for the single requests. Next time, we will repeat these test with TigerGraph under the same load of 22 clients and we expect (given the laws of parallelism and the overhead this will generate for executing parallel requests) our results will be even better.We are very proud of these preliminary benchmark results for our v1.0 GA release. RedisGraph was started just two years ago by Roi Lipman (our own graph expert) during a hackathon at Redis. Originally, the module used a hexastore implementation but over time we saw way more potential in the sparse matrix approach and the usage of GraphBlas. We now have formal validation for this decision and RedisGraph has matured into a solid graph database that shows performance improvements under load of 6 to 60 times faster than existing graph solutions for a large dataset (twitter) and 20 to 65 times faster on a normal data set (graph500).On top of that, RedisGraph outperforms Neo4j, Neptune, JanusGraph and ArangoDB on a single request response time with improvements 36 to 15,000 times faster. We achieved 2X and 0.8X faster single request response times compared to TigerGraph, which uses all 32 cores to process the single request compared to RedisGraph which uses only a single core. It’s also important to note that none of our queries timed out on the large data set, and none of them created out of memory exceptions.During these tests, our engineers also profiled RedisGraph and found some low-hanging fruit that will make our performance even better. Amongst other things, our upcoming roadmap includes:Happy graphing!RedisGraph TeamJanuary 17, 2019After we published these RedisGraph benchmarking results, the folks at TigerGraph replied with some thoughts. While we appreciate hearing our competition’s perspective, we’d like to address their accusation of fake news. Read our benchmark update blog here."
302,https://redis.com/blog/announcing-redis-enterprise-release-redisgraph-streams-redis-5-0-popular-redis-java-python-clients/,"Announcing Redis Enterprise Release with RedisGraph, Streams (Redis 5.0) and Popular Redis Java and Python Clients","November 16, 2018",Madhukar Kumar,"As we gear up for 2019, we are very excited to announce the latest release of Redis Enterprise which marks a significant milestone in our journey towards a zero latency future, and comes packed with all sorts of goodies. Let’s review three capabilities that make the new Redis Enterprise especially interesting.RedisGraph is a Redis module that represents connected data as adjacency matrices instead of the more common adjacency lists per data point representation. By using sparse matrices and employing the power of GraphBLAS (a highly optimized library for sparse matrix operations), RedisGraph delivers a fast and efficient way to store, manage and process graphs — we’re talking up to 600 times faster than other graph databases. RedisGraph is now part of a brand new Redis Enterprise module available for our VPC and software customers, and can also be used under our source available license. You can read about the benchmarks on our blog here and get more details about RedisGraph here.Streams is a new data structure that was introduced with open source Redis 5.0 and is now available in the latest version of Redis Enterprise. Similar to the Lists data type, Streams has an ordered collection of elements called messages. Unlike Lists, however, messages are added exclusively to the end of Streams. In Streams, messages are stored using unique sub-second timestamp-based identifiers, and the order of the messages is immutable. With blocking read functionality, Stream provides a pub/sub like functionality but with fan out scaling ability. Streams has many interesting real-world uses like log aggregation, real-time sentiment analysis and more. We cannot wait to see how the community will put it to use to process data generated at incredible velocities. You can read more about Streams here.As part of the current Redis Enterprise release, we are also incredibly proud to collaborate with the creators of the two most popular Redis clients — Jonathan Leibiusky, the creator of Jedis, and Andy McCurdy, the creator of Redis-py. We will continue to contribute to these projects as part of our commitment to the Redis community. We plan to ensure that all new features released as part of Redis will be supported in these clients, including Streams and modules. We believe it is important to make sure that these highly popular libraries will be constantly maintained and up to date with the latest capabilities of Redis."
303,https://redis.com/blog/use-redis-connect-bi-tools-windows-using-cdata-software-odbc-driver/,How to Use Redis to Connect to BI Tools on Windows Using CData Software ODBC Driver,"November 19, 2018",Jerod Johnson,"Redis is well known as an in-memory database that’s designed for various use cases, including session management, caching, high speed transactions, pub/sub, data streaming, analytics, and more. Redis is also popular for its support of client libraries and its wide-ranging list of programming languages. You can develop apps with most popular programming languages and connect them to your Redis database with zero hassle.Another reason Redis is versatile is its extensibility  as a multi-model database. For example, you can import a RediSearch module into Redis and get an extremely fast, in-memory, search engine. Similarly, you can also use Redis as a graph database, with the help of the RedisGraph module.All of this, of course, begs the question: Can you use Redis as a relational database? The answer is simple: An ODBC connector to Redis can power numerous applications, such as Tableau, DataBind Charts, Crystal Reports, LINQPad, Entity Framework 6, etc.; with the ODBC driver, you can obtain an SQL-like access to Redis, so you can even connect the ubiquitous Microsoft Excel to Redis data.The good news is, Redis’ partner — CData Software — has developed an ODBC driver for Redis, making Redis even more accessible.With the CData ODBC Driver for Redis, users can connect to data sources from various BI, analytics and reporting tools, but stored in Redis. In order to configure the driver for Windows, download the appropriate ODBC driver from the options listed here and follow the simple steps listed below.Before the driver is able to help you connect to Redis and Redis Enterprise instances, you must read through and agree to the end user license agreement. After you click “I agree,” you will be able to begin the installation process.After you have accepted the EULA, you will encounter several screens prompting you to select an installation destination as well as the components you want to install.The destination folder for the installation should be pre-populated, so as long as you have enough space available, you will be able to proceed by clicking “next.”The subsequent screen will allow you to select which components you would like to install. Here, it is important to note that the bitness — whether 32-bit or 64-bit — of your ODBC Driver does not refer to bitness of your machine. Instead, it refers to the bitness of the applications you will use with the driver. According to Johnson, it makes sense for most users to install both versions of the driver.On this same screen, you will also notice that there are help files included, which provide more details about the driver’s features and how they are meant to be used. There are some demo applications as well, which provide simple, straightforward examples of what using the driver can look like. Then, there is the final component — the SQLBroker, a lightweight application that allows you to connect to your Redis data remotely using Tabular Data Stream or MySQL protocol. After you have selected these components, click “next” to continue.On the following screen, you will be prompted to select the start menu folder for the driver’s shortcuts, and to enter a name for this new folder. Then, click “next” again, and you should see a screen letting you know that the driver is ready to install. Click “install” to continue, and you will observe all of the components you previously selected being installed. Once the installation is complete, you should see an option to configure an ODBC data source for the driver, which can be used to connect to Redis data from any number of other applications. Click “finish” to open the data source configuration wizard.Once you open the wizard, you will see a new screen listing the connection properties used to connect to your Redis instance:Now, you should start by entering the IP address or fully qualified domain name for the Redis server. Then, enter the port for your Redis instance. If your Redis instance is protected by a password, you can indicate that under “Auth Scheme.” If it isn’t, you can simply set the Auth Scheme to “none.”Once you have made these basic customizations, you can go on to explore the other connection properties. For example, take a look at the Define Tables property:This property enables you to group keys based on a key pattern; these groups will then be treated as a table. In other words, table patterns set up key patterns that will be used to define tables by discovery. For example, suppose you specify that Table_1 will include all of the keys that begin with prefix:, and that Table_2 will include all of the keys beginning with prefix:prefix2. If you set the Table Pattern property to prefix:*, then the ODBC driver will only highlight keys that begin with prefix:. If you use the default value instead, then the driver will expose all keys that use a colon to separate hierarchies.There are also numerous other connection properties available, which will allow you to do anything from connecting through a proxy or firewall to configuring any logging and secure connectivity.Now that you have indicated your installation and connection preferences, you are ready to test the driver’s functionality. At the top of the screen within the data source configuration wizard, click “test connection”:If the connection test is successful — which it should be if you have correctly followed the above steps — click “OK” to save your connection settings and close the wizard.With the DSN configured, you are now ready to connect to your Redis data from any number of third-party BI, reporting, ETL and custom applications. If you have any questions, you can reach out to CData Software’s support team at support@cdata.com. Good luck with the installation process!"
304,https://redis.com/blog/share-redis-geek-story-redisconf-19/,How to share your Redis geek story at RedisConf ‘19,"December 11, 2018",Redis,"It’s that time of year! No, I don’t mean the seasons changing. It’s RedisConf CfP time!This is your opportunity to talk about Redis in front of a gathering of fellow geeks. RedisConf ‘19 will be held April 2-3 at San Francisco’s Pier 27. That might seem like a long time from now, but we’re already busy putting together a schedule of sessions to ensure that April will be great.In the past, we’ve run into people who wanted to talk at RedisConf but were worried that they didn’t know everything about Redis, or that their work wasn’t impressive enough to submit. This is proof positive of how impostor syndrome runs rampant in the tech community. If Redis has solved a problem for you, made your work easier or brought you joy in some other way, we want to hear about it! Don’t worry if it’s not some massive, earth-shattering revelation. And well, if it is earth-shattering, that’s good, too.The process for submitting a presentation is quite simple; I’ve outlined the steps below:There are a few extra pages that are optional, if you want to take additional steps like submitting a co-speaker, but once you get done with step 4, you’re submitted. It should take you about 15 minutes.After you submit your presentation, our committee will take a look at it and provide feedback if necessary. It’s a very simple process that looks great on a resume/CV or when you’re trying to impress a date: “You know, I presented at RedisConf” is a great conversation starter."
305,https://redis.com/blog/redis-now-available-edgex-foundry-iot-edge-platform/,Redis Now Available for the EdgeX Foundry IoT Edge Platform,"December 13, 2018",André Srinivasan,"I’m thrilled to share that Redis is now available as an embedded data service for the EdgeX Foundry IoT Edge Platform. By leveraging Redis, the EdgeX Core Services delivers an exceptional performance, a small runtime footprint and the ability to ingest millions of device events at the IoT edge with <1ms latency to IoT solutions. The combination of EdgeX and Redis has been optimized to live on fog nodes, edge gateway devices and even IoT devices in some cases. Redis’ blazing fast performance and small memory footprint (<5MB) enables data analytics to be closer to the source and respond faster to real-time business needs.In the wild of the IoT edge space, there are many diverse conditions and requirements that will tax any database trying to provide necessary data services. These conditions will inevitably require the use of multiple data models, as well as edge computing involving deep learning, image recognition and other complex computing requirements. As a multi-model database, Redis has the ability to handle these various data models gracefully, removing the complexities of an architecture with polyglot persistence.EdgeX with the Redis open source database is available immediately under the 3-Clause BSD license.In tandem with our partnership with EdgeX, Redis is introducing a new commercial product called RedisEdge, a database built specifically to address the demanding conditions at the IoT edge. RedisEdge is a commercial product, fully compatible with Redis Enterprise, which makes it ideally suited to extend to a clustered Redis Enterprise in the cloud for IoT solutions that require a hybrid edge and cloud architecture.We are in the early stages of a disruptive trend, with virtually all major cloud and technology players making large bets on IoT and edge computing. This is the fourth wave of computing over the past 50 years: Mainframe → Client/Server → Cloud → Edge. It is anticipated edge computing will accelerate over the next decade as many compute models transition from cloud computing to a hybrid edge/cloud architecture to optimize the latency, bandwidth, security and locality needs of IoT applications. Gartner projects that by 2020, processing and analytics for industrial IoT use cases will increase from <10% to 60% at the IoT edge. Redis partners like Microsoft Azure and EdgeX Foundry — which already incorporate RedisEdge and Redis into their IoT edge platforms — are ahead of the game and are therefore ready for this massive ground shift toward edge computing.IoT edge application developers should not have to deal with the complexities of the IoT edge stack, including messaging and networking protocols. With RedisEdge, application developers can focus on their applications and business needs; Redis will take care of the rest.Read more about bringing cloud principals to the edge here. If you have any questions about RedisEdge, please reach out to us at support@redis.com!"
306,https://redis.com/blog/benchmarking-redisgraph-1-0-update/,Benchmarking RedisGraph 1.0: Update,"January 17, 2019",Pieter Cailliau,"After we published our recent RedisGraph benchmarking results, the folks at TigerGraph replied with some thoughts. While we appreciate hearing our competition’s perspective, we’d like to address their accusation of fake news.In our blog post, we introduced a parallel requests benchmark. This test is the same K-hop neighborhood count query test created by TigerGraph, but for our test, we had 22 clients execute the queries against RedisGraph concurrently. We introduced the concurrent requests test because we believe it is representative of almost any real-time, real-world scenario.The results we published measured the overall benchmark time of 300 seeds for RedisGraph. This is the time it took from submitting the first query until the last query returned. For TigerGraph however, we extrapolated the overall benchmark time instead of executing the benchmark. We wrongly assumed that in this concurrent test it could be seen as if the requests were executed sequentially in TigerGraph, since we observed that certain queries in TigerGraph consumed all 32 cores of the machine. The overall benchmark time results that we first published for TigerGraph were based on the average query time that TigerGraph had published multiplied by the total number of seeds.We acknowledge this was an unfair assumption, so we decided to execute the full benchmark against TigerGraph.For your convenience, below are the results we originally published.  The times presented below are overall benchmark times (for RedisGraph these were measured, for TigerGraph they were extrapolated as described above).The numbers below represent the overall benchmark time we recently measured for TigerGraph.While we found that RedisGraph is still faster than TigerGraph, this new data finds that we are 5-15 times faster (as opposed to the 6-65x estimate we originally published). The biggest change was in the 1-hop queries, whereas for longer path queries the times decreased slightly. This is due to the fact that 1-hop queries are far less computationally expensive than multiple hop queries, allowing for more concurrency. But to quote TigerGraph: “In the real world, if you only need to do one hop, a key-value database or RDBMS is sufficient enough; you don’t need a graph product.”In addition to the overall benchmark time, we would also like to publish the average query time from our parallel request benchmark, since overall benchmark time is in fact not the best metric for this test. This is because the population of seeds (300) was too small for 22 parallel requests. If, for example, the longest running query was submitted as the last query, then the other 21 clients would be idle, which  would influence overall benchmark time negatively.A more correct metric is average query time, which averages the individual response times of all 300 seeds. Ideally, we would not include the first 22 and last 22 response times in this average, but since the population of seeds is already so small, we chose to leave them in. Below are the average query times we measured for both RedisGraph and TigerGraph. These are new results that weren’t included in our first benchmark publication, and we used no extrapolation to determine the time results below.This shows that RedisGraph’s average query times are 5-20 times faster than TigerGraph under parallel load. We believe this is what really matters for our RedisGraph users — a database that enables the highest throughput at the lowest latency.Furthermore, we’re making our modifications to the benchmarking code originally created by TigerGraph available to everyone and documenting a known issue about our variable length path query behaviour. We would also like to confirm that any results published in our original blog post that were not for TigerGraph or RedisGraph were the results that TigerGraph originally published — no boosters were given. And yes, in addition to RedisGraph, TigerGraph was another solution that never timed out in any test."
307,https://redis.com/blog/redis-satisfies-need-speed/,Processing Large Volumes of Data With Real-Time Record Tracing,"January 30, 2019",Michal Cholewa,"After aggregating massive amounts of disparate data coming in across hundreds or thousands of diverse systems and locations, DigitalRoute’s Usage Data Platform (UDP) then analyzes and acts upon all that data… in real time! Not only does the UDP transform the data, it also filters and forwards the data, creating an intelligent layer across a company’s infrastructure. Sounds daunting, right?It is daunting, but tracking customer consumption in large and complex environments is exactly what DigitalRoute’s Usage Data Platform was built to do, not only for organizations in the telecom industry, but other industries too such as utilities, financial services, and travel.Our technology processes 300 billion transactions per day and much of that data is streaming. Needless to say, our platform has a real need for speed. Industry benchmarks demand times of less than 500ms for end-to-end processing of what can easily reach thousands of transactions per second per customer. Even more demanding are our customers, who are counting on us to provide real-time insights that they can then act upon in real time to improve user experience.My application development team recently introduced a new trace feature to DigitalRoute’s platform offerings. This feature tracks the path of any given record through a customer’s system: where it originated, where it currently resides, and what types of transformations have been applied to it along the way. Since one single transaction can be propagated to many areas of a system for fine-grain correlations, we needed a robust back-end capable of efficiently storing this information and presenting it to the requesting application on the fly. And like every other aspect of our platform, it needed to be fast.Initially, we used Akka Clusters running on top of our own “abstract executor” as the back-end tool for these communications, but this deployment turned out to be quite painful for us to manage because each time connection was lost we had to perform a series of cumbersome administrative steps to recover the cluster. After receiving a couple of problem reports, we decided to give Redis’ pub/sub messaging capabilities a try.Under a pub/sub communications pattern, the publisher publishes that the state of the data has changed (along with the changes themselves) and the subscribers, which are in constant listening mode, update their internal states in response. Redis’ pub/sub message broker proved to be extremely simple to set up and very, very fast. We use it to exchange trace messages. Because our interface has many instances, Redis channels listen to traces reported by active workers at any given point in time. Then, using the reported ID, we can find a sorted set holding traces in correct order. We benefit from the ability to subscribe to a channel at any time, and listen to any new trace reported by any worker; there’s no need to create separate services.After hearing about Redis Streams at RedisConf18, I did some research and believe that this new data structure will be an even better fit for our use case. The pub/sub messaging pattern is one of fire and forget, but Redis Streams persists data—even when data consumers are offline or disconnected. This built-in data persistence is a perfect complement to the historical view our tracing feature is looking to provide. Redis Streams also creates a many-to-many data channel between producers and consumers, which is ideal for us because our platform most typically deals with multi-sourced data destined for consumption by multiple consumers.With Redis lending a helping hand, it’s full speed ahead for DigitalRoute!See how you too can use Redis Streams in your apps and follow Digital Route on their Twitter to see what’s next for them."
308,https://redis.com/blog/redis-labs-named-january-2019-gartner-peer-insights-customers-choice-operational-database-management-systems/,Redis Labs Named a January 2019 Gartner Peer Insights Customers’ Choice for Operational Database Management Systems,"February 1, 2019",Miguel Allende,"I love my job. As a Customer Advocacy Manager, I get to hear first-hand accounts from the people we impact, including challenges they’ve had to overcome, their innovative implementations and the success they’ve achieved with our product. But working in the customer advocacy field can be volatile. To be frank, the success of my position is largely dependent on the happiness of our customers. Technically, everyone’s job here is dependent on our customers’ success. But my role feels the direct impact, whether positive or negative, of each customer experience. I can’t share a customer’s achievements (a key part of my job) if they haven’t had success with our products.When we were named a January 2019 Gartner Peer Insights Customers’ Choice for Operational Database Management Systems (OPDBMS), I quickly realized how lucky I am to work with the Redis developer community. In less than a year, we received over 100 reviews while maintaining a 4.6 rating*. Our team at Redis Labs takes great pride in this distinction because we wouldn’t have been able to get there without the support of such an active and passionate community.In its announcement, Gartner explains, “The Gartner Peer Insights Customers’ Choice is a recognition of vendors in this market by verified end-user professionals, taking into account both the number of reviews and the overall user ratings.” To ensure fair evaluation, Gartner maintains rigorous criteria for recognizing vendors with a high customer satisfaction rate.Here’s what some customers who contributed to this recognition had to say about Redis:Read more reviews written by the professionals who use Redis Enterprise here.In short, we couldn’t have been named a Customers’ Choice without our active community, and we look forward to continuing to foster those relationships. To learn more, check out the Gartner Peer Insights’ announcement.To all of our customers who submitted reviews, thank you! Your feedback helps mold our products and shape the customer journey. We look forward to further building on the experience that contributed to this distinction!If you have a Redis story to share, we encourage you to join the Gartner Peer Insights crowd and weigh in.*Reviews and ratings are current as of January 30, 2019.The GARTNER PEER INSIGHTS CUSTOMERS’ CHOICE badge is a trademark and service mark of Gartner, Inc., and/or its affiliates, and is used herein with permission. All rights reserved. Gartner Peer Insights Customers’ Choice constitute the subjective opinions of individual end-user reviews, ratings, and data applied against a documented methodology; they neither represent the views of, nor constitute an endorsement by, Gartner or its affiliates."
309,https://redis.com/blog/redis-labs-modules-license-changes/,Redis Labs’ Modules License Changes,"February 21, 2019",Yiftach Shoolman,"For the latest information, please read the RSALv2 + SSPL license blog.Early in August 2018, Redis was one of the first open source companies to realize that the current open source licensing scheme falls short when it comes to use by cloud providers. We wanted to make sure open source companies could continue to contribute to their projects, while still maintaining sustainable business in the cloud era. That’s why we changed the license of our Redis Modules from AGPL to Apache2 modified with Commons Clause.It was not an easy move for us, and we probably didn’t communicate the change clearly enough. This caused some confusion when some people incorrectly assumed that the Redis core went proprietary, which was never the case (see more here).However, over time, other respected open source companies, like MongoDB and Confluent, created their own proposals for modern variants to open source licensing. Each company took a different approach, but all shared the same goal — stopping cloud providers from taking successful open source projects that were developed by others, packaging them into proprietary services, and using their market power to generate significant revenue streams.Since last summer’s announcement, we’ve continued to manage our Redis Modules projects in the same way we manage our other open source projects. Our open and transparent manner helped the majority of our community accept the license change. Certain large enterprises even preferred it over AGPL, which we had previously used for those modules. During this period, we also received honest feedback from multiple users about how we could further improve our license to favor developers’ needs.We identified three areas that needed to be addressed:Given all of these considerations, and after many discussions with members of our community, we decided to change the license of our Redis Modules to Redis Source Available License (RSAL).RSAL is a software license created by Redis for certain Redis Modules running on top of open source Redis. RSAL grants equivalent rights to permissive open source licenses for the vast majority of users. With RSAL, developers can use the software; modify the source code’ integrate it with an application; and use, distribute or sell their application. The only restriction is that the application cannot be a database, a caching engine, a stream processing engine, a search engine, an indexing engine or an ML/DL/AI serving engine.For additional information, please see our detailed FAQs.With RSAL in place, the Redis licensing model looks like this:This change has zero effect on the Redis core license, which is and will always be licensed under the 3-Clause-BSD. Unlike many other open source database companies, we have built a dedicated team (led by Salvatore Sanfilippo, the creator of Redis), who manages the Redis core in a completely independent manner. Additionally, we have chosen not to limit the functionality of open source Redis by moving core components to closed source. Consequently, open source Redis includes all the ingredients needed to run a distributed database system — replication, auto-failover, data-persistence and clustering.This open approach sometimes works against our commercial interest, since the cloud providers don’t have to do much in order to offer a viable Redis service. But we have a much bigger vision of helping modern applications provide instant experiences to their users. The only way to guarantee end-to-end instant  application response time, which today is considered anything under 100msec, is by ensuring your database constantly and consistently responds to application requests in less than 1msec. Of course, we believe there’s only one database that can do this — and that is Redis!Most recently, we are seeing some cloud providers think differently about how they can collaborate with open source vendors. We believe those cloud providers that build the right collaboration infrastructure will be the ones that eventually benefit the most from open source projects."
310,https://redis.com/blog/redis-labs-multi-model-database-recognized-nosql-leader/,Redis Labs’ Multi-Model Database Recognized as a NoSQL Leader,"March 14, 2019",Priya Balakrishnan,"We’re honored to announce that Forrester Research this week cited Redis as a Leader in The Forrester WaveTM: Big Data NoSQL, Q1 2019. In this evaluation, we believe that the leading independent research firm placed us as a leader based on Redis’ high-performance and multi-model approach, which powers a broad set of enterprise applications. In fact, we received the third-highest score out of 15 vendors in the current offering category, with Forrester noting that “Customer references like its innovation for machine learning apps, performance, scale, customer support, and support for diverse NoSQL use cases.”For this research, Forrester identified, analyzed and scored key-value, document and graph database vendors across 26 criteria, evaluating each provider’s offering, strategy and market presence. The report highlights some of Redis and Redis Enterprise’s key capabilities, stating “Redis is a multi-model, open source, in-memory database platform whose key development is currently sponsored by Redis. Redis supports both relaxed and strong consistency, a flexible schema-less model, high availability, and ease of deployment. An enterprise version encapsulates the open source software and provides additional capabilities for geo-distributed active-active deployments (multi-cloud, hybrid, on-premises) with high availability and linear scaling, while supporting the open source API.”NoSQL has become critical for business to support a new generation of applications — which depend on real-time data to deliver an instant experience, requiring databases that can be deployed on multiple clouds, on-premises or in a hybrid architecture. This demand helped the category grow leaps and bounds over the past decade, as NoSQL went from supporting simple schema-less apps to becoming a mission-critical data platform for large Fortune 1000 companies (including the 70% of Fortune 10 companies that use Redis Enterprise as their primary database). According to Forrester, half of global data and analytics technology decision makers either have implemented or are implementing flexible NoSQL platforms.To learn more, read our press release or click to download the full research."
311,https://redis.com/blog/learn-easily-build-deploy-real-time-applications-redis-enterprise-red-hat-openshift-container-platform/,Learn How to Easily Build and Deploy Real-time Applications with Redis Enterprise on Red Hat OpenShift Container Platform,"March 15, 2019",Sheryl Sage,"On March 29, Rob Szumski, Principal Product Manager at Red Hat will join Redis for a webinar to discuss how Kubernetes and OpenShift will help you accelerate application delivery with Redis Enterprise. This session will cover how containers and OpenShift automates Kubernetes tasks and how the Redis Enterprise Operator lets you run stateful databases so you can more easily manage the lifecycle of your Redis deployments in your Kubernetes cluster.Kubernetes is hard. Not everyone can be Kelsey Hightower. Over the past few years developers have been leading the charge using open source software and innovative tooling to build cloud-native applications. As a result, DevOps teams are challenged to manage portable services across different clouds, without taking on the risk of deploying unsupported technologies.The introduction of Operators for Kubernetes and OpenShift makes it easier for DevOps, to extend Kubernetes using custom resources and custom controllers to manage your applications that run on Kubernetes. The central idea of an Operator is to encode the best practices for deploying and managing container-based applications as code. This frees up DevOps teams from doing all the grunt work of managing applications and allows them to focus on higher-level tasks.Red Hat recently announced OperatorHub.io, which fills an important gap for the Kubernetes community with a curated set of Operators. By standardizing the Operator Framework and lifecycle, Red Hat is working with the broader community of developers and ISVs to provide DevOps teams with the essential Kubernetes tooling for administrators to provision, update, scale, and manage the lifecycle of apps and services no matter where they reside.Redis and Red Hat share a common vision, around our open source ecosystem, to provide more advanced tooling to manage complex stateful applications with Kubernetes and OpenShift. Redis has worked very closely with Red Hat to simplify and automate the creation and management of highly available Redis Enterprise on the OpenShift Container Platform with built-in support for Redis Enterprise Operator for Kubernetes. Additionally, to help customers reduce risk and ensure business continuity, Redis Enterprise Active-Active support was recently introduced for geographically distributed applications to persist Redis Application data with Conflict-free Replicated Databases (CRDB) on OpenShift.Join usIn addition to the presentation, Adi Foulger, Principal Architect at Redis, will provide a Demo featuring the Redis Enterprise Operator on OpenShift. By the end of the talk you will have a better understanding of where the Kubernetes and OpenShift is heading to make it easier to manage applications that run on Kubernetes and how to manage Redis Enterprise in your Kubernetes and OpenShift cluster.Register here"
312,https://redis.com/blog/introducing-redis-cloud-pro-essential-versions/,Introducing Redis Cloud Pro and Essential Versions,"March 25, 2019",Allegra Dan,"As we continue to enhance our product offerings to deliver further ease of use and simplicity for our customers, we are happy to announce enhanced versions and new names for our cloud offerings. Starting today, we are unifying Redis Enterprise Cloud and Memcached Enterprise Cloud and naming them Redis Cloud Essentials . We are also renaming Redis Enterprise VPC to Redis Cloud Pro.In addition to rolling out new features for each cloud version, we will now offer Redis Cloud Pro as a fully hosted Database-as-a-Service (DBaaS). This means that customers can get started on Redis instantly without having to first create their own cloud accounts.Our two cloud products are suitable for varying use cases – from basic, small data sets up to large, global and distributed implementations. Customers can choose their deployment type (multi-tenant or hosted VPC), and we’ll deliver the relevant set of features based on their business needs.Redis Cloud Essentials is a fully hosted, fully managed, multi-tenant DBaaS in the cloud. Its pricing is based on memory size, either at fixed plan rates or using our pay-as-you-go model.Redis Cloud Essentials offers the blazing high performance that Redis is known for and includes all of its open source features. In addition, it also includes Redis Enterprise features, like high availability and infinite scalability, within a fully hosted cloud environment.Signing up for Redis Cloud Essentials is short and simple. Select your preferred cloud provider, region and plan, and you are ready to go. You can check it out yourself here .Redis Cloud Pro is also a fully hosted, fully managed DBaaS, with the addition of a dedicated virtual private cloud (VPC). Its pricing is based on memory size and maximum throughput, either at annual reserved rates or using our pay-as-you-go model.Redis Cloud Pro contains all Redis Cloud Essentials features, as well as Redis on Flash, active-active geo-distribution, active-passive geo-distribution, built-in full-text search and integrated modules (RedisJSON, RedisBloom and RedisGraph).Signing up for Redis Cloud Pro is also more simple. In just a few minutes, you can choose your cloud and region and then select the performance characteristics required for your database.Since we figured you’d have some questions about these enhancements, we’ve answered a few of them below and provided a handy comparison chart. Feel free to get in touch with our cloud specialists if you need further details.Can I still deploy Redis Cloud Pro on my own cloud account?Yes, you can deploy Redis Cloud Pro on your own cloud account if you prefer to do so. Please contact Redis’ cloud specialists, and they can set up your account to accommodate this.Who should use Redis Cloud Pro?Redis Cloud Pro is the right solution for companies interested in one or more of the following scenarios:Can I migrate my existing Redis implementation to Redis’ cloud plans?Yes, existing Redis implementations can be migrated to any of our cloud plans, and you can migrate your databases between the two cloud plans as well. For more information, please contact Redis’ support team."
313,https://redis.com/blog/how-intels-persistent-memory-and-redis-is-a-new-game-changer-in-the-database-industry/,How Intel’s persistent memory and Redis is a game changer in the Database industry,"April 2, 2019",Priya Balakrishnan,"Over the past decade, our customers have used Redis in some amazing ways—as a cache (of course!), session store, message broker, recommendation engine, secondary index, streaming platform, and, increasingly, single source of truth database. Redis’ extreme versatility across countless scenarios is due to its ability (unlike any other database in the market) to process data and deliver insights at sub-millisecond speeds, irrespective of data volume.The ability of Redis to instantaneously process data represents the future of all databases, but the performance efficiency gained by in-memory computing can sometimes come at a cost (literally!), especially if the volume of data needed for instant processing is substantially large and resides in DRAM. It was with this consideration that Redis introduced Redis on Flash in 2016. Redis on Flash stores Redis keys, data dictionaries, and “hot” (frequently accessed) data in RAM, and “cold” data on Flash SSDs, all while still maintaining sub-millisecond performance. Many of our customers including Whitepages, Malwarebytes, Dynamic Yield, BioCatch, Inovonics, and Etermax, all representing a variety of use cases with data sets in the terabytes, have achieved 40-80% savings on infrastructure costs with Redis on Flash.But even with the incredible success of Redis on Flash, we have not remained complacent. Redis has been working very closely with Intel to ensure that their latest game-changing memory technology— Intel® OptaneTM DC persistent memory—is immediately available to Redis Enterprise users upon its general release with the second-generation Intel® Xeon® Scalable platforms. Which is happening now! And we’re ready!Intel Optane DC Persistent Memory delivers a new persistent memory tier between DRAM and SSD that can provide up to 6TBs of non-volatile memory capacity in a two-socket server plus up to 1.5TB of DRAM,—at a performance level comparable to traditional DRAM memory. More simply put, this new tier extends a standard machine’s memory capacity to up to 7.5TBs of byte-addressable memory (DRAM + persistent memory), while providing persistence. This technology comes in a DIMM form factor and is available as a 128, 256, and 512GB persistent memory module.If you’ve been limiting your use of Redis because of memory costs, you no longer need to agonize over the trade-off. Intel Optane DC persistent memory allows you to keep more data per node, dramatically reducing your infrastructure costs while maintaining performance SLAs, keeping sub-millisecond latency at high throughputs of 1M ops/sec – a typical throughput for Redis Enterprise customers.How do we know? We worked with Intel to test Redis Enterprise on its new persistent memory tier:Over the multiple tests executed, we were able to consistently reproduce the same 1ms latency on these large data sets at high throughput. This equates to approximately 43% savings in hardware costs with little to no impact on performance—even on throughputs running into millions of operations per second!You can see why this is a game-changer for Redis Enterprise users, who want more data closer to compute so they can use Redis Enterprise’s multi-model capabilities to their fullest and use it as a single source of truth database for its many robust enterprise capabilities.There are two distinct operating modes available to customers of Intel’s persistent memory:Given that Redis is based on an in-memory engine (i.e. all its data structures are byte-addressable, with no special serialization/deserialization processes), it was relatively easy to adapt the Redis Enterprise stack and Redis on Flash to work with Intel’s new technology, which is also byte-addressable by design.Want to learn more? If you’re in the bay area, join us at RedisConf 2019 in San Francisco (April 2-3). We’ll be talking about this technology in depth at the event and you’ll have direct access to both the Intel team and Redis experts. Even if you haven’t registered, show up and we’ll accommodate you! If you can’t make it to our conference, download our white paper, which delves into a good amount of detail on the persistent memory technology. Last but not least, you can always reach out to the experts at our office for more information. We’re happy to inform and educate in any way that works for you!"
314,https://redis.com/blog/rdbtools/,Redis Labs has acquired visualization solution RDBTools from HashedIn,"April 3, 2019",Alvin Richards,"After hearing all your suggestions, concerns and gripes and after looking at our data and understand that this is what the typical Redis user needs… it’s finally here.Today at RedisConf 19 we announced that Redis, the home of Redis, has acquired RDBTools from HashedIn. You can read all about HashedIn announcement here. RDBTools is the leading product for people to view and analyze their data with awesome features like Memory Analysis. All aimed at Developers to get the most out of Redis.Not familiar with RDBTools? Well here’s a quick dip into what you can doRich visualization is a part of gaining insights into your Redis deployments, here’s an example of Memory analysis.You can also understand your Redis deployments and see the relationships between Master and Replica instances and Shards.You can deploy RDBTools in a number of ways. There are Desktop clients for Windows, Mac and Linux. A Docker image also exists, to ease the deployment – including into Kubernetes clusters. And finally there is an image on the AWS marketplace.We will continue to provide a free version of RDBTools and have the following features and support planned out:We also plan to provide RDBTools as a fully-managed service.So grab a free copy here, and use it with Redis Open Source, Redis Enterprise or point at your favorite Redis Managed Services (GCP, Azure and AWS)."
315,https://redis.com/blog/redis-named-loved-database-third-consecutive-year/,Redis Named the Most Loved Database for Third Consecutive Year,"April 11, 2019",Redis,"This year Redis continues to rack up milestones. A few days ago we celebrated the 10th anniversary of Salvatore Sanfilippo first posting a link on Hacker News about starting the Redis project. Now coming fresh off our fifth annual – and biggest ever RedisConf—we are  thrilled to receive the news that Redis has been named the most loved database in Stack Overflow’s 2019 Developer Survey for the third consecutive year!In this year’s survey, Redis significantly increased its share of developers as the database they want to continue working with, up nearly seven percent to 71.3% of the total respondents. Redis continues to be the sixth most commonly used database and the fourth most wanted database.We truly appreciate this recognition because this feedback comes directly from the developer community. This is significant because everything we do begins with us asking ourselves, “How will this help the Redis community?”This year Stack Overflow received nearly 89,000 responses from developers in 179 countries. In combination with more than 1.5 billion launches on Docker Hub, it is exciting to see Redis continue to gain widespread love and adoption by developers worldwide.Additionally we continue to innovate and develop Redis to be the high performance, multi-model database developers must have to build the fastest and most powerful applications. This year we’ve seen amazing industry recognition of these new capabilities including:If you are interested in learning more about Redis and experience the performance and simplicity that Redis is most loved for, you can sign up and get 30mb of Redis for free in the cloud. It’s yours as long as you use it—no strings attached. Thank you again to the entire Redis community for your tremendous support and we can’t wait for what else is in store for the rest of 2019."
316,https://redis.com/blog/announcing-redis-enterprise-5-4-2-improved-crdb-compatibility-open-source-redis/,Announcing Redis Enterprise 5.4.2 with Improved CRDB Compatibility for Open Source Redis,"April 16, 2019",Paz Yanover,"We just released a new version of Redis Enterprise – v5.4.2 and are happy to announce that we’ve added a much requested feature. One of the big highlights of this release is improved compatibility for active-active Redis (through CRDBs, or conflict-free replicated databases) COUNTER operations with open source Redis.Redis Enterprise 5.4.2 also contains other new features and capabilities, such as:Prior to version 5.4.2, we treated COUNTERs as a different data type in Redis. In some scenarios, this created conflicts with the way users were used to working with Redis. For instance, unlike SETting a value and then incrementing it using the INCR command in open source Redis, with CRDBs, once you SET a value you couldn’t increment it because it was not treated as a COUNTER.Since one of the main ideas behind CRDBs is to allow applications to smoothly migrate from a standard Redis deployment to an active-active deployment, we realized that this design must be changed. From now on, any CRDB COUNTER operations will be treated in the exact same manner they are treated in Redis. For example, Redis will check the encoding of the data before applying an operation, so an INCR operation will only succeed if the value is encoded as an integer. CRDBs will now follow this same algorithm.The above CRDB improvement also applies to other Redis data types, such as Redis Hashes that contain and use Strings.In order to update String data type behavior for CRDBs, we made changes to its internal structure and the communication protocol between CRDB instances. To enjoy these improvements, you’ll need to install Redis Enterprise version 5.4.2 and then upgrade your existing CRDB deployment. This will automatically execute the required Redis server restart. Newly created CRDBs will benefit from active-active compatibility.The CRDB protocol is backward-compatible, which means that Redis Enterprise 5.4.2 CRDB instances that use the new protocol can understand write operations from instances on versions below 5.4.2. However, it is not forward-compatible, so CRDB instances with the old protocol cannot understand write operations of instances with the newer protocol version. As a result, after you upgrade the CRDB protocol on one instance, other instances that are not yet upgraded may not be able to sync from the upgraded instance, although the upgraded instance will receive updates from both upgraded and non-upgraded instances.Therefore, we highly recommend that you upgrade all instances of a specific CRDB within a reasonable time frame to avoid temporary inconsistencies between them.Note that:In order to upgrade the protocol of your CRDBs, follow these upgrade instructions.If you’re interested in a more detailed view of what else is new in Redis Enterprise 5.4.2, take a look at our technical documentation or check out our release notes.For any questions, don’t hesitate to drop us a line at pm.group@redis.com!"
317,https://redis.com/blog/using-redis-to-optimize-feature-rollouts-and-error-triaging/,Using Redis to Optimize Feature Rollouts and Error Triaging,"April 24, 2019",Shabih Syed,"In an effort to run, run, run… you don’t want to make the $460 million dollar mistake that Knights Capital made back in 2012. This single-day computer system failure of a leading financial market-maker offers several lessons for the broader IT community, including the critical importance of your system components’ design, implementation and DevOps details. In this two-part blog, I’ll share some ideas to help development teams keep their continuous integration and continuous deployment (CI/CD) processes fool-proof. In particular, I’ll show how you can manage continuous updates by using feature toggles and feature context to dictate code routing, store log data for easy access and create an error database with fast lookups — all with the help of Redis.Imagine, you are a director of engineering managing a team of several developers responsible for the front-end of a web app with thousands of concurrent users. Your app is deployed in AWS and you push weekly updates. The business cannot afford to have any disruptions to the web app, so if an error occurs, your team has to roll-back its latest update instantly.You have to identify the culprit code quickly, have the appropriate developer fix it and make the change part of a subsequent release. Also, the product team is always requesting new features to be made generally available asap. So, how can you react to errors swiftly, and deploy feature requests safely at the speed the business demands?At the 2019 Game Developers Conference (GDC), I attended a session that described a well-thought-out process to perform weekly software releases reliably. The session was titled “Debugging in the Large: Cross-Platform Stability at 70M+ Monthly Active Users” and it was co-presented by Chris Swiedler from Roblox, a Redis customer. Chris shared an interesting insight into how his team modifies application behavior at Roblox without changing code in case they run into production issues. They use feature flags, which is very similar to Martin Fowler’s “feature toggle” approach.Let’s breakdown Figure 2, which outlines an approach that could be part of your CI/CD and triage process.This strategy can be helpful for:But this approach can be taken one step further to help distributed development teams release new features safely and roll them back when required with minimal impact.Redis Enterprise fits the bill when you need a fast, persistent database. Its capabilities include:In my next installment for this series, I’ll offer more details and code snippets to show specifically how feature toggling, feature context, error databases and log databases built with Redis can make your CI/CD triage process more effective and efficient."
318,https://redis.com/blog/multi-model-action-using-redisgraph-redisearch-together/,Multi-Model in Action: Using RedisGraph and RediSearch together,"May 1, 2019",Redis,"At RedisConf19, I demoed a solution for running full-text RediSearch over nodes in RedisGraph. The days leading up to the event are a bit of a fog, but now, a few weeks later, I realized we should explain more about how we did this and release the source code.In this demo, I showed a little interface that allows you to search for animals and see how they are related via the biological classification system (kingdom, phylum, class, order and so on). The full-text portion was based on the first English paragraph from wikipedia. So, as an example, let’s say you searched for “cat house pet” (Domestic Cat, Felis catus) and “baleen blue” (Blue Whale, Balaenoptera musculus), you would see that they are both just mammals, while if you searched for “cat house pet” and “snow central asia cat” (Snow Leopard, Panthera uncia), you would see that they both belong to the same family, Felidae.This demo project was surprisingly easy, but I should point out that the integration between RediSearch and RedisGraph is still early and not ready for production at the time of writing. I encourage you to figure out if this approach could meet your needs, with the understanding that the integration between RediSearch and RedisGraph will mature and expand in the coming months.In the meantime, let’s talk about the components of the demo. The first thing to accomplish is to build both RediSearch and RedisGraph based on the correct branches in the repository. For RediSearch, that’s just the current master branch, while for RedisGraph you’ll need the redisconf branch. If you’d like to replicate the solution for your own needs, you’ll want to build both modules from source. The website for RedisGraph and RediSearch each have detailed instructions on how to do this, which takes a bit of time, but isn’t hard.In your redis.conf file, you’ll need to ensure that RedisGraph is loaded before RediSearch. To do this, go to the redis.conf file and in the modules section, put the loadmodule directive for RediSearch before that of RedisGraph. This way, when RedisGraph loads, it will detect the presence of the search module. After you’ve finished editing redis.conf, you can go ahead and restart the Redis server.In the demo repo, I’ve included the data set we cobbled together by using the redisgraph-bulk-loader script to load the following from CSV into RedisGraph. This dataset only includes the mammals in the animal kingdom, due to the lower quality of data available for animals outside of mammals (Non-mammals species rarely have good Wikipedia descriptions).Here is how you load the data:$ cd redisgraph-bulk-loader/
$ python3 bulk_insert.py MAMMALS -q -n /path/to/demo/dataload/Class.csv -n /path/to/demo/dataload/Family.csv -n /path/to/demo/dataload/Genus.csv -n /path/to/demo/dataload/Order.csv -n /path/to/demo/dataload/Species.csv -r /path/to/demo/dataload/IN_CLASS.csv -r /path/to/demo/dataload/IN_FAMILY.csv -r /path/to/demo/dataload/IN_GENUS.csv -r /path/to/demo/dataload/IN_ORDER.csv -a yourpassword
1 nodes created with label 'Class'
157 nodes created with label 'Family'
1272 nodes created with label 'Genus'
29 nodes created with label 'Order'
5616 nodes created with label 'Species'
29 relations created for type 'IN_CLASS'
1272 relations created for type 'IN_FAMILY'
5616 relations created for type 'IN_GENUS'
157 relations created for type 'IN_ORDER'
Construction of graph 'MAMMALS' complete: 7075 nodes created, 7074 relations created in 0.443749 seconds
$ redis-cli -a yourpassword GRAPH.QUERY MAMMALS ""CALL db.idx.fulltext.createNodeIndex('Species','description')""
Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.
1) (empty list or set)
2) (empty list or set)
3) 1) ""Query internal execution time: 324.970000 milliseconds""(gist: https://gist.github.com/stockholmux/0727a4a784a46f8cb9e8329d393a513a)At this point, the key MAMMALS contains our entire graph. A couple of important notes:Now, let’s get a UI up and running. As with practically any Node.js application, we start with npm install. This might take a few seconds, as we’re managing not only the server-side files for Node.js, but also the front-end Vue.js components. If you haven’t spent much time in the front-end Javascript world recently, it’s no longer a FTP and HTML file affair. Modern front-enders do take their tooling seriously, so we’ll need Vue CLI installed (I suggest following the Vue CLI getting started guide).After you’ve got your front-end tooling in place, let’s float back to npm and build the front end by running:$ npm run buildThis will create the dist directory that we’ll serve all our front-end files from. Now that we have our data in Redis, our front-end files are ready to serve, so we can kick on the server:$ node server.js -p 6379 -a yourpassword -h yourhostOrlocalhostLet’s pause for a moment and cover a few points about this server we just turned on. It’s built on Express.js and primarily uses websockets to communicate back and forth. I also integrated the Visible Engine debugging tool, which allows you to see commands being executed in a separate browser window. You can play with that demo by pointing your browser at http://localhost:4444.All in all, it’s remarkably short for what it does – only 75 lines of code. Our solution doesn’t need to be that long because all we’re really doing is accepting websocket connections, running Redis commands based on the passed messages, and passing those messages back with the results. Redis(Graph) is doing all the hard work here. Let’s take a look at the commands we’re executing.To search for keywords, we run this command:> GRAPH.QUERY MAMMALS ""CALL db.idx.fulltext.queryNodes('Species','cat house pet')""This is pretty unsophisticated. Our key is MAMMALS and we using a special Cypher syntax CALL to a specific function, which passes in the label of the nodes we’re looking for and finally the actual search string. You can pass in valid RediSearch queries, but keep in mind that this is only full-text search at the moment, so don’t use geospatial, tag or numeric clauses.Once we’ve identified the two animals we’re comparing, we use a straight Cypher query:> GRAPH.QUERY MAMMALS ""MATCH (s:Species)-[*]->(x)<-[*]-(c:Species) WHERE c.fullname = 'Felis catus' AND s.fullname = 'Balaenoptera borealis'  RETURN x.name, labels(x) LIMIT 1""In the server.js file, these queries are represented as Javascript template strings. Nothing is hidden, it’s simply user input through string interpolation to a query. However, if deploying something like this in production, you’d want to be careful with accepting user input.If you plan to tinker with the front-end code, make sure you edit the /src directory, not /dist. After editing, you’ll need to run npm run build again or use the development server (npm run serve) that auto-compiles changes to front-end code and serves it up on another port. Again, not much going on here, it’s a pretty standard Vue.js and Bootstrap application. The only really relevant files are /src/App.js, /src/components/panels.vue and /src/components/search.vue.That’s it. A simple demo to show off a powerful feature that integrates two distinct models – graph and full-text search. I encourage you to play with the demo and sub in your own datasets."
319,https://redis.com/blog/whats-new-redisgraph-1-2-0/,What’s New in RedisGraph 1.2.0,"May 2, 2019",Roi Lipman,"Last week, we released RedisGraph 1.2.0. If you’ve been using RedisGraph or considering trying out the module, this is a good time to see what we’ve been up to as we continue to enhance its graph processing capabilities. Let’s review some of the more notable additions in the new release:Up until this release, connecting node A to node B via an edge of type R and then forming another connection of the same type between the two was not possible. as the latter would overwrite the former one, because of the way connections used to be represented in RedisGraph.For every relation type, e.g. `friend`, `works_at` or `owns`, RedisGraph maintains a mapping matrix M, in which M[I,J] = EdgeID where I and J are the source and destination node IDs, respectively. Forming the first connection of type R between A and B would have M[I,J] set to X, and introducing a second edge of type R between the two would overwrite M[I,J] with Y.With release 1.2.0, RedisGraph now enables multiple edges of the same type. Our mapping matrix M can hold a pointer to an array of edge IDs, M[I,J] = [ID0, ID1, … IDn], specifying each edge of type R connecting I to J.To avoid high memory fragmentation, consider a graph with 500 million edges none of which share source or destination nodes, i.e. no multiple edges of the same type between any two nodes. A naive approach would have allocated 500M arrays of length 1, increasing our memory consumption significantly. To encounter this, our mapping matrix entry type remained the same (UINT64).  Starting with an empty graph – M[I,J] is empty – when the first edge E is formed, connecting I to J, M[I,J] = ID(E) is a simple constant and no extra memory is allocated. At some point, I is connected once again to J with an edge X of the same type as E, M[I,J] = [ID(E), ID(X)], an array is allocated and a pointer is placed in M[I,J].Whenever a graph entity (Node, Edge) is created, an internal unique ID is assigned to it. This ID is immutable throughout the entity life, and you can retrieve an entity ID by using the ID function: MATCH (N) RETURN N, ID(N) LIMIT 10. Similarly, finding a node by its ID: “MATCH (N) WHERE ID(N) = 9 RETURN N” used to perform a full scan, inspecting each node in the graph.Our latest release of RedisGraph introduces an optimization that replaces these full-scan and filter operations with a single `seek node by id` operation, which runs in constant time.In RedisGraph, traversals are performed by matrix multiplication. For example, consider the search pattern: (A)-[X]->(B)-[Y]->(c). To find all source nodes (A) that are connected to destination nodes (C), we’re simply multiplying matrix [X] by matrix [Y], X*Y=Z, and Z’s rows and columns represent A and C respectively.If we’re interested in just a handful of C nodes, we’ll want to avoid performing X*Y, as these might be very large matrices. As such, we’ll tack on a small filter matrix F, (F*X)*Y. By multiplying F*X first, we’re reducing any intermediate matrix computed dimensions, dramatically reducing computation time.Depending on the result of F*X*Y, we might find ourselves computing a different filter matrix F and re-evaluating this expression to get additional C nodes. This process may repeat itself several times before we have enough data. Now, whenever a search pattern contains left-to-right and right-to-left edges: (A)-[X]->(B)<-[Y]-(C), we’ll have to transpose Y, (F*X)*Transpose(Y) = Z.Previous versions of RedisGraph transposed Y on every evaluation of this expression, which was costly. Now, as of release 1.2.0, we’ll only transpose once on the first evaluation. Although this results in higher memory consumption per query, the latency gain is definitely worth it.Overall RedisGraph 1.2.0 (release notes) contains both overdue features and some interesting optimizations, give it a try let us know what you think!"
320,https://redis.com/blog/learn-redis-free-redis-university-summer-session/,Learn Redis for Free at the Redis University Summer Session,"May 29, 2019",Kyle Banker,"Summer is a great time for relaxing, unwinding, and picking up a good book. And, personally, I always try to learn something new. But I’ve found it has to be manageable! I admit to failing miserably the summer I tried taking on even the abridged version of Edward Gibbon’s masterpiece on the Roman Empire. I had a lot more success several summers later when I read (and did) The Little Schemer, a slim volume you might consider the unofficial “gateway drug” to functional programming.In the interest of providing you with a manageable yet edifying summer learning experience, we’re presenting the Redis University Summer Session. This June, Redis University will be offering four free online courses. Whether you’re an experienced Redis user or completely new to the world’s most-loved database, we have a course to help you make the most of those lazy summer days.RU101: Introduction to Redis Data StructuresThe magic of Redis is in its data structures, and this course presents those data structures in all their glory. How do you use bit fields and sorted sets? How can you perform geo queries? What’s the time complexity of a list insert? You’ll find the answers to all of these questions and more in this introductory course.RU102J: Redis for Java DevelopersOften the best way to learn a technology is to build something real. In this Java course, the first in our language series, you’ll help build a solar power dashboard app for monitoring and ingesting real-time data. In the process, we’ll cover a number of Redis development patterns, including DAO design, pipelining, transactions, time-series data, and Lua scripting. A prime course for any application developer working in Java.RU201: RediSearchIf you need a fast, in-memory full-text index, then you should strongly consider the RediSearch module. If you don’t know how to get started with RediSearch, then RU201 should be your first port of call. Among other things, you’ll learn about how RediSearch implements common text indexing strategies such as stemming, synonyms, and phonetic matching. You’ll also master the RediSearch query language, allowing you to perform complex text searches with ease and elegance.RU202: Redis StreamsStreams is one of the latest additions to core Redis, and with the prevalence of streaming data, it’s a hotly-anticipated new feature. How do you get data streaming into Redis, and how do you consume it? This course teaches a variety of stream consumption patterns, including an extensive how-to on consumer groups. A fantastic introduction for distributed systems and messaging geeks.There are few better ways to learn Redis than with a Redis University course. We invite you to sign up now, and we’ll see you in class this summer!"
321,https://redis.com/blog/on-types-and-transactions/,On Types and Transactions,"June 4, 2019",Redis,"Transactions in any database are intimidating. It requires a level of understanding beyond just what is stored, but also when it is stored. Unlike the happy world that results when countless layers of abstraction can shield you from complexity, transactions require you to go deeper. Redis is not unusual in this regard. In fact, its entirely different way of thinking about transactions causes a lot of people to say it doesn’t have transactions at all. Redis has them, just with an approach that’s totally different from the rollbacks you’ve probably grown up with.To view transactions at 10,000 ft, you have to understand a few things about Redis. One is that it is single-threaded (well, with a list of exceptions that is perhaps continuing to grow). This means that if it’s doing something, that’s all it’s doing. Of course with Redis, “doing something” is best measured in milli- or nano-seconds. Second, keep in mind that Redis has tunable durability, with some options providing very good durability and some that are totally ephemeral. This obviously has an effect on transactions. Third, it lacks rollbacks, but can fail a transaction if a key changes before it starts. This inverted way of controlling transactions lets you pull data back to the client, and evaluate it logically to ensure that the data did not change before the transaction started.The biggest tripping point for most people is that individual commands in transactions can have errors. This can create situations where each command is executed but one or more of the command executions has errors. Knowing this is key to understanding and controlling for these situations.First, let’s take a look at the distinction between a syntax error and a semantic error in Redis. A syntax error is just that – the syntax is wrong in a way that is known without access to the data. For example, sending a command that doesn’t exist or violating the argument key/value sequence. Syntax errors result in a transaction never starting.Take this example:127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> STE foo bar
(error) ERR unknown command `STE`, with args beginning with: `foo`, `bar`,
127.0.0.1:6379> EXEC
(error) EXECABORT Transaction discarded because of previous errors.Redis knows that STE is not a command, so it can throw the whole thing out without ever even having to evaluate the underlying data, and will reject the entire MULTI/EXEC block. Other syntactical errors that are caught immediately by Redis include argument count and (some) argument pattern problems. These transaction errors are very safe — with MULTI, all subsequent commands are queued up to run when the EXEC is called, so anything that triggered an EXECABORT was never run.The next, and more expansive, category is semantic transaction errors, which behave differently than syntactical errors. They come about when Redis cannot catch the error statically, generally requiring Redis to evaluate the underlying data. A classic example of this behavior is this:127.0.0.1:6379> SET foo ""hello world""
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> INCR foo
QUEUED
127.0.0.1:6379> SET bar baz
QUEUED
127.0.0.1:6379> EXEC
1) (error) ERR value is not an integer or out of range
2) OKObviously, a real-world competent developer would never intentionally try to increment the string “hello world.” But when you think about an end-to-end application where user input is accepted, expecting a number but not validated as a number, this example becomes more realistic. The other insidious thing about this is that the second SET command was executed but the INCR was not. This is not what most developers want nor expect from the transaction. The good news is that this type of error can be controlled for with the use of the WATCH command. WATCH enables you to observe keys for changes, and if they occur, it will immediately send an error to the client. This is very powerful and allows you to bring data to the client and evaluate it. In this case, you can evaluate if foo is able to be incremented (a.k.a. a whole number).Take a look at this pseudocode:1 > SET foo 1234
2 > WATCH foo
3 If watchError goto 2 else continue
4 > GET foo
5 If the result of line 4 is a not a number, throw an error, otherwise continue
6 > MULTI
7 > INCR foo
8 > SET bar baz
9 > EXECNow, if foo is changed by any connected client between lines 3 and 9 (but not during 9, which allows for the transaction itself to change watched data), the process will jump back to 2. After line 4, it will look at the contents of foo in the application and determine if it is possible to increment. Because it will immediately retry if a change happens, this ensures that you won’t get any errors due to the contents of foo being wrong.This relates to how Redis works with whole numbers/integers vs. floating point numbers. I’ve talked about this a little before, but basically if your data is a floating point number then you have to use INCRBYFLOAT instead of INCR or INCRBY. However, if the value is ever exactly equal to a whole number, then you can use INCR or INCRBY once again. It’s important to note that you can increment non-floating point numbers by non-floating point numbers using INCRBYFLOAT and it doesn’t affect anything. In essence, the INCR family of commands will never create a “1.0.” In a MULTI / EXEC situation, it’s safer to use INCRBYFLOAT over INCR or INCRBY, as it won’t create errors. The only reason to use INCR or INCRBY in this case is to ensure an error is thrown if you have a floating number – in that case, you have to use WATCH anyway.One note about evaluating the data in this way: it’s not free. If you’re pulling numbers to the client side, the evaluation is minimal. But let’s say you have an unknown key and you evaluate it for suitability, but instead of 5-7 bytes that make up a number, you have a 500 MB binary blob. That’s going to take a while to push over the wire and evaluate. It’s an edge case, but it’s something to keep in mind.This same pattern (WATCH / MULTI / EXEC) can be useful to guard against command / type mismatches (a.k.a. WRONGTYPE), turning up in the results of your transaction. With the INCR problem, you had to track if a String contained data that could be INCR’ed or INCRBYFLOAT’ed using some sort of client-side evaluation of the data. Alternatively, you can be much more straightforward and just use the TYPE command to evaluate the data’s type instead of the data itself (nicely avoiding the unexpected 500 MB evaluation).Let’s take a look at how this might work:1 > HSET foo bar 1234
2 > WATCH foo
3 If watchError goto 2 else continue
4 > TYPE foo
5 If the result of line 4 is a not “hash”, throw an error, otherwise continue
6 > MULTI
7 > HSET foo baz helloworld
8 > HLEN foo
9 > EXECIn this case, you know that your HSET and HLEN commands will work, since you’ve verified the type and it hasn’t changed by the time you run the EXEC. Of course, there is the matter of the HINCRBY/HINCRBYFLOAT command, where you’ll need to combine the previous technique using HGET instead of GET to evaluate the field for INCR-ability.Interestingly, BITFIELD has controls for handling out-of-bounds values. You can either wrap around or saturate based on the type declared in the command itself, or you can use the FAIL option. This, strangely, doesn’t produce an error but ignores the INCR, leaving the value as before. BITFIELD, however, does have another gotcha. The command is quite complex and Redis only does some basic syntax checking, so if you pass it the wrong syntax, this will not be evaluated at the time it’s added to the transaction, but when the command is executed inside the transaction. This results in a syntax error that doesn’t cancel the transaction and is returned in the values. The only way to guard against these types of errors is to ensure that your syntax is correct at the client level before you throw it into a transaction.By and large, Redis requires no initialization step for data. To some, this is alarming, but for most people, they get it. If a key is blank and you add data to it, the newly created data structure is defined by the command used to add data. This has slowly started to change with the advent of modules, which make this convention less definite. For example, RedisGraph requires you to add some nodes and relationships before you can query the graph.Take this example:> MULTI
OK
> GRAPH.QUERY mygraph ""MATCH (r:Rider)-[:rides]->(t:Team) WHERE t.name = 'Yamaha' RETURN r,t""
QUEUED
> LPUSH teamqueries Yamaha
QUEUED
> EXEC
1) (error) key doesn't contains a graph object.
2) (integer) 1Indeed, you can see this behavior as well with Redis Streams. For example:> MULTI
OK
> XGROUP CREATE my-stream  my-consumer-group $
QUEUED
> LPUSH my-stream-groups my-consumer-group
QUEUED
> EXEC
1) (error) ERR The XGROUP subcommand requires the key to exist. Note that for CREATE you may want to use the MKSTREAM option to create an empty stream automatically.
2) (integer) 1Thankfully, these issues are resolved rather simply, by using the WATCH, TYPE, MULTI/EXEC pattern described above. You would just check the type matches (with these examples) and match graphdata or stream.Redis transactions with MULTI and EXEC are really not that complicated. However, you do need to watch out for a few gotchas to make sure your transaction behaves as expected. If you take anything away from this article, remember that you can make bulletproof Redis transactions by assuming nothing about the type, contents or existence of the referenced keys."
322,https://redis.com/blog/5-steps-selecting-high-performance-nosql-database/,5 Steps for Selecting a High-Performance NoSQL Database,"June 14, 2019",Shabih Syed,"Development teams building online and operational applications increasingly choose a new class of databases to support them. It’s called “NoSQL,” or “Not Only SQL”, and includes options such as Redis, MongoDB and others. Selecting the right database from among the available NoSQL solutions is one of the most important decisions you can make when designing a new application. So, if you are evaluating NoSQL databases, read on for some recommendations that will aid in your selection.When choosing your database, there are five high-level steps you should follow:The goal of a NoSQL database might be to: support personalized digital experiences for thousands of users on mobile devices; store data for a back-end payment processing application;  manage ephemeral data that has a certain time to live; or store persistent data as a system of record. You could even involve multiple types of databases in the same data pipeline for a particular scenario.Regardless of your use case, it is important to define the specific function of the NoSQL database in your data pipeline, including how data will be collected, ingested and made available for analysis.In today’s age, users expect instantaneous experiences. Typically, this requires consistent response times of <100ms from your application. Otherwise, it will be perceived as slow, and you might lose users’ interest. However, some applications — such as gaming, communications, and financial trading systems — demand response times as low as 13ms from their databases.In addition to latency, you also need to identify throughput requirements. For example, can your database process thousands of simultaneous data streams with latencies as low as 50ms or better?Understanding the demands that will be put on the database is important to ensuring the quality of the user experience.Typically, developers choose NoSQL databases because they require semi-structured or unstructured data with flexible schema, simple query patterns, high-velocity transactions, a large volume of data, and quick and cheap scalability via distributed computing and storage. You can further narrow down your choices through the CAP theorem, which is defined as follows on Wikipedia:From the CAP theorem, you can prioritize CA, AP or CP characteristics. This helps you determine which database might be a good fit for your application.A managed services solution handles the day-to-day management of your database with experienced resources. This enables your own resources to focus on the innovation and efficiencies that are required in your applications. If you go this direction, evaluate third-party options that offer database-as-a-service options, and pick a provider that can handle your throughput and latency requirements while guaranteeing uptime.Of course, outsourcing may not always be the option, in which case, you should consider which database provider(s) offerw a software version with support for provisioning, scheduling and managing containers at scale. Be sure to check for your most desired capabilities, such as scalability, active-active deployment, throughput and latency – and verify them during a proof-of-concept trial.Ideally, you want a database provider that will let you run the database in any environment of your choice (whether that be public or private), with full control over your data and configuration. Your database software should also be available as a Docker image, which will allow your enterprise developers to use it in their Docker-based microservices architecture.If you use a private platform-as-a-service (PaaS), make sure your database provider supports seamless scaling and effortless high availability in private PaaS environments, such as Pivotal, Bluemix, Heroku and others.If you select a managed service provider, confirm that they support clustered deployments across multi-cloud providers, including AWS, Azure and Google.Benefits of making the right NoSQL database choiceSome of the advantages of a well-thought-out decision include:Need help selecting the right NoSQL database?Redis is the home of Redis, the world’s most popular in-memory database, and commercial provider of Redis Enterprise. Consistently ranked as a leader in top analyst reports on NoSQL, in-memory databases, operational databases and database-as-a-service, Redis is trusted by over 7,000 enterprises, including seven of the Fortune 500’s top ten.The enterprise edition of Redis simplifies the development of highly performant, reliable and seamlessly scalable real-time applications, including e-commerce, mobile, social, personalization, internet of things (IoT), fraud mitigation and many others.The fastest way to deploy Redis Enterprise is as a hosted and serverless DBaaS in the public cloud or private cloud with support for active-active geo distribution and Redis-on-Flash, as well as search, graph or document data models. Redis Enterprise software can be installed on any cloud or private data center and also as containers, on Pivotal CF, OpenShift or as AWS AMI.Contact us today or get started with a free trial."
323,https://redis.com/blog/redis-ai-first-steps/,Redis AI First Steps,"June 17, 2019",Redis,"At RedisConf 2019, Redis introduced a new module called RedisAI. The idea is to bring together machine learning (ML) and deep learning (DL) and execute artificial intelligence (AI) models as close as possible to where your data currently lives. This sounds amazing, but what if you’re brand new to all of this? What if you’re interested in machine learning, but you’re not quite sure what the heck it all means? How do you make sense of it all? Your boss’ boss is saying, “we need to integrate machine learning,” and last week you thought that meant he wanted you to run an extra compile step or something. Now you’re sitting here trying to understand a lot of new terms and how you can bring this into your organization.Okay, first, take a deep breath and let’s take a step back. Today, we’ll start digging into some AI lingo and show you how to add the RedisAI module into your already existing Redis system and start to play around with this stuff.So what exactly is machine learning? The broad concept refers to building algorithms that read data and make predictions based on that data as well as any new data that comes in. During the initial “training” period, someone looks at the model’s predictions and tells the system if each guess is good or bad. Deep learning involves building algorithms and feeding the model data, but rather than outside training, the system trains itself.Training these systems is complex and usually happens in a totally different application (don’t worry, you don’t need to write that application unless you enjoy linear algebra). One of the most popular systems for machine learning is called TensorFlow and it’s open source. TensorFlow helps you build, train and deploy ML applications and has a great community to help you get started.You probably already interact with ML systems more than you realize. If you have a Netflix account and you click “like” or “dislike” on a certain movie or show, you’re training a model to better predict what types of movies and shows you’d enjoy. Netflix’s suggestions are directly related to the things you say you like. Sadly, this is why my queue is filled with things like True and the Rainbow Kingdom (thanks kids).Also, if you’ve been on the internet in the past couple of years, you’ve no doubt seen the explosion of chatbots. These are also ML/DL tools. They’re trained to answer questions and in some cases can hold pretty good conversations with a person around related topics. Of course, training can sometimes go off the rails, but if nothing else, it’s fun to watch!Traditionally, all this data needs to be moved around, and that can present some serious DevOps challenges. Think about a chatbot saving the state of your conversation. That data needs to live somewhere because it’s important to the bot. It needs this conversation and context to help craft what it’s going to say next. We’d need to deserialize the data, run it through the model, and then serialize the data once it’s sent back to Redis. Doing all this uses CPU cycles and network overhead to go from one app to another and translate data between systems.RedisAI gives you new data structures and allows Redis to manage requests to run models and execute them. Essentially, we’re running your models right where your data lives. No network overhead, no serializing/deserializing.I’m an old-fashioned kinda guy and I like to build tools like this myself. So when I want to play around with a new module for Redis, I go to the source and build it. We’ve got a couple of different ways you can get RedisAI running locally:If you’re a Docker user, it’s trivial to get an instance of Redis with RedisAI on your system:$ docker pull redisai/redisai
$ docker run -p 6379:6379 -it --rm redisai/redisaiYou’ll still want to clone the RedisAI repo to get access to examples.Let’s get our hands dirty in the command line. Now, before you can make the code, you need to install two particular things: cmake and git-lfs. On my Macbook, this was as simple as using Homebrew:$ brew install git-lfs cmakeYou also need to make sure you’re running redis-server version 4.09 or greater. If you need to check, type:$ redis-server --versionNow that you’ve checked your server version and know you’re good there, go out and clone the AI repository:$ git clone git@github.com:RedisAI/RedisAI.gitCd yourself into the RedisAI repo and get the dependencies:$ bash get_deps.shNow build:$ mkdir build
$ cd build
$ cmake -DDEPS_PATH=../deps/install ..
$ make
$ cd ..If all goes according to plan, you’re ready to load the module and start playing. The process should be pretty smooth. The only issues I ran into were not having CMake installed. Once I did that, the whole process worked.Now, before you go any further, you should make sure the Redis server isn’t running. I had forgotten that I always start it as a service on my machine, so it was just running in the background! When I was trying to load the module, everything seemed like it was working fine, but the module wasn’t actually loading. With the  Redis server already running in the background, things got wonky on me and the module wasn’t loading. If you’ve used Homebrew to install Redis, stopping the Redis server should be a simple command.  brew services stop Redis on my Mac does the trick.Once you’ve stopped Redis, you can run:$ redis-server --loadmodule build/redisai.soYou could make your life a little easier by just using the MODULE LOAD command from the CLI:> MODULE LOAD path/to/build/redisai.soNow, load the module into the server and you can start playing around!Congratulations on making it this far, now the fun begins. You have everything you need to experiment with RedisAI. If you’re new to all of this, you might not be sure of the next step, so RedisAI comes with an example you can run to see things working.If these are your first steps into the AI world, the next thing you need to do is build and train a model. These steps are out of scope for Redis and this article (but if you’re interested, there’s some great material available), so we’re going to skip all that and give you some data that you can play around with right away.I’ve set up a repo on Github that you can download and play around with an example Inside that folder you’ll find everything you need to get started:Our example project is a CLI image classification app. We’ll give it an image and the app should be able to figure out what’s in the image. For example, we have an image of a panda, and when we give the app the picture, the app should tell us that there’s a giant panda in the image.To get this up and running, cd into the JS folder and run either yarn or npm install. Everything you’ll need to get it going will be installed. Then all you have to do is run:$ node mobilenet.js ../img/panda.jpgThe panda.jpg image is the header image of this blog post. If all goes well, your output should look similar to this:Awesome, we got something back! Great? What does this mean exactly, what did we do? We supplied our node app a picture of a panda and a computer. Our system was trained to look at images and tell us what’s in the picture. So what we’ve done here is supply our app 2 images and the app was able to tell us what it “saw” in the images.Quickly, we’ve been able to set up RedisAI in our Redis instance so you can give it data and watch it figure out and classify the images you give it. This example is the basic building block of facial recognition and image recognition. Now with RedisAI we’re able to work where our data lives, in Redis."
324,https://redis.com/blog/why-modern-finance-applications-need-redis-enterprise/,Why Modern Finance Applications Need Redis Enterprise,"June 18, 2019",Shabih Syed,"Data is the foundation of all digital transformation initiatives in finance, and Redis Enterprise is the industry’s high-performance database of choice. This enterprise edition of Redis simplifies the development of highly performant, reliable and seamlessly scalable real-time applications, including mobile, social, personalization, internet of things (IoT), fraud mitigation and many other solutions.The popularity of NoSQL databasesAccording to Forrester, half of global data and analytics technology decision makers either have implemented or are implementing flexible NoSQL platforms, like Redis Enterprise. NoSQL databases are critical for a new generation of financial applications built with microservices architectures.  These systems depend on real-time data to deliver instant experiences and require databases that can be deployed on multiple clouds, on-premises or in a hybrid architecture.Removing data silosFinancial services companies struggle to harness data for useful insights across all relevant data in their fragmented data architectures. Thankfully, Redis Enterprise addresses this challenge head-on. It is an elegant multi-model database for heterogeneous data, — breaking down data silos, simplifying operations and enabling real-time analytics for modern applications. Redis Enterprise empowers application developers with great flexibility and sub-millisecond response times by bringing a variety of data models together into a unified database platform. The database delivers predictable performance for large volumes of structured, unstructured or semi-structured and constantly changing data while scaling easily to millions of operations per second.Top 6 database use cases for financial services companiesThe world-class Redis Enterprise solution is ideally suited to serve as the foundation for several different types of applications, including:Time-sensitive financial transactions need Redis’ versatility and low latency to deliver massive calculations and handle every type of data processing requirement with high performance.Caching is an important technique to reduce the load on your most expensive database resources by positioning application data as close as possible to the user. Redis is the best choice for caching due to its lean footprint, immediate time-to-value and rapid response times.Ask any question of all your financial data and get real-time answers at blazing speeds with built-in rapid-fire search capabilities. Redis Enterprise supports simultaneous indexing and advanced searches over fast-moving text, numeric and geospatial data.With financial companies facing huge risks of data breaches, your transactions require fraud detection and prevention. Redis Enterprise provides instant perception and mitigation by accelerating analysis across the widest variety of data sets and data types.Redis Enterprise is ideal for session store use cases that require in-memory replication because it gives you both the high availability and the durability you need.Finally, Redis Enterprise can deliver custom, personalized experiences to improve the stickiness of your apps. It supports dynamic pricing, credit risk analysis, custom advertising, content discovery, catalog recommendation and user content.Deployment options for Redis EnterpriseThe fastest way to deploy Redis Enterprise is as a hosted and serverless DBaaS in the public cloud or private cloud with support for active-active geo distribution, Redis-on-Flash, search, graph and document data models. Redis Enterprise software can be installed on any cloud or private data center and also as containers, on Pivotal CF, OpenShift or as AWS AMI.Contact us today for more information, or get started with a Free Trial."
325,https://redis.com/blog/5-steps-building-great-redis-module/,5 Steps to Building a Great Redis Module,"July 22, 2019",Redis,"Here are five things to keep in mind when writing a Redis module. While this list is non-exhaustive, my aim is to offer a good way to get started if you don’t yet have much experience with module building.Redis already has plenty of tools that allow you to build the exact solution you need. One example could be locks. Using SET with the NX option, you can create a lock key, and by combining it with EXPIRE, you get a lock lease. This can be very useful when solving coordination problems. When built-in commands are not enough, you might also resort to Lua scripts, which add full programmability to composite operations that are then executed atomically by Redis.Modules go a step further, giving you even more flexibility and speed, thanks to their ability to access lower-level APIs compared to Lua, but they’re more challenging to maintain and distribute. Go for a module only when Lua can’t fully solve your use case.Modules can add new commands to Redis that execute arbitrary C functions (to be precise, you can also use Rust, Zig or any C-ABI compatible language). What you do in your function is up to you. A basic, but useful, starting point could be implementing a command that is similar to an existing one but does something more. An example of this could be SETNE (which was first mentioned by a user in this GitHub Pull Request). SETNE behaves exactly like SET, but when the new value is equal to the current one, it does not modify the key, thus avoiding producing a spurious keyspace notification. In general, to get some practice, think about small additions you could make to existing commands to help with specific use cases.Most of those small additions would be best implemented as Lua scripts, but it’s a good way to gain some experience in case you can’t come up with compelling module ideas right from the start. A couple exercises left to the reader: SETEQ, HINCRDATEBY.The most effective way a module can add functionality to Redis is by adding a new data type. Redis has a strong focus on proper design of data structures and their related algorithms and properties. While you might not know what the exact implementation of the Set data type is, you know for sure that set membership (SISMEMBER) is always going to be fast regardless of Set size (i.e., it has sub-linear asymptotic complexity), for example.This is the basis behind our own modules:These are serious modules, but not every module that introduces a new data type has to be this complex. There are plenty of simpler data types that could be useful as a module. A basic example could be a different implementation of an already present data type in Redis, like using an ArrayList to implement Lists, for example.Don’t forget that wrong usage of your module’s commands is going to be as important to prepare for as correct usage. Redis users like to try commands by hand to get a better understanding, and typing in wrong arguments is part of that process. Your API should be easy to use and hard to misuse, but when the inevitable happens, make sure to report meaningful error messages.Take a look at how standard commands behave within Redis and see if you can come up with something that works on the same assumptions. This will lessen the mental overhead required to use your commands. One example is that, in Redis, most commands have sensible behavior when called on a non-existent key: INCR will assume a missing key has value 0 so it will set it to 1, SADD will assume a missing key is an empty set, and so forth.Modules can interact with the Redis ecosystem. Make sure to read the documentation to learn how to get the details right, especially if your module implements a new data type. Here are the two most important aspects to get right.When you’re declaring a new command, you must specify a few flags to tell Redis what your command is going to do when invoked. Is it going to just read data or also write it? Is it going to allocate memory or just modify existing data? Make sure to fill those options correctly. For example, in out-of-memory (OOM) situations, deny-oom is an important flag that will tell Redis to deny access to a command that allocates memory, otherwise the whole process will be killed by the OOM killer! Even the read-only flag is important. New client-side caching functionality will use it to decide whether to enable tracking for a given key or not.When Redis is run in a master/replica setup, the master must know which commands it should send to replicas or not. Not every command should be replicated, and some might need to be replicated only under specific conditions. For instance, I mentioned above the SETNE command that would set a key value only if the new value is different from the current one (otherwise it does nothing). In this case, the command should be replicated only when it is effectively applying a change to the key. There is no reason to make each replica execute it if it would not perform any write. Redis can’t know what to do from the outside, so you must make proper use of RedisModule_ReplicateVerbatim and related functions.It doesn’t matter how useful your module is if no one understands how to use it. Polishing your API can help immensely in that regard, but first you need to convince potential users that the module is at least worth trying out. A good module should have good documentation that explains the general goal of the module and lists detailed information for each command.If you take a look at redis.io, you will see that each command lists its relative BigO complexity and has a few extra notes for when a command has particularly big or small constants, or when there are notable edge cases. Try to replicate that format, especially with regards to the syntax for command examples. Notice how each example uses lowercase names for placeholders, while uppercase ones denote keywords that must be used verbatim, with optional values between square brackets. Look at the documentation of SET to see an example of this.Always keep in mind that the first design principle behind Redis is simplicity. This doesn’t mean your module should never explore other options and occasionally sacrifice simplicity for other benefits (modules exist precisely to let Redis users experiment), but always be mindful of what you’re giving up.Generally speaking, when you sacrifice simplicity for ease of use you’re also implicitly constraining the ways in which your users will be able to use your module. In Redis, most utility generally doesn’t come from a given command used in isolation, but rather in how users can combine different commands together. Smaller, clearer, simpler commands will always be easier to combine and thus yield greater results in the grand scheme of things. For this reason, I recommend increasing ease of use by properly applying the techniques described above before resorting to this kind of trade-off.Another potential trade-off could be in favor of efficiency. This may be worth exploring and is one that Redis occasionally makes itself. A few built-in data types have two internal representations — one optimized for when the data type only has a few elements in it, while the second one is for when the key grows over a certain threshold. Two representations (plus the mechanism to switch between the two) are certainly more complex than just one, but the benefits might be worth it. This is especially true since the added complexity doesn’t show up in the user interface, as users will interact with the data type in the same way regardless of which internal representation is in use.Take a look at which modules already exist, and see if you can find inspiration. We published an SDK for writing modules in Rust and also wrote about doing it in Zig, so don’t worry if you don’t (want to) know C. We also have talks on YouTube (Rust, Zig), if you prefer listening over reading.If you do end up writing a module, please make sure to send a pull request to antirez/redis-doc to have it added on redis.io and, if you feel like it, shoot me a tweet @croloris. I’ll be happy to try out your module."
326,https://redis.com/blog/springone-platform-insiders-guide/,4 Reasons to Visit SpringOne Platform 2019: An Insiders’ Guide,"September 27, 2019",Sheryl Sage,"As we head into Fall, I can’t think of a better place to start thinking about your new digital initiatives than at SpringOne Platform 2019. The crowds converge in Austin, Texas, the live music capital of the world, from October 7 – 10, and it’s shaping up to be a can’t-miss event for all things cloud native, microservices, and Kubernetes.Redis collaboration and partnership with Pivotal continues to gain momentum as the best way to build and deploy modern apps and microservices in your Pivotal environment.  So, naturally Redis Enterprise on the Pivotal Platform—with key capabilities like Active-Active and Redis Streams—will be featured at the conference. But SpringOne has so much more going on!To help you make the most of the event, I’ve put together four essential tips:Join us to sample some local craft beers (and delicious wine) at the Redis booth, on Tuesday, Oct. 8, from 4 p.m. – 6 p.m. We’ll be demonstrating Redis Enterprise support for geo-distributed apps and microservices. And we’ll be talking about the advanced features of the Redis Enterprise, Kubernetes Operator, and Pivotal Platform integrations.Drop by anytime for an in-person demo or white-boarding session with our solutions architects. You can also register to get a copy of “Redis for Dummies” book,  and pick up a token for SpringOne’s innovative swagless charity drive.Even better, schedule a 1:1 meeting with our sales reps or SAs, and score an exclusive invitation to our offsite Redis Happy Hour on Wednesday, Oct. 9, from 6:30-8 p.m.SpringOne is packed with sessions on building cloud-native services and Kubernetes implementations, dedicated to helping you transform how you build software.But what about the data layer? One of the biggest microservices challenges is achieving data consistency. So you won’t want to miss a pair of Redis sessions on achieving data consistency with Active-Active for Geo-Distributed session stores and using Redis Streams for fast data ingest.Adi Foulger, Principal Architect, Redis, will address the challenges of operating geo-distributed applications that are both accurate and efficiently distributed. The session will provide insights into the Redis Enterprise architecture and demo Active-active Redis clusters across two geo-distributed Pivotal Platform foundations. (Wednesday, Oct. 9, 5:40 p.m. – 6:10 p.m., Room 11AB)In this session, I’ll show you how Redis Enterprise can solve common challenges associated with ingestion and processing of large volumes of high-velocity data. (Thursday, Oct. 10, 11:50 a.m. – 12:20 p.m., Room 11AB)If you’re excited to get the latest take on cloud-native architectures in the enterprise, SpringOne is the place to be. Learn how to build resilient applications from experts leveraging agile practices and patterns in real-world production environments.. Visionary speakers like Barbara Sanders, Vice President of Technology and Chief Architect at The Home Depot (on the main stage) and Jay Piskorik, Director of Platform Engineering at DICK’S Sporting Goods (in a session called Pivotal Vanguard Customer Deep Dive Expedition: Pearls of Wisdom, Wednesday, Oct. 9, 4:20 p.m., in room 17AB), will share their digital transformation and cloud strategies. And that’s only the beginning: Explore hundreds of breakout sessions, plus hands-on labs in the Pivotal booth.Sign up for Spring developer pre-conference training, Monday and Tuesday, October 7 – 8, from  9 a.m. – 5 p.m., at a special rate. The lab-based curriculum is delivered by Pivotal Certified Instructors.Finally, learn more about how to get started with free Redis University online training at the Redis booth.Can’t join us at SpringOne this year? Watch this video or read this tutorial on why the new Redis Enterprise Kubernetes Operator for PKS makes it easy to manage distributed data."
327,https://redis.com/blog/unlocking-timeseries-data-redis/,Unlocking Time-Series Data with Redis,"October 4, 2019",Redis,"(Note: This blog post was adapted from a webinar I presented in June. To go even deeper into RedisTimeSeries, register and watch the webinar now!)Most developers know that Redis real-time response capabilities makes it well suited for working with time-series data. But what exactly is time-series data? Plenty of definitions stretch into page after page of explanation, but I think it can be dramatically simplified:Basically, time-series data is data that encodes time as an index, and where each recorded time has a numeric value. If you visualize it in two columns, one column would have some sort of time index, usually timestamps in the Unix epoch form. And the other column would have some sort of numeric value.Very simple.Critically, you can analyze time-series data using time bounds, for example, to see happened between January 1st and January 3rd. You can also get granular, into seconds, sometimes even milliseconds. You can also separate your data into time units, to see what happened on an hourly basis. Then, if you don’t want to look at every single event in your time-series data, you can put an aggregation on top of that, to get averages per hour, for example.Many people picture stock charts when they think of time-series data. It’s a good way to look at how a stock performed during a given time period. One time-series data use case that I look at a lot is the CPU load on a server during any specified interval. Time-series data is also a good way to look at sensor data and other Internet of Things (IoT) information. Any time you’re looking at trends over time, that’s usually sourced in some sort of time-series database or time-series structure.Now let’s focus on Redis and time series. It all started with Sorted Sets, one of the built-in data structures in Redis. People started using Sorted Sets for time-series data early on, looking something like this:> ZADD mySortedSet 1559938522 1000This example includes the ZADD command, mySortedSets as the key, and a timestamp, which as the score. And finally the member, which was the value.That was great, but you could get only ranges, you couldn’t do averages or downsampling.Sets cannot have repeats. Here, if you have two different timestamps with the same value, the set is based on the member (in this case what we’re defining as the value.) So in the example below, the second one would actually be an upsert—it would overwrite the first one. That doesn’t work for time-series data and people had some rude awakenings when they used it that way:> ZADD mySortedSet 1559938522 1000
> ZADD mySortedSet 1559938534 1000Developers came up with a number of workarounds that were computationally complex and really hard to implement. There had to be an easier way.Then, about two years ago, Redis 4.0 debuted with Redis Streams, which was designed to solve problems in building applications with the  unified log architecture and for interprocess messaging.Redis Streams offered important advantages over Sorted Sets for time-series use cases. It allowed for auto-generated IDs, no duplicates, and per-sample field/value pairs.> XADD myStream * myValue 1000> XADD myStream * myValue 1000 anotherField helloAs you can see in the first command, we set the field myField to 1000. In the second command a new entry was created with myValue set to 1000 as well as anotherField set to hello. Each of these are entries in the stream located at the key myStream.But this still lacked important features and wasn’t really designed for time-series data. You can easily get time ranges, but not a whole lot else.Now let’s rewind a bit to talk about the Redis modules API, which came out a little bit before Streams and allowed Redis to have additional communities and data types. Redis users could build modules that would act as first-class citizens inside Redis. Existing modules include everything from RediSearch to RedisGraph to RedisJSON. And now there’s RedisTimeSeries, which basically creates an entire time-series database inside Redis.Before we get to how to use the RedisTimeSeries module, it’s important to understand what’s going on under the hood.The first thing you need to learn about is ‘the chunk.’ You actually never manipulate a chunk directly, but RedisTimeSeries stores all the data in these chunks. Each chunk consists of two correlated arrays (one for timestamps and one for sample values) in a doubly linked list.For example, let’s say I want to put a timestamp into my time-series database. It goes on the first row in the two arrays. If you have additional samples, they would just go into the array.Chunks are a fixed size. When  chunk is full, additional data automatically goes to the next chunk. Adding to the beginning or end of a linked list is computationally trivial, so when new chunks are added it is very lightweight.But unlike most Redis data types, it’s best practice to first create your time-series key. In this case, my command is TS.CREATE. And then I have myTS, which is the key that I’m using here.So let’s say that we want to add in some metadata to this key. Imagine we’re running a vegetable nursery and we want to track cabbage number 47 in greenhouse number 4; we would call this metadata  labels. This would apply to every single sample across the entire time series:Another important part of working with time-series data is retention. Let’s say that we don’t care about anything older than 60 seconds. RedisTimeSeries can trim off things that are outside the retention time periods you specify.We can add values using an operation called TS.ADD. The first argument is the key myTS and the asterisk is syntax borrowed from Redis Streams indicating that Redis will auto generate the timestamp. In this case, the value is 834.Lets let’s add another sample, and specify a timestamp. Note that timestamps are effectively append-only, so you can’t go in and add something past the most recently used timestamp. The subsequent TS.ADD would have to be a timestamp greater than that value.Next, to get bounded results, you would ask for all the samples between the two timestamps. Using our example, you can see the first timestamp has a value of 834 and the second one has 1000.That’s useful, but maybe you want the average for every 30-second time period. Here, avg is is our keyword and the 917, of course, is the average of 834 and 1,000.But what happens when you have a lot more data? You might not want to run that TS.RANGE command all the time and just want to granularly extract that data.Well, we have the ability to create rules! myTS is my key: That’s the source. The destination is myTS2 and that’s the second key. All the chunks here represent 30 seconds of time and RedisTimeSeries will automatically put that into the secondary key of destination Key. So as every 30 seconds passes, you’ll get one more sample added to myTS2.But wait, there’s more! It’s not just limited to averages. You can sum, you can get the minimum, you can get the maximum, you can get a range. You can get the count—how many— and the first or the last. And all those different aggregation functions also apply for TS RANGE.Let’s take a look at what else RedisTimeSeries can do. The commands TS.INCRBY and TS.DECRBY are for counting over time. TS.INCRBY increments the previous entry by some value. Let’s say you know that in 10 seconds you have collected 10 widgets. you would run TS.INCRBY on a key. In this way you don’t have to know the previous value and you can keep a running tally. The same applies for TS.DECRBY, only in reverse.TS.GET, meanwhile, grabs the last value. And TS.ALTER lets you change the metadata for keys you have created, including fields, values retention, and more.TS.MRANGE and TS.MGET are interesting but a bit complicated to explain. RedisTimeSeries tracks all the different time-series keys in the database. TS.MRANGE lets you specify a key/value pair of labels. So in our greenhouse example, you could get the temperature readings for Greenhouse 4 and then use TS.MRANGE to look at different keys across the entire keyspace. Similarly, TS.MGET lets you get the single most recent values by labels. You can connect RedisTimeSeries with different parts of your infrastructure, such as Prometheus and Grafana, which is a great way to power monitoring dashboards.Even as we find our customers using time-series data for more and more use cases, many companies are still storing their time-series type in relational databases. That’s simply not a great fit from a technology perspective when it comes to scaling things. It might work fine when just two or three people are looking at a dashboard, but when you want thousands of people throughout the organization to all look at the same analytical dashboard dashboards, ad hoc queries of a relational database often can’t keep up.That’s why we see RedisTimeSeries being used to cache time-series data that would otherwise be used in a slower database, and also to get other Redis benefits, including the choice whether to persist your data or keep it ephemeral.."
328,https://redis.com/blog/redis-developer-certification-is-here/,Redis Developer Certification Is Here!,"November 7, 2019",Kyle Banker,"Today, we’re pleased to announce the availability of Redis Developer Certification. The new Redis Developer Certification program allows software developers to demonstrate their mastery of Redis. The program comprises an extensive study guide, a practice test, and a formal certification exam. Engineers who complete the exam with a passing score become Redis Certified Developers and receive a digital badge and certificate as proof of their accomplishment. The certification never expires.Redis is one of the most popular databases in the world. A key component in technology stacks in thousands of companies large and small, it’s a high-performance, multi-purpose, in-memory database that scales easily but delivers sophisticated functionality. Often used as a database, cache, and message broker, Redis has been voted the most-loved database three years in a row, rated the #1 database container, and the #1 cloud database.And yet we’ve found that most folks use only a tiny portion of the total Redis feature set. Studying and passing our new certification exam is the perfect opportunity to learn the full breadth and power of Redis’s data structures. Moreover, the certification process prepares to you use Redis in the most efficient way possible, as performance and debugging are among the core competencies tested in the exam.Getting certified as a Redis developer adds a powerful set of skills to your quiver. You’ll be able to apply these skills to optimize a whole host of data management problems, to the delight of your team and your users. This Certification is also a great way to show your Redis mastery to potential employers and engineering teams as you progress in your career.Since this is a certification for developers, the exam focuses on Redis’s core functionality and includes data structures, data modeling, performance awareness, debugging, and writing code against clustered Redis deployments. The exam does not cover administration; that will be addressed in a separate certification we’re working on for 2020.We’ve put together a thorough study guide to help you prepare for the exam. For each exam criterion, the guide includes links to many Redis resources, plus suggestions for practice. (For additional background, you might consider taking a few Redis University courses, although this is by no means required.)Interested in trying your hand at some exam questions right now? Here are two, pulled straight from the practice exam (scroll to the bottom of this post for the answers):Which of the following Redis operations has the greatest time complexity?A. RPUSH collection fooB. ZADD collection 0 fooC. LRANGE collection 0 -1D. XADD log * a fooSuppose you’re using Redis to store a series of events. The events themselves are comprised of a set of field/value pairs. For each event, you need to generate a unique ID. You also need to efficiently access these events by ID. Which Redis data structure meets these requirements?A. ListB. SetC. HyperLogLogD. StreamTo become a Redis Certified Developer, enroll in our free exam prep and scheduling course. This will provide you with all the required study materials, a practice test, and specific instructions for scheduling your exam. (Only when you’re ready, of course!) The exam costs $120.Whether you’ve been using Redis for years or have simply taken a few courses via Redis University, this is a fantastic opportunity for all Redis developers. It’s definitely a challenge, but also a chance to credibly level up your Redis skill set. We wish you, in advance, the best of luck on your exam and we can’t wait to welcome you into the fellowship of Redis Certified Developers.Sign up now to get started!(Answers to sample questions: 1. C. 2. D.)"
329,https://redis.com/blog/aws-eu-central-outage/,AWS eu-central-1 Was Down and Our Customers Didn’t Even Know,"November 14, 2019",Yiftach Shoolman,"Early on Tuesday, November 12, the Redis DevOps team started to get bursts of alerts from our production clusters located in the Amazon Web Services eu-central region (Frankfurt, Germany). Upon further examination, they realized a significant outage was happening in AWS’ eu-central-1 data center, which was confirmed on the AWS status page, and eventually covered in the press:Cloud outages are not new to us. We have been running production Redis clusters since early 2013 and during that time we’ve experienced more than 3,000 instances failures and over 100 complete data center outages. We deeply appreciate the hard work the cloud providers do to stabilize their infrastructure, but we also know that failures are inevitable. That’s why we invested extensive engineering resources over the years to make sure that our Redis Enterprise Cloud service is built to provide an industry-leading five-nines (99.999%) availability.Before sharing details about how we overcome outages, let’s take a quick look at the Redis Enterprise Cloud architecture. The service deploys and manages multiple Redis Enterprise clusters across all three major public clouds (AWS, Microsoft Azure, and Google Cloud) and regions. Each Redis Enterprise cluster manages multiple databases of various configurations (for example: single-instance Redis, high-availability Redis, masters-only Redis cluster, and high-availability Redis cluster) in a multi-tenant and isolated manner. We like to think of it as an orchestration platform for Redis databases. That means a failure in one of our clusters can potentially affect hundreds or even thousands of our customers’ databases.Given the stakes, we promote a multi-AZ (multiple availability zones) database configuration to help customers avoid database failures due to infrastructure outages like what happened on Tuesday.In this specific case, although all our AWS eu-central clusters were affected, all of our customers were deployed across multiple availability zones and therefore the outage didn’t affect the clusters’ operation and availability—even when in one of the clusters, 5 of 15 nodes clusters went down! Furthermore, hundreds of auto-failover events went smoothly without any data loss (of course) and without a single urgent support ticket. A few users complained that it took more time than expected to execute some admin operations, but that is not surprising during an outage like this.How is this possible? Here are the principles behind running Redis Enterprise in a multi-AZ configuration:1. In-memory replication. All Redis Enterprise databases use pure in-memory replication (a feature we contributed back to the OSS project that will soon be a part of Redis 6.0). In-memory replication makes replication twice as fast, which minimizes the time Redis is exposed to double failure events.2. Master and replica instances of the same dataset (hash-slots) are deployed on different nodes. In a multi-AZ deployment they are also deployed in a different availability zone.3. Each cluster node is connected to external persistent memory to allow fast recovery in case of a complete cluster failure.(Luckily that did not happen this time.)4. Our clusters always include uneven number of nodes; this design allows us to deal with a split-brain situation, as in the case of a network split event.5. For the same reason, in a multi-AZ configuration, our clusters are deployed across an uneven number of zones.6. We make sure that the number of nodes deployed in each zone is always less than half of the number of cluster nodes. This guarantees that a single-AZ failure will not result in a quorum loss.A typical Redis Enterprise Cloud multi-AZ configuration might look like this:Every modern database should provide multi-AZ capabilities, but that alone is not enough to guarantee 99.999% availability. Five-nines availability, or less than 26.3 seconds of downtime a month, cannot be achieved without a true Active-Active multi-region deployment that lets customers instantly  recover from a complete region failure. The following figure summarizes the types of SLA we provide with Redis Enterprise Cloud:It’s always satisfying when a capability we worked so hard to build actually works as designed under extreme and unexpected production conditions. This event represents yet another example of how Redis Enterprise can be used as primary database for mission-critical use cases, especially for customers who require sub-millisecond latency at extremely high throughput.Learn more about how Redis Enterprise works."
330,https://redis.com/blog/how-to-ace-the-redis-certified-developer-exam/,How to Ace the Redis Certified Developer Exam,"November 19, 2019",Kyle Banker,"As you may have heard, we recently announced the Redis Certified Developer program. Anyone can sign up for this certification program for free, but only engineers who take and pass the certification exam will become Redis Certified Developers.So, how do you prepare for the exam? How do you maximize your chances of passing and earning this coveted designation? Whether you’re already a veteran Redis expert or are still earning your spurs, there’s a process to put you on the right path and help ensure that you’re ready for your exam.To begin, you’ll want to study the study guide and spend time working directly with Redis. Once you think you’re up to speed, take the practice exam to see how you do. If you need more background, Redis University courses are a great way to boost your knowledge. Let’s take a look at all four of these areas:Not surprisingly, the best way to prepare for the Redis Certified Developer exam is to start with the study guide. This comprehensive document covers all of the Redis topics you’ll encounter in the exam and includes many suggestions for study and practice. The guide is divided into seven sections. To give you a sense of what’s in store, check out this brief overview of each section:1. General computer science, database, and Redis knowledgeYou don’t need a Computer Science degree to pass this exam, but there is some required foundational knowledge. This includes knowing the difference between a bit and a byte; recognizing that Redis is an in-memory database that supports persistence; and understanding time-complexity and Big-O notation (note that these topics are also covered in RU101: An Introduction to Redis Data Structures).2. Redis keysWhat is a Redis key? What kinds of operations can you perform on keys? How many keys exist in your Redis database? What’s the TTL on a given key? These are a few of the questions you should be comfortable answering.3. Data structuresData structures are the crown jewels of Redis—so the exam expects you to know the available data structures and what they do. While we don’t expect you to memorize every command, we do want you to be able to recognize the most common operations for each data structure.The exam also requires an understanding of the time complexity of these operations. Developers need to be aware of these time complexities to write efficient code against Redis.4. Data modelingHow do you store a domain object in Redis? What about creating unique IDs for these objects? How do you create a leaderboard? Or a queue? Or a stream of unique events?Being an effective Redis developer means knowing how to use the built-in data structures to solve a variety of domain problems. Since there’s often more than one way to do it, the study guide outlines a set of specific solutions for you to learn from.5. DebuggingAll good Redis developers need to know how to debug their code, and this section covers a few of those techniques and commands, such as MONITOR. We also intro the Redis protocol, known as RESP.6. Performance and correctness“Make it work, make it right, make it fast.”If you’re familiar with Redis data structures, patterns, and debugging, then you can probably “make it work.” But getting everything right and making your code fast requires another level of understanding. In this section of the study guide, we review pipelines, transactions, and eviction policies. We also address the commands that you might want to avoid in production.7. Working with Redis clustersWriting code against a clustered Redis deployment, such as Redis Enterprise, requires you to understand shards, hash slots, and hash tags (and, no, we’re not talking about the awkward social networking thingy). As you may know, most multi-key operations can be executed against a clustered Redis deployment only if all of the keys involved exist on the same shard. This section teaches you the techniques required to run such operations, including Lua scripts, transactions, and GEO commands.As you’re working through the study guide, it helps to have the Redis CLI at hand. Running Redis commands, and experimenting with the data structures themselves, is one of the best ways to absorb the knowledge you need.It’s also extremely helpful to write some actual code against Redis. Choose the programming language you’re most comfortable with (note: apart from Lua, there are no language-specific questions on the exam). Try to use all of the data structures. Go beyond the basics.Wondering what to try first? Experiment with key expiry. Make sure you’re comfortable with blocking commands (e.g., BRPOP), Pub/Sub, and Redis Streams. Try iterating over the keys using SCAN and a cursor. And see about writing Lua script or two.Once you’ve reviewed the study guide and gotten some hands-on experience, you should be ready to tackle the practice exam. This is an essential part of your preparation for the actual certification test. Taking the practice exam will show you the kinds of questions to expect on the certification exam. And as you get your results, you’ll also be able to identify any knowledge gaps and areas of weakness.Each practice exam question includes an explanation. It’s important to study these explanations as you go through the exam. The explanations often elaborate on the core concepts being tested.Let’s be clear: Redis University courses are not required to take the certification exam. And not all of the topics covered in the exam are discussed in our online courses. Still, taking a few of these courses can go a long way towards making you a Redis expert and preparing you to pass the exam. We recommend starting with the two data structures courses, plus one of our language courses:Data structures coursesLanguage coursesChoose one:If you follow all of the suggestions in the post, you’ll have a great chance of passing your certification exam. Of course, on the day of the exam, be sure that you’re relaxed and well rested. And if you have any questions during your exam preparation process, please don’t hesitate to send us a note.Again, we wish you the best of luck!"
331,https://redis.com/blog/15-people-to-follow-for-the-most-interesting-reinvent-takeaways/,15 People to Follow for the Most Interesting re:Invent Takeaways,"November 27, 2019",Steve Naventi,"AWS re:Invent brings together over 65,000 cloud professionals each year, but it can be hard to shift through the noise and news that this annual Las Vegas conference generates. To help you out, we’ve compiled the top people we’re following for re:Invent commentary. Follow them for insightful, analytical, humorous, and fun takes on all things AWS and re:Invent.Jeff BarrJeff, the Chief Evangelist for AWS, has been an essential guide for developers on the latest tools and how to get the most out of AWS services since its origins. He showed up to re:Invent last year with purple hair — this year we predict it’ll be orange.Twitter: @jeffbarrBlog: https://aws.amazon.com/blogs/aws/author/jbarr/Jeff BlankenburgJeff is an Alexa Evangelist at AWS. He tweets frequently about all things tech, developers, and nerds, and will no doubt have news to share on what’s coming next in the voice-enabled future.Twitter: @jeffblankenburgBlog: https://blankenblog.com/Vicki BoykisVicki is a consultant and frequent presenter on data science and machine learning. She also writes a newsletter, Normcore Tech, analyzing the future of data science and the tech industry.Twitter: @vboykisNewsletter: https://vicki.substack.com/GitHub: https://github.com/veekaybeeSam CharringtonSam is an expert and frequent speaker on machine learning and AI. He hosts the This Week in Machine Learning & AI (TWIML) podcast. As AWS continues to iterates and develop new AI, ML, and deep learning services, Sam is sure to provide essential analysis.Twitter: @samcharringtonWebsite: https://twimlai.com/Cloud OpinionThe anonymous Cloud Opinion delivers parody content on AWS and the rise of the cloud, and will certainly have unconventional, fun takes on re:Invent and break through to the real meaning of the presentations and announcements.Twitter: @cloud_opinionYan CuiYan is a consultant and expert on AWS Lambda and serverless computing, and wrote Production-Ready Serverless: Operational Best Practices. As stateless architecture are growing in adoption, will we see examples in large-scale production this year at re:Invent?Twitter: @theburningmonkWebsite: https://theburningmonk.com/GitHub: https://github.com/theburningmonkEmily FreemanEmily is the Modern Operations Advocacy Manager at Microsoft. She wrote the bestselling book DevOps for Dummies, and her human approach to building software will provide balance to all the product-centric announcements.Twitter: @editingemilyWebsite: https://emilyfreeman.io/Abby FullerAbby is a Principal Technologist focusing on container work at AWS. Follow her for insightful takes on all happenings at re:Invent in addition to the future of operating containers in AWS.Twitter: @abbyfullerGitHub: https://github.com/abby-fullerArun GuptaArun is an AWS evangelist focusing on open source, containers, and serverless. He’ll likely be live-tweeting some sessions and keynotes.Twitter: @arunguptaGitHub: https://github.com/arun-guptaLydia LeongLydia is the Gartner analyst who has covered the rise of cloud computing and managed infrastructure services within global enterprises.Twitter: @cloudpunditWebsite: https://cloudpundit.com/RedMonkOne analyst firm, RedMonk, has charted the rise of developers as the kings within their organizations, or more appropriately the technologies from cloud to containers that has enabled this transformation. You should also follow their entire team directly: @monkchips, @drkellyannfitz, @sogrady, and @rstephensme (clockwise from top left).Twitter: @redmonkWebsite: https://redmonk.com/Corey QuinnCorey, the Cloud Economist at Duckbill Group, knows AWS inside and out, explaining the ecosystem in a fun and satirical Tweetstorms. If you run into him in-person, you might have to capture the moment with a selfie.Twitter: @QuinnyPigWebsite: https://www.lastweekinaws.com/And of course don’t forget to follow @Redisinc for all the happenings in our booth, sessions, parties, and how you can win some exciting giveaways!Cover image by Ken Yam, Unsplash."
332,https://redis.com/blog/aws-vs-open-source/,AWS and Open Source: It’s Complicated,"December 17, 2019",Redis,"It’s that time of the year again. No, I’m not referring to the winter holidays, but to the reverberations of the announcements coming out of AWS re:Invent. Oftentimes when AWS makes a big move, IT ecosystems get shaken. In the database space, this year the effect focused on Apache Cassandra, after AWS announced a Cassandra-compatible serverless managed solution.Along with the announcement, AWS also released a blog post explaining how its offering is going to help the Cassandra ecosystem by increasing the demand for Cassandra-like solutions and by allowing AWS to contribute back improvements to the open source community.This is not the first time AWS has done something like this. Back in March, AWS announced Open Distro, a hostile fork of Elasticsearch, and published a similar blog post where the company argued its actions were aimed at stopping Elastic from tainting the open source project with proprietary extensions.Now, with the Cassandra announcement, I’m getting a feeling of deja vu. I’m not here to address any business implications, but from the developer-community perspective, it seems that every time AWS announces a new database offering based on an open source project, it feels the need to restate how the company is a great OSS citizen—but the repeated effort just makes me feel more skeptical about the overall effect on open source projects in the cloud era. To be fair, at least in the case of Redis, AWS did contribute something back to the main project. The upcoming addition of SSL support to Redis 6 is the result of a collaboration between software engineers from both AWS and Redis (plus Alibaba, and more), as antirez himself tweeted:While legions of AWS employees storm Twitter to assure everybody they’re the good guys, other people in the community think differently. In particular, the people at ScyllaDB (a C++14 implementation of Cassandra) seem very concerned (learn more about the company’s take in this blog post). Long story short the AWS offering is based on Amazon DynamoDB and uses only some of the original Cassandra code as a form of translation layer to allow Cassandra clients to connect almost transparently.“Almost transparently” because some original Cassandra features are not supported by this implementation. Quoting the Cassandra experts from ScyllaDB:This is the inevitable result of offering a hybrid solution. In my opinion, it’s not OK because it’s likely to cause a split in the Cassandra community. I saw it happen with Redis. AWS offers a managed Redis solution called ElastiCache. As the name implies, it’s heavily geared towards caching workloads and it doesn’t include support for some key features that make Redis a viable persistent message broker or even a primary database.The situation is complicated, and not as straightforward as it sounds. At AWS re:Invent, I lost count of the number of attendees who showed up at the Redis booth who didn’t know that in Redis you can indeed tune persistence to achieve durability guarantees comparable with any other operational or analytical database. Once they learned that, they would ask what else they could do with Redis, and we’d talk about the different data types, Redis Streams (which are supported by ElastiCache, but become much more useful with strong persistence settings), and how Redis modules let you add new data types to Redis, such as full-text search indexes. We’d also cover the many additional modules written by the Redis open source community (including redis-cell, redis-cuckoofilter, and cthulhu).As you might have guessed, ElastiCache doesn’t support any modules, even those from the community:The prevalence of ElastiCache has created a split in the community of Redis users. The people who experience Redis only via AWS are seeing  an incomplete vision of the open source project’s direction and benefits. I feel the broader community would benefit if they knew that, Yes, Redis is a great caching solution, but you can do so much more with it.We’ve now seen AWS fork and carve out features from several open source databases. In addition to Cassandra and Elasticsearch, AWS also has a MongoDB-compatible offering that is also not feature complete. Feature disparity might seem a minor technical detail, but as a Developer Advocate for Redis I can see the impact it can have on the community. In the case of Redis, the feature disparity effectively relegates the most loved database by developers worldwide to being primarily a caching front-end for the databases that AWS wants its customers to use.When it comes to open source, the real question is not whether or not AWS will contribute code back, but rather what will be the overall impact of its actions on the open source community. Frankly, all these blog posts, tweets, and metal pins just look like red herrings to me."
333,https://redis.com/blog/new-microsoft-azure-qualified-proof-of-concept-poc-program/,New Microsoft Azure Qualified Proof of Concept (PoC) Program,"December 19, 2019",Cassie Zimmerman,"Earlier this year, we released Redis Enterprise as a fully managed Database-as-a-Service (DBaaS) on Microsoft Azure, and are now authorized to sell Redis Enterprise through the Microsoft One Commercial Partner program.Since the announcement, Microsoft and Redis have partnered to make it easy for users to run, scale, and manage Redis Enterprise on Azure. We are energized by the positive feedback we’ve gotten from customers and partners who are beginning to move their Redis Enterprise databases to Azure, and we want to make it easier for everyone to see the benefits of running Redis Enterprise on Azure.That’s why we are so excited to announce participation in the Azure Sponsorship Program. The program will grant qualified users access to complimentary infrastructure for thorough evaluations of Redis Enterprise on Azure. This program offers evaluation subscriptions to Azure for up to 90 days for qualified proofs of concept (PoC).It’s your risk-free chance to experience the benefits enjoyed by leading e-commerce platforms that are embracing Redis Enterprise on Azure for multiple real-time use cases. As part of its broader strategy to leverage the cloud, for example, this large enterprise retail company is taking advantage of the simplicity and blazing fast performance of running Redis Enterprise on Azure.This retailer and other customers are able to take advantage of key features of Redis Enterprise on Microsoft Azure, including:To learn more about running Redis Enterprise on Azure, read Run, Operate, and Scale Modern Applications with Redis Enterprise on Microsoft Azure or download Redis Enterprise on Azure today.Interested in taking advantage of the Azure Sponsorship Program? Send us a note at channel@redis.com to be connected with a Redis Azure specialist who can help you get started."
334,https://redis.com/blog/how-holidayme-uses-redis-enterprise-as-its-primary-database/,How HolidayMe Uses Redis Enterprise As Its Primary Database,"December 20, 2019",Miguel Allende,"Nadia is looking to take a summer holiday to Switzerland from her home in Bangalore, India. To plan her trip, she heads online to HolidayMe.com, an online travel agency based in Dubai, Riyadh, and Pune that curates thousands of expert-designed itineraries for customers to personalize. Juggling logistics for airfare, lodging, and attractions in an unfamiliar country can be complicated, and Nadia hopes HolidayMe will make it easier to organize everything in one place.But there are a number of inhibitors that can make the booking process aggravating. If Nadia can’t remember the name of a particular attraction she’s interested in visiting and HolidayMe’s autocomplete lags, or if the website lags and her search results don’t load instantly, she might grow frustrated and turn to another website to book her travels.To minimize these issues and ensure a great user experience for all its customers, HolidayMe relies on Redis Enterprise as its primary database, not just a database cache. To learn more about how Redis Enterprise helped HolidayMe speed up its data output by 50 to 60 times and move toward a more modern microservices architecture, read the case study and listen to The New Stack’s podcast with HolidayMe CTO Rajat Panwar.The online travel agency has been using open source Redis since the company was founded in 2014, originally as a caching system. Hoping to speed data output, the team transitioned to using Redis Enterprise as its primary database for all its customer-facing interactions. The company uses Redis’ Hashes, Sets, and Lists to process data and update geographic information, and even created its own key structures for specific queries.HolidayMe also wanted to build a search autocomplete mechanism. But the team wasn’t satisfied with the latency yielded by its original toolchain that included MongoDB, Apache Lucene, and Elasticsearch, so it turned to RediSearch to create that functionality.Read the full HolidayMe case studyBut that’s not all. You can hear HolidayMe Chief Technology Officer Rajat Panwar discuss using RediSearch in a new episode of The New Stack Makers podcast, in conversation with Redis Chief Product Officer Alvin Richards and The New Stack’s Alex Williams and B. Cameron Gain. “We were able to migrate our in-house autocomplete (function) that we managed previously and we took that completely to RediSearch,” Panwar says. “Honestly, it didn’t take us more than two days in terms of having a complete transformation, and it was up and running. And it’s giving us the best latency.”See Redis on Why NoSQL Is a Safe Bet for Today’s Multi-Environment Deployments on The New Stack to learn more about the podcast, or listen below:"
335,https://redis.com/blog/redis-labs-2019-year-in-review/,Redis Labs’ 2019 Year in Review,"December 30, 2019",Ofer Bengal,"Is it just us, or did 2019 seem to fly by? Maybe it just seemed that way because 2019 was such a big year for us here at Redis: We continued to invest heavily in Redis Open Source and its ecosystem, we introduced important new features to Redis Enterprise, partnered with Google Cloud to offer Redis Enterprise as a native service on the GCP console, hosted events all around the world, welcomed legions of new customers, grew our staff and bolstered our executive team, and much more.And as 2020 rolls into town, we’re proud to say that Redis is still the fastest NoSQL database, still the most-loved database by developers, still the #1 database container, and the #1 cloud database. The Redis world has a lot to look forward to in the new year, including more proof of how development and operations teams are increasingly taking Redis beyond caching to serve as a primary database powering their most critical applications. In the meantime, here’s a recap 2019’s biggest and most memorable moments:2019 saw slew of big announcements for Redis Enterprise, further demonstrating that the power of Redis goes beyond just caching.We started the year by announcing that Redis Enterprise for Intel Optane DC persistent memory was available across multiple cloud services or as downloadable software. Compared to a traditional DRAM-only memory configuration, Redis Enterprise on Intel Optane offers comparable performance with up to 40% memory cost savings.We continued by announcing several new modules designed to enrich the data models and use cases supported by Redis:We then introduced RedisInsight, a free(!) browser-based UI and management system designed to help ease Redis deployments. Finally, RedisEdge enables deploying Redis on tiny Raspberry Pi-based edge devices while processing hundreds of thousands of events per second with enhanced functionality, including RediStreams, RedisTimeSeries, and RedisAI— programmable with RedisGears.We also set a new performance standard for Redis Enterprise. In our latest benchmark published in June, Redis Enterprise delivered more than 200 million ops/sec, with less than 1 millisecond latency, on as few as 40 AWS instances. This represents a 2.6X performance improvement from our previous record in just 15 months!During 2019 we also invested in enterprise-grade Kubernetes deployment of Redis. Redis Enterprise Operator ensures that your cluster is properly utilizing the Kubernetes Stateful Set with support for anti-affinity, multi-AZ (failure domains), rolling upgrades, and Active-Active deployment across multiple Kubernetes clusters. And we recently announced Automated Cluster Recovery, which not only improves the availability of your Redis deployment but also allows users to manage a stateful service as if it were stateless. Our Kubenetes distro is available across all the leading Kubernetes platforms: GKE, AKS, EKS, PKS, RedHat OpenShift, and native Kubernetes.Furthermore, we worked hard to help customers use Redis Enterprise across multiple clouds in just a few clicks, with the unification of Redis Enterprise Cloud and Memcached Enterprise Cloud into Redis Cloud Essentials. We also renamed Redis Enterprise VPC as Redis Cloud Pro, supporting Active-Active, Redis on Flash, and modules.At Google Cloud Next, Google Cloud CEO Thomas Kurian invited me on stage to announce an expanded partnership with the goal of giving our joint customers a simplified and streamlined experience for building and running modern high-performance applications. A few months ago we announced the first step of this cooperation by making Redis Enterprise available on the Google Cloud Marketplace, and allowing Google Cloud customers to consume enterprise grade Redis services with their existing Google credit.2019 officially marked Redis’ 10th open source anniversary, exactly a decade since Redis was first shared on Hacker News.During 2019 we continued our strong investment in the Redis core and its ecosystem, and to round out the year, Redis Creator Salvatore Sanfilippo announced the first release candidate of Redis 6 in December. Some of the new features include SSL, ACLs, RESP3, client-side caching, new module APIs, a better expire cycle, a new distributed-queue capability with Disque modules, and more.We’ve also made it easy to demonstrate your mastery of Redis with our new Redis Developer Certification program, which launched in November. Getting certified shows you’ve mastered a wide variety of Redis skills, and passing the exam earns you a digital badge and certificate—perfect for LinkedIn and job applications.In April we held our fifth annual RedisConf, gathering more than 1,500 Redis Geeks on San Francisco’s Pier 27. Attendees were treated to special keynotes from Redis creator Salvatore Sanfilippo and Redis Co-founders Ofer Bengal and Yiftach Shoolman, plus more than 100 breakout sessions led by Redis experts and customers as well as world-class networking. Be sure to save the date for RedisConf 2020, coming May 12-14 at SVN West in San Francisco.Meanwhile, we brought Redis to you with Redis Days in Tel Aviv, New York, and London. Attendees boosted their skills in hands-on training days and heard from Redis customers on innovative ways they are using Redis. If you’re in the neighborhood, don’t miss our first 2020 Redis Days in January: Seattle and Bangalore!Finally, at AWS re:Invent, we went all in on the theme Growth Happens: as your company and database needs scale, Redis Enterprise offers the best Redis experience for dealing with that growth smoothly and effectively. We had a blast in Vegas, interviewing people on the show floor, watching our sponsored sumo wrestler win the Sumo Logic Slam Jam, and hearing top customers like Gap and Alliance Data share how they get the most from Redis Enterprise.We’re excited to continue building Redis for the next decade, fueled by our $60 Million Series E funding round led by Francisco Partners in February, bringing our total funding to $146 million.And in April, we announced that Redis has achieved co-sell ready status through the Microsoft One Commercial Partner program, an elite group of independent software vendors selected by Microsoft for intensive joint sales, support and go-to-market initiatives.We also expanded our workforce and leadership team! In January, Alvin Richards, previously Chief Education Officer, was promoted to Chief Product Officer. In February, Rafael Torres joined as CFO and, in April, Howard Ting was named Chief Marketing Officer. Overall, the Redis team grew more than 50%  in 2019!We’ve also proud to serve all the customers who adopted Redis Enterprise in 2019. Our momentum in the Fortune 1000 continued to increase, with companies such as Deutsche Börse, which expanded its use of Redis Enterprise as an intelligent cache to rapidly process and organize data, and Turner Broadcasting, which has boosted its real-time analytics performance and uses Redis Enterprise to help ingest data quickly. And we’re especially excited about the dramatic increase in organizations using Redis Enterprise as a primary database, such as HolidayMe and Charter Communications.It’s also nice that other folks noticed our big year. Redis was named to a number of key industry lists in 2019, including Deloitte’s North America Technology Fast 500 (for the third consecutive year!) and as a Leader by Forrester in the NoSQL Wave.Even more important, perhaps, Redis was named most-loved database for the third consecutive year in Stack Overflow’s 2019 Developer Survey. Plus we were also named to Gartner’s January 2019 Peer Insights Customers’ Choice list, based on how our customers review their experiences with us. In less than a year, we received more than 100 reviews while maintaining a 4.6 ranking.We’re especially proud of accolades that celebrate our workplace culture. Redis was named a great place to work on 3 continents, earning a place on Deloitte’s EMEA Fast 500 list, Dun & Bradstreet’s list of 10 best startups to work in Israel, and in Sequoia Consulting Group’s 2019 Healthiest Employees of the Bay Area program. Earlier in the year, global investment firm Battery Ventures included Redis on its list of 50 Highest Rated Private Cloud Computing Companies to Work For, which ranks companies based on Glassdoor employee feedback.2019 was a great year for Redis-related content of all kinds, highlighted by the publication of our new ebook—Redis Microservices for Dummies! Written by Developer Evangelists Kyle Davis and Loris Cro, this free 64-page volume can help you understand the fundamentals of a microservices architecture and how to use Redis to optimize your data layer to make your apps more scalable.And finally, to help the Redis community stay up to date, we relaunched two newsletters in 2019: RedisWatch and Redis Enterprise. Subscribe to the Redis Watch newsletter for a monthly dose of technical goodness on everything Redis. Sign up for our Redis Enterprise newsletter—relaunched in December—for the latest on Redis, including product updates event highlights, top blog posts, and more.2019 was a great year for Redis and Redis, and we owe it all to the incredibly vibrant community of developers, operations teams, and DevOps practitioners who use Redis and Redis Enterprise to address an incredible variety of business and technical use cases—not to mention Redis’ customers, partners, and dedicated roster of employees. We’re excited to continue building and growing Redis Enterprise as a primary database powering the most critical applications. We can’t wait to see what 2020—and the new decade—brings."
336,https://redis.com/blog/how-redis-fits-with-a-microservices-architecture/,How Redis Fits with a Microservices Architecture,"January 9, 2020",Redis,"This blog post was adapted from our new e-book, “Redis Microservices for Dummies” by Kyle Davis with Loris Cro. The excerpt was originally published on The New Stack on December 20, 2019. Download the complete e-book here.Many of today’s widely used database systems were developed in an era where a company adopted a single database across the entire enterprise. This single database system would store and run all the functions of the enterprise in one place. You can probably picture it: a room full of refrigerator-sized machines, many sporting oversize reel-to-reel tape drives.But Redis evolved differently than many other popular database systems. Built in the NoSQL era, Redis is a flexible and versatile database specifically designed not to bother storing massive amounts of data that will be mostly idle. A microservices architecture has related goals: each service is designed to fit a particular use—not to run everything in the business.Redis is designed to store active data that will change and move often, with an indefinite structure with no concept of relations. A Redis database has a small footprint and can serve massive throughput even with minimal resources. Similarly, an individual service in a microservices architecture concerns itself only with input and output and data private to that service, meaning that Redis databases can back a wide range of different microservices, each with their own individual data store. That’s important, because the very nature of having many services means that each service must perform as fast as possible to make up for the connection and latency overhead introduced by inter-service communication.A key characteristic of a microservices architecture is that each individual service stands on its own—the service is not tightly coupled with another service. That means microservices must maintain their own state, and to maintain state you need a database.Microservices architectures can comprise hundreds or even thousands of services, and overhead is the enemy of scale. An infrastructure that consumes lots of resources just to run would dilute the benefits of a microservices architecture.Ideally the service data would be completely isolated from other data layers, allowing for uncoupled scaling and cross-service contention for slow resources. Since services are specifically designed to fill a single role (in terms of business processes), the state they store is inherently non-relational and well suited to NoSQL data models. Redis may not be a blanket solution for all data storage in a microservices architecture, but it certainly fits well with many of the requirements.Once you have built a service, it needs to talk to other services. In a traditional microservices environment, this occurs over private HTTP endpoints using REST or similar conventions. Once a request is received, the service begins processing the request.While the HTTP approach works and is widely used, there is an alternate method of communicating where services write to and read from log-like structures. In this case, that’s Redis Streams, which allows for a completely asynchronous pattern where every service announces events on its own stream, and listens only to streams belonging to services it’s interested in. Bidirectional communication at that point is achieved by two services observing each other’s stream.Even in services that do not use Redis for storage or communication, however, Redis can still play a vital role. To deliver a low-latency final response, each individual service must respond as fast as possible to its own requests, often outside the performance threshold of traditional databases. Redis, in this case, plays the role of a cache, where the developers of the service decide where data is not always required to be retrieved directly from the primary database, but instead can be pulled from Redis much more quickly.Similarly, external data services that need to be accessed through an API will also likely be far too slow, and Redis can be used here to prevent unneeded and lengthy calls from impacting the system’s overall performance.For more detail on how Redis can help power your microservices architecture, download your own copy of the “Redis Microservices for Dummies” e-book and listen to authors Kyle Davis and Loris Cro discuss the book on the The New Stack Context podcast."
337,https://redis.com/blog/redis-microservices-for-dummies-podcast-kyle-davis-loris-cro-the-new-stackj/,"Listen to Kyle Davis and Loris Cro Discuss Their New Book, “Redis Microservices for Dummies” [Podcast]","January 14, 2020",Sheryl Sage,"The data layer is one of the first things to consider when moving to a microservices architecture. That’s because in a microservices architecture, each service manages its own data and is responsible for its own private data store. Understanding the role of the database in each service is critical. Depending on the service, for example, database could be the single source of truth, a temporary store, or something in between.That’s one of the most important concepts in our new free e-book: Redis Microservices for Dummies. Authors—and Redis developer evangelists—Kyle Davis and Loris Cro walk readers through key microservices terms and concepts and how a microservices architecture can help make your apps more scalable, easier to manage and update, and more resilient. You’ll also learn how to use Redis to optimize your data layer.Find out more about Redis and microservices architectures with Kyle and Loris on The New Stack Context podcast (player embedded below), in conversation with The New Stack’s Alex Williams, Libby Clark, and Joab Jackson. The group discusses common questions about using Redis in a microservices architecture application state and deconstructs the notion of whether you really need a separate “primary database,” among other things.“The mind-expanding moment for people is saying, ‘Do you need a ‘primary’ database at all? Can you use Redis to store things in a durable set?” Kyle says. “And this is something that when people start understanding, like, ‘Oh, I can take an entire layer out of this stack that I’m building this service on,’ it really kind of changes their mindset, because it’s operationally easier, it’s less to develop.”And for a quick preview what’s in the e-book before you download it, check out these excerpts:Download Redis Microservices for Dummies here!If you enjoy podcasts, be sure to also take a listen to the Redis Stars Podcast, where developers and community members share the many ways they are using Redis."
338,https://redis.com/blog/what-you-missed-at-redis-day-seattle/,What You Missed at Redis Day Seattle,"January 22, 2020",Jane Paek,"A little snow couldn’t stop the Redis community from coming out in force to our first Redis Day of the year in Seattle. Held on January 13–14 at the Hyatt Regency, Redis Day Seattle brought together some 300 developers, engineers, software architects, programmers, and business professionals for intensive training, thought leadership, and networking.On day one, Redis users from beginners to experts packed the conference room for a full schedule of hands-on training covering everything from Redis basics to Redis Streaming Architectures, Probabilistic Data Structures, RediSearch, and RedisTimeSeries. The following day, attendees heard from representatives of Zulily, Twilio, MDmetrix, AWS, and other cutting-edge customers on how Redis helps solve some of their business problems.We had a grand time in Seattle, and can’t wait to be back again. For now though, here’s a recap on what went down.Redis Day Seattle was a great way to discover how to use Redis beyond traditional use cases, and demonstrated how Redis has been embraced by many frameworks, like Protobuf, Rust, and ASP.NET. Attendees were treated to out-of-the-box presenters, like Adam McCormick from Sinclair Broadcasting, who started his presentation by having the entire audience stand up and sit down with the progression of pages deployed. That didn’t take long—Sinclair publishes a new webpage every 15 seconds on more than 16 million websites!One point that struck many attendees was the wide variety of businesses from completely different industries that all have one thing in common: Redis is part of their technology stack. From Seattle-based Zulily, Mohamed Elmergawi shared how the ecommerce giant uses Redis to build a super-fast, super-reliable global customer session service that handles millions of requests per day while delivering a great customer experience. For medtech company MDmetrix, RedisGraph helps track medical treatments so hospitals can carefully track doses, reducing overmedication and drug addiction, said Co-Founder and CTO Matthew Goos. Cloud service company Twilio, meanwhile, uses APIs to connect people around the world via voice, SMS, video, and WhatsApp, said Principal Software Engineer Scott Haines. And I got the chance to explain how to use Redis to power a metering and rate limiting app to protect your APIs—using an example of keeping folks from drinking too much.Many attendees were surprised to learn about Redis’ capabilities beyond caching. Cameron Vander Wal, a software engineer at the survey firm Qualtrics, for example, said he was surprised to learn that Redis had a streaming capability. “We’ve really only used Redis for a simple key-value search,” he said. “To have a message queue like that seems pretty useful.”Redis Day Seattle attendees were also treated to a special keynote from Redis CTO and Co-Founder Yiftach Shoolman, who traced the growth of NoSQL data models like TimeSeries, Graph, JSON, Key-value, and Search, as discussed some of the most-desired features of the upcoming Redis 6. Later in the day, Redis’ Security Product Manager Jamie Scott explained some of the new security features in Redis 6, including TLS and ACLs. And, of course, it wouldn’t be a Seattle tech event without tech staples Amazon Web Services and Microsoft, who presented on Redis Commands and ASP.NET Core with Redis, respectively.The second day closed with a networking happy hour where developers, presenters, and experts alike could mingle and share their Redis stories. It was a great opportunity to bounce ideas off Redis users with a wide variety of experiences, all eager to learn something new. Plus, everyone left happy with a brand new Redis t-shirt and laptop stickers.Despite the blanket of fluffy white stuff, Redis Day Seattle turned out to be a great event, bringing together the Pacific Northwest Redis community to kick off the new year and the new decade, Redis style.Interested in attending future Redis events? Save the date for RedisConf 2020 in San Francisco:RedisConf 2020When: May 12–14, 2020Where: SVN West, 10 S Van Ness Ave, San Francisco, CA 94103"
339,https://redis.com/blog/getting-started-redis-6-access-control-lists-acls/,Getting Started with Redis 6 Access Control Lists (ACLs),"January 31, 2020",Redis,"With the arrival of Redis 6 come a few new features for better security and compliance. The easiest one to explain is probably support for SSL, which enables secure communication between your application servers and Redis. But the most notable new feature is Access Control Lists (ACLs).With ACLs you can create multiple users and specify login passwords, and the keys and commands they are allowed to use for each. This makes it trivial to implement security and safety features such as:The list could go on and on, but these three examples should be enough to get your gears spinning.Before we jump into how to use ACLs, let’s quickly dive into the technical implications behind ACLs.The first notable point is that ACLs are a self-contained mechanism that you can opt-in to when the server is started. This means that there won’t be a performance hit if you don’t enable ACLs, which can be of the utmost importance for those who use Redis as a heavy-duty caching engine.The second point is that ACLs are also very lightweight when enabled. Enabling restrictions on commands doesn’t require any pattern matching as Redis keeps a bitmap of enabled commands for each user, making the check extremely fast. While enabling restrictions on key names does require pattern matching, the computational cost will still be minimal for the common use case of using key prefixes for namespacing.The last point is about the code itself. Most of the code related to ACLs is in acl.c, a file with fewer than 1,800 lines of C code (as of publication), making the feature easy to inspect for those who want a deep understanding of the tools they use.Before v6, Redis did not include the notion of users, and the only authentication strategy was a single login password. To make the transition smoother for existing codebases that rely on the previous behavior, Redis 6 includes a default user named ‘default’.This means that by setting up a password for the ‘default’ user, along with any command or key restriction you might find appropriate, you will be able to immediately transition to Redis 6 from previous versions without having to change a single line of code in your applications.To put this in practical terms, before Redis 6 the AUTH command took only one argument: the server password. Now, in Redis 6, AUTH can take either a single argument, for retrocompatibility, or two (username and password). When called with a single argument it will assume you’re providing a password for the ‘default’ user.Let’s start by creating a basic user with all permissions:127.0.0.1:6379> acl setuser antirez on >hunter2 allcommands allkeysThis command creates a new user called ‘antirez’, enables the user for login by providing the ‘on’ option, sets the user’s password to ‘hunter2’, and enables all permissions.Using the same syntax, it is also possible to apply changes to the user in a free-form way by simply concatenating the changes you want to apply to it:127.0.0.1:6379> acl setuser antirez nocommands +get +setIn this invocation we first reset the list of enabled commands (so we went from all commands to no commands allowed at all), and then we enabled GET and SET. Note that all the other properties (the user being enabled, restrictions on key patterns) are left untouched.Now let’s add some restrictions on keys:127.0.0.1:6379> acl setuser antirez resetkeys ~secret ~redis:*After this command, the user will only be allowed to call SET and GET on the ‘secret’ key and all other keys that are prefixed with ‘redis:’.Another important characteristic of ACLs is the ability to set up robust password management schemes for your users.In the previous example, we set the password ‘hunter2’ for the user ‘antirez’. Let’s see what happens if we discover that the password was compromised, maybe because it got disclosed on IRC, for example.A reasonable first step would be to immediately disable the user:127.0.0.1:6379> acl setuser antirez offNote how this command doesn’t clobber the existing set of permissions, it just disables the ability of the user to log in. After this is done, the next step is to remove the existing password and add a new one:127.0.0.1:6379> acl setuser antirez on resetpass >hunter3With this command we re-enabled the user, removed any previously set password, and added ‘hunter3’, all in one atomic command.Let’s say that some time has passed and it’s now time to rotate out ‘hunter3’ in favor of a newer password. Since this time the password has not been compromised, we want to transition away from it without causing a service disruption. To allow a seamless transition, we need to add the new password while leaving the old one enabled as the change rolls out to our services:127.0.0.1:6379> acl setuser antirez >newpassAfter running this command, the user ‘antirez’ will be able to authenticate with both ‘hunter3’ and ‘newpass’. After the transition is completed (i.e., all services have been restarted and are now using the new password), it’s time to remove the old password:127.0.0.1:6379> acl setuser antirez <hunter3And with this, you’re done. ACLs also offer the ability of managing passwords by hash value, instead of having to remember the plaintext value, and also provide facilities for generating secure passwords automatically. Use the ‘acl help’ command for more details.To try out ACLs all you need is Redis 6.While this is just a simple introduction to ACLs, there are certainly more usage scenarios than what’s presented here. As with many other Redis features, they take only a moment to learn, but require more time to master. Try your hand at working through how ACLs can integrate with your existing user management schemes, and let us know on Twitter if you have any feedback."
340,https://redis.com/blog/multiplexing-explained/,Multiplexing Explained,"February 12, 2020",Redis,"A critical, but important piece of your Redis-powered application is the client library. Client libraries are the glue between the software you are writing and Redis. They perform a few primary major duties:The first point is standardized—there is a specification for RESP and all clients must conform or, well, nothing works. The second point is unique to each library—this is what makes Redis feel natural to your programming language; even client libraries for the same language may implement this differently. It’s art rather than science. The third point, connection management, is where you see an oddly large variance among different libraries for such a technical point.The Redis connections are persistent between the client (your app) and the Redis server. In contrast, many other APIs rely on a single-use connection that is used once then disposed. If you’ve ever used a REST interface, it follows this model. The persistent connection is fast because it doesn’t have to deal with overhead of creating and destroying connections. It does present some challenges, however: the client library needs to manage how the connection is reused (or not) and shared (or not). Opinions on this management as well as runtime characteristics of languages explain why the landscape remains fairly wide open on this point of client architecture.There are three basic schools of thought regarding connection management:Unmanaged connections are those that defer the management of the connection to the application itself. A prime example would be the node_redis library, which provides very little in terms of managing the connection aside from basic reconnection logic. The Node.js world is JavaScript, which is asynchronous by nature and single threaded, so much of the scaling of a Node.js applications occurs by running multiple instances of the application.Pooled connections keep a series of connections to the Redis server ready at any given time and then allow for the application to pluck one of these connections from the pool, use it, and return it when done. Jedis for Java uses this technique as Java is threaded and it allows for a more logical sharing of the connections across threads.Finally, we have multiplexing. In multiplexing, you take many threads and share a single connection. StackExchange.Redis for the .NET ecosystem uses this model. This may sound counter productive, but let’s look more closely at how it works and what it means for your application.Visually, you can think of multiplexing a bit like a rope being braided. Many strands are arranged in a particular way to yield a single strand at the other end. In a multithreaded runtime, you’re not exclusively giving any thread full control over the communication with the Redis server. Instead, you’re letting the client library take communication from those threads and intelligently merge it into a single connection. Then, as communication is returned from the Redis server, you’re unwinding the responses back to each individual thread.This gives the client a few obvious advantages. Multiplexing can handle a large number of independent execution threads that get created and destroyed arbitrarily without having to create and destroy connections (which is expensive for both your application and Redis). Secondly, unlike a pooled interface, you don’t have to worry about getting and returning the connection from the pool.Less apparently, there are other advantages. Multiplexing allows for a form of implicit pipelining. Pipelining, in the Redis sense, meaning sending commands to the server without regard for the response being received. If you’ve ever been through a drive-through window and rattled off your entire order into the speaker, this is like pipelining. You don’t wait for confirmation from the restaurant employee for each item, instead they just read back the entire order at the end. This is naturally just faster as it removes the latency between sending and awaiting the response to each item.When using a multiplexer, all commands are pressed into the same connection at all times, so unrelated threads with unrelated Redis commands are sent immediately to the server, no waiting for a connection from the pool to be available or any responses to come back. And your application is none the wiser to all of this.Like most things in computing, multiplexing does not come without a cost. Using a single connection is not always advantageous. Certain operations in Redis intentionally take a long time to respond: these are collectively known as client-blocking operations. Client-blocking operations withhold a response until a condition is met, usually until a new item is added to a structure or when a timeout elapses, whichever comes first. These commands are BLPOP, BRPOP, BRPOPLPUSH, BZPOPMIN, BZPOPMAX, XREAD…BLOCK, and XREADGROUP…BLOCK.If you think about this from the perspective of multiplexing, as soon as one of these commands is issued, all traffic between all threads of your application and the Redis server are placed on hold until new data arrives or the timeout is met. Not good! For this reason StackExchange.Redis does not support these commands (and according to the current documentation, will never support them).If you ever played with the Redis Pub/Sub command you’ll notice that the SUBSCRIBE command works somewhat like this, so how does the multiplexer manage that? In effect, it creates a single dedicated subscription connection to Redis, then multiplexes any published messages out to the relevant threads as they come in.Finally, the multiplexer has different dynamics than other clients when very large pieces of data are sent or received from Redis. Imagine sending a 500MB chunk of data to Redis. Redis itself, being single threaded, will be devoted to receiving this data, but your client application cannot continue to add to the pipeline until the entire 500MB is finished on that end as well. The same goes for receiving large pieces of data from Redis.StackExchange.Redis is a good client and multiplexing is an interesting architecture for a Redis client library. It is important to know what you are dealing with though: on one hand multiplexing solves a common problem (latency) and, on the other hand, it limits some functionality of Redis.It’s also useful to understand how current and future variants in the Redis ecosystem will interact with this client architecture. Redis Enterprise is based on a zero-latency proxy process that does some automatic pipelining internally on the cluster side, which mutes some of multiplexing’s advantages.Additionally, the upcoming release of Redis 6 will raise two new challenges to the multiplexer model. In Redis 6, ACLs will control what keys and commands individual users can use, so a multiplexed connection will be counterproductive if it must constantly switch user contexts. Redis 6 also introduces threaded I/O, which means the processing delta between the single connection on the client side and multi-threaded server-side connections will likely grow.On the other hand, there are many well-written existing applications and libraries in the .NET ecosystem that will automatically take advantage of optimization in Redis 6 and will continue to operate lightning fast without code changes. And be aware that the author of the StackExchange.Redis library, Marc Gravell, recently hinted in a tweet that he’s considering some changes in the new version that may change the architecture of the library away from multiplexing."
341,https://redis.com/blog/how-a-leading-fintech-company-leverages-redis-enterprise-to-boost-uptime-and-quadruple-throughput/,How a Leading FinTech Company Leverages Redis Enterprise to Boost Uptime and Quadruple Throughput,"February 13, 2020",Miguel Allende,"Alice loves to flash her favorite clothing store’s corporate credit card. As a frequent shopper, Alice receives points for her purchases that she can redeem for discounts, free shipping, and other perks. And the benefits go both ways—the store can collect information about her shopping habits to tailor its marketing, loyalty programs, and promotional materials.The liaison between Alice, the clothing store, and the branded credit card is a leading fintech company, which manages more than 300 branded credit card programs and their integrated marketing campaigns. This fintech firm recognized that the aging content management system it used to run those campaigns was lagging compared to its competitors. The company needed a high-performing, scalable, and stable database to manage all customer information and marketing campaigns, as well as scale a more efficient client-onboarding process.The database also needed to provide all the normal credit card services so customers like Alice could enjoy a seamless experience. An outage—or even more detrimental, a data loss—would be a major problem, damaging the company’s services and reputation as it competes with other fintech firms focusing on technological innovation.Before switching to Redis Enterprise, the company relied on a monolithic architecture nearly a decade old. The IT team decided to transition to a microservices architecture to improve scalability and allow for specific parts of its content management system to be updated independently. The transition—which took close to a year and was completed in November, 2018—encouraged the team to look for new highly available solutions. The team eventually settled on Redis, MongoDB, and Couchbase, each powering a different use case.Redis stood out, though, for being super easy to set up and maintain. Redis Enterprise removes business operations from the forefront of the team’s minds. Letting them focus on innovation instead of “How do we operate?”The team was thrilled and surprised that Redis Enterprise installation was completed in only two days, just as promised. They quickly learned that training developers—even those who had never used Redis before—was similarly fast and intuitive. The ability for a developer to go from never-heard-of-Redis to actually being hands-on in the code and contributing to the project was a huge differentiator, according to the team.Because of Redis Enterprise’s low maintenance requirements, operations issues are no longer top of mind for the team, allowing it to streamline client onboarding and profitably serve less-lucrative clients it would once have had to pass over.It’s a big change for a massive application. The company’s content management system pushes out more than 200,000 publishing transactions an hour. The repository contains millions of snippets of personalized content for each client’s brand.Redis Enterprise powers the queueing system that sends that content out to customers. With just two 3-node clusters across a pair of data centers, the company processes roughly 1,000 ops/sec, while also syncing communication between the company’s publishing and deployment servers. Since transitioning to Redis Enterprise, the company has enjoyed dramatic performance improvements. Throughput is 4 times faster, and system uptime jumped from 70% to almost 99%. And Redis Enterprise has also enabled the company to scale up more efficiently by minimizing its database footprint—a single 3-node cluster supports as many as 200 web application servers at any time.Critically, the performance improvements are also helping speed the company’s time to market, because it’s able to publish content faster. When it needs to get a marketing campaign or banner out there right away, the tool no longer slows the team down. It helps them get there faster.One member of the firm’s team called Redis Enterprise a “fire hose” because it’s always putting out fires. But even as the company relies on Redis Enterprise to solve today’s problems and help streamline its content management system, it plans to standardize on Redis Enterprise as its primary enterprise caching tool, replacing Oracle Coherence. The team is also hoping to develop a chatbot application using Redis Enterprise.Looking to learn more about microservices architectures? Check out how Mutualink’s microservice architecture helps save lives and how online travel agency HolidayMe uses Redis as a primary database."
342,https://redis.com/blog/bullet-proofing-lua-scripts-in-redispy/,Bullet-Proofing Lua Scripts in RedisPy,"February 19, 2020",Redis,"Lua scripting is a hugely powerful feature of Redis. At the same time, though, Lua scripting can be tricky to “get right.”There are many ways to run a script that works most of the time, which can also be articulated as a script that fails some of the time. Let’s take a look at a Lua scripting corner case in Python with RedisPy where your script can fail despite doing what looks correct. Of course, at the end, we’ll show you the bullet-proof way to do it when you need everything to always work.We need to review Redis’ Lua scripting engine and how you run a script. First and most basic is EVAL. This command accepts the full Lua source followed by the keys count, keys, and finally any arguments that are passed into the script. Sending the source code over and over is a waste of bandwidth, so SCRIPT LOAD lets you send the Lua source once and receive a SHA-1 digest that you can use later to identify and run this script with EVALSHA. This command functions just like EVAL, but points to the SHA-1 digest. Redis uses an entirely separate, keyspace-less cache for the scripts: both EVAL and SCRIPT LOAD take the source code, compile it, and store the byte code representation in the cache, but EVAL first checks the script cache so it won’t trigger recompilation of the script if it is already stored in the script cache.The problem with EVALSHA is that if you try to run a script that doesn’t exist in the script cache, you’ll get the following error:(error) NOSCRIPT No matching script. Please use EVAL.If you don’t want to immediately run the script, you can use the SCRIPT EXISTS command to see if a given SHA-1 digest represents a cached script. In effect, an application that uses Lua scripting in Redis always needs to be ready to supply Redis with the full Lua source at any given time, and there is no way to ensure that a given script is never evicted from the script cache.RedisPy is a full featured client library that improves Lua’s ergonomics in Redis—let’s see what it does to make your life easier. For one thing, you don’t have to manage SHA-1 hashes or loading scripts on your own most of the time—RedisPy abstracts this away in a clean, Pythonic style. Let’s look at it from the Python REPL:>> import redis
>>> r = redis.Redis()
>>> mylua = """"""
... return ""hello world""
... """"""
>>> hello = r.register_script(mylua)
>>> hello()
b'hello world'What is happening here from the Redis perspective? If you run MONITOR while inputing this script line-by-line, you’ll see activity only when you invoke hello(). The output should look something like this (if your script cache is empty):""EVALSHA"" ""0a4e337ee79a86930eb054981e3acc8a22d0674d"" ""0""
""SCRIPT"" ""LOAD"" ""nreturn ""hello world""n""
""EVALSHA"" ""0a4e337ee79a86930eb054981e3acc8a22d0674d"" ""0""This shows what RedisPy is doing: trying to run the script, getting a NOSCRIPT error, loading the script, then running EVALSHA again. If you run hello() again, it will show only a single EVALSHA. So, you can see this abstraction is saving you a little bit of code and making your code more readable.RedisPy also gives you an abstraction around pipelining and transactions. You create a pipeline object, then just perform your operations normally. To demonstrate, first, I’m going to run SCRIPT FLUSH (which empties the entire script cache) from redis-cli then back to the REPL:>> import redis
>>> mylua = """"""
... return ""hello world""
... """"""
>>> r = redis.Redis()
>>> mypipeline = r.pipeline()
>>> mypipeline.get('foo')
Pipeline<ConnectionPool<Connection<host=localhost,port=6379,db=0>>>
>>> hello = mypipeline.register_script(mylua)
>>> hello()
Pipeline<ConnectionPool<Connection<host=localhost,port=6379,db=0>>>
>>> mypipeline.execute()
[None, b'hello world']A possible point of confusion is that pipelines and transactions are two very different things, yet RedisPy uses the same construct for both: despite the word “pipeline,” the commands in the script above will run as a MULTI/EXEC transaction. If you were to change r.pipeline() to r.pipeline(transaction=False), you would have a pipeline. (That is not how I would have chosen to do this, but countless lines of Python have already worn this path.)So, what is happening on the Redis side? When you press Return on mypipeline.execute(), the following will be shown by MONITOR:""SCRIPT"" ""EXISTS"" ""0a4e337ee79a86930eb054981e3acc8a22d0674d""
""SCRIPT"" ""LOAD"" ""nreturn ""hello world""n""
""MULTI""
""GET"" ""foo""
""EVALSHA"" ""0a4e337ee79a86930eb054981e3acc8a22d0674d"" ""0""
""EXEC""Something more sophisticated is going on here than in our previous example. Indeed, if you look at the source code to RedisPy, it’s doing something quite smart. Because in either a pipeline or a transaction you can’t get a reply back until it’s all over, RedisPy is checking for the existence (SCRIPT EXISTS) of scripts that will be executed before it starts the transaction. If the script doesn’t yet exist in the cache, then RedisPy uses SCRIPT LOAD to cache the script for later execution. Only after this dance is finished will it start the transaction. In the middle of the transaction, we’re using EVALSHA to invoke the script.Now, we all know that many, many clients can connect to a single Redis server. While Redis is single threaded for the most part, it uses an event loop, so it’s doing only one thing at a time—but that thing may not be your one thing. Outside of a MULTI/EXEC transaction or commands inside a Lua script, there is no guarantee that commands sent rapidly by your application will be executed atomically by Redis. Other connected clients could sneak in commands between what is sent by your application.Let’s take a look at what is really going on with the script check/load sequence in RedisPy:Note that at steps 3 and 6 Redis is “doing other stuff.” That could be just an idle loop or it could be serving other commands to other applications. The place where Redis cannot do anything else is during step 8. Now, steps 3 and 6 typically consume a very short amount of time—micro- or milliseconds—but still a non-zero value.It is possible that other ‘things’ can happen during this process since Redis is doing other stuff.For example, imagine you’re running a chaotic script connected to the same Redis server. This script, for some unknown reason, is running a tight loop of SCRIPT FLUSH commands. So in our stepwise example above, it’s possible that the “other stuff” mentioned in step 6 could be a SCRIPT FLUSH. Despite just doing a SCRIPT LOAD, the MULTI/EXEC block could start running with a missing script.A chaotic SCRIPT FLUSHing script is not the only scenario where this could be an issue. Imagine that you are running a great many Lua scripts (not usually a best practice, but it does happen) and are tight on space in your script cache. Then another application comes along and does a similar process and SCRIPT LOADs its scripts and evicts your script from the cache before your MULTI/EXEC runs. You’re in the same situation as the tight-loop SCRIPT FLUSH.It’s important to note that RedisPy is doing nothing wrong here. There is no mechanism to use EVALSHA in a MULTI/EXEC transaction and be sure that the script exists before starting.Nevertheless, the end result of all these scenarios is that you’re left with a Lua script that sometimes works and sometimes fails, which is pretty scary. However, this doesn’t mean that you should never run Lua in a transaction. There is a bullet-proof way to do this—incur the bandwidth penalty and run EVAL with the full Lua source inside the transaction.Let’s see how this approach looks in Python:>> mypipeline = r.pipeline()
>>> mypipeline.get('foo')
Pipeline<ConnectionPool<Connection<host=localhost,port=6379,db=0>>>
>>> mypipeline.eval(""""""
... return ""hello world""
... """""", 0)
Pipeline<ConnectionPool<Connection<host=localhost,port=6379,db=0>>>
>>> mypipeline.execute()
[None, b'hello world']One note: if you’re running the same Lua script multiple times within a single transaction, you can safely use EVALSHA on subsequent invocations of the same script within the MULTI/EXEC block. Remember, EVAL will still cache the script. This will save you a little bandwidth in what is, for sure, a very unusual circumstance.Up until this point we’ve talked about MULTI/EXEC transactions—what about Lua scripts within a pipeline? In that case the risk is perhaps even more pronounced. Remember, pipelines do not provide any atomic guarantees so there are more opportunities for chaotic SCRIPT FLUSHing or cache fills and evictions to occur.So, should you avoid using the ergonomic register_script function of RedisPy with transactions or pipelines?I wouldn’t go that far. The real answer, though unsatisfying, is that it depends.If your Lua script is doing something mission-critical, then yes, bite the bullet, accept the bandwidth penalty, and stick with good old EVAL. But if the script is doing something you can account for later by looking at your transaction or pipeline response, then go for it. And remember, outside these pipeline and transaction scenarios, the register_script provides tons of value to your code in a way that isn’t really risky."
343,https://redis.com/blog/rediscover-redis-at-redisconf-2020/,Important Announcement: RedisConf 2020 is Now a Virtual Event,"March 6, 2020",Mike Kwon,"Note: Originally published on February 21, 2020, this blog post previously held information about RedisConf 2020, planned for May 12–14 in San Francisco. Due to an abundance of caution surrounding the novel coronavirus (COVID-19), we have decided to transform the conference into a virtual event. Read on for more information.With the health and wellbeing of the Redis community top of mind, Redis has decided to present RedisConf 2020 as a virtual event due to the growing concern around the novel coronavirus (COVID-19).RedisConf 2020 Takeaway will deliver the same inspiring content already in the works—including keynotes, training, breakout sessions, access to Redis experts, and more.We are still working through many of the details for the virtual event. As we design a valuable virtual experience over the coming weeks, we will update information here, along with more details in our FAQ.If you have already registered for RedisConf 2020, your registration fee will automatically be refunded within the next few days. You will also automatically be registered for RedisConf 2020 Takeaway.While we are disappointed that we will not be together in-person with our community this year, we are excited to continue using technology to bring our community together for a shared experience, and look forward to hosting you at RedisConf 2020 Takeaway.For questions or concerns, please reach out to us at redisconf@redis.com.We understand that you probably have a lot of questions about what RedisConf 2020 Takeaway will look like. We’ve compiled a list of answers here, and stay tuned to the FAQ page on our website for the latest information.What will happen to my training day and/or conference pass?What is the cost to join RedisConf 2020 Takeaway?Will Training Day be offered virtually?What’s on the schedule for RedisConf 2020 Takeaway?When will RedisConf 2020 Takeaway take place?Who do I contact for more information?"
344,https://redis.com/blog/redis-powered-microservices-architecture-proves-its-worth-in-two-very-different-projects-for-z3-works/,Redis-Powered Microservices Architecture Proves Its Worth for Z3 Works,"March 10, 2020",Miguel Allende,"(As organizations look to modernize their applications, many are turning to a microservices architecture to deconstruct their legacy apps into collections of loosely coupled services. This profound change inspired us to reach out to Redis users in various stages of this journey to microservices architectures. We are telling their microservices stories in a series of blog posts, which began in late 2019.)If you thought microservices architectures could be used for only a limited set of projects, a quick chat with the folks at Z3 Works might convince you of the approach’s versatility. At the Brazilian software agency, Co-Founder Marcelo Nozari’s 20-person team designs and builds software for projects across a variety of industries, then passes off the code to his clients to deploy.Marcelo’s team helps a home-security company collect data from its video cameras, doorbell activity, and more, for example, providing real-time notifications to its clients. And Z3 Works’ newest project is helping a retailer transition its monolithic applications to a microservices architecture. While each project is in a different stage of production, microservices play a critical role in ensuring that the applications run effectively and in real-time.For Marcelo, a microservices architecture means creating clear, separate services to help better manage the project. “Whenever you have smaller projects and smaller services, microservices makes it easier for you to deploy and avoid crashing everything else,” Marcelo says. The microservices architecture allows Marcelo’s team to work on small components and update and fix code as quickly and as often as needed.To show the value Z3 is getting from its microservices approach, let’s take a closer look at a couple of very different projects:If your family is on a month-long vacation, chances are you’ll want to check on your home every so often. That can create a challenge for home-security companies. With so many different pieces in a home security system—from doorbells to security cameras to motion detectors to alarms—these companies need to compile all interactions in a single, up-to-date application.At the same time, it’s increasingly important for security companies to provide real-time notifications to their clients. That lets clients learn about incidents as trivial as a package delivery while they’re on vacation—so they can ask their neighbors to bring the box in to prevent theft—as well as alerting them to critical threats such as a potential burglary.Z3 Works partnered with a home security company that needed to manage all this data. The application Marcelo’s team built pulls data from 3–5 different kinds of data sources (including more than 7,000 different cameras!) into a Redis database running in the cloud. The system uses Redis as a caching layer, as speed is critical—customers expect an instant notification of any activity at their homes. “For doorbell pushes, for example, Redis makes it really fast to get this image from the cache and send it to the apps as a notification for users,” Marcelo says. Redis Pub/Sub also acts as a message broker for image requests, communicating between software functions responsible for fetching and those deploying the images.The home-security project has been in production for about a year, Marcelo says, processing up to a terabyte of video every day and ingesting up to 3 million pieces of data per hour into a 50 GB database. Redis is a critical component of the home security vendor’s microservices architecture application—it uses Redis Pub/Sub to communicate between different services of the application and hold the tokens and counters for the vendor APIs, which are central to the data integrations, Marcelo says. If Redis went down, the application would no longer be able to process the data and deliver it to the user, who wouldn’t be able to see any new footage.By using a microservices architecture, the application avoids bottlenecks and the team can easily fix code and small components as needed. Redis’ speed, high availability, and fast failover makes it critical for implementing a microservices architecture. Plus, Z3 Works can scale the client’s application as needed, making it more effective in the long run.Marcelo’s team recently started working with a major retailer to transition its monolithic architecture to microservices as part of an overall migration to the cloud. This project is still in the planning stages, Marcelo says, but the new architecture will combine some 10 different services.With Redis-based microservices architecture projects spanning multiple industries, Z3 Works makes it clear that the combination is a powerful approach to turbocharge a wide variety of applications and use cases.Looking to learn more about microservices? Check out how Mutualink uses Redis for a life-saving microservices architecture, and hear Redis Developer Advocates Kyle Davis and Loris Cro discuss their new free e-book, “Redis Microservices for Dummies,” on The New Stack podcast."
345,https://redis.com/blog/redisearch-version-1-6-adds-features-improves-performance/,"RediSearch Version 1.6 Adds Features, Improves Performance","March 12, 2020",Pieter Cailliau,"The popular RediSearch module was designed to extend Redis’s capabilities by adding a secondary index with super-fast full-text search capabilities. RediSearch has been extremely well received, collecting more than 150,000 Docker pulls, 2,000 GitHub stars, 230 forks, and 10 drivers in some 10 different languages. And customers are already taking advantage—see this video on how GAP scaled 100X using Redis.Today, we’re proud to announce RediSearch 1.6, which refactors the original module to boost performance and adds some important new functionality—including aliasing, a low-level API, and improved query validation—as well as making the Fork garbage collection the module’s default—all designed to make development with RediSearch even more powerful and convenient.We originally created RediSearch to support a single (or limited in amount) secondary index that would scale by scaling Redis. Over the years, Redis users have turned to RediSearch for a wide variety of use cases that frequently delete and recreate the same index while querying it or that create thousands of small, short-lived indices. In RediSearch 1.6, we refactored our codebase to better support these use cases. In doing so, we added better query validation and improved performance by up to 73%! We also added two major new features: Aliasing lets Redis refer to an existing index and easily switch to another index, while a new low-level API makes RediSearch available as a library that can be used by other Redis modules written in C or Rust. That means developers won’t have to learn additional search query languages to use any Redis module. With this library, modules can add secondary indexing capabilities easily. RedisGraph 2.0 is the first generally available Redis module to exploit this feature by offering full-text search capabilities. The full list of added features in RediSearch v.1.6 can be found on GitHub.Based on testing with our full-text search benchmark (FTSB), RediSearch 1.6 brings significant performance advantages compared to version 1.4. Specifically, RediSearch 1.6 increased simple full-text search throughput by up to 63%, while cutting latency (q50) by up to 64%. For aggregate queries, throughput increased from 15% to 64%.(For more on the performance improvements in RediSearch 1.6, see our blog post on RediSearch 1.6 Boosts Performance Up to 64%.)One of RediSearch’s most powerful capabilities is that every update to a document atomically updates the index. Unlike other, Lucene-based search engines, the RediSearch index does not have to play catch up with the data. In other words, you always read your own writes.In certain applications, however, it’s more efficient or convenient to reload an entire index rather than tracking the differences between two bulk loads. Updating your applications to connect to the newly loaded index in runtime without downtime is almost impossible, so we introduced aliasing to make RediSearch even more flexible and powerful. Significantly, ElasticSearch users will find the addition of aliasing to RediSearch will ease their migration path to Redis, as shown in the example below.Aliasing lets you redirect application queries from a logical index name to a physical underlying index. Updating an alias allows you to transparently redirect your application queries to another physical index, without any downtime!This simple example explains how aliasing works with RediSearch commands:redis:6379> FT.CREATE idxA SCHEMA title TEXT
OK
redis:6379> FT.ADD idxA a:doc1 1.0 FIELDS title ""plump fiction""
OK
redis:6379> FT.ALIASADD movies idxA
OK
redis:6379> FT.SEARCH movies ""@title:fiction""
1) (integer) 1
2) ""a:doc1""
3) 1) ""title""
2) ""plump fiction""
redis:6379> FT.CREATE idxB SCHEMA title TEXT
OK
redis:6379> FT.ADD idxB b:doc1 1.0 FIELDS title ""pulp fiction""
OK
redis:6379> FT.ALIASUPDATE movies idxB
OK
redis:6379> FT.SEARCH movies ""@title:fiction""
1) (integer) 1
2) ""b:doc1""
3) 1) ""title""
2) ""pulp fiction""Going forward, there is an opportunity to enhance this feature with the ability to alias more than one index, for example, to address querying two indices responsible for a distinct set of documents.Since the creation of Redis’ module API, Redis and the Redis community have created a large set of modules. Some add a whole new database model to Redis, some let you execute code with the data, and others add new data structures to Redis. We noticed, however, that several modules started building their own proprietary way of indexing data. In RedisGraph, for example, indices on properties of nodes in the graph are maintained in a ziplist and RedisTimeSeries uses sorted sets for querying time series data that has certain label conditions.These implementations provide basic search functionality, but for RediSearch 1.6 we wanted to remove code duplication and enhance search functionality in key use cases with a common query language. In graph databases, for example, it’s common to do a fuzzy search on properties of nodes to enable a graph-aided search. Time-series use cases, meanwhile, often involve all time series where a label matches a certain prefix. Other Redis modules could benefit from secondary indexing support. Imagine what you could do if RedisJSON had full-text search capabilities!So for RedisSearch 1.6 we created a low-level API that can be consumed by other Redis modules. RedisGraph v2.0 is the first generally available Redis module that uses this module. Graph-aided search lets you find nodes in a graph for which properties match a full-text search query and rank them based on the connections these nodes have in the graph.LinkedIn’s search functionality is a great example of graph-aided search: People and companies that are closer to you in your network show up higher in your search results. We continue to work on low-level API development for RedisTimeSeries, and you can already try out the preview version of RedisJSON 2.0 with RediSearch embedded.Like most inverted-index based search engines, when you delete or update a document, RediSearch effectively marks the document as to be deleted. A garbage collection process runs regularly to reclaim the memory used to store deleted documents. Marking the document as removed and making the physical removal asynchronous reduces the command execution time while preserving the correctness of queries.In RediSearch 1.4, the default behavior was to lock the main thread, which introduced spikes in read latencies during garbage collection. To overcome this, RediSearch 1.4 offered an optional Fork GC that runs in parallel with the main query process, allowing uninterrupted scanning for any deleted documents. This approach provides superior performance for high-traffic environments, where many queries are issued and/or many writes are performed, so we have made this behavior the default in RediSearch 1.6.You can see the benefits of Fork GC in this two-phase test. In phase one, we insert 10 million new documents. After this phase we drop the index. In phase two, the traffic is mixed: inserting and updating 5 million documents. For each write operation however, there is also an update operation (a 50% update rate), so this also totals 10 million operations.The chart clearly shows how legacy garbage collection performance quickly degrades when there is a 50% update rate. In general, we have observed an 8% improvement in low update rates and up to a 70% performance improvement at high update rates with the new Fork garbage collection:Finally, RediSearch 1.6 also boasts improved query validation. If a query is determined to contain logical or syntax errors, an appropriate error message is returned to the user, rather than letting the query execute with no results. RediSearch 1.6 also returns an error when an unrecognized keyword is submitted. This query validation will enhance the developer experience and lower the RediSearch learning curve. Also note that RediSearch 1.6 is supported by RedisInsight, our recently announced browser-based management interface for your Redis deployment.Looking forward, we have many interesting features on the roadmap, such as support for polygon search and creating higher parallelism for read queries on a single shard. That will help us achieve higher throughput for aggregation queries without having to increase the number of shards.First up, however, is schema-based RediSearch. In the current version of RediSearch, if you want the index to be in sync with the hashes representing your document, you have to add the documents by issuing either an ADD or an ADDHASH command. If updates to the hash occur afterwards without these commands being executed, they won’t be reflected in the index. With schema-based RediSearch, you will be able to define rules for which hashes need to be indexed automatically. On each write to a hash, RediSearch will either synchronously or asynchronously update the index of hashes that match the rules.Even more important than the new features, of course, Version 1.6 is the fastest RediSearch ever (learn more about RediSearch 1.6 performance improvements here.) Your applications will experience more consistent latencies, with fewer peaks, across all types of queries.Finally, we want to thank the Redis community members who helped make RediSearch 1.6 more robust by spotting bugs in earlier release candidates."
346,https://redis.com/blog/redisearch-1-6-boosts-performance-up-to-64/,RediSearch 1.6 Boosts Performance Up to 64%,"March 12, 2020",Pieter Cailliau and Filipe Oliveira,"The newly introduced RediSearch 1.6 adds some important new functionality, including aliasing, a low-level API, and improved query validation, as well as making Fork garbage collection the module’s default. Even more important, though, the original code in RediSearch 1.6 has been refactored to significantly boost performance. The improved performance leads to a better user experience as applications can be more reactive than ever with search.(For more on the new features in RediSearch 1.6, see our blog post on Announcing RediSearch Version 1.6.)Just how big is the performance bump in RediSearch 1.6? That’s what this post is all about, and we’ll get to the results in a moment. But in order to fully understand the performance issues, we needed a way to properly compare database search speed.To compare RediSearch version 1.6 against the previous 1.4 release, we created a full-text search benchmark. FTSB (licensed under MIT) was designed specifically to help developers, system architects, and DevOps practitioners find the best search engine for their workloads. It allows us to generate datasets and then benchmark read and write performance on common search queries. It supports two data sets with different characteristics:To achieve reproducible results, the data to be inserted and the queries to be run are pre-generated, and native Go clients are used wherever possible to connect to each database. In order to design a common benchmarking suite, all latency results include the network round-trip time (RTT) to measure duration to and from the client, so that we can properly compare additional databases.We encourage you to run these benchmarks for yourself to independently verify the results on your hardware and datasets of choice. More importantly, we’re looking for feedback and extension requests to the tool with further use cases (e.g. ecommerce, JSON data, geodata, etc.) and problem domains. If you are the developer of a full-text search engine and want to include your database in the FTSB, feel free to open an issue or a pull request to add it.We ran the performance benchmarks on Amazon Web Services instances, provisioned through our benchmark testing infrastructure. Both the benchmarking client and database servers were running on separate c5.24xlarge instances. The tests were executed on a single node Redis Enterprise cluster setup, version 5.4.10-22, with data distributed across 10 master shards.In addition to this primary benchmark/performance analysis scenario, we also enabled running baseline benchmarks on network, memory, CPU, and I/O, in order to understand the underlying network and virtual machine characteristics. We represent our benchmarking infrastructure as code so that it is easily replicable and stable.The table below displays the size of the datasets we used in this benchmark. It also shows the overall indexing rate for each dataset. You can see that RediSearch 1.6 did not degrade performance on ingestion:FSTB currently supports three full-text search queries, as shown in the chart below (remember that the latency results include RTT). Note that we plan to extend this benchmark suite with a more diversified set of read queries.We can observe in the table above that RediSearch 1.6’s improved q99 makes it more predictable, with fewer latency peaks.The chart below shows that compared to RediSearch 1.4, RediSearch 1.6 increases throughput by 48% to 63%.Similarly, the latency (q50) drops by 51% to 64%.In addition to the simple search queries, we also added a set of aggregation queries. The full details of what each query does can be found in our benchmark repository (again, remember that the latency results include RTT):Here too, RediSearch 1.6 improves performance compared to RediSearch 1.4. The throughput increases by 15% to 64%:Similarly, the latency (q50) decreases by 17% to 73%:Put it all together, and you can see that RediSearch 1.6 brings significant performance advantages compared to version 1.4. Specifically, RediSearch 1.6 increased simple full-text search throughput by up to 63% while cutting latency (q50) by up to 64%. For aggregate queries, throughput increased from 15% to 64%.Almost as important, the introduction of full-text search benchmarking helped us to constantly monitor the performance between different versions of RediSearch and it has proven to be of extreme value in eliminating performance bottlenecks and hardening our solution."
347,https://redis.com/blog/redistimeseries-version-1-2-is-here/,RedisTimeSeries Version 1.2 Is Here!,"March 12, 2020",Pieter Cailliau,"When you think about it, “life is a time series.”So maybe it shouldn’t be surprising that even though the RedisTimeSeries module—which simplifies the use of Redis for time-series use cases like IoT, application monitoring, and telemetry—has been generally available only since June of 2019, we are already seeing a large number of interesting use cases. For example, the New York Times is using RedisTimeSeries in its internal Photon project, as is Amazon Web Service’s IoT Greengrass project (see the GitHub repo here).But we’re not here to rest on our laurels. Instead, we’re announcing general availability of a major new version of the module, RedisTimeSeries 1.2, which debuts with a long list of powerful new features. But given the widespread interest, our main goal for RedisTimeSeries 1.2 was to increase usability without degrading performance. Because Redis is an in-memory database, storing a large volume of time-series data in DRAM can become expensive. So in version 1.2 we added compression that reduces its memory requirements up to 90% in most scenarios—in theory, it could compress your time series data by up to 98%! We made our API more consistent and intuitive, and removed redundant response data to further improve performance. Together, the API changes and the compression improves read performance up to 70%. Put it all together and the new version of RedisTimeSeries could save users serious money on infrastructure costs without affecting performance.(Check out our companion blog post on RedisTimeSeries 1.2 Benchmarks for more info on how ingestion throughput is independent of the number of time series in the dataset, of the compression, and of the number of samples in a time series.)But let’s start with how the compression works, because it’s kind of cool.A time series in RedisTimeSeries consists of a linked list of chunks that each contain a fixed number of samples. A sample is a tuple of a timestamp and a value representing a measurement at a specific time. Time is represented as a timestamp and the value as a float (double-precision floating point format). The chunks themselves are indexed in a Radix tree. You can read more about the internals here.Uncompressed, the timestamp and the value each consume 8 bytes (or 16 bytes/128 bits in total) for each sample. For the timestamps we use double-delta compression and for the values we use XOR (“Exclusive or”) compression. Both techniques are based upon the Facebook Gorilla paper and are documented in our source code, but we’ll explain how it works below.In many time-series use cases, samples are collected at fixed intervals. Imagine we are collecting measurements from a temperature sensor every 5 seconds. In milliseconds, the delta between two consecutive samples would be 5,000. Because the intervals remain constant, however, the delta between two deltas—the double-delta—would be 0.Occasionally data will arrive at second 6 or second 4, but usually this is the exception.  Rather than storing this ΔΔ (double-delta) in its entirety in 64 bits, variable-length encoding is used according to the following pseudo code:If ΔΔ is zero, then store a single ‘0’ bit
Else If ΔΔ is between [-63, 64], store ‘10’ followed by the value (7 bits)
Else If ΔΔ is between [-512,511], store ‘110’ followed by the value (10 bits)
Else if ΔΔ is between [-4096,4095], store ‘1110’ followed by the value (13 bits)
Else if ΔΔ is between [-32768,32767], store ‘11110’ followed by the value (16 bits)
Else store ‘11111’ followed by D using 64 bitsThe basis of the value compression is the assumption that the difference between consecutive values is typically small and happens gradually instead of abruptly. In addition, floats are inherently wasteful and contain many repeating zeros that can be eliminated. So, when two consecutive values are XORed, only a few meaningful bits will be present in the result. For simplicity, the example below uses a single-precision double—RedisTimeSeries uses double-precision doubles:You can see that the XOR operation between the first two equal numbers is, obviously, 0. But also there are often a number of leading and trailing 0s around the meaningful XOR value. The variable-length encoding of the XOR differences of the values removes leading and trailing zeros.There is also a control bit that can further reduce the number of bits consumed by a sample. If the block of meaningful bits falls within the block of the previous value’s meaningful bits, i.e., there are at least as many leading zeros and as many trailing zeros as in the previous value, then the number of leading and trailing zeros of the previous sample can be used:If XOR is zero (same value)
store single ‘0’ bit 
Else 
calculate the number of leading and trailing zeros in the XOR, 
store bit ‘1’ followed by
If the block of meaningful bits falls within the block of previous meaningful bits, 
store control bit `0`
Else store control bit `1`, 
store the length of the number of leading zeros in the next 5 bits, 
store the length of the meaningful XOR value in the next 6 bits. 
Finally store the meaningful bits of the XOR value.So how much memory reduction can this technique give you? Not surprisingly, the actual reduction depends on your use case, but we noticed a 94% reduction in our benchmark datasets. According to page six of Facebook’s Gorilla paper, each sample consumes an average of 1.37 bytes, compared to 16 bytes. This results in a 90% memory reduction for the most common use cases.The theoretical limit is reached when the double-delta is 0 and the XOR is also 0. In that case, each sample uses 2 bits instead of 128, resulting in a memory reduction of 98.4%! In the worst case scenario, a sample consumes 145 bits: 69 bits for the timestamp and 76 bits for the value. (For this extreme case to happen, the double-delta would have to be over MAX_32 and float values would be extremely complex and different. Because we don’t want to exclude even extreme use cases, you can still use the `UNCOMPRESSED` option when you create a time series.)Time-series use cases are mainly append-only, which allows for optimizations such as the compression described above. But with this compression approach, inserting a sample with a timestamp earlier than the last sample will not only introduce more complex logic and consume more CPU cycles but also require more complex memory management.That’s why with RedisTimeSeries 1.2, we made the API more strict and no longer allow clients to rewrite the last sample of the time series. That might sound limiting, but most of the changes are intuitive and actually enhance the developer experience.As a consequence, the automatic calculation of the aggregated sample for a downsampled time series now happens at the source time series. Only when the aggregation time window has passed, will the sample representing the aggregation be written to the downsampled time series. This means that writes happen less often to the downsampled time series, further enhancing ingestion performance.With downsampling, you can reduce your memory consumption by aggregating samples that lay further in the past. But you can also model counters for time windows of fixed sizes, for example the number of website pageviews per hour. You can construct these counters easily with RedisTimeSeries. In the source time series you use TS.ADD to add a sample with the value you would like the counter to be incremented and a downsampling rule with a sum aggregation over the fixed window size:redis:6379> TS.CREATE ts RETENTION 20000
OK
redis:6379> TS.CREATE counter
OK
redis:6379> TS.CREATERULE ts counter AGGREGATION sum 5000
OK
redis:6379> TS.ADD ts * 5
(integer) 1580394077750
redis:6379> TS.ADD ts * 2
(integer) 1580394079257
redis:6379> TS.ADD ts * 3
(integer) 1580394085716
redis:6379> TS.RANGE counter - +
1) 1) (integer) 1580394075000
   2) ""7""
redis:6379> TS.ADD ts * 1
(integer) 1580394095233
redis:6379> TS.RANGE counter - +
1) 1) (integer) 1580394075000
   2) ""7""
2) 1) (integer) 1580394085000
   2) ""3""
redis:6379> TS.RANGE ts - +
1) 1) (integer) 1580394077750
   2) ""5""
2) 1) (integer) 1580394079257
   2) ""2""
3) 1) (integer) 1580394085716
   2) ""3""
4) 1) (integer) 1580394095233
   2) ""1""
redis:6379>In both cases the downsampled series will have the exact counter you’re looking for.The addition of Gorilla compression and API enhancements have dramatically reduced memory consumption. (For more information on how this works, see the companion blog post on RedisTimeSeries 1.2 Benchmarks.)But we’re not done yet. Looking forward, we plan to replace the RedisTimeSeries proprietary secondary indexing using sorted sets with the new low-level API used in RediSearch. This will let users do full-text search queries on the labels linked the time series using a rich and proven query language. We are also working to complete our API to be in line with other Redis data structures by introducing the TS.REVRANGE and TS.MREVRANGE commands.Most Redis modules focus on solving one particular problem extremely well and extremely quickly—combining them enables a large set of interesting new use cases. We are currently exploring combining RedisTimeSeries and RedisAI to do real-time anomaly detection and unsupervised learning. Lastly, we are working to expose a low-level API for RedisGears that will allow you to do cross-time series aggregation in the most performant way."
348,https://redis.com/blog/business-continuity-during-covid-19/,Redis Labs Business Continuity During COVID-19,"March 13, 2020",Ofer Bengal,"Earlier today, I sent this note to Redis’ customers, partners, and other members of the Redis community:To the Redis community:As the coronavirus (COVID-19) continues to disrupt communities throughout the world in unprecedented ways, we at Redis would like to take this opportunity to reiterate our commitment to the health and safety of the global Redis community, including our customers, developers, partners, employees, and their families. During these uncertain times, we are taking all necessary steps to ensure reliable service and support to our customers and partners. We recognize you have entrusted us to power your mission-critical applications and take this responsibility very seriously.Over the past eight years, we have developed, refined, and extended our business continuity plans to ensure the seamless and uninterrupted delivery of service and support to our 7,700 customers and partners around the world. The following measures are in place to ensure your continued success:Redis is a distributed global company, with five offices on three continents and employees who work remotely around the world. The art of virtual collaboration and remote work is in our DNA. In response to COVID-19, we have decided to temporarily move to a fully remote work environment. Our connectivity and workforce productivity tools have been thoroughly tested to ensure employees have secure and continuous access to our systems, data, and team members. We are here to serve you, and confident our service levels and productivity will not be impacted during this period.Redis Enterprise Cloud, our fully managed database-as-a-service offering, is designed to deliver the highest levels of resiliency and availability. Our globally distributed infrastructure and DevOps team ensures uninterrupted 24x7x365 operations. Despite more than 100 data center outages and 3,000 node-failure events, Redis Enterprise Cloud has never experienced any notable data loss (when all high-availability features were enabled) over the lifetime of the service. With our Active-Active SLA, we can confidently deliver 99.999% uptime—and that promise remains unchanged.To do our part to contain the coronavirus, we made the logical decision to move our annual RedisConf to a virtual event: RedisConf 2020 Takeaway. We are committed to deliver an amazing online experience for our community to rediscover what Redis can do for their applications and businesses.Thank you for your business and trust. We recognize these are turbulent times for many people in our community and we want you to know the entire Redis team is working hard and creatively to ensure your Redis deployment isn’t something you have to worry about.Warm regards,Ofer Bengal, CEO of Redis"
349,https://redis.com/blog/enter-the-rediscover-redis-competition/,Enter the Rediscover Redis Competition!,"March 24, 2020",Britiana Andrade,"As we draw ever closer to RedisConf 2020 Takeaway starting May 12, we want to hear from you about the cool things you use Redis for. The Redis community is one of the most inspiring and creative groups of developers in the world—and we want to showcase the community’s achievements. That’s why we’re excited to announce the Rediscover Redis Competition!We’re challenging members of the Redis community to show off how they have rediscovered Redis. Almost every developer knows how to use Redis as a cache to build top-performing applications. But not everyone is leveraging Redis to its fullest potential.We’re collecting stories from the Redis community to highlight examples of Redis’ versatility and demonstrate how Redis is behind some of our most successful applications. And you can get some pretty sweet prizes from participating—the top three submissions get their projects featured in the keynote at RedisConf Takeaway and a Valve Index VR Kit (a $999 value)!So what do you have to do? With this year’s Rediscover Redis theme at RedisConf Takeaway, we want you to submit your project explaining how you have “rediscovered” Redis and taken advantage of more of its limitless potential.While you can submit your project in a variety of forms, we’re encouraging video submissions—with every valid submission we receive, Redis will donate $100 to the COVID-19 Solidarity Response Fund, which supports the World Health Organization’s work to ensure patients get proper care, frontline workers get essential supplies, and development of a vaccine and treatments can be accelerated.Every Rediscover Redis Competition submission must include a few key elements:Here’s what we’re grading on:Note: Redis reserves the right to use submitted videos for marketing purposesSubmissions are due by end of day Friday, May 8, 2020. Winners will be announced at the RedisConf keynote on May 12.Wondering what would make a great video? Check out the winning Redis Day Bangalore Hackathon projects. Redis Geeks were challenged to demonstrate how to use Redis beyond caching in three categories: Event-Driven Architecture; Redis Modules; and Integrations, Plug-ins, Clients, and Frameworks.The three winning projects were:That’s all there is to it! We’re excited to see your projects and hear how you’re using Redis beyond caching.Even if you don’t submit a video, don’t miss RedisConf 2020 Takeaway, starting May 12. RedisConf is a learning conference for developers and cloud professionals from the makers of the world’s most-loved database. It offers the perfect opportunity to dive into the latest innovations and trends in data platforms, share your ideas, learn from valuable experiences of Redis users across the globe, and get hands-on training. Register for free here."
350,https://redis.com/blog/digging-into-redis-enterprise-on-google-cloud/,Digging into Redis Enterprise on Google Cloud,"March 27, 2020",Redis,"It used to be that Redis Enterprise was just one of many offerings in the Google Cloud Marketplace, but now thanks to the partnership between Redis and Google, Redis Enterprise is integrated into the Google Cloud Console. This integration means you not only can enjoy unified billing, but also that you can use your Google Cloud credits to pay for Redis Enterprise.Let’s take a look at how to enable the Redis Enterprise API in your Google Cloud project.The starting point is the sidebar of your Cloud Console. At the bottom you will find Redis Enterprise alongside other third-party services. Click on Redis Enterprise to go to a page where you can set up your billing information.I won’t go into the details of this step. It’s straightforward if you have a personal account, but you will have to coordinate with your IT management if you’re part of a big organization. Once you have set up billing and enabled the API, you can start creating Redis Enterprise databases.Once you set up your Redis Enterprise API, you’re ready to see how it works. The sample application in this GitHub repository was written for the new Redis Microservices for Dummies book. It implements some of the functionality of a fully automated library using a microservices architecture. You can request books and return them, and all the code can be run on your computer. The application also depends on a Redis database, so we will create one on our new Redis Enterprise service.Click the “MANAGE ON PROVIDER” button to go to the Redis’ website, where you can create a new subscription and corresponding database.Select a name for your subscription and specify whether you want Redis on Flash and where your nodes should be located. If you want more information about a specific topic, the (i) icons provide more details.The next screen lets you decide the settings for the database, including the name, protocol, and resource consumption. This is also where you can enable Redis modules, if you need any. Make sure to look at the persistence options, you may need to enable them, depending on your use case.Saving the changes brings you to the review page where you can double check that you configured everything as needed. After you’ve made sure that everything looks right, it’s time to give the final confirmation.It takes a moment to set up the entire Redis Enterprise cluster, so after a few minutes the system will send you an email confirming that everything is up and running.After you receive the confirmation email, it’s time to get the endpoint information to connect to your database. In the Redis Enterprise Cloud interface you can see the database you just created and its connection options. For the next step you will need to know your endpoint and password to access the database.Now you need to get the code so that you can run it. Git fans won’t need any pointers, but others can go on the GitHub page and easily download the project as a Zip file using the green “Clone or download” button.Install the dependenciesAfter you put the files on your local machine, it’s just a matter of installing the dependencies. The repository is a Python project, so you will need to have a Python interpreter on your machine. On a Mac you can get it with Homebrew, while on Windows you can use Chocolatey.Once Python is installed on your system, to get the project-specific dependencies all you have to do is navigate into the project’s main directory and run:$ pip install -r requirements.txtYou can now run the application!To run the application, you need to launch main.py. Launching it with the -h argument shows all the options at your disposal. In this case you want to specify –address and –password.Now it’s time to copy all the details about our new database from the Redis Enterprise Cloud control panel and use them to invoke main.py. Note that the script requires you to specify a unique name when you launch it. This is an application-specific setting not related to Redis, that allows multiple instances to run at the same time. (If you want to learn more about the library application, download Redis Microservices for Dummies.) For our purposes, any name will be fine, so let’s use “worker1”.$ python main.py --address redis://YOUR-URL.redis.com:1234
--password your-password worker1Note how the address needs to be prefixed with redis://.If you do this correctly, the terminal will show a “Ready to process events…” line.It’s now time to play with it!Request some booksTo interface with the system, you need to open a new tab in your terminal to run get_books.py. This script lets you request and return books. It still needs the same connection options as the previous script, but after that it expects an action, a username, and a series of book names. Let’s see what the help says:Below are a series of invocations that would make sense, where two users request a few books, some of which are already taken by the other. Feel free to copy and paste these commands or to come up with your own sequence.$ python get_books.py --address redis://YOUR-URL.redis.com:1234
--password your-password request loris fight-club the-witcher lotr
OK$ python get_books.py –address redis://YOUR-URL.redis.com:1234–password your-password return loris fight-clubOK$ python get_books.py --address redis://YOUR-URL.redis.com:1234
--password your-password request itamar fight-club the-witcher hitchhikers-guide
OKIf you now look in the other tab, where main.py is running, you can see the result of each action:Note how Itamar, the second user, was not able to obtain The Witcher because it was taken by Loris.This demo shows how easy it is to provision a new Redis Enterprise cluster in Google Cloud and connect it to an application. The next step is to learn more about what you can do with Redis. You can find inspiration by checking out the Redis website, taking a course at Redis University, or watching a talk on the Redis YouTube channel."
351,https://redis.com/blog/redisgraph-and-redis/,"RedisGraph and Redis: What, Why, and How","March 31, 2020",Guy Royse,"Like many software engineers, I enjoy a good game of Dungeons & Dragons. I love powering up my character and facing increasingly more powerful foes. Doing this right requires gold and experience. And the best way to get those things is a good old-fashioned dungeon crawl.If you need some context on D&D, this video might help. The short but overly simplified version: in D&D you explore underground complexes, fight monsters therein, and take their gold.Unfortunately, dungeons aren’t set up for the convenience of adventurers like us. Sometimes the monster is challenging but has no treasure. Sometimes there is treasure just lying about. And sometimes the room is empty. So how do we find out where in the dungeon to get the treasure we seek?A fun way to solve this problem is to represent a dungeon as a graph using a graph database like RedisGraph. From a data perspective, a dungeon is a collection of entities (rooms, monsters, and treasure) and their relationships. Graphs are great at modeling this sort of data. With a graph database, we can query a graph of a dungeon to find the creepy critters and the sparkling hoozits that will level up our characters.Perhaps you’re not familiar with graphs and graph databases? Well then, join me on an adventure as we explore the what, why, and how.Graph databases are actually pretty easy to understand. I think they’re actually easier to understand than relational databases. But if you’ve spent a lot of time in Relational Land (and many of us have, myself included) you might need to unlearn a thing or two. Just take all that relational stuff, shove it in a different part of your head, and make room to let the new ideas stream on in.Done? OK. Let’s dive into graph databases!A graph database contains a graph made up of nodes and edges. Explanations of graphs, nodes, and edges can get a bit abstract because, fundamentally, they’re pretty abstract ideas. So I’m going to use examples to make them a little more concrete.Nodes are the nouns, the things, of your data. They have a label telling you what type of thing they are. They can also have attributes that provide additional information about the node. Let’s look at a couple of nodes with their labels and attributes:Below we have two nodes. The first node has the label “room” and a single attribute telling us a bit about the room. In this case its name: “The Den of the Ogre King.” The second node has a label of “monster” and two attributes, one telling us that the monster’s name is “Ralph the Ogre King” and another that slaying him is worth 1,200 experience points.Pretty straightforward. Nodes are sort of like objects in a programming language like Java or C#. They have a type and properties.Now, let’s add in an edge and see what that does:The edge has a type of “contains” and a direction that goes from the room to the monster. Its purpose is to establish a relationship between the room and the monster. The type is the nature of that relationship and is, in many ways, like the label of a node. I like to think that edges are verbs—transitive verbs to be specific—in that they connect the nouns together: the room contains a monster. This adds a relationship between the nodes.The direction of the edge is arbitrary. Either way, it establishes the relationship. I could have just as easily created an edge with a type of “is_contained_by” pointing the opposite direction. But then my sentence would be: the monster is contained by the room. Which is in the passive voice. And, as I was taught by my English teacher all those years ago, the passive voice is to be avoided because it’s more verbose and harder to understand.Collectively, these nodes and edges are called a graph. The simplest (and probably least interesting) graph has no nodes at all. And without nodes, of course, it can’t have edges.On the other hand, graphs can get quite complex. Nodes can have multiple edges going to and from them. A pair of nodes can even have multiple edges between them. And nodes can be isolated, without any edges at all!Look at that monster of a graph! It shows three rooms, a secret door, and a treasure pile. Complete with a guardian named Ralph.I’ve been busy modeling my ridiculous example of a dungeon, all of its connected rooms, the secrets, the monsters, and the treasure. And if you were building a text-based online game like a MUD, this would be a great way to model the state of it.But most developers aren’t building text-based games from the ‘80s. Most of us are building more practical things. What sorts of practical problems can graph databases solve? All sorts. Here are a few examples:These are all good candidates for graph databases because they have complex relationships that would, ironically, be difficult to model with a relational database. This is one of the main strengths of graph databases: they model relationships really well.But graph databases have another important strength: they are without schema. This can make them easier to work with once they are in production. For example:And can you even imagine converting a one-to-many relationship to a many-to-many relationship with a relational database? With a graph database, you don’t need to care about one-to-many and many-to-many. Things just relate to each other. If they need to relate, add an edge. That’s it.Yes. Yes you can. You might be familiar with Redis modules. Modules are extensions that you can install to extend the capabilities of Redis—often by adding new commands and data structures but sometimes quite a bit more. Redis has created several and they add all sorts of capabilities. One of them, RedisGraph, provides a data structure that is a graph database.I don’t feel right writing a blog post without some code in it, so I’m going to show some interactions with a graph using Cypher, the query language that RedisGraph uses. I’m going to use RedisInsight to do this because it has a cool visualization tool that’s worth checking out, but you can use redis-cli if you like.NOTE: If you do use redis-cli, be sure to begin all your Cypher queries with GRAPH.QUERY key “your cypher query here”.> CREATE (:monster { name: 'Ralph the Ogre King', xp: 1200 })The stuff between the parentheses is the node to be created and, after the colon, monster is the label for the node. The attributes of the node follow the label and are formatted in a very JavaScript-like way.Now that we’ve created a monster, let’s query him:> MATCH (m:monster) RETURN mThe MATCH here matches all nodes with the label of monster and assigns them to m. In this case, m just has one node in it. The RETURN, unsurprisingly, returns what we tell it.This could have been even simpler. Since we have only a single node in our graph, we don’t need to check the label:> MATCH (n) RETURN nThis query returns all nodes.Cypher is kind of neat because its queries look sort of like a graph. Since nodes are represented as circles, when we CREATE or MATCH them, we wrap them in parentheses to suggest a circle. This idea is carried forward when we create nodes with edges.Let’s start over with an empty graph and create a graph that has Ralph in his den:> CREATE (:room { name: 'The Den of the Ogre King' })-[:contains]->(:monster { name: 'Ralph the Ogre King', xp: 1200 })Here we are creating two nodes: Ralph and his den. But we are also defining a relationship between them, an edge, with what looks like an arrow with a label on it. The arrow points in the direction of the relationship and the square brackets contain the type of the relationship. In this case the room contains a monster.We can query this structure like we did before, but now with the edges as well:> MATCH (r:room)-[c:contains]->(m:monster) RETURN r, c, mAnd, since our graph contains only two nodes and a single relationship, we could have made an even simpler query:> MATCH (n1)-[e]->(n2) RETURN n1, e, n2This query returns all nodes and all edges.There’s a lot more to RedisGraph, including many more-sophisticated queries. You should dig deeper in the documentation as there’s a lot of neat stuff in there. However, here’s a useful query for us treasure seekers:> MATCH (r:room)-[:contains]->(:monster)-[:guards]->(:treasure) RETURN rWe’ve seen the sorts of problems that graph databases can solve, and we’ve seen how to create and find nodes and edges. But how does it work internally? Well, that’s a big question that I’m not going to answer. Instead, I’d like to send you to the documentation, where you can learn about sparse adjacency matrices, matrix multiplication, and GraphBLAS.Finally, I encourage you to more deeply explore the treasure-filled domain that is RedisGraph. Install it. Build something cool. Share it with the world. And if you build a MUD, can I play?"
352,https://redis.com/blog/redistimeseries-version-1-2-benchmarks/,RedisTimeSeries Version 1.2 Benchmarks,"March 12, 2020",Pieter Cailliau and Filipe Oliveira,"To benchmark the performance of our newly released RedisTimeSeries 1.2 module, we used the Time Series Benchmark Suite (TSBS). A collection of Go programs based on the work made public by InfluxDB and TimescaleDB, TSBS is designed to let developers generate datasets and then benchmark read and write performance. TSBS supports many other time-series databases, which makes it straightforward to compare databases.For more on RedisTimeSeries 1.2, see RedisTimeSeries Version 1.2 Is Here!This post will delve deep into the benchmarking process, but here’s the key thing to remember: RedisTimeSeries is fast…seriously fast! And that makes RedisTimeSeries by far the best option for working with time-series data in Redis:To compare RedisTimeSeries 1.2 with version 1.0.3, we choose three datasets: The first two have the same number of samples per time series but differ in cardinality.Note: The maximum cardinality of a time-series dataset is defined as the maximum number of distinct elements that the dataset can contain or reference in any given point in time. For example, if a smart city has 100 Internet of Things (IoT) devices, each reporting 10 metrics (air temperature, Co2 level, etc.), spread across 50 geographical points, then the maximum cardinality of this dataset would be 50,000 [100 (deviceId) x 10 (metricId) x 50 (GeoLocationId)].We chose these two datasets to benchmark query/ingestion performance versus the cardinality. The third dataset has the same cardinality as the first, but has three times as many samples in each time series. This dataset was used to benchmark the relationship between ingestion time and the number of samples in a time series.The performance benchmarks were run on Amazon Web Services instances, provisioned through Redis’ benchmark testing infrastructure. Both the benchmarking client and database servers were running on separate c5.24xlarge instances. The database for these tests was running on a single machine with Redis Enterprise version 5.4.10-22 installed. The database consisted of 10 master shards.In addition to these primary benchmark/performance analysis scenarios, we also enable running baseline benchmarks on network, memory, CPU, and I/O, in order to understand the underlying network and virtual machine characteristics. We represent our benchmarking infrastructure as code so that it is stable and easily reproducible.The table below compares the throughput between the RedisTimeSeries version 1.0.3 and the new version 1.2 for all three datasets. You can see that the difference between the two versions is minimal. We did, however, introduce compression, which consumed 5% more additional CPU cycles. From this, we can conclude that if the shards are not CPU bound, then there is no throughput degradation by the compression.The three images below track throughput, latency, and memory consumption during the ingestion of the third (and largest) dataset. We inserted 800 million samples into a single database over the course of less than two hours. What is important here is that the latency and throughput do not degrade when there are more samples in a time series.The last row of the chart compares throughput over the first two datasets. There is almost no difference, which tells us that the performance does not degrade when the cardinality increases. Most other time-series databases degrade performance when the cardinality increases because of the underlying database and indexing technologies they use.TSBS includes a range of different read queries. The charts below represent the query rate and query latency of multi-range queries comparing RedisTimeSeries version 1.0.3 to version 1.2. They show that query latency can improve up to 50% and throughput can increase up to 70%, depending on the query complexity, the number of accessed time series to calculate the response, and query time range. In general, the more complex the query, the more visible the performance gain.This behavior is due to both compression and changes to the API. Since more data fits in less memory space, fewer blocks of memory accesses are required to answer the same queries. Similarly, changes in the API’s default behavior of not returning the labels of each time series leads to substantial reductions in the load and overall CPU time on each TS.MRANGE command.The addition of compression in RedisTimeSeries 1.2 makes it interesting to compare memory utilization in these three datasets. The result is a 94% reduction in memory consumption for all three datasets in this benchmark. Of course, this is a lab setup where timestamps are generated in fixed time intervals, which is ideal for double-delta compression (for more on double-delta compressions, see RedisTimeSeries Version 1.2 Is Here!). As noted, a memory reduction of 90% is common for real-world use cases.RedisTimeSeries is seriously fastWhen we launched RedisTimeSeries last summer, we benchmarked it against time-series modelling options with vanilla data structures in Redis, such as sorted sets and hashes or streams. In memory consumption, it already outperformed the other modeling techniques apart from Streams, which consumed half the memory that RedisTimeSeries did. With the introduction of Gorilla compression (more on that in this post: RedisTimeSeries Version 1.2 Is Here!), RedisTimeSeries is by far the best way to persist time series data in Redis.In addition to demonstrating that there is no performance degradation by compression, the benchmark also showed there is no performance degradation by cardinality or by the number of samples in time series. The combination of all these characteristics is unique in the time-series database landscape. Add in the greatly improved read performance, and you’ll definitely want to check out RedisTimeSeries for yourself.Finally, it’s important to note that the time-series benchmarking ecosystem is rich and community-driven—and we’re excited to be a part of it. Having a common ground for benchmarking has proven to be of extreme value in eliminating performance bottlenecks and hardening every solution in RedisTimeSeries 1.2. We have already started contributing to better understanding latency and application responsiveness on TSBS, and plan to propose further extensions to the current benchmarks."
353,https://redis.com/blog/redis-game-mechanics-scoring/,Redis Game Mechanics: Scoring,"April 1, 2020",Redis,"First, let me warn you that I am not a game developer. So this post, which has been rattling around in my head for a number of years now, is partly a look at how a theoretical game would work with Redis but also a metaphor to help readers understand how Redis works.Phew!With all that out of the way, let’s talk about implementing a game that is powered by Redis. We’re going to call this game Super Redis Brothers—it’s an action platformer connected to a central server. It stars Redisman, who needs to collect coins, use those coins for power-ups, and avoid being killed by radiation. Redisman started life as Sal, an Italian software developer who was bitten by a Redis module, turned into the stick-figure Redisman, and transported into this topsy-turvy world. Today we’re going to cover the aspects of scoring in Super Redis Brothers (aka collecting coins).Setting aside all the other mechanics of the game, how do you manage the coin count of your character? You could use something as simple as a string to store and manipulate the coin count. Getting a coin could be something like:> INCR pID123Getting more than one point at a time would employ the INCRBY command, and losing points by either DECRBY or INCRBY with a negative increment. This is all fine and good, but what happens if you want to implement some sort of leaderboard or player-matching system? Using a simple string in Redis would not be very effective. What we need is a Sorted Set: this would allow us to pull a leaderboard quite efficiently, but can we use it to also track the scores of the individual players?Let’s take a look!Sorted Sets consist of members, each with a numerical score all stored under one key in Redis. Like normal Sets, members cannot have repeats, but it is possible to have multiple members with the same score. While using string/counter operations in Redis is a O(1) operation, write operations on Sorted Sets are O(log(n)) operations (where n is relative to the members in the set), so still respectable. All sorted set commands in Redis use the prefix “Z”.Modeling the game mechanics of collecting coins, we’ll use a single key for all the players’ scores, the member is the player identifier and the score for this member is the number of coins that player has. As players collect coins, we’ll use ZINCRBY to increment the scores by the number of points collected. If a player has yet to collect a coin, then Redis will start with zero and increment accordingly. Note that sorted sets allow for a zero score (a distinct difference from a member that is not in the set) as well as a negative number. Let’s take a look at this visually.(Note: Obviously, I am not a game artist!)So, as Redisman comes in contact with the 10 coin, we use ZINRCBY to increase the score of the member by 10. The same thing happens when Redisman comes in contact with the 20 coin, except with the appropriate value.Also in the above graphic you can see the heads-up coin display. The initial value can be retrieved by using ZSCORE, which, when supplied with the key and member name will return the score. Upon collecting points, ZINCRBY will return the new score of the member, so the result can be used to update the coins heads-up display.Let’s look at some more operations. Say we want to implement a power up that exchanges 10 coins for freezing enemies. This can be implemented by using the same ZINCRBY command, except with a negative increment argument. Another encounter might be something that reduces your coin count to zero (think Sonic the Hedgehog losing his rings). In this case we’d use ZADD with a score of 0. ZADD might seem counterintuitive in this situation, but think of ZADD as an upsert—this operation will update a score if the member already exists or add it with the new score when it doesn’t exist. Like ZINCRBY, ZADD will return the score, so we can also use this to update the heads up display.Note that there’s a key oversimplification in the picture above: In the current setup, if Redisman has 0 coins and collides with the enemy freeze powerup, then his coins will drop to -10, which is probably not what we want. In pseudocode, we’ll be doing:currentScore = ZSCORE scores pID1234
increment = -10
member = pID1234

//multiplying by -1 to invert negative numbers
If (currentScore >= increment*-1) {
Return ZINCRBY scores increment member
}All the script pseudocode is doing is checking first if there are enough coins, and if so, then allow the negative increment to go through. This is a really great job for a simple Lua script:> EVAL ""if tonumber(redis.call('zscore',KEYS[1],ARGV[2])) >= (ARGV[1]*-1) then return tonumber(redis.call('zincrby',KEYS[1],ARGV[1],ARGV[2])) end"" 1 scores -10 pID1234If you’re not familiar with Lua this might be a little overwhelming, but is about as simple as you can get: get the score, make sure it won’t go below zero, if so, do decrement. The nice thing about Lua is that it runs atomicly. With this script there is no way for Redis to modify anything between the ZSCORE and the ZINCRBY. In a production scenario, you would probably use SCRIPT LOAD and EVALSHA instead of EVAL, but the principal is the same.Sorted Sets are a near-perfect data structure for leaderboards. In this example, we’re using the coins as the score in a Sorted Set, so the leaderboard functionality is practically built-in. Leaderboards are most often rendered from high score to low, so we’ll use the ZREVRANGE command.> ZREVRANGE scores 0 9 WITHSCOREThis will show the top ten players with the highest scoring player first. It will keep track of ties as well, so it’s possible that the top ten will have 10 players all with the same score.Showing the top ten might not be optimal in all situations, though, especially for new players with a low score. What a player probably wants to see is the people they are slightly better than and those who are slightly worse than them. To do this, first you want to get the rank of the player in question. You can use ZREVRANK, then feed the results into ZREVRANGE with the number of results you want above and below the player in question.You might have noticed that the examples in this post always refer to the same key scores. This highlights a flaw in this method when used in an actual game at scale. Because you are touching the same key over and over, you could create a very hot key. In clustered Redis, the key determines which shard or node the data is stored on. Activity on a single key will end up on the same machine until a resharding occurs. In this case, even if you had a cluster composed of many nodes and shards, all the scoring activity would be on a single shard, so you’ll eventually be limited by the resources of this shard. The number will be high, but could be reached in a heavily used game with frequent coin gathering or spending across multiple players.There are a few solutions: You can go back to using INCR and a single key for each player, then using SCAN to periodically update a Sorted Set, but that would lose the real-time-ness of the leaderboard and create extra processes to make the Sorted Set update.Another option would be to use a Redis Enterprise Active-Active deployment in a slightly unusual way. Active-Active in Redis Enterprise is designed primarily to facilitate geo-distribution by having multiple replica peer clusters spread over wide geographic area to reduce intrinsic latency: read and write to a nearby instance and Redis Enterprise takes care of any conflicts using the concept of Conflict-free Replicated Data Types (CRDTs). Using this method, you could just as easily set up several peer clusters then round-robin write to these different clusters. The writes would be accepted at the speed of the individual cluster then resolved to the different clusters. While this wouldn’t be exactly real-time, it would provide a seamless way of scaling the number of writes at a single key, especially if the workload has a bursting traffic pattern.There you have it, remote score recording made easy with Redis. Sorted Sets make a really keen data structure for recording and displaying scores, both for individual users and in a leaderboard. Just keep in mind your strategies for managing potentially whitehot keys. Then go collect those coins!"
354,https://redis.com/blog/fewer-unsecured-redis-servers/,The Number of Unsecured Redis Servers Keeps Declining,"April 2, 2020",Itamar Haber,"Earlier today, security firm TrendMicro published a blog post with findings from the company’s security researchers. The team had used Shodan, the popular search engine for internet-connected or Internet of Things (IoT) devices, to identify more than 8,000 unsecure Redis servers across various clouds. When left unsecured, such servers could be used by cybercriminals for nefarious purposes.That number is substantial, but it demonstrates a constant, if not declining, number of exposed servers overall. About three years ago, I used the same method to count 9,916 such servers, and a year later Shodan had confirmed that newer versions of Redis, specifically version 4.0, had “drastically improved/lowered the numbers of publicly accessible Redis instances”—counting 17,443 unsecured servers worldwide.This drastic improvement is due to the protected mode introduced in Redis version 4.0 and backported to Redis version 3.2.0. This special mode, enabled by default, prevents unsecure access to the server from the outside. Looking at the top Redis versions used by unsecure servers, we see that a significant part of the exposed server are still using outdated versions.Taken from the Shodan report, the table above shows the exposed Redis servers’ version (right side) and their respective counts (left). The fact that there are exposed servers running Redis v3.2.0 and above is concerning, of course. It means that the administrators of these Redis servers deliberately disabled protected mode and didn’t set passwords for their databases. The motivations for intentionally doing that are unclear; it seems no amount of security protections are enough to fully overcome the possibility of human error.That said, we’re continuing to improve on open source Redis’ security. Managed service provided by Redis is secure out of the box, and eliminates the need for users to figure out their own security practices. In addition, security  is a key focus of the next version of open source Redis, expected to be released later this April. The upcoming Redis version 6.0 adds Transport Layer Security (TLS) encryption to all three communication channels: client-server, master-replica, and cluster bus. Furthermore, Redis 6.0 includes a new Access Control List (ACL) mechanism integrated into the server, which allows the definition of fine-grained user permissions for accessing and manipulating data.Redis always treated security risks with extreme seriousness and our enterprise products are designed to help prevent them. The Redis Enterprise stack blocks the creation of passwordless databases. Additional security features of Redis Enterprise include TLS encryption and tight integration with the organizational identity management system, as discussed here and here.Looked at in context, the blog post from Trend Micro actually shows an encouraging and declining trend of the number of open source Redis servers that are exposed. Newer versions, meanwhile, improve open source Redis’ security model by reducing the attack surface, encrypting communication channels, and adding permissions management.By its very nature, however, open source software—including Redis—can be easily configured with all, some, or none of the available security features. So when it comes to using Redis in production, users should carefully review their security settings—or opt for the production-proven and security-hardened Redis Enterprise."
355,https://redis.com/blog/redis-university-announces-self-paced-on-demand-courses/,Redis University Announces Self-Paced On-Demand Courses,"April 3, 2020",Simon Prickett,"Redis University is pleased to announce the immediate availability of free, self-paced, on-demand online courses. A lot has changed in recent weeks, and many of us are now at home, perhaps a bit more than we’d like to be.To help make this shelter-in-place time as productive as possible, we figured this would be a great time to give the Redis community the chance to complete as many courses as you can, all at your own pace. Take as many courses as you like—just be sure to complete them before they close on May 12, 2020.Right now, for a limited time, five of our courses are immediately available for self-paced learning:For those new to Redis, this course offers a thorough introduction to Redis and its data structures.Check out the intro video here, and learn more about the course here.A logical continuation to RU101, the Redis Streams course covers the most sophisticated Redis data structure: Streams.Check out the intro video here, and learn more about the course here.In RU201, you’ll learn all about full-text indexing in Redis using the RediSearch module.Check out the intro video here, and learn more about the course here.Write a full-fledged application with Java and Redis! This language course describes many of the patterns you’ll use to write efficient Java code against Redis.Check out the intro video here, and learn more about the course here.This course takes a similar approach to RU102J, but is for developers wishing to learn more about best practices for using Redis in Node.js applications.Check out the intro video here, and learn more about the course here.Redis University turns two years old in April, and there are few better ways to learn Redis than with a Redis University course. We’ve had tens of thousands of enrollments in more than one hundred countries, and more new courses and ways to learn are in the works. For now, we hope that our new self-paced approach makes it even easier and more convenient to level up your Redis skills!Sign up and start learning more about Redis now!"
356,https://redis.com/blog/redisgraph-2-0-boosts-performance-up-to-6x/,RedisGraph 2.0 Boosts Performance Up to 6x,"April 7, 2020",Pieter Cailliau and Filipe Oliveira,"The newly announced RedisGraph 2.0 module brings a number of improvements, including increased Cypher support, full-text search, and it enables graph visualization. Just as important, however, the latest version of RedisGraph delivers significant performance improvements: with latency improvements up to 6x and throughput improvements up to 5x. Let’s take a look at those performance gains and the benchmarks used to demonstrate them.When we released RedisGraph 1.0 back in November 2018, we shared benchmark results based on the k-hop neighborhood count query. The new RedisGraph 2.0 includes new features and functionalities that support more-comprehensive test suites, like the ones provided by the Linked Data Benchmark Council (LDBC)—more on that below. But we still rely on the k-hop benchmark in order to compare RedisGraph 2.0 to v1.2.The K-hop neighborhood count query is a graph local query that counts the number of nodes a single start node (seed) is connected to at a certain depth, and counts only nodes that are k-hops away, as shown here:K-hop neighborhoods queries are very useful in analytic tasks on large-scale graphs, like finding relations in a social network, or recommending friends or advertising links according to common properties.We’ve kept the previous benchmark Graph 500 dataset with scale 22, with the following graph characteristics:To gain deeper insight and coverage of the database performance, we extended the seed coverage of the benchmark to 100,000 random deterministic seeds instead of 300. To make it easy for anyone to replicate our results, here is a public link to the persistent store graph 500 dataset with scale 22 on RDB format.For each tested version, we performed:All queries were under a concurrent parallel load of 22 clients. We reported the median (q50) and achievable throughput.To get steady-state results, we discarded the previous Python benchmark client in favor of the memtier_benchmark, which provides low overhead and full-latency-spectrum latency metrics.All benchmark variations were run on Amazon Web Services instances, provisioned through our benchmark-testing infrastructure. Both the benchmarking client and database servers were running on separate c5.12xlarge instances. The tests were executed on a single-shard setup, with RedisGraph versions 1.2 and 2.0.5.In addition to the primary benchmark/performance analysis scenarios described above, we also enable running baseline benchmarks on network, memory, CPU, and I/O, in order to understand the underlying network and virtual machine characteristics. We represent our benchmarking infrastructure as code so that it is stable and easily reproducible.The RedisGraph 1.2 vs 2.0 benchmark values point towards significant improvements on parallel workloads (multiple clients). We’ve measured latency improvements up to 6x and throughput improvements up to 5x when performing graph traversals. The more parallel and computationally expensive the workload, the better RedisGraph 2.0 performs when compared to the previous version.RedisGraph 2.0 incorporates the latest version 3.2.0 of SuiteSparse:GraphBLAS —SuiteSparse’s implementation of GraphBLAS— which RedisGraph uses for sparse matrix operations. With this version it is now possible to exploit shared-memory parallelism by recurring to OpenMP in order to gain significant performance advantages.Internal testing on CPU intensive queries showed that RedisGraph spends around 75% of its total CPU time on SuiteSparse:GraphBLAS. To demonstrate the performance gains of SuiteSparse:GraphBLAS parallel OpenMP-based implementation in RedisGraph, we tested the 6-hop query neighborhood count due to it being computationally expensive.As visible below, using a single-threaded SuiteSparse:GraphBLAS translated into q50 latencies of 537ms, and 175ms with 22 OpenMP threads, with latency reductions of up to 6x, at no extra cost for RedisGraph. The improvements are even more noticeable in the higher-latency spectrum (high-quantile values like q99):Because RedisGraph 2.0 supports many more Cypher features, we decided to start to embrace the Linked Data Benchmark Council (LDBC) benchmarks. LDBC has gathered strong industrial participation for its mission to standardize the evaluation of graph data management systems.  In this section we wanted to give an update on the progress we’ve made.The LDBC Social Network Benchmark (SNB) in the LDBC is a logical choice for RedisGraph since it enables complex read queries that touch a significant amount of data, have complex graph dependencies, and require the graph database to support complex and innovative query patterns and algorithms.In the LDBC SNB benchmark, the nodes and edges distribution are guided by a degree-distribution function similar to the one found in Facebook. This benchmark accounts for an important aspect of simulating social networks, the fact that persons with similar interests and behaviors tend to be connected (known as the Homophily principle).Concurrent with the SNB’s read queries is a write workload, which reproduces real-world social network scenarios such as adding a friendship among persons or liking and commenting on a post.Currently from the LDBC SNB benchmark queries we support 100% of the write queries and 52% of the read queries, as seen in the following table:The soon-to-be-supported Cypher features: OPTIONAL MATCH (the equivalent of outer join in SQL), shortestPath (an implementation of the well-known Shortest path problem), and list and pattern comprehensions will enable us to support 100% of the SNB’s complex read queries.RedisGraph 2.0 brings significant performance gains compared to version 1.2, which can lead to 6X faster queries. Not only did the sequential performance improve, RedisGraph is also able to exploit shared-memory parallelism with the inclusion of the latest versions of SuiteSparse:GraphBLAS, leading to even further performance gains.In conjunction with the performance improvements, the work we did to increase the Cypher support gets us close to supporting the richer, community-driven LDBC benchmark, which we are excited to be part of and use to improve and harden our solution."
357,https://redis.com/blog/7-redisconf-2020-speakers-you-wont-want-to-miss/,7 RedisConf 2020 Speakers You Won’t Want to Miss,"April 9, 2020",Redis,"Every year we’re proud to bring together an A-list group of Redis Geeks, experts, and contributors to speak at RedisConf, the largest gathering of the Redis community. This year RedisConf is going virtual. The main conference with live keynotes will be on May 12 and online training will be held on May 13. In addition to our own speakers, we’ll be joined by a number of guest speakers from across the Redis community and we’ll be adding more leading up to the big day. You don’t want to miss this event, so be sure to register for free here.To give you an idea of what we have planned, I’ve highlighted seven amazing speakers who’ll be taking the (virtual) stage.Corporate Vice President of Microsoft’s Developer DivisionSession topic: Keynote presentation on May 12Keynote speaker Julia Liuson leads technical and business strategy, product development, and engineering teams for Microsoft’s Visual Studio, Visual Studio Code, .NET and .NET core framework, and developer services and platform for Microsoft Azure—including all programming languages, runtime, tools for building Azure, AI, mobile and Windows-oriented applications. Visual Studio and Visual Studio Code are consistently voted the top code-editing tools for developers in the industry.Julia was the first woman promoted to Corporate Vice President of Engineering leading development teams at Microsoft. She received the Asian American Executive of the Year award in 2013 and was inducted into the Women In Technology International Hall of Fame in 2019.Before stepping into her current role, Julia served in a variety of leadership roles on product and engineering in the Microsoft Visual Studio product line, including leading the effort to make the .NET platform open source and cross platform. She also worked as General Manager for Microsoft Server and Tools Business in Shanghai for two years while running engineering teams on both sides of the Pacific.Connect with Julia on LinkedIn.Creator of Microservices.io, and the author of Microservice PatternsSession topic: Microservices Architectures Workshop on Training Day (May 13)Chris Richardson is a renowned microservices expert, and his RedisConf 2019 session was so popular that we’re bringing him back again this year. Besides being a Java Champion, a JavaOne Rock Star, and the author of POJOs in Action—which describes how to build enterprise Java applications with frameworks such as Spring and Hibernate—Chris was also the founder of the original CloudFoundry.com, an early Java Platform-as-a-Service (PaaS) for Amazon EC2.Chris is the creator of Microservices.io, a pattern language for microservices, and the author of Microservice Patterns. He provides microservices consulting and training to organizations adopting a microservices architecture and is working on his third startup, Eventuate, an application platform for developing transactional microservices.Don’t miss Chris’ training day session on microservices architectures, where he’ll walk you through their essential characteristics, benefits, drawbacks, and when to use them.Connect with Chris on Twitter, LinkedIn, his website, or GitHub.Developer Advocate at HashiCorpSession topic: Securing Your Network with a Service Mesh on May 12Nicole is a developer advocate for HashiCorp, where she focuses on the experience of developers and infrastructure engineers. She has a passion for solving problems at scale  and making infrastructure repeatable and automated. She’s experienced in building software in Go, Ruby, and Python, and besides frequently speaking and attending developer conferences, she is also active in the online developer community via platforms like Twitter.Nicole is active within the Kubernetes ecosystem, and prior to HashiCorp she worked at Microsoft, WP Engine, and Rackspace. Her session will explain what service meshes are and why you should use one, and show you how to deploy Consul Connect inside a Kubernetes cluster and connect your applications inside Kubernetes to your Redis instance running on virtual machines.Connect with Nicole on Twitter, LinkedIn, her website, or GitHub.Senior Principal Software Engineer at TwilioSession topics: Workshop: Machine Learning Fundamentals with Redis and Apache Spark (May 13) and Talk: The Happy Marriage of Redis and Protobuf: A True Story About  Communication, Keys, and Give/Take (May 12)If you didn’t catch Scott at Redis Day Seattle in January, you’re in luck—he’ll be presenting his talk again and leading a training day session at RedisConf 2020 Takeaway! During his session on May 12, he’ll walk you through a design paradigm that might make you smile, using Google Protocol Buffers (“Protobuf”), Sorted Sets, Lists/Sets, binary Redis keys, and Redis Streams.Scott is a full-stack engineer with a focus on real-time, highly available analytics, and insights systems. He works at Twilio where he helped drive Apache Spark adoption and develop streaming pipeline architecture best practices.Connect with Scott on Twitter, LinkedIn, his blog, or GitHub.VP of IoT and Cloud Platforms at ZenerchiSession topic: Creating a Model of Human Physiology Using RedisGraph on May 12Carlos is a veteran software developer and Co-Founder of Zenerchi, a creator of biomedical interface software. He also wrote the Hydra framework, a Node.js package that leverages Redis to facilitate building distributed applications, including microservices architectures.This will be Carlos’ third RedisConf appearance, and this year his presentation will focus on modeling human physiology with Internet of Things and wearable tech devices using RedisGraph. He’ll demonstrate how RedisGraph is powering Zenerchi’s biotech platform, which produces advanced 3D, VR, and AR visualizations.Connect with Carlos on Twitter, LinkedIn, his blog, or GitHub.Tech Leader Manager at DoorDashSession topic: Improving Cache Speed at Scale on May 12Zohaib is a long-time Redis user who relies on Redis for caching on his current team, DoorDash Platform Service. Prior to DoorDash, Zohaib worked at companies like   Microsoft, Flutter (acquired by Google), and Bumpin (a social media platform).Catch Zohaib’s presentation on techniques his DoorDash team uses to improve tail latencies. He’ll cover hot keys, stampede (Thundering Herd), predictive cache expiry, and compression, among other topics.Connect with Zohaib on Twitter, LinkedIn, or GitHub.CTO/Co-Founder at Gretel.aiSession topic: Joinability Analysis at Scale with HyperLogLog on May 12John is a long-time Redis user and the CTO and Co-Founder of Gretel.ai. He’s been architecting and building for more than 15 years in disciplines such as cybersecurity, data  analysis, and privacy engineering. Previously, he was the CTO and Co-Founder of an enterprise security startup and Principal Architect at Arbor Networks for cloud solutions. He’s been building with Redis for more than eight years and has used it for multiple use cases far beyond caching scenarios.Catch John’s presentation on HyperLogLog, where he’ll demonstrate how the lightweight data structure—along with some basic math operations—can perform real-time joinability analysis between multiple massive datasets.Connect with John on LinkedIn and Medium.Be sure to catch these seven speakers and dozens more at RedisConf 2020 Takeaway, held on May 12–13. RedisConf is an online learning conference for developers and cloud professionals from the makers of the world’s most-loved database. It offers the perfect opportunity to dive into the latest innovations and trends in data platforms, share your ideas, learn from valuable experiences of Redis users across the globe, and get hands-on training—all from the comfort of your living room. Register for free here."
358,https://redis.com/blog/the-deluge-designing-for-peak-usage/,The Deluge: Designing for Peak Usage,"April 10, 2020",Guy Royse,"A few years back, I was at a conference and a small group of us were talking about scaling software. One individual in the discussion introduced the idea of a company’s “day of deluge.” The day of deluge is the day your organization, and its software systems, experiences their maximum load. If you’re an American pizza delivery chain, the day of deluge is probably Super Bowl Sunday. If you’re a florist, it’s likely Valentine’s Day. Perhaps the most famous deluge day of all is Black Friday—that special day right after Thanksgiving when consumers across America swarm stores to gobble up the goods therein.But your day of deluge isn’t always on the calendar. For several years, I worked as a software engineer at a major financial services company—a property and casualty insurer to be specific—and our deluges were literal catastrophes. Catastrophes with names like Hurricane Sandy and Hurricane Katrina.We had some warning. Hurricanes aren’t scheduled, but they also don’t sneak up on you any more. Even so, they put enormous strains on our organization and our claims processing systems. And our finances.Similar events are happening to many organizations now. The COVID-19 virus is forcing hundreds of millions of people to stay home. The entire internet—and companies that offer services over the internet—are seeing unprecedented loads as many of those people work from home. We designed our software for an antediluvian world but must now deal with the flood. How can we better design it for this environment?I don’t know that I have the answer. I’m not sure anyone has the one true answer. Software is a domain of trade-offs, choices, and consequences. However, I do have some thoughts that might help.The first and easiest scaling trick is scaling up your hardware. Get a more powerful machine with more processor, more memory… more of everything. While this works well if your Twitch streaming rig needs some extra oomph, it is of limited use for applications at even moderate scale. The box can only get so big. Eventually, you’ll start to see this pattern:Diminishing returns and then a plateau. You keep upping the processors and the memory and the everything, but it just doesn’t get any faster. I worked at a company that had architected itself into a corner and this was its solution. It worked in the short term. But the costs increased dramatically even as less and less capability was being added.The next solution is to scale out your hardware. Instead of buying bigger and bigger boxes, buy more boxes. By adding nodes, you can handle more requests. This common technique can carry you pretty far. If you do it right, it can carry you most of the way. But if you don’t do it right, you start running into diminishing returns and plateaus again.What you need is a solution that allows you to scale linearly. Like this:But how can we do that? Read on.The secret to linear scalability is statelessness. State—whether it be on disk, flash, or in memory—is the thing that causes applications to wait. It is the thing that our threads and our processes and our servers fight over.And this waiting is the source of the diminishing returns. If we could eliminate state, that would solve all of our problems, right? Technically, yes, but only in the way that forbidding passengers on airplanes would reduce fatalities in air crashes. State is necessary.Every application has to store and manage state of some sort. I’m using a word processor to store my words right now. That’s state. Without state, an application is useless.Instead of trying to achieve the impossible goal of complete statelessness, I like to flip this idea on its head and ask: how do we avoid statefulness? This lets us think about minimizing state, and handling the state we must accept in ways that don’t tie up our application. Optimizing how we manage our state lets us get closer to the goal of linear scalability.This is something that both developers and architects must consider. As a developer, here are four simple things you can do to write less stateful code:As a software architect, you can discourage the writing of stateful code among your developers using these three ideas:These techniques help but they’re only part of the equation. They mostly push the state out of the code. But every application has to store its state somehow. So how can we store our state and still get that near linear scalability?Scaling storage can be a bit more fiddly. Sometimes, you can get away with just scaling up your database cluster. There can be diminishing returns down that path, but your particular deluge might not encounter them.If that doesn’t work (and for many applications it won’t), you have two basic options:(There’s also option 3. Use distributed data types with a really, really, really ridiculously fast database.)Distributed data types are an interesting topic and there are a few to check out, but I’m just going to talk about one: conflict-free replicated data types, or CRDTs.CRDTs allow updates to be replicated between nodes in a database without needing a master node. All nodes are equal and any node can accept an update. However, reads from any given node are not guaranteed to have all updates at any given time as they might not have replicated yet. But, it will have eventual consistency. This allows for some really nice scalability with the tradeoff that your reads might not be current. For many applications, this is an excellent exchange.Redis is an obvious answer to the second option. It runs in-memory, which is orders of magnitude faster than anything stored on disk in both latency and throughput. It’s single-threaded and event-based, just like your application is if you followed the aforementioned advice. It does have state, but it’s fast enough that it takes a lot of load to cause clients to start blocking.For the third technique—the why-not-both option—Redis Enterprise solves that neatly with Active-Active replication that uses CRDTs.Did I? Oh. You probably mean caching and queueing.Caching is something we’re all familiar with. Take common reads from your database and save them in memory. Caching like this moves slow and remote disk access to fast and local memory access. It’s really just a way of taking a slow database and turning it into a faster database for certain operations. If you’re using something akin to a traditional relational database, caching is often a great option to improve your performance.Queuing is a little different and is a notable part of dealing with peak load. Many tasks are long running—like sending an email. A common way to do this is to put a request to send an email in a queue. Then, another node will pick up that message and send the email. This keeps the user happy, as they get a faster response, and it keeps the requesting node from tying up a bunch of threads while the email is being sent.However, this can also be effective for shorter-running processes. By breaking out applications into lots of little pieces—each on their own node—and connecting them with queues, we are able to make lots of little, linearly scalable services (i.e. microservices). These services can then be scaled out—perhaps even automatically based on load—to meet the load required.There’s lots more that I could talk about here. Streaming comes to mind. Microservices is a massive topic that my peers, Kyle and Loris, wrote a book about. Autoscaling is a great tool for DevOps teams to automatically expand and contract your pool of nodes to meet demand.The idea of a day of deluge has always fascinated me. I like what it tells me about the nature of so many businesses. Until I was introduced to this idea, it never occurred to me that pizza and wing places had to gear up for Super Bowl Sunday.I’d love to hear about your company’s day of deluge and how you’ve addressed them. Feel free to reach out to me on Twitter and share!"
359,https://redis.com/blog/computing-jaccard-similarity-with-redis/,Computing Jaccard Similarity with Redis,"April 13, 2020",Guy Royse,"Many things in the world have fancy-sounding names, but are actually really simple ideas. Jaccard similarity is one of those things. It’s a simple calculation—created by botanist Paul Jaccard in 1901—that you can make with sets to determine how similar they are.Finding similar things is useful—you might want to find people with similar tastes as part of a matchmaking (romantic or otherwise) process or to find similar documents to detect plagiarism.Redis, with its Set data structure, has most of what you need to calculate Jaccard similarity. But before we get to the Redis bits, let’s look at how to calculate it.If you have even the slightest background in set theory, this will be a review. We’ll start with sets. A set is a collection of unique objects. In mathematical notation, they are represented with curly braces and a list of the members of the set. Here’s a simple set of movies I like:You can count a set’s members. This is the cardinality of the set. The above set has a cardinality of 6.Of course, you can have more than one set. Here is a simple set of movies that my wife likes:You can see that she and I have some movie preferences in common, but she also has several that I’m not as fond of. And vice versa. You can also see that her set has a cardinality of 7.Now that we have two sets we can start doing some interesting stuff. First, we can combine the sets. This is called the union. The union is itself a set, and like all sets, has a cardinality (in this case 9). We can write the union of these two sets as an equation using the union symbol as shown below:We can also determine what the sets have in common. This is called the intersection. The intersection is also a set and also has a cardinality—which is 4—just like a union. We can write the intersection of our two sets as an equation using the intersection symbol as shown below:There’s lots more to set theory, but this is all we need to know to understand and calculate a Jaccard similarity.The Jaccard similarity is the ratio of the cardinality of the intersection to the cardinality of the union. In our example, my wife and I have 4 movies in common and 9 movies between us. 4 divided by 9 yields approximately 0.444 or 44.4%. So, our taste in movies is 44.4% similar.And that’s it. That’s all a Jaccard similarity is.Let’s calculate it using Redis Set commands.Instead of using movies that my wife and I like, this time I’m going to use states in which Scully and Mulder have seen UFOs. (This data is totally made up. I’m pretty sure they saw UFOs in every state.)First, we need to create a couple of sets for Scully and Mulder:> SADD ufos:scully ""California"" ""Nevada"" ""Oregon""
> SADD ufos:scully ""Wyoming"" ""New Mexico"" ""Ohio""Nevada, New Mexico, Ohio. Sounds legit:> SADD ufos:mulder ""Florida"" ""Kansas"" ""South Carolina""
> SADD ufos:mulder ""West Virginia"" ""New Mexico"" ""Ohio""New Mexico and Ohio again.Now we need to create some new sets that are the intersection and union of these two sets. We’ll need to store them so we can get their cardinality:> SINTERSTORE ufos:intersection ufos:scully ufos:mulder
(integer) 2> SUNIONSTORE ufos:union ufos:scully ufos:mulder
(integer) 10Now that we have the intersection and the union we can get their respective cardinalities:> SCARD ufos:intersection
(integer) 2> SCARD ufos:union
(integer) 10Or, we can skip that call and just use the results of the SINTERSTORE and SUNIONSTORE commands. Either way, we just divide these two numbers and get the result. Which is 20%.You might not be super familiar with the SINTERSTORE and SUNIONSTORE commands and their possible consequences, so I thought some education might be in order.SINTERSTORE and SUNIONSTORE do the same basic thing as SINTER and SUNION except they store the result at a key (hence the STORE at the end). Using them is easy. Just pass in the desired key of the set to be created and one or more keys containing sets you want to operate against:> SUNIONSTORE destination_key key1 key2 key2 …
(integer) 51While SINTER and SUNION return the intersection and union of the sets they are handed, SINTERSTORE and SUNIONSTORE create new sets (and return the size of said sets). These sets could be quite large and will be written out to the AOF and replicated (if you’re doing those things.)In our example, they are intended to be intermediate values. So, if they are big and they need to be written to the AOF and replicated, this could be a significant performance hit.SINTER and SUNION don’t have this problem. They just return the set they create without the storing of it. We could have built our solution using them and just counted the members returned to get the cardinality. Of course, these sets could also be quite large, and returning them to the client could be a rather expensive operation.There’s often more complexity hidden within once you dive into a problem. We could have solved our Jaccard similarity problem with SINTER and SUNION, or we could have chosen SINTERSTORE and SUNIONSTORE. Size, writing to the AOF, and replication are all factors here. So which is the right approach? As usual, the answer is “it depends.”First and foremost, remember that we are using sets. More significantly, we are using intersection and union commands. The key names above would work only on a single instance without clustering. If you are using clustering, you need to be sure to slot your keys so they all go to the same shard. For more information, check out Kyle Davis’ great blog post on Redis Clustering Best Practices with Keys.Second: It’s not terribly satisfying to have to do the final computation outside of Redis. If we’re going to use Redis to compute Jaccard similarity then we want to use Redis, not Redis and client-side code. We can accomplish this with Lua.If you just want to use Lua for the last mile of this calculation, the code below will work without a problem:> EVAL ""local inter_card = redis.pcall('scard', KEYS[1]); local union_card = redis.pcall('scard', KEYS[2]); local similarity = inter_card / union_card; return tostring(similarity)"" 2 ufos:intersection ufos:unionHowever, if you want to avoid creating the intermediate sets using SUNIONSTORE and SINTERSTORE, there are other ways to do this. But there are tradeoffs. I have a GitHub repo with a couple of ways of solving this that you can explore if you want to go deeper. (It’s in JavaScript. I hope you’ll forgive me.)And just like that, you can now calculate Jaccard similarity and use it for all sorts of cool things. Looking for some fun data to try it against? Check out this large dataset of UFO sightings!"
360,https://redis.com/blog/why-should-you-care-about-kubernetes-k8s/,Why Should You Care About Kubernetes?,"April 14, 2020",Redis,"Is your development team working with microservices architectures? Or are you still trying to wrap your mind around how to get started? Or maybe you’re already at the disillusionment stage where you’re back to writing monolithic applications, like Kelsey Hightower predicted?Regardless of which stage you’re currently in, you most certainly know that microservices require the use of an orchestration platform like Kubernetes (also called K8s). But the real question is whether this is also true in the opposite direction: can Kubernetes be useful in situations that don’t employ a microservices architecture?Adopting Kubernetes is not a lighthearted decision, and it’s reasonable to want to avoid the extra complexity, especially if it turns out to be not a good long-term investment. Still, I’d like to make the argument that gaining some K8s know-how will still prove useful in the eventuality that microservices, and even containers, fall out of fashion.Kubernetes is made up of many different tools, most of which are optional. At its core, K8s is a tool that keeps a cluster of services up and running, allowing you to ask for modifications to the cluster by editing one or more configuration files that concretely define what “up and running” means.Before Kubernetes, we relied on a mix of process monitoring tools and ad-hoc automation tools to keep services up in our systems. Then cloud providers started offering simplified ways of deploying services, like Google Cloud’s AppEngine, Amazon Web Services’ BeanStalk, and Microsoft Azure’s App Service. All these vendor-specific systems used to have significant limitations compared to what is possible nowadays. I still remember my first AppEngine instance back in 2013, where the only languages supported were Python and Go, and the only database available was Google Datastore.In other words, even if you’re not doing microservices, having Kubernetes at your disposal is a good way to maintain your clusters without having to rely on the services your cloud provider offers. For example, it’s trivial to spawn a small Redis instance alongside your containers by applying the K8s sidecar pattern.Orchestration tools like Kubernetes have also proven extremely useful for handling scaling requirements. The recent Covid-19 pandemic that forced people to stay at home caused huge traffic spikes for many popular online services, yet as of mid-April, no major outages have been reported. This is the result of amazing engineering on the part of the online service providers who have been able to keep their systems linearly scalable.To keep a system linearly scalable, you need to ensure that important services can be scaled horizontally (i.e. you can spawn multiple instances at the same time). One common prerequisite for horizontal scalability is statelessness. A service that keeps state in memory requires special operational care that is often not worth the effort when the state could be offloaded to a dedicated database system. Distributed caching and sessions are a good example of this, a topic which I recently discussed at NDC London.In other words, modern applications still need a high degree of automation when it comes to dealing with failures and scalability, and Kubernetes is a very valid way of achieving that, regardless of the architectural choices made during development.You might have expected this point to be clickbait designed to grab your attention, but I’m actually serious. Containers solve two problems for which new technologies may be about to provide arguably better solutions.Application portabilityThe first problem solved by containers is application portability. Moving an application around without containers is not fun, as any DevOps professional can tell you. The main problem is that applications often have a lot of dependencies whose deployment details differ for each operating system (even if we’re just talking about different distributions of Linux!). For example, even a simple Django application will require you to install Python, a few Python packages, and any system dependency those might have. Bundling the application in a container lets you ferry around a binary file that contains everything—sidestepping a lot of problems.Thankfully, not all languages require convoluted steps like Python, Ruby, or JavaScript occasionally do. If you write an application in Go or Rust, for example, you will produce a single binary that can even be compiled to be completely static (i.e. without any form of runtime dependency). Once an application becomes a single binary, wrapping it in a container does little more than add an extra layer of complexity.Resource managementThe second problem solved by containers is limiting an application’s access to resources. Without containers it’s more difficult, but still possible, to limit the amount of DRAM that an application can consume, or restrict access to a specific directory. Containers make it easy to specify these restrictions simply as configuration options.One new technology trying to address the same issue is WebAssembly, a new type of virtual machine with security as a primary goal. A WebAssembly application will need to explicitly ask permission to make use of memory or any other resource. WebAssembly support is being rolled out in major web browsers, but it’s not limited to the web. You can run WebAssembly applications without a browser and open source developers have been experimenting with the idea of writing an entire OS kernel with support for WebAssembly. To complete the picture, it’s possible to create WebAssembly executables from many programming languages, including Go and Rust.Kubernetes without containers?Yes, that’s a real possibility. In a world where most applications compile down to a single binary with proper attention to security already baked-in, containers could become a vestigial abstraction layer with no further use.That said, you will still need orchestration, and I can see Kubernetes’ continued relevance. But let’s assume that a new orchestration technology takes over. Would investing in K8s then be a waste? In my opinion, if done the right way, it won’t.Kubernetes has a few concepts that are universally useful. Let me give you some examples:All these concepts are fundamental building blocks for any distributed system. Now, not every Kubernetes-related technology is as obviously timeless as the ones mentioned above. For example, the final cost-benefit analysis of service meshes like Istio is left to posterity. I don’t necessarily recommend buying into every single new K8s-related trend, but the core ideas are solid and will probably outlive K8s itself.We often hear Kubernetes mentioned alongside microservices architectures, and to some people the two concepts are almost synonymous. In reality, though, K8s is a very useful piece of tooling for medium-to-big organizations that want to streamline service deployment and orchestration regardless of the architectural paradigms they use. Even if you’re skeptical about microservices, investing in K8s should prove a good choice in the long run because modern applications will always require good engineering practices (e.g. statelessness) to be efficiently orchestrated.Of course, many people already have complete faith in Kubernetes, and for them we have created an Operator that supports OSS K8s and a few other flavors, with the goal of supporting all major Kubernetes offerings in the near future. For those who want to be more cautious, we can offer Redis Enterprise as a managed service in your cloud of choice. Even if you remain skeptical of K8s, they’re worth checking out."
361,https://redis.com/blog/introducing-redis-enterprise-5-6-customized-installation-hyperloglog-on-active-active-and-more/,"Introducing Redis Enterprise 5.6: Customized Installation, HyperLogLog on Active-Active, and more!","April 16, 2020",Alon Magrafta,"As we continue to enhance Redis Enterprise by adding new exciting features for our customers, we are happy to announce general availability of the latest major version: Redis Enterprise Software 5.6.0. This release includes:These important new capabilities bring big benefits to a wide variety of Redis users. For a complete list of new features and changes, check out the RS 5.6.0 Release Notes.The Redis Enterprise Software installer now allows you to specify custom installation paths and a custom installation user, group, or both. This provides the flexibility customers require to align with their companies’ security best practices and internal processes and procedures.When running the installer, you can specify the installation, configuration, and var directories, as well as the operating system user and group. Remote users (such as LDAP, etc.) are also supported. To install Redis Enterprise with these new customizations, run:sudo ./install.sh --install-dir <path> --config-dir <path> --var-dir <path> --os-user <user> --os-group <group>You can find all the details in the Redis Enterprise Software installer documentation.Redis Enterprise Software 5.6.0 adds support for HyperLogLog in Active-Active Redis databases.What is HyperLogLog?HyperLogLog is a data structure that probabilistically solves the count-distinct problem. Hyperloglog is available as a Redis data structure, and you use it when you need to count many distinct elements and you’re willing to sacrifice some accuracy (it has a standard error of 0.81%) in exchange for significant memory savings.The HyperLogLog data structure has three main operations:Data types on Active-ActiveWith Active-Active Redis databases, your applications can read and write to the same data set from different geographical locations seamlessly and with latency less than 1 millisecond, without changing the way the application connects to the database. With the addition of HyperLogLog, Active-Active databases now support ten  Redis data types: Floats, Geospatial, Hashes, HyperLogLog, Integers, Counters, Lists, Sets, Strings and Sorted Sets. (Stay tuned: In Redis Enterprise 6.0, Active-Active will also support Streams!)For more information, check out the HyperLogLog on Active-Active documentation.Redis Enterprise Software 5.6.0 adds support for Redis on Flash (RoF) databases with Redis modules. RedisJSON is the first module to run on a RoF database.Redis on Flash offers users major cost savings to users with especially large Redis databases. Where standard Redis databases are held entirely in DRAM, Redis on Flash enables your Redis databases to span both DRAM and dedicated solid-state Flash memory drives (SSDs). This enables you to affordably run much larger datasets with DRAM-like latency and performance.RedisJSON is a Redis module that implements the JSON Data Interchange Standard as a native data type. It allows storing, updating, and fetching JSON values from Redis keys (documents). Using RedisJSON on Redis on Flash allows you to benefit from the high-performance of Redis with high data volumes at a much lower cost. Once again, please stay tuned—we’re working on making more Redis modules support Redis on Flash.The Redis OSS cluster API ensures the fastest possible response times by allowing clients to connect directly to each shard in a cluster. Redis Enterprise databases have long supported the Redis OSS cluster API. Today, we’re announcing cluster API support for Active-Active and Replica Of databases! This increased local performance may be of special interest to customers who need a very high number of operations per second and very low latency, even beyond “regular” Redis’ exceptional performance.Working in OSS Cluster mode improves the performance of client operations against your database, whether it’s a standard database, an Active-Active database, or a Replica Of database. You can create or modify an Active-Active Redis database in OSS cluster mode using the web UI (see below) or using the crdb-cli tool with the –oss-cluster option. Using the crdb-cli tool will affect all the database’s instances.For more information, check out the Redis OSS cluster API documentation.Create and edit databases via the web UIStarting with Redis Enterprise Software 5.6.0, you can configure databases with the OSS cluster API using the web UI. (Note that for standard or Replica Of databases, this configuration applies only to the local database). Here is what the UI looks like for standard or Replica Of databases:When creating new Active-Active databases this configuration will apply to all its instances. When editing a database via the configuration page, it will apply only to the local instance. Here is what the UI looks like for Active-Active databases:These independent enhancements and new features add up to important new capabilities for a wide variety of Redis users. Try them out and see which ones make the biggest difference to you."
362,https://redis.com/blog/tracking-bigfoot-with-redis-and-geospatial-data/,Tracking Bigfoot with Redis and Geospatial Data,"April 23, 2020",Guy Royse,"I think one of the coolest features of Redis—one that surprised me when I discovered it—is the geospatial data structure. Since I thought it was cool, I thought y’all might as well. So, I’m going to share it with you.Redis’ geospatial data structures are fairly straightforward to work with but have some interesting nooks and crannies. We’re going to cover both. And, since I can’t write a blog post without a fun theme for my examples, this time I’m going to be using my favorite dataset of all: Bigfoot sightings.The Bigfoot dataset has lots of great stuff in it. Each row is an account of a Bigfoot sighting and includes the full text of the account, when it happened, and even a classification. The classification is my favorite bit although the accounts are fun to read, too!It’s not relevant to what we are doing today—I’m sharing this just for the fun of it— but here are the three classes of Bigfoot sightings:Today, we’ll be playing with just the longitude, the latitude, and the report number of the Bigfoot sightings. If you want to load the data up and play along, I’ve set up a repository with the data, the code to transform it, and instructions on running it. Of course, you’ll need an install of Redis as well.A Geo Set is the key data structure for working with geospatial data in Redis. It holds a named set of locations on the globe. Adding and updating members to this set is easy. Just use the GEOADD command:> GEOADD bigfoot:sightings:locations -89.15173 37.61335 report:40120This command asks Redis to add or update a member named report:40120 to the Geo Set bigfoot:sightings:locations at longitude -89.15173 and latitude 37.61335. If the member doesn’t exist, it will be inserted. If it does, its location will be updated.Note: This type of operation is called an upsert—a portmanteau of update and insert—and is a popular pattern in Redis. Another pattern you’ll see repeatedly is the variadic command—a command that takes a variable number of arguments.GEOADD is a variadic command as you can also upsert multiple members. Like this:> GEOADD bigfoot:sightings:locations -89.15173 37.61335 report:40120
-88.55 41.33 report:12140Note: The longitude and latitude are presented in the command in that order. Longitude—then the latitude. If you’re like a lot of people, your instinct will be to enter the latitude first. This is easy to get wrong. Watch for it.Great. We’ve upserted a Bigfoot sighting in southern Illinois and one just west of Chicago. If you’re playing along at home you can look them up with these commands:> HGETALL bigfoot:sightings:report:40120
> HGETALL bigfoot:sightings:report:12140You can also look them up from their original source on the Bigfoot Field Researchers Organization’s website. There isn’t a way to find them by report number, but it is part of the link so you can probably figure it out.Anyhow, that covers creating and updating. But what about reading and querying? There are several ways to do this. If you just want to pull out the coordinates you can use the GEOPOS command:> GEOPOS bigfoot:sightings:locations report:40120You can also query for members variadicly:> GEOPOS bigfoot:sightings:locations report:40120 report:12140This will return the coordinates with, perhaps, a bit more precision than they were entered with. That’s an artifact of how they are stored and is something we’ll talk about in a bit:Again, note that the two sets of coordinates returned are longitude first and then latitude.You might want to do something a little more sophisticated. Maybe you want to determine the distance between a couple of Bigfoot sightings. You can find out with the GEODIST command:> GEODIST bigfoot:sightings:locations report:40120 report:12140Just hand GEODIST a key and two members and it will tell you how far apart they are in meters. If you’re not interested in meters and prefer kilometers or Freedom Units (i.e. feet and miles), you can just specify which you want at the end of the command:> GEODIST bigfoot:sightings:locations report:40120 report:12140 m
> GEODIST bigfoot:sightings:locations report:40120 report:12140 km
> GEODIST bigfoot:sightings:locations report:40120 report:12140 ft
> GEODIST bigfoot:sightings:locations report:40120 report:12140 miThese two sightings are about 260 miles apart.You can also find members of the Geo Set in a radius around a specific point. That point can be a coordinate pair or another member of the Geo Set. Southeastern Ohio is actually a hotbed of Bigfoot sightings. Let’s see how many sightings there are near Athens—the largest city in that part of the state:> GEORADIUS bigfoot:sightings:locations -82.109149 39.319950 25 miQuite a few:1) ""report:4982""
2) ""report:9042""
...snip...
15) ""report:8017""
16) ""report:10945""Here’s the same command using a member instead of coordinates:> GEORADIUSBYMEMBER bigfoot:sightings:locations report:9042 25 miYou can also request extra information on those locations by adding WITHCOORD and/or WITHDIST at the end:> GEORADIUS bigfoot:sightings:locations -82.109149 39.319950 25 mi WITHCOORD WITHDISTSo that’s GEOADD, GEOPOS, GEODIST, GEORADIUS, and GEORADIUSBYMEMBER. These commands allow you to create, read, and update geospatial data in Redis. However, you might notice I have talked only about three of the four legs of CRUD. Where’s the GEO-something-or-other command to remove members? Well, that’s one of the first interesting bits.So before we can talk about delete, we need to talk about geohashing. Geohashing is a clever way to store coordinates in a single integer and then represent that as a base-32 encoded string.Geohashing stores the coordinates one bit at time as you subdivide the Earth. The first division splits the globe in twain along the prime meridian. The most significant integer is 0 if the location is in the western hemisphere or a 1 if it is in the eastern hemisphere. The next bit splits along the northern and southern hemispheres. Northern gets a 1 and southern gets a 0. You keep dividing like this, east and west then north and south, and keep adding bits until you are at a resolution you are happy with. Then, you encode the bits.Redis can do this for you with the GEOHASH commands. It works just like GEOPOS but instead of returning coordinates, it returns a geohash. And, of course, it’s variadic:> GEOHASH bigfoot:sightings:locations report:40120 report:12140
1) ""dn8tgr39wh0""
2) ""dp350gzueq0""These are base-32 encoded numbers. This means that a Geo Set could be represented as a set with a numeric value. Redis has a data type like that: the Sorted Set. And, that’s exactly how Redis implements Geo Sets.Behind the scenes, Geo Sets are Sorted Sets. The number in a sorted set is a 64-bit floating-point number. The integer representing the geohash is stored in that float and can safely be no larger than a 52-bit integer (which is plenty). When you call GEOHASH, Redis gets the number and base-32 encodes it. When you call GEOPOS, Redis gets the number and converts it to coordinates, which is why the coordinates you enter aren’t exactly the ones you get back.And since Geo Sets are Sorted Sets, all the Sorted Set commands work on Geo Sets—although some come with caveats. For example, want to get the underlying integer for the geohash?> ZSCORE bigfoot:sightings:locations report:40120
""1781261397121617""Want to get all the members?> ZRANGE bigfoot:sightings:locations 0 -1
1) ""report:8059""
2) ""report:4886""
3) ""report:1031""
...snip...Actually, don’t do that. Use ZSCAN instead:> ZSCAN bigfoot:sightings:locations 0 COUNT 5So, how would you remove a member? With ZREM:> ZREM bigfoot:sightings:locations report:40120 report:12140And now when you go to get them, they’re gone:> GEOPOS bigfoot:sightings:locations report:40120 report:12140
1) (nil)
2) (nil)That’s pretty much everything you can do to a Geo Set. But it certainly isn’t everything you can do with a Geo Set. There are plenty of applications for geospatial data. In addition to the rather whimsical tracking of Bigfoot, you could also combine geospatial data with Pub/Sub to do real-time tracking of whatever you’d like—be that users of a mobile app, trucks in your fleet, or non-cryptozoological animals in an environmental study."
363,https://redis.com/blog/ingesting-iot-data-efficiently-with-redis-streams/,Ingesting IoT Data Efficiently with Redis Streams,"April 27, 2020",Redis,"Before joining Redis, I worked for a consultant firm working with technologies like the Internet of Things (IoT). If you’re not familiar with the concept, IoT is all about smart devices connected to the internet. Many of these devices include sensors that produce metrics about the real world. One notable IoT use case, for example, are bike sharing companies, where the bike itself is connected to the internet and constantly notifies its owners of its location and other conditions. But perhaps the biggest, most promising uses of IoT technology come in factories, or the industrial internet of things (IIoT).While the IoT opens a window on a great range of possibilities, with market predictions topping $1 trillion within a few years, and some 41.6 billion IoT devices expected to generate 79.4 zettabytes of data, there are still challenges to be overcome when implementing services based on IoT, primarily around handling IoT data at scale.Going back to my consultant experience, I remember meeting with a data science team from a company that had issues with its IoT solution. The main conundrum the team faced was about storing data in a NoSQL document-based database. They were trying to decide between two data models, where one would optimize for query speed, while the other would optimize for space efficiency.The first model consisted in storing one datapoint per document, like this:This approach would yield the fastest query speed, but on the downside it would take up a lot of storage space. The other model, designed to use space more efficiently, looked like this:As you can see, this second model is more complex: each document holds multiple data points in a single chunk. This model required more-complicated queries to account for the chunks, but in exchange it provided some space savings.At the time I didn’t have a good solution to this problem, but now I realize that with Redis Streams the problem doesn’t exist in the first place.When you have a stream of miscellaneous data that is time-directed and intrinsically immutable, as is usually the case with IoT data, Redis Streams are probably the right data structure for the initial ingestion.A Redis Stream key contains an indexed list of entries. Each index ID is a millisecond-precise timestamp followed by a sequence number for events that happen within the same millisecond. Each entry is a sequence of field-value pairs, almost the same as a Redis Hash. “Almost” because Redis Hashes are a good way to visualize what you can put in a stream except for one small difference: a Redis Stream entry can have multiple instances of the same field, while fields in a Redis Hash are unique.The following command shows how to add an entry to a Redis Stream. Check out the documentation to learn more.> XADD mystream * time 123123123 lon 0.123 lat 0.123 battery 0.66Redis Streams are indexed using a radix tree data structure that compresses index IDs and  allows for constant-time access to entries.Redis Streams also employ another smart space-saving mechanism that applies to field names.You might not think about it often, but the names that you choose for your entry fields impact how much space each entry takes. With SQL databases the problem doesn’t come up because the schema is fixed at the table level, but the downside is that you lose flexibility. With NoSQL, document-based databases you get more flexibility but then documents have the overhead of saving the strings representing field names over and over again.In Redis Streams, each entry in a stream can have a different set of fields, so you can have as much flexibility as you need, but if you keep the set of fields stable, Redis will not store multiple copies of their names. Depending on the number of fields you have, the amount of overhead you avoid with this feature can be significant.As a simple example, imagine you have 1 million entries, 20 fields each, with an average field-name length of 15 bytes. This translates to roughly 300 megabytes of overhead you avoid for each million items.While Redis Streams are a perfect fit for internet of things use cases, they are hardly limited to only IoT. Inversely there are many more data structures in Redis that can help you save space when ingesting data at scale, including probabilistic data structures, for example.After you have mastered Redis Streams, consider using the RedisTimeSeries module for storing numeric time-series data derived from the raw data you ingested. If you want to learn more about both Redis Streams and RedisTimeSeries, sign up for our training day at RedisConf 2020 Takeaway, occuring online on May 12-13."
364,https://redis.com/blog/redisconf-2020-training-day-brush-up-on-your-redis-skills/,RedisConf 2020 Training Day: Brush Up on Your Redis Skills,"April 29, 2020",Redis,"RedisConf 2020 Takeaway is just around the corner, and free registration is still open to attend the world’s largest annual gathering of the Redis community—online and on demand. This year we’re inviting you to Rediscover Redis and leverage developers’ favorite database to its full potential beyond caching use cases.The main conference, with more than 50 breakout sessions from experts and active community members, is on May 12. (Learn more about seven RedisConf speakers).But that’s just the beginning for RedisConf 2020. On our May 13 Training Day, you’ll be able to watch more than a dozen technical workshops and tune in to livestream Q&As with Redis experts. And don’t miss the chance to use those skills in the Redis ‘Beyond Cache’ Hackathon, where you could win $5,000!For Training Day, we’re premiering 15 on-demand videos, many covering important topics for the first time, including security, advanced RediSearch, and a special 90-minute Redis Developer Certification Cram course. Here’s a sample of what will be on tap:Intro to RedisGraphSpeaker: Guy Royse, Developer Advocate at RedisA great introduction to graph databases, demonstrating the unique set of problems graph databases are able to solve. In this session, Guy will cover what graph databases are, what kind of problems they can solve, and how RedisGraph is both distinct from and compatible with other graph databases. You will also learn the basics of Cypher, an open graph query language.RediSearch Advanced TopicsSpeaker: Kyle Davis, Head of Developer Advocacy at RedisWhile an introduction to RediSearch has been a mainstay on previous RedisConf Training Day agendas, RedisConf 2020 Takeaway will mark our first advanced RediSearch course. Tune into my session, where I’ll demonstrate aggregations, conditional updates, and ephemeral indexing (get a preview on the latter in my recent blog post).Redis Security Best PracticesSpeaker: Jamie Scott, Senior Product Manager at RedisThere are lots of new security features in Redis 6, and Jamie will cover how to scale Access Control Lists (ACLs), Transport Layer Security (TLS), and Redis-secure design patterns. Plus, you’ll learn some holistic best practices for maximizing Redis security.Redis Developer Certification CramSpeaker: Kyle Banker, Senior Director of Developer Education at RedisNow’s the perfect time to get certified in one of the 20 fastest-growing tech skills. While the Redis Developer Certification Program includes an extensive study guide, practice test, and a formal certification exam, we’ve condensed all the studying down into this special cram session. Kyle will review the certification study guide, work through sample test questions, and review the exam experience.Livestream Q&As9 a.m. – 5:10 p.m. PT on May 13While our Training Day sessions are available to view on demand, the experts teaching them will be available for live office hours all day on May 13. Get your questions answered on everything from RedisJSON to Active-Active to PyTorch and RedisAI.We hosted our first-ever Hackathon at Redis Day Bangalore, challenging 37 competitors across a dozen teams to demonstrate how to use Redis “beyond cache.” If you weren’t in Bangalore, don’t worry—we’re now hosting our first fully online hackathon, and awarding $15,000 in prizes!This year’s RedisConf theme is Rediscover, and we want you to demonstrate how you’ve pushed the boundaries of Redis beyond caching. Entries must meet at least one of two categories:Prizes: The Grand prize (first-place) winner will receive $5,000, while second place gets $3,000, third receives $1,500, and fourth $500. A People’s Choice winner will also receive $5,000. Plus, all the winning teams will receive Redis swag—one Redis t-shirt and sticker per team member—and their projects will be showcased on the Hackathon website and in a Redis blog post.Judges will score eligible submissions by quality of the idea, design, and technical implementation (get more details on what they’re looking for).How to enter: The Hackathon is open now, so sign up and get started right away. Tune into RedisConf 2020 Takeaway from May 12–13 to hear the latest product announcements and build new skills you can use to your advantage when developing your project.Redis “Beyond Cache” Hackathon schedule:Get more information on the hackathon and sign up now!"
365,https://redis.com/blog/you-dont-need-transaction-rollbacks-in-redis/,You Don’t Need Transaction Rollbacks in Redis,"May 4, 2020",Redis,"Redis features two main mechanisms for executing multiple operations atomically: MULTI/EXEC transactions and Lua scripts. One peculiarity of transactions in Redis that often trips up newcomers is the absence of a rollback mechanism. In my tenure as a Developer Advocate at Redis, I’ve talked to a few engineers with traditional SQL backgrounds who found this troubling, so with this blog I want to share my opinion on the subject, and argue that you don’t need rollbacks in Redis.Transactions in Redis start with the MULTI command. Once it’s sent, the connection switches mode and all subsequent commands sent through the connection will be queued by Redis instead of being immediately executed, with the exception of DISCARD and EXEC (which will instead cause the transaction to abort or commit, respectively). Committing the transaction means executing the previously queued commands.MULTI
SET mykey hello
INCRBY counter 10
EXECTransactions (and Lua scripts) ensure two important things:One last basic thing to keep in mind about transactions: Redis will continue to serve other clients even when a MULTI transaction has been initiated. Redis will stop applying other clients’ commands only briefly when the transaction gets committed by calling EXEC. This is very different from SQL databases, where transactions engage various mechanisms within the DBMS to provide varying degrees of isolation assurances, and where clients can read values from the database while performing the transaction. In Redis, transactions are “one shot”—in other words, just a sequence of commands that get executed all at once. So, how do you create a transaction that depends on the data present in Redis? For this purpose, Redis implements WATCH, a command for performing optimistic locking.Optimistic locking with WATCHLet me show you on a practical level why you can’t read values from Redis while in a transaction:MULTI
SET counter 42
GET counter
EXECIf you run this series of commands in redis-cli, the reply from “GET counter” will be “QUEUED”, and the value “42” will be returned only as a result of calling EXEC, alongside the “OK” returned from executing the SET command.To write a transaction that depends on data read from Redis, you must use WATCH. Once run, the command will ensure that the subsequent transaction will be executed only if the keys being WATCHed have not changed before EXEC gets called.For example, this is how you would implement an atomic increment operation if INCRBY did not exist:WATCH counter
GET counter
MULTI
SET counter <the value obtained from GET + any increment>
EXECIn this example, we first create a WATCH trigger over the “counter” key, then we GET its value. Notice how GET happens before we start the transaction’s body, meaning it will be executed immediately and it will return the key’s current value. At this point, we start the transaction using MULTI and apply the change by computing on the clientside what the new value of “counter” should be.If multiple clients were trying to concurrently apply this same transaction to the “counter” key, some transactions would be automatically discarded by Redis. At this point it would usually be the client’s job to retry the transaction. This is similar to SQL transactions, where higher isolation levels will occasionally cause the transaction to abort, leaving the client the task of retrying it.While WATCH can be very useful for performing articulated transactions, it’s usually easier and more efficient to use a Lua script when you need to perform multiple operations that depend on data in Redis. With a Lua script, you send the logic to Redis (in the form of the script itself) and have Redis execute the code locally, instead of pushing data to the client as we were doing in the example above. This is faster for several reasons, but here’s the big one: Lua scripts can read data from Redis without needing optimistic locking.This how the previous transaction would be implemented as a Lua one-liner:EVAL ""redis.call('SET', KEYS[1], tonumber(redis.call('GET', KEYS[1]) or 0) + tonumber(ARGV[1]))"" 1 counter 42There are, in my opinion, a couple of reasonable situations where you might legitimately prefer transactions with optimistic locking over Lua:Unless both these points are true for your application, I recommend you choose Lua over WATCH.To recap: MULTI/EXEC transactions (without WATCH) and Lua scripts never get discarded by Redis, while MULTI/EXEC + WATCH will cause Redis to abort transactions that depend on values changed after the corresponding keys were WATCHed. Lua scripts are more powerful than simple (i.e. WATCH-less) transactions because they can also read values from Redis, and are more efficient than “WATCHed” transactions because they don’t require optimistic locking to read values.The key point about optimistic locking is that when a WATCHed key is changed, the whole transaction is discarded immediately when the client commits it using EXEC. Redis has a main, single-threaded command execution loop, so when the transaction queue is being executed no other command will run. This means that Redis transactions have a true serializable isolation level, and also means that no rollback mechanism is required to implement WATCH.But what happens when there’s an error in a transaction? The answer is that Redis will continue to execute all commands and report all errors that happened.To be more precise, there are some types of errors that Redis can catch before the client calls EXEC. One basic example are blatant syntax errors:MULTI
GOT key? (NOTE: Redis has no GOT command and, after season 8, it never will)
EXECBut not all errors can be discovered by inspecting the command syntax, and those could cause the transaction to misbehave. As an example:MULTI
SET counter banana
INCRBY counter 10
EXECThe example above will be executed but the INCRBY command will fail because the “counter” key doesn’t contain a number. This type of error can be discovered only when running the transaction (nevermind that in this simplified example we are the ones setting the wrong initial value).This is the moment where one might say that rollbacks would be nice to have. I might agree if not for two considerations:The second point is particularly important because it also applies to SQL: SQL DBMSs offer many mechanisms to help protect data integrity, but even they can’t completely protect you from programming errors. On both platforms, the burden of writing correct transactions remains on you.If that seems to conflict with your experience using SQL databases, let’s look at the difference between relying on errors to enforce constraints vs. relying on errors to protect the data from bugs in your code.It’s common practice in SQL to use indexes to implement constraints on the data and rely on those indexes on the client side for correctness. A common example would be to add a “UNIQUE” constraint to a “username” column to ensure that each user has a different username. At that point clients would try to insert new users and expect the insertion to fail when another user with the same name already exists.This is a perfectly legitimate use of a SQL database, but relying on the constraint to implement application logic is very different than expecting rollbacks to protect you from mistakes in the transaction logic itself.At AWS re:Invent 2019, when an attendee asked me “Why doesn’t Redis have rollbacks?” my answer was based on enumerating why people use rollbacks in SQL. In my opinion, there are only two main reasons to do so:First reason to use rollbacks: concurrencyMost common SQL databases are multithreaded applications, and when a client requests a high isolation level, the DBMS prefers to trigger an exception rather than stop serving all other clients. This makes sense for the SQL ecosystem because SQL transactions are “chatty”: a client locks a few rows, reads a few values, computes what changes to apply, and finally commits the transaction.In Redis, transactions are not meant to be as interactive. The single-threaded nature of the main event loop in Redis ensures that while the transaction is running, no other command gets executed. This ensures that all transactions are truly serializable without violating the isolation level. When a transaction uses optimistic locking, Redis will be able to abort it before executing any command in the transaction queue—which doesn’t require a rollback.Second reason to use rollbacks: leveraging index constraintsIn SQL, it’s common to use index constraints to implement logic in the application. I mentioned UNIQUE, but the same applies to foreign key constraints and more. The premise is that the application relies on the database to have been properly configured and leverages index constraints to implement the logic in an efficient way. But I’m sure everyone has seen applications misbehave when somebody forgets to put in a UNIQUE constraint, for example.While SQL DBMSs do a great job of protecting data integrity, you can’t expect to be protected from all errors in your transaction code. There is a significant class of errors that don’t violate type checking or index constraints.Redis has no built-in index system (Redis modules are a different story and don’t apply here). To force uniqueness, for example, you would use a Set (or equivalent) data type. This means the correct way to express an operation in Redis looks different from the equivalent in SQL. Redis’ data model and execution model are different enough from SQL that the same logical operation would be expressed in different ways depending on the platform, but the application must always be in sync with the state of the database.An application that tries to INCRBY a key that contains a non-numeric value is the same as an application that expects a SQL schema inconsistent with what’s on the database. If you have gremlins in your Redis database making unexpected changes, lock them out using access control lists (ACLs).If you come from a SQL background, you might understandably be surprised by how transactions work in Redis. Given that NoSQL has demonstrated that relational databases are not the only valuable model for storing data, don’t make the mistake of assuming that any diversion from what SQL offers is inherently inferior. SQL transactions are chatty, based on a multi-threaded model and interoperate with other subsystems to leverage rollbacks in case of failures. In contrast, Redis transactions are more focused on performance and there is no indexing subsystem to leverage for enforcing constraints. Because of these differences, the transaction-writing “style” that you use in Redis is fundamentally different from the SQL one.This means that the lack of rollbacks in Redis doesn’t limit expressiveness. All reasonable SQL transactions can be rewritten to a functionally equivalent Redis transaction, but it’s not always trivial to do in practice. Reasoning about a problem originally articulated in SQL in Redis requires you to think about your data in a different way, and you also need to account for the different execution model.Finally, it’s true that rollbacks can be useful to protect your data from programming errors, but they are not meant to be a solution to that problem. As a multi-model database based on a key-value structure, Redis doesn’t offer the same level of “type checking” ease that SQL does, but there are techniques to help with that, as Kyle Davis, Head of Developer Advocacy at Redis, explained in this recent blog post: Bullet-Proofing Lua Scripts in RedisPy.That said, your applications need to be in sync with what’s in the database, both when using a relational database and when using Redis. For Redis, the utility of rollbacks would not outweigh the costs in terms of performance and additional complexity. If you ever wondered how Redis can be so much faster than other databases, here’s yet another reason why."
366,https://redis.com/blog/redisconf-2020-takeaway-is-almost-here/,RedisConf 2020 Takeaway Is Almost Here!,"May 6, 2020",Mike Kwon,"We are only a few days away from RedisConf 2020 Takeaway, the largest annual gathering of the Redis community! This year is all about Rediscovering Redis and leveraging developers’ favorite database to its full potential—beyond traditional caching use cases.You still have time to sign up for two free days of programming—on Tuesday, May 12, we’ll be premiering more than 50 sessions for you to watch on demand and streaming live Q&As and keynotes with industry experts and active community members. Then on May 13, stay tuned for 15 pre-recorded technical training workshops and live Q&As with Redis experts. Can’t make it next week? Don’t worry—all the sessions and training workshops will be available on demand after the conference.In case you weren’t already excited, here are the latest goodies we wanted to share about what to expect from RedisConf 2020 Takeaway, including the RedisConf virtual world, our special Ask the Expert sessions, and more!Register now!While we won’t be able to meet together in-person this year, we’re working on bringing you the next best thing. At RedisConf 2020 Takeaway, you’ll be able to create your own Redis avatar and interact with fellow Redis Geeks in an online landscape filled with keynote and training content, live interactions, games, and more. You’ll be able to navigate throughout the world, and maybe even play a round of ping-pong along the way.For the first time, we’re offering exclusive consultations with our Redis engineers, solution architects, and product managers. Sign up to get your specific questions answered in a private session! Our experts will be able to advise you on the following topics:Find more information on the RedisConf 2020 Takeaway sign-up page. Don’t miss out—the experts’ calendars are filling up fast, so reserve your spot today!Along with Julia Liuson, Corporate Vice President of Microsoft’s Developer Division, Atlassian CTO Sri Viswanath will also be taking the virtual stage for a keynote during RedisConf 2020 Takeaway.At Atlassian since January 2016, Sri helms the company’s cloud-native journey, leading the building and scaling Atlassian’s cloud platform. Previously, Sri served as CTO and Senior Vice President of Engineering at Groupon, Vice President of R&D for Mobile Computing at VMware, and Senior Vice President of Engineering at Ning—where he was instrumental in the company’s acquisition by Glam.He also led development of a number of successful open-source and B-to-B products at Sun Microsystems, served on the Board of Directors for SendGrid, and holds a number of patents. Sri currently serves on the Board of Directors for Splunk and earned a M.S. in Management from Stanford University and a M.S. in Computer Science from Clemson University.He’ll be presenting his live keynote address on May 12 starting at 12:30 p.m. Pacific Time, so make sure you tune in! (Julia Liuson will be presenting her live morning keynote in the 9–10:30 a.m. PT morning block). (Learn more about seven other RedisConf speakers you won’t want to miss).This year we’re hosting our first fully online hackathon! Diving into RedisConf’s Rediscover theme, we’re asking developers to demonstrate how to push the boundaries of Redis beyond caching. RedisConf 2020 Takeaway is the perfect place to hear the latest product announcements and build new skills you can use to your advantage when developing your project. Then submit a project that uses either Redis modules or event-driven architectures by May 27 for a chance to win up to $5,000.Learn more about what the judges are looking for, the hackathon schedule, and submission guidelines in this blog post, and enter the hackathon now!We’re collecting stories from the Redis community to highlight examples of Redis’ versatility and demonstrate how Redis powers some of the world’s most successful applications. Enter the Rediscover Redis competition and explain how you use Redis beyond caching for a pretty sweet prize: your project featured in the keynote at RedisConf 2020 Takeaway and a Valve Index VR Kit (a $999 value)!Plus, for every valid submission we’re donating $100 to the COVID-19 Solidarity Response Fund, which supports the World Health Organization’s work to ensure patients get proper care, frontline workers get essential supplies, and development of a vaccine and treatments can be accelerated.Hurry—submissions are due Friday, May 8, 2020! Winners will be announced at the RedisConf keynote on May 12. Learn more information about the competition in this blog post: Enter the Rediscover Redis Competition!"
367,https://redis.com/blog/introducing-redisgraph-2-0/,Introducing RedisGraph 2.0,"April 7, 2020",Pieter Cailliau and Alex Milowski,"Redis is a noSQL database that enables users to store a variety of data in a variety of data structures. We saw a need for our customers to create highly connected data and derive insight by using graph technology. Originally released in 2018, RedisGraph is the result of that effort.RedisGraph is based on a unique approach and architecture that translates the Cypher query language to matrix operations executed over a GraphBLAS-based engine. By using sparse matrices to represent graphs and the power of GraphBLAS to process them, RedisGraph delivers a fast and efficient way to manage and process graph data shown to have significant advantages over other approaches in some applications.Since the release of version 1.0 of RedisGraph, the project has gathered 1,000 GitHub stars and over 100,000 Docker pulls. During that time we’ve also been hard at work building the next version, and we’re excited to announce RedisGraph 2.0, designed to fulfill the promise of RedisGraph 1.0 in the areas most important to our customers. Graph databases can support a wide range of use cases from resource management to health care. We’ve been working hard to support the queries necessary for a range of applications in resource management, fraud prevention, graph-aided search, and knowledge graphs.There are too many RedisGraph enhancements since version 1.0 to list them all here. We’ll summarize them via examples of the improvements to our support of Cypher, full-graph response, the integration of RediSearch for full-text search over graphs. We’ll also detail the enhancements to RedisInsight to support graphs and introduce two RedisGraph partners: Linkurious and Graphileon.To learn more about the significant performance enhancements in RedisGraph 2.0 and get more info on the benchmarking process, check out RedisGraph 2.0 Performance Benchmarks.The release of RediSearch 1.6 introduced a new low-level API that allows other modules to consume RediSearch for secondary indexing and full-text search purposes. RedisGraph 2.0 is the first generally available module to take full advantage of this capability. Prior to this release, RedisGraph had indexing capabilities via skiplists that was quite performant, but was limited to exact matches and did not allow for prefix or fuzzy matching. Also, there was no way to leverage two separate indexes on a given label as compound indexes.RedisGraph 2.0 supports full-text search on property values. An index can be created for a specific node and property combination:CALL db.idx.fulltext.createNodeIndex('Person','name')and then can be queried via Cypher as shown here:CALL db.idx.fulltext.queryNodes('Person','Bob')
YIELD node AS p
RETURN p.nameWhat follows the CALL expression can be a variety of Cypher expressions. The nodes returned can be combined with a match expression to further qualify the search results. Enriching the search results with the connections of a graph is called graph-aided search.An example of graph-aided search is finding someone who is connected to an individual by a certain number of degrees of separation. A common example of this is the search functionality in LinkedIn, where people who are more closely connected to you are listed towards the top of the search matches.The following query demonstrates how this could be implemented:CALL db.idx.fulltext.queryNodes('Person','%yif%') YIELD node
MATCH p=(node)-[:CONNECTED*1..3]->(:Person {name:'Pieter'})
RETURN p.name, p.title, length(p) AS connectedness
ORDER BY connectedness ASC LIMIT 20Instead of returning only a tabular result of properties values, RedisGraph 2.0 can now return nodes, relations, and other data types. For example, previously you could return only tabular results (e.g., properties values):MATCH (me:Person)-->(friend:Person)-->(fof:Person)
WHERE me.name='Pieter'
RETURN friend.name, fof.nameWith a full-graph response, you can return nodes and relations directly. For example, the previous query can return the friend, the friend of the friend, and the relationship between them:MATCH (me:Person)-->(friend:Person)-[f:FRIEND]->(fof:Person)
WHERE me.name='Pieter'
RETURN friend, f, fofThis new feature enables such uses like Object Graph Mapping (OGM), querying subgraphs, and visualizations where the receiving application needs all the nodes and their relationships.To demonstrate the power of this full-graph response, we want to showcase some the integration with RedisInsight and the partnerships with Linkurious and Graphilean enabled by this feature.RedisGraph in RedisInsightOne such application is RedisInsight, where a user can now enter a query and receive a visualization (as shown above). The full-graph response allows RedisInsight to present a visualization and allows the inspection of all the properties of the nodes and edges returned by the query without having to know these properties in advance.RedisInsight 1.2 provides direct support for exploring and querying the graphs stored in Redis. The new support for full-graph response allows RedisInsight to present the query subgraph as a visualization directly to the user. Subsequently, a user can click on nodes and edges in the graph to inspect property values or expand the graph further.LinkuriousRedis has partnered with the French software developer Linkurious, which provides an enterprise platform, Linkurious Enterprise, for fraud, money laundering, and cyber threat protection via graph analytics. Linkurious has developed the OGMA library that powers the visualizations inside RedisInsight. This library allows users to visually interact with graphs stored in RedisGraph.In addition, the full-graph response now available in RedisGraph 2.0 allows integration with Linkurious’ graph visualization and analysis platform. We expect even deeper integration in the future.In addition, the full-graph response now available in RedisGraph 2.0 allows integration with Linkurious’ graph visualization and analysis platform. We expect even deeper integration in the future.GraphileonOn January 27th, 2020, Redis and Dutch software company Graphileon announced a partnership to let RedisGraph users manage their data and build applications in Graphileon’s advanced graphing tools. The combination of RedisGraph and Graphileon enables our customers to enjoy fast and easy data management and to query and analyze the data using the power of the Cypher query language all the while leveraging the specific strengths of RedisGraph.The list of Cypher enhancements added to RedisGraph 2.0 is really long. Notable additions include a variety of operators and functions, support for simple case statements, enhanced merge support, counting of aggregates, support for naming paths, and reusing entities in pattern matching. There’s no way to explain all the additions in a blog post, so let’s look at an example of Triadic closure to highlight some of the most interesting ones.The improved support for merges, paths, and operators allows for new support for graph manipulation. We can follow a particular shared relationship between two unrelated nodes to infer a new relationship. For example, if two people know a common person, a social media service might suggest an introduction. Similarly, if two people are managed by the same person, they are peers in the organization. Computing these new relationships are examples of triadic closure.Consider the two graphs below. The graph on the left shows a “friend” relationship (i.e., who knows whom). The graph on the right shows an organizational structure (i.e., who manages whom). In both graphs, the relationship between the “B” and “C” nodes is not present in the graph.We can query for suggested introductions in the left-hand graph by following the “KNOWS” relationship:MATCH (b:Person)-[:KNOWS]->(a:Person)-[:KNOWS]->(c:Person)
WHERE NOT (b)-[:KNOWS]-(c) AND b <> c
RETURN b, cThe query follows the “KNOWS” relationship from node B to node C via A, but qualifies that we want nodes that do not have an existing relationship (or cycles) via the “WHERE NOT” clause.In the right-hand graph, we can create a peer relationship (via MERGE) by finding a path where we have two co-managed persons without an existing peer relationship:MATCH (b:Person)<-[:MANAGES]-(a:Person)-[:MANAGES]->(c:Person)
WHERE NOT (b)-[:PEER]-(c) AND b <> c
MERGE (b)-[:PEER]-(c)We can further qualify the relationship by adding a role and direction to the edge:MATCH (b:Person)<-[:MANAGES]-(a:Person)-[:MANAGES]->(c:Person)
WHERE NOT (b)-[:PEER]-(c) AND b <> c
WITH b, c,
CASE b.level
WHEN 'Director' THEN 'mentor'
ELSE 'peer'
END AS role
MERGE (b)-[:PEER {role:role}]->(c)Stay tuned to the Redis blog for more detailed examples of new RedisGraph 2.0 features over the next few months. We’re also working on more enhancements to RedisGraph performance, Cypher coverage (such as OPTIONAL MATCH), and adding graph algorithms via the open-source LAgraph algorithm collection for GraphBLAS in upcoming releases.RedisGraph 2.0 is available via Docker, in Redis Enterprise Software (RS) 5.4.14, and in Redis Cloud Pro. If you tried RedisGraph 1.0, check out the latest version to see what’s new and improved. If you’re interested in graph databases, consider looking into the power of RedisGraph 2.0. If you’re a Redis user new to graph databases, now would be a good time to find out what RedisGraph could do for your existing applications."
368,https://redis.com/blog/what-your-database-needs-to-do-now/,What Your Database Needs to Do Now,"May 7, 2020",Haley Kim,"Jenny is looking to buy a new pair of sandals for the summer, so she checks out her favorite retailer’s website. As she browses the selection and clicks on the shoes she’s interested in, she notices that each page takes forever to load. Frustrated, she goes to another retailer’s website, where she makes her purchase.This is just one example demonstrating why instant database performance is so critical. Research indicates that apps have roughly 100 milliseconds—one-third of the time it takes to blink—before users begin to feel that they’re waiting for a response. The problem is that a round-trip request and response to internet servers can easily take 50ms. The processing of the request can take another 50ms, which basically leaves no time for the database to respond. That means that modern databases have to respond in less than 1ms to avoid becoming a performance bottleneck!Instant performance is one of the many requirements of modern databases, and one of a number of trends transforming the demands on the databases in your organization’s technology stack. Today’s databases face unprecedented challenges: they must work with mountains of disparate data, return results in the blink of an eye, and cope with hugely complex technology environments, all while remaining easy to work with and highly available.Keeping up with industry trends is important to ensure you don’t fall behind. That’s why leaders, decision makers, and experts in every industry spend sizeable amounts of time dissecting and analyzing the latest trends in the database space.We’ve made it easy to stay ahead of the curve with our new e-book, 9 Things Your Database Must Do. You’ll learn how:Learn about the eight other requirements of modern databases by downloading the new e-book for free now. And while you’re at it, check out some of our other resources:"
369,https://redis.com/blog/secure-enclaves-future-of-data-security/,Secure Enclaves Could Be the Future of Data Security,"May 12, 2020",Jamie Scott,"If you work in cybersecurity, or if you’ve interacted with your organization’s cybersecurity team, you’ve likely heard the question, “Is everything encrypted in transit and at rest?” Encryption in transit protects data while in motion, and encryption at rest protects data in storage.In highly regulated industries, this is standard operating procedure. In less regulated industries, it’s still a prudent practice when handling customer data.But there’s another realm of encryption that few talk about, and that’s encryption in use. Encryption in use protects data when it is being processed in memory. Practically speaking, encryption in use prevents someone who has access to a server from being able to access data through a memory dump or memory forensics, which can be performed on any process running on a server. (To truly understand the importance of encryption in use, you should approach it with the mindset that your organization has already been compromised and security is about minimizing the potential damage.)Today, encryption in use is typically done with client-side encryption. However, there is an emerging technology, called secure enclaves, that promises to solve many of the limitations of client-side encryption.Client-side encryption, or the practice of encrypting data within an application before storing it in a database, such as Redis, is the most widely adopted method to achieve encryption in use.Client-side encryption also protects against insider threats. Reducing who can access your data transforms who you have to trust when conducting business operations. With client-side encryption, you no longer have to trust an administrator on the operating system of your database. Similarly, client-side encryption helps remove third parties from who or what you need to trust to run your application, called the trusted computing base. You no longer have to trust your cloud provider not to misuse your data because it has access to the operating system or hypervisor, you can ensure that cloud providers cannot access your data.These benefits apply to both client side encryption and the use of a secure enclave.However, there are two big limitations to client-side encryption. First, functions that need to operate on data—such as simple searching functions, comparisons, and incremental operations—don’t work with client-side encryption. The command line example below shows how incrementing an encrypted number fails because the data is no longer recognized after being encrypted client side:$ echo ""33"" >> secrets.txt
$ openssl aes-256-cbc -a -salt -in secrets.txt -out secrets.txt.enc
$ enter aes-256-cbc encryption password:*****
$ Verifying - enter aes-256-cbc encryption password:*****
$ cat secrets.txt.enc
U2FsdGVkX1+zYi/m14irl+JeZokh75XxRAG4HBA56bk=
$ redis-cli set mysecret U2FsdGVkX1+zYi/m14irl+JeZokh75XxRAG4HBA56bk=
OK
$ redis-cli incrby mysecret 1
(error) ERR value is not an integer or out of rangeIn addition, multiple services often have to access the same database, which brings with it the complexity and investment of managing encryption keys across multiple applications. This raises the administrative overhead of deploying client-side encryption.Secure enclaves promise to help reduce the barriers to encryption in use. Secure enclaves are private allocations of memory protected from use by external processes. An ecosystem of hardware vendors, cloud providers, and software makers is now forming to make the use of secure enclaves more accessible to the software community. As of April 2020, support for secure enclaves is available in some on-premises hardware, in a subset of Microsoft Azure virtual machines, and in dedicated hardware instances in Alibaba Cloud and IBM Cloud.So, what do secure enclaves need to achieve broad success? According to the theory of the diffusion of innovation, many emerging technologies struggle to cross the chasm between attracting early adopters and making headway with the early majority. As secure enclave technology develops and finds new followers, it is approaching that chasm, which has been the graveyard of many promising technologies. But given the technology’s momentum and the ecosystem building around it, we’re really excited about the prospects for secure enclaves.Four critical things must happen for secure enclaves to successfully cross that chasm:Redis is partnering with those in the community who are developing the secure enclave ecosystem. We’re working to take Redis and Redis Enterprise into the rapidly forming secure enclave ecosystem by partnering with Anjuna, a vendor that moves existing applications as-is into enclaves with no re-coding required. While we’re still in the early stages of innovation, we aim to encourage a broader awareness to help secure enclaves cross the chasm from early adoption to early majority and promote the technology’s ongoing success.We see three key benefits of secure enclaves for the Redis community:To fully protect your in-memory data, more than just the data in Redis needs to be encrypted. Application services developed against Redis must also be developed in a secure enclave. You will still be exposed to memory attacks if your applications are not developed using this technology.This has been a simplified explanation of the benefits of secure enclaves, designed specifically for the Redis community. Like any emerging technology, you should be aware of secure enclave’s limitations as well as its functionality.If you’d like to find out more about secure enclaves on a technical level, a paper called Improving Cloud Security Using Secure Enclaves, out of UC Berkeley, is a good place to start. You can also find out more by visiting Intel’s Software Guard Extensions (SGX) site."
370,https://redis.com/blog/redis-enterprise-on-azure-cache-for-redis/,What Customers Can Expect from Redis Enterprise on Azure Cache for Redis,"May 12, 2020",Cassie Zimmerman and Amiram Mizne,"Earlier today, Microsoft and Redis announced private preview availability of Redis Enterprise on Azure Cache for Redis, a significant milestone in the deepening strategic collaboration between our two organizations. The integration will be featured as two additional Enterprise tiers to the existing Azure Cache for Redis service.More than ever, both developers and operators look to the cloud to address unprecedented demands on existing services as well as the need to re-invent and innovate. Financial service companies, healthcare providers, and retailers all benefit from instant-on, fully managed, highly scalable, and highly available services in these uncertain times. Operators don’t want the burden of deploying new infrastructure and don’t have the time to go through lengthy procurement cycles. Developers want access to the tools they need to build new apps and scale existing ones using well-known toolchains with an established, reliable developer experience.Microsoft Azure is one of the most globally distributed and secure clouds on the market. The integration of Redis Enterprise features into Azure Cache for Redis combines Azure’s global presence, flexibility, and security and compliance, with Redis Enterprise’s unmatched availability, performance, and extended data-structure functionality. It addresses key scalability and innovation challenges critical to enterprises, including ensuring cross-regional reliability, cost-effective scaling, enabling new data structures, and expanding existing services in the future with a hybrid-cloud approach.This joint offering already allows select customers to launch Redis Enterprise in the Azure portal just as they would any other native Azure service. Using the Azure Portal user interface, creating a new Redis Enterprise database and connecting it to your app is as easy as 1-2-3.Microsoft offers this fully managed, cloud-hosted version of Redis Enterprise as new Enterprise tiers of the well-established Azure Cache for Redis. These new tiers enhance the already popular open source based Azure product suite with support for the new Redis 6.0, Redis modules, Redis-on-Flash, as well as active geo-replication for seamless failover. As part of the collaboration, Microsoft will extend its continued commitment to the Redis open source project with community engagement and contributions adding to Redis Lab’s existing efforts as the home for Redis and the primary contributor to the Redis project.Once in public preview later this year, the Enterprise tiers in Azure Cache for Redis will include:This collaboration will also ease the procurement process for customers. The integrated Enterprise tiers will be available for customers to purchase in a similar way to existing Azure Cache for Redis tiers, eliminating the lengthy contractual processing required to sign a new vendor. Customers will also be introduced to new:The service is fully managed by Microsoft Azure and supported jointly by the Microsoft Azure and Redis support and engineering teams.This milestone is a culmination of more than a year of close collaboration between the Microsoft Azure and Redis teams. We are very excited about what our teams have already achieved. We look forward to providing our customers—and the greater Redis community—with continually improving ways to deploy, operate, procure, and manage your data infrastructure, support innovative use cases, and address unprecedented demands for scale and performance.To learn more about this exciting new offering, please:"
371,https://redis.com/blog/the-experience-you-missed-at-redisconf-2020-takeaway/,The Experience You Missed at RedisConf 2020 Takeaway,"May 15, 2020",Bao Phan,"Every year the Redis community comes together for RedisConf, the world’s largest annual gathering of Redis Geeks. Although we couldn’t meet in person this year, we’re proud to have delivered our most-attended conference yet, filled with learning and networking to nearly 4,000 attendees from 103 countries. RedisConf 2020 Takeaway was held online from May 12–13, with keynotes, live Q&As, and sessions on Day 1 and a full roster of training classes on Day 2.This year’s theme was Rediscover, and we invited attendees to expand their knowledge of Redis and see how it can be used far beyond caching to power some of the world’s most powerful applications.If you missed all the excitement, not to worry—all sessions continue to be available for on-demanding viewing on the conference site. Here are some of the highlights, including images of the virtual experience, sound bites from the keynotes, and more!What made RedisConf Takeaway different from other virtual conferences? With its virtual reality style interface, attendees hung out with fellow Redis Geeks almost like they would at an in-person conference! Using custom avatars, attendees interacted in a variety of ways to interact, from hanging out in themed Group Discussion rooms on topics like Redis 6, Redis Modules, and K8s & Containers, to talking to other attendees in the Lounge and playing more than 1,500 games of Redis Pong. And you guys were social—sending more than 3,700 chat messages and 12,212 emojis!These features let conference attendees meet fellow Redis community members, like Patrick Heslin from Algo-Logic, who attended RedisConf to learn more about the platform, environment, and community. His favorite sessions were on security, as he says he always insisted on keeping clients secure. “It was fun to meet new people in a unique virtual environment,” he said. “We made good connections.”We also awarded points for various activities in Redis World, from attending an on-demand or live session, asking a question in a live Q&A, or playing a game of ping pong. High scorers earned prizes like gift cards and Redis swag.But it wasn’t all fun and games—in the Video Library, attendees watched more than 50+ on-demand sessions on topics like “Creating a Model of Human Physiology using RedisGraph” from Carlos Justiniano and “Leveraging Redis v6 Tracking for Awesome Client-Side Caching” from Ben Malec on Day 1. Each video had its own room, where you could see the other people watching it with you and chat with them. Many of the speakers were also available for live Q&As to answer any questions not covered in the videos.On Day 2, we released 15 video training sessions designed to help you improve your Redis knowledge. Sessions varied from introductions for Redis newcomers—like “Redis 101: Getting Started with Redis” and “A Practical Introduction to RedisGraph”—to brand-new training content like “RediSearch Advanced Topics” for more experienced Redis users. An exclusive Redis Certification Cram session helped Redis Geeks prepare for the exam. As on Day 1, the Redis experts were answering questions all day.And for Redis users with specific technical questions—Redis experts spent both days circulating the rooms to help out, and made themselves available for exclusive, one-on-one, Ask the Expert sessions in private meeting rooms.Throughout May 12 and 13, attendees tuned in to more than 30 live streams, including a pair of keynotes, interviews, and Q&As. In the morning keynote on May 12, Redis CEO Ofer Bengal and Microsoft CVP of Developer Division Julia Liuson announced a new partnership to deliver two new tiers of Redis Enterprise on Azure Cache for Redis. Plus, Redis Creator Salvatore Sanfilippo demonstrated new Redis 6 features like access control lists and tracking/client-side caching, and Redis’ Chief Product Officer Alvin Richards talked about new Redis Enterprise 6 features that make it more secure and easier to deploy than ever before.During the afternoon keynote on May 12, Redis CMO Howard Ting chatted with Atlassian CTO Sri Viswanath on transitioning to microservices and the worldwide adjustment to working from home. Ting also interviewed Matthew Goos, Co-Founder and CTO of MDmetrix, one of the winners of the Rediscover Redis competition.We hope RedisConf 2020 Takeaway attendees had fun and learned something new about Redis. If you’d like share your feedback on the event, don’t hesitate to reach out to us on Twitter with the hashtag #RedisConf."
372,https://redis.com/blog/expanded-microsoft-partnership-highlights-redisconf-2020-takeaway/,Expanded Microsoft Partnership Highlights RedisConf 2020 Takeaway,"May 18, 2020",Mike Anand,"RedisConf 2020 Takeaway is in the books. This innovative virtual event became a unique forum for sharing the latest Redis news, product previews, and technical insights with thousands of online attendees, all in the context of rediscovering the wide range of things Redis can do, far beyond the most-common caching use case. Highlights include Salvatore Sanfilippo’s demo of the new Redis 6.0, and the formal rollout of Redis Enterprise 6.0, as well as general availability of new technologies such as RedisGears 1.0 and RedisAI, a preview of RedisRaft, and an early-stage partnership around secure enclaves with Anjuna.The one clear “takeaway,” if you will, is that Redis and Redis Enterprise continue to push boundaries and gain momentum. Redis Enterprise 6.0 is the most secure, easiest to deploy version of Redis ever, empowering developers to address a wider variety of enterprise use cases—including as a primary data store—than ever before. The new GA versions of RedisGears and RedisAI push the boundaries of what developers can do with Redis to achieve more with data.More on that in a moment, but the biggest news, of course, was revealed in the opening keynote (watch the video of the entire presentation below). Redis Co-Founder and CEO Ofer Bengal shared many stories of our customers going beyond cache with Redis, and introduced Microsoft Corporate Vice President of the Developer Division Julia Liuson to discuss the expanded partnership between the two companies, adding a pair of new Enterprise Tiers to the familiar and popular Azure Cache for Redis.To see the Microsoft Azure announcement, jump to 1:16:10Microsoft is working to “empower every developer and every development team on the planet to build more applications,” Liuson told the online attendees. “We are also committed to building the most developer friendly cloud, bringing developer tools for development, productivity, and collaboration.”She said that developers love the fully managed Azure Cache for Redis offering, which will be bolstered with two new Enterprise Tiers. The Enterprise Tier will be an in-memory offer using Redis Enterprise, while the Enterprise SSD Tier will be built on top of the Redis on Flash technology, and deliver up to 10 times larger cache sizes and similar performance at a lower price per gigabyte.“The new Azure Redis Enterprise provides significantly improved developer productivity,” Liuson said. “Developers can easily use the latest version of Redis and use all of its native data structures and modules,” including RedisBloom, RedisTimeSeries, and RediSearch.“Developers can also deploy a Redis cluster within minutes, to scale without additional steps,” Liuson added. Developers who are already familiar with Azure Cache for Redis can leverage the same interface and management experience so they can immediately start working with the new service, she said.Resiliency, security, and complianceBecause Redis is frequently used in mission-critical applications, resiliency, security, and compliance are not optional. Liuson promised to “deliver enterprise-grade SLAs offering additional uptime, through Redis Enterprise’s active geo-replication technology across Azure regions. With this joint partnership, the Enterprise Tier will support all of the most stringent requirements on security and compliance that is demanded by our most security-minded customers.”It’s not just a technology partnership, though. “We want to make it easy for customers to acquire and use open source technologies directly in Azure,” Liuson said, “and deliver deep integration around identity, security, and unified billing.” Liuson emphasized that this will significantly increase both operational efficiency and developer productivity.A simplified user experienceThe new tiers will also provide a simplified administration and billing experience, as well as streamlined support. Customers can set Redis cache size in real-time based on their needs, potentially delivering cost savings with Azure’s consumption-based pricing model. “Redis Enterprise usage will simply show up in customers’ monthly Azure bill,” Liuson promised. The companies will also deliver a simplified and streamlined support experience—customers will contact Microsoft, which will then pull Redis in for any software questions.For more on how it works, watch the video demo of Azure Cache Redis Enterprise from the keynote. The new tiers are available now in private preview. “We are working hard for public preview this fall,” Liuson concluded, “and general availability by winter.”An appearance by Salvatore Sanfilippo, the original creator of Redis, is always a RedisConf highlight, and this year he was able to demo the brand-new Redis 6.0, released just two weeks ago.Sanfilippo noted many fixes and powerful new features like access control lists (ACLs), client-side caching, cluster manager in the Redis CLI, replication improvements, and even a Gopher implementation, among others. He also showed off the longest common subsequence algorithm, which is often used to compare and visualize genetic sequences, of the RNA of viruses, for example.With all that, Sanfilippo wondered, “is Redis becoming too complex?” He did a quick check of the number of lines of code in the system. The answer: “I think we are still small! … What I want to stress is that Redis is still the simple thing … it looks like a toy … but it can actually solve really interesting problems when developers apply their creativity.”Sanfilippo also took an in-depth look at tracking. “It’s just 300 lines of code and a bit more comments,” he noted, “but it could be the most significant feature of Redis 6.0, perhaps because it can change a lot of things.” Watch his live-coding explanation and demo of tracking—basically the server-side support for the client-side caching protocol—in the video below:After Sanfilippo discussed what’s new in Redis 6, Redis Chief Product Officer Alvin Richards took the virtual stage to talk about how Redis Enterprise 6.0, which extends the open source version in a number of key ways for enterprise customers to  increasingly use it as a powerful primary database, not just as a cache. It’s no secret that Redis is often used as a cache to speed up other database systems, but over the years Redis has come a long way toward becoming an increasingly credible database option, adding a number of critical enterprise-grade features. Redis Enterprise 6.0 highlights the security and management side of things, giving customers better control of user and enterprise access and security.Specifically, that means ACLs with role-based access control (RBAC) to simplify administration at scale. Richards also noted that Redis Enterprise 6 includes TLS 1.3 support and Mutual TLS Authentication.Meanwhile, as data volume and velocity keep growing, customers increasingly need to work with event-based streams, and Redis Enterprise 6 extends Active-Active to support Redis Streams to take advantage of that trend.Much remains to be done of course, and Richards also discussed new security initiatives with Intel and Anjuna around secure enclaves, which bring hardware and software together into an integrated security approach to encryption in use. (For more on this, see our blog post on Secure Enclaves Could Be the Future of Security.)Finally, he touched on our work on a new RedisRaft module to support strong consistency for large-scale enterprise applications, based on the popular Raft consensus-based algorithm for replicating state across many machines. “We are announcing that sometime early this summer, we will be open sourcing RedisRaft,” he said.Of course, as Chief Product Officer, Richards couldn’t sign off without offering a peek at what’s coming next:“Now that we’ve got role-based access control,” Richards said, “many people will want to configure them through external identity providers. So Active Directory or LDAP. The next step is to be able to configure those roles, not just within a Redis Enterprise cluster, but from outside the Redis Enterprise cluster. Again, this simplifies the administration burden.”Richards concluded that we’re also adding TLS 1.3 support so we get the latest ciphers, and we’re adding mutual TLS authentication. And, finally, we’re extending Active-Active to support RediSearch, which lets you do full-text search, stemming, fuzzy matching, and much more.Much of the keynote focused on doing more with your applications, but Redis Co-Founder and CTO Yiftach Shoolman addressed the issue of “doing more with data.” For one thing, he said, “it’s about time to take AI to production,” and move beyond AI training to AI inferencing. Shoolman noted that analysts estimate that revenue associated with AI inferencing will soon surpass the revenue associated with AI training. That’s because the AI training model is complex, he said, while inference lets you tweak existing AI models using your own reference data and get to production much faster.That’s where the new Redis AI module (co-developed by Redis and Tensorwerk) comes in. By placing the AI serving engine inside Redis, RedisAI reduces the time spent on external processes and can deliver up to 10x more inferences than other AI serving platforms at a much lower latency. This increased performance can help drive better business outcomes for leading AI-driven applications such as fraud detection, transaction scoring, ad serving, recommendation engines, image recognition, autonomous vehicles, and game monetization.Learn more about RedisAI.Finally, to orchestrate the flow in this new data architecture, Shoolman said, “you need to have a fully programmable serverless engine that runs in a fully distributed manner, closer to where your data lives, preferably inside the database.” Redis needs serverless to enable cluster-wide data processing, reliable event processing, and to orchestrate your AI transactions—at the speed of Redis.RedisGears, announced a year ago, is a serverless engine for infinite programmability in Redis, and Yiftach announced that RedisGears is now generally available. RedisGears lets you program everything you want in Redis, deploy functions to every environment, simplify your architecture and reduce deployment costs, and run your serverless engine where your data lives.Learn more about RedisGears.Put it all together and once thing is clear: While many people know that Redis is an awesome caching system, that’s only the beginning of what Redis can do. According to Ofer Bengal, maybe that’s because running in-memory may still be perceived as not robust enough for a primary database, or perhaps we need to do more to expose the Redis community to the enhanced capabilities added to Redis over the years.Whatever the cause, Bengal said, “with so many embedded tools and capabilities, Redis can serve as your primary database for many of your modern apps and complex use cases, while guaranteeing the best performance. … We hope you will rediscover Redis and its unique capabilities as a database.”More on RedisConf 2020 TakeawayFor more on everything that went down at the event, check out this press coverage:You can also watch many of the presentations from RedisConf 2020 Takeaway on the conference site."
373,https://redis.com/blog/redisgears-serverless-engine-for-redis/,Announcing RedisGears 1.0: A Serverless Engine for Redis,"May 19, 2020",Pieter Cailliau and Meir Shpilraien,"We are happy to announce the general availability of RedisGears, a serverless engine that provides infinite programmability in Redis. Developers can use RedisGears to improve application performance and process data in real time, while architects can leverage it to drive architectural simplicity.As a dynamic framework for the execution of functions that implement data flows in Redis, RedisGears abstracts away the data’s distribution and deployment to speed data processing using multiple models in Redis. RedisGears lets you program everything you want in Redis, deploy functions to every environment, simplify your architecture and reduce deployment costs, and run your serverless engine where your data lives.RedisGears can be deployed for a variety of use cases:In addition to announcing RedisGears, we’re also unveiling its first recipe. A “recipe” is a set of functions—and any dependencies they might have—that together address a higher-level problem or use case. Our first recipe is rgsync. Also referred to as write-behind, this capability lets you treat Redis as your frontend database, while RedisGears guarantees that all changes are written to your existing databases or data warehouse systems.To help you understand the power of RedisGears, we’ll begin with an explanation of RedisGears architecture and its benefits. Then we’ll discuss how these benefits apply to write-behind and we’ll demonstrate its behavior via a demo application we created.At the core of RedisGears is an engine that executes user-provided flows, or functions, through a programmable interface. Functions can be executed by the engine in an ad-hoc map-reduce fashion, or triggered by different events for event-driven processing. The data stored in Redis can be read and written by functions, and a built-in coordinator facilitates processing distributed data in a cluster.In broad strokes, this diagram depicts RedisGears’ components:RedisGears has three main components:On top of these three core components, RedisGears includes a fast low-level C-API for programmability. You can integrate this C-API via Python today, with more languages in the works.RedisGears minimizes the execution time and the data flow between shards by running your functions as close as possible to your data. By putting your serverless engine in memory, where your Redis data lives, it eliminates the time-consuming round trips needed to fetch data, speeding processing of events and streams.RedisGears lets you “write once, deploy anywhere.” You can write your functions for a standalone Redis database and deploy them to production without having to adapt your script for a clustered database.Combining real-time data with a serverless engine lets you process data across data structures and data models without the overhead of multiple clients and database connectors. This simplifies your architecture and reduces deployment costs.The ability to cope with sudden spikes in the number of users/requests is something modern companies and organizations must consider. Black Friday and Cyber Monday traffic, for example, can dwarf that of ordinary days.Failure to plan for such peaks can lead to poor performance, unexpected downtime, and ultimately lost revenue. On the other hand, over-scaling your solution to these peaks can also be expensive. The key is to find a cost-efficient solution that can meet your demands and requirements.Traditional relational/disk-based databases are often unable to deal with significant increases in load. This is where RedisGears comes into play. RedisGears’ write-behind capability relies on Redis to do the heavy lifting, asynchronously managing the updates and easing the load and diminishing the spikes on the backend database. RedisGears also guarantees that all changes are written to your existing databases or data warehouse systems, protecting your application from database failure and boosting the performance of your application to the speed of Redis. This simplifies your application logic drastically since it now only needs to talk to a single frontend database, Redis. The write-behind capability comes initially with support for Oracle, MySQL, SQL, SQLite, Snowflake, and Cassandra.The diagram below displays the architecture of RedisGears’ write-behind capability:It operates as follows:Together those two functions make up what we call a “recipe” for RedisGears. (Note that the recipe for write-behind is bundled in the rgsync (RedisGears sync) package, along with several other database-syncing recipes.)As noted above, step three happens only when the event was successfully added to the stream. This means that if something goes wrong after the client gets the acknowledgement of the write operation, Redis replication, auto-failover, and data persistence mechanisms guarantee that the update event will not be lost. By default, the write-behind RedisGears capability provides the at least once delivery property for writes, meaning that data will be written once to the target, but possibly more than that in case of failure. It is possible to set the RedisGears function to provide exactly once delivery semantics if needed, ensuring that any given write operation is executed only once data is on the target database.Improving application performance with write-behindTo showcase the benefits of write-behind, we developed a demo application in which we’ve added endpoints to enable two scenarios:In this example, we used MySQL as the backend database for ease of testing and reproduction.To simulate peaks in the application, we’ve created a spike test with k6, in which we simulate a short burst going from 1 to 48 concurrent users.To check how the overall system handled the spike, we tracked the achieved HTTP load and latency on the application as well as the underlying database system performance. The graph below showcases both scenarios—the left interval presents results for the MySQL-only solution, while the right interval presents results for the write-behind scenario with RedisGears.This chart displays some important findings:We are really excited about RedisGears and write-behind. We believe that the write-behind use case is only the beginning of the infinite problems that RedisGears can solve.We hope that this blog post has encouraged you to try RedisGears. Please check out RedisGears.io, which contains tons of examples and hints on how to get started. You can find more cool demos here:A new version of RedisInsight will be released soon and will contain support for RedisGears to execute functions and to view the registered functions in RedisGears. We’ll leave you with a quick GIF of what you can expect:Happy coding!"
374,https://redis.com/blog/meet-the-winners-of-the-rediscover-redis-competition/,Meet the Winners of the Rediscover Redis Competition,"May 20, 2020",Britiana Andrade,"In honor of the Rediscover theme of RedisConf 2020 Takeaway, the Rediscover Redis Competition collected stories from the Redis community to highlight examples of Redis’ versatility and demonstrate how Redis powers some of the world’s most innovative applications. For every valid entry received, we donated $100 to the COVID-19 Solidarity Response Fund, which supports the World Health Organization’s work to ensure patients get proper care, frontline workers get essential supplies, and development of a vaccine and treatments can be accelerated.Redis CMO Howard Ting highlighted the three winners in the afternoon keynote at RedisConf 2020 Takeaway, and each won a Valve Index VR Kit (a $999 value). In no particular order, the three winners were Matthew Goos for MDmetrix’s COVID-19 Mission Control; Randall Shane for Referred.ai; and Luiz Santos for ProvTransaction. Read on for more information about their projects:Matthew Goos is a longtime technologist with more than 25 years of experience, and is the Co-Founder and CTO of MDmetrix, an interactive data analytics platform that helps clinicians and institutions understand patterns in their data.While MDmetrix uses Redis exclusively for all data storage, RedisGraph is particularly critical, as it stores all the analytical data. MDmetrix built Mission Control to combat the spread of COVID-19 by providing analytics to institutions and physicians around the country to improve patient care and optimize utilization of resources.Randall Shane is an adjunct professor and machine learning and AI developer at Boise State University. He built a platform designed to help patients find trusted doctors that match their preferences by using a natural language processing candidate referral digestion and interpretation system.Referred.ai uses Redis for the entire application, including data storage as well as computational bolstering for the natural language processing.Luiz is a senior electrical engineer and software developer working in information security. His project, ProvTransaction, is an approval platform for online banking systems, which allows free generation and safe storage of password token seeds—saving banks millions of dollars a year.Redis is the backbone behind the project’s component token database, storing OATH validation components embedding the random token seeds. It acts as a critical cryptographic authentication server and replaces a centralized monolithic one.If you didn’t get a chance to enter the Rediscover Redis competition, there’s still time to enter the Redis ‘Beyond Cache’ Hackathon, where we’re asking developers to demonstrate how to use Redis beyond caching by creating apps with Redis data structures, event-driven architecture, and/or Redis modules. Submit your project by May 27 for a chance to win up to $5,000. Learn more about what the judges are looking for, the hackathon schedule, and submission guidelines in this blog post, and enter the hackathon now!And if you missed RedisConf 2020 Takeaway, not to worry—all 50+ sessions continue to be available for on-demanding viewing on the conference site, and read more about the virtual conference experience on our blog."
375,https://redis.com/blog/the-best-on-demand-sessions-from-redisconf-2020-takeaway/,The Best On-Demand Sessions from RedisConf 2020 Takeaway,"May 28, 2020",Mike Anand,"While RedisConf 2020 Takeaway has come and gone, the good news is that the content—40+ breakout sessions, 15 training sessions, two keynotes, and 25+ live Q&As—won’t be going anywhere. After premiering on May 12–13, nearly everything is still available to view on-demand in the Video Library on RedisConf.com.This year’s conference theme was Rediscover, and many sessions demonstrated how Redis powers some of the world’s most innovative applications, and has many business-critical use cases far beyond caching. During the live event, nearly 4,000 attendees watched more than 13,000 sessions and spent more than 4,000 hours in the virtual world. Plus, you all asked nearly 400 questions during the live Q&As (and blew off steam with 1,500+ games of ping-pong).We’re grateful to all the speakers who took time to record videos and/or go live for the largest annual gathering of the Redis community, and we wanted to highlight a few community sessions to check out if you haven’t yet already. Happy viewing!Ana Margarita Medina, GremlinAs we move more systems to the cloud, it can become harder than ever to detect, track, and manage the inevitable failures. Ana shows you how to use chaos engineering—using controlled experiments to test failure scenarios—to ensure your apps scale properly and durably and can handle failures.Vishy Kasar, AppleMany of the new features of Redis 6 are security-focused, and Vishy does a great job walking you through how to take advantage of them. He explains current use cases that required better security, and then explains—in tutorial-like fashion—how to use access control lists (ACLs) and transport layer security (TSL).Phyo Kyaw and Afshin Salek, UberThis is an informative session from Uber engineers on how they used Redis Streams to replace Kafka and Flink to get better performance and easier provisioning. Phyo and Afshin discuss the overall architecture and design, lessons learned from using Redis Streams, and other best practices for using Redis Streams.Carlos Justiniano, ZenerchiCarlos is a three-time RedisConf speaker. We’re always big fans of his presentations, and this year is no different. Learn about how RedisGraph powers advanced medical simulations to build a model of human physiology at his biotech company Zenerchi. If you’re familiar with RedisGraph and curious about its use cases, take 30 minutes to watch this one.Jayesh Ahire, Researcher and Consultant“Fake news” is a phrase we’ve all become familiar with. Jayesh shows us how to build a service using RedisAI and Twilio to predict if an article is fake news, including a demo of the application!Kyle Bebak, Elementary RoboticsIf you’ve ever wanted to learn how to build a message bus—a way for producers to communicate with receivers by adding messages to a persistent data structure—this session is a great tutorial. Kyle shows you how to do this using Redis Streams and FastAPI, a Python web framework.Daniel D’Agostino, Cleverbit SoftwareBrand-new to RedisGraph? Check out Daniel’s session, where he’ll cover three important basics: what graph databases are, what problems graph databases solve, and how RedisGraph is distinct from yet compatible with other graph databases. Plus, you’ll leave with a few basics of Cypher, the open-graph query language, which is used in RedisGraph.John Myers, Gretal.AiJohn is a longtime Redis user, and his session combines microservices, Redis, and streaming technologies to demonstrate how to perform real-time joinability analysis between multiple massive data sets with HyperLogLog and some basic math.Zohaib Sibte Hassan, DoorDashDoorDash has billions of requests coming in at any given moment—here’s an inside look at how the DoorDash team manages them. Zohaib explains some of the techniques the team uses to improve tail latencies, deal with hot keys, and prevent cache stampedes.Nicole Hubbard, HashiCorpIf you’ve never used a service mesh—which manages the authentication, authorizing, and encryption of all your microservices no matter the location—Nicole explains why you should consider getting started. She shows you how to deploy Consul Connect inside a Kubernetes cluster and connect your applications inside Kubernetes to your Redis instance running on virtual machines.Scott Haines, TwilioScott previously spoke at Redis Day Seattle, and this time he shows you how to get started with solving enterprise-level machine learning problems with Apache Spark and Redis. You’ll learn about logistic and linear regression; tree models like decision trees and forests; and Redis Streams.Alex Kalinin, FacebookThis is another great multi-part training series, where you’ll learn how to get started with image recognition with the PyTorch Deep Learning framework and RedisAI for serving models. Alex leads multiple demos: first building an end-to-end image recognition system; then custom networks with a transfer learning model; and finally a convolutional neural network.Ready to start watching? All these sessions from Redis community members are available to view at RedisConf.com in the Video Library. Registration is free!And for more information on RedisConf 2020 Takeaway, check out the experience you missed and a recap of the keynote on our blog."
376,https://redis.com/blog/redis-is-the-most-loved-database-for-the-4th-year-in-a-row/,Redis is the Most Loved Database for the 4th Year in a Row,"June 3, 2020",Redis,"What’s better than a 3-peat? Being named Most Loved Database four years in a row!Stack Overflow’s Annual Developer Survey takes the pulse of the coding community on their most-loved, most-wanted, and most-dreaded technologies. More than 65,000 developers responded, with a special effort this year to reach beyond its traditional audience and hear from a more diverse population.With that extended community in mind, Redis is especially honored to be named the Most Loved Database by developers once again, marking our fourth consecutive year earning this designation. We also held our position as the sixth Most Popular Database overall among developers. We’re proud to be recognized for our focus on creating a database that is easy to learn, easy to use, and easy to manage—no matter where you are from, what gender you identify with, or your programming skill level.The broader reach of this year’s survey gives the tech community valuable insights into where opportunities lie for improvement. Redis garnered 66% of the Most-Loved votes in 2020, a slight decrease from the 71% in 2019. Over the coming year, we’ll also be examining how we can better reach and support a wider variety of developers.We’re encouraged by the recent success of our RedisConf 2020 Takeaway conference, where nearly 4,000 attendees gathered to Rediscover Redis beyond caching. This was the first virtual edition of our annual conference, enabling more of the Redis community to participate than ever before. Attendees joined from 103 countries to listen to live keynotes, watch 60+ live and on-demand breakout and training sessions, and engage with fellow Redis professionals to share experiences and answer questions. Our event survey revealed that 92% of attendees would “recommend this event,” and we received many requests to continue offering virtual learning opportunities. More excitingly, 64% of attendees intend to explore Redis beyond caching and 57% are more likely to use Redis than before—we’re looking forward to the 2021 survey already!We hear you. Stay tuned. In the meantime, if it’s been a while since you’ve taken a look at what’s new, we invite you to Rediscover Redis for yourself. After all, it’s the world’s most loved database, not just the most loved cache."
377,https://redis.com/blog/redisraft-new-strong-consistency-deployment-option/,"Introducing RedisRaft, a New Strong-Consistency Deployment Option","June 23, 2020",Yossi Gottlieb,"RedisRaft (under development) is a new module for open source Redis that makes it possible to operate a number of Redis servers as a single fault-tolerant, strongly consistent cluster. As its name suggests, it is based on the Raft consensus algorithm and an open-source C library that implements it.RedisRaft brings a new strong-consistency with strict serialization deployment option to Redis and the Redis ecosystem. The new module makes it possible to use Redis along with Redis’ existing clients, libraries, and data types in beyond-cache scenarios requiring a high level of reliability and consistency.RedisRaft started as an experimental “side project” shortly before Redis 5 was released. The Redis module API was introduced in Redis 4, primarily designed to support modules that implement new data types and commands. We wanted to explore how far we could stretch the API, and use modules to extend Redis in even more radical ways. But we also wanted to end up with something useful—a strong consistency deployment option for Redis.A RedisRaft cluster offers the same level of consistency and reliability expected from reliable, well-known data stores such as ZooKeeper or Etcd. In a nutshell, in RedisRaft:As expected from reliable data stores offering this level of consistency, such guarantees bring trade-offs in performance and availability. In RedisRaft:Once a RedisRaft module is loaded into Redis, it takes over communication between cluster nodes, replication of the Raft log or snapshots, persistence, and so on. The Redis core remains unaware of this and as far as it is concerned, it is operating as a standalone server with no clustering, persistence, or replication.Setting up a three-node RedisRaft cluster is as easy as starting three Redis servers, as shown here:redis-server --loadmodule /path/to/redisraft.soThis is how to connect to the first server and create the Raft cluster:10.0.0.1:6379> RAFT.CLUSTER INIT
OK 989645460313dd2ddb051f033c791222Then you connect to the other two servers and join them to the cluster:10.0.0.2:6379> RAFT.CLUSTER JOIN 10.0.0.1:6379
OK
10.0.0.3:6379> RAFT.CLUSTER JOIN 10.0.0.1:6379
OKOnce RedisRaft is set up, we can write data to our cluster:10.0.0.1:6379> INCR counter:1
(integer) 1Receiving a reply shows that our write has been replicated to at least a majority of the cluster nodes (2 nodes in our case), and committed to their persistent storage.Raft is based on a strong-leader concept, which means all client operations should go to the leader node and initiate from it. In this case, our client was indeed connected to the leader, but cluster leadership is dynamic and clients may not necessarily know who the leader is.Trying the same operation on a follower (non-leader) node creates this response:10.0.0.3:6379> INCR counter:1
(error) MOVED 10.0.0.1:6379So as a client, we can now catch this error, re-establish a connection with the leader node as specified, and retry our command.But what if we don’t want to modify our existing application? Luckily, RedisRaft can also be configured to automatically handle this for us. By enabling follower proxy mode, we can have cluster nodes automatically forward our request to the leader and provide the reply when it is available:10.0.0.3:6379> RAFT.CONFIG SET follower-proxy yes
OK
10.0.0.3:6379> INCR counter:1
(integer) 2This is much simpler, of course, but impacts latency and network load as hitting a non-leader node creates an extra network hop.When setting up our cluster, we’ve actually performed three distinct operations:RedisRaft cluster configuration is not static, and it is possible to add or remove additional nodes after the cluster has been created and while it’s active.For example, we may need to replace our third node. First, we join a new node that’s going to replace it. Note that we “add-then-remove” rather than “remove-then-add” to avoid leaving the cluster and our precious data in a degraded redundancy state during the transition:10.0.0.4:6379> RAFT.CLUSTER JOIN 10.0.0.1:6379
OKNext, we look up the ID that was randomly assigned to our third node:redis-cli -h 10.0.0.1 --raw RAFT.INFO | grep 10.0.0.3
node2:id=1739451728,state=connected,voting=yes,addr=10.0.0.3,port=6379,
last_conn_secs=3537,conn_errors=0,conn_oks=1And then we remove it:10.0.0.1:6379> RAFT.NODE REMOVE 1739451728
OKAs part of RedisRaft development work, we have been collaborating with Kyle Kingsbury (a.k.a Aphyr) to analyze and test RedisRaft using Jepsen, a well-known framework for testing the safety and correctness of distributed systems. This collaboration has so far resulted in this published analysis.Though still under development, most of RedisRaft’s basic functionality is in place. We are currently working towards a first preview version, which is expected to be available in a couple of months. When generally available, RedisRaft will be released under a dual license, either GNU AGPLv3 or Redis Source Available License (RSAL)."
378,https://redis.com/blog/announcing-the-winners-of-the-redis-beyond-cache-hackathon/,Announcing the Winners of the Redis Beyond Cache Hackathon,"June 29, 2020",Drew Kreiger,"The results are in!For the Redis Beyond Cache Hackathon, we challenged participants to build applications that use Redis modules like RedisGears, RedisAI, RediSearch, RedisGraph, RedisTimeSeries, RedisJSON and RedisBloom, as well as Redis event-driven capabilities like Redis Streams, Redis for Task Queues, and Pub/Sub. After more than 320 participants submitted 48 entries in our first online hackathon—of which 26 met our strict criteria—we held the first round of voting on DevPost, at which point five applications moved on to the final round. From June 11–18, we collected more than 2,600 votes, 640 likes, and 360 comments on our LinkedIn poll.We awarded three prizes: the Grand Prize, the Judge’s Choice, and People’s Choice. The judges—Redis CTO of Incubations Guy Korland; HashedIn CTO Sripathi Krishnan; Redis Solution Architect Julien Ruaux; Redis Technical Account Manager Tug Grall; and Redis Software Engineer Gavrie Philipson—scored entries on three equally weighted criteria:For the People’s Choice Award, LinkedIn votes counted for 50% of an application’s final score and judges’ voting for the other 50%. The Grand Prize went to the project with the highest overall score.The People’s Choice award was a close race—while FaceMark earned 902 votes, Redisafe was not far behind with 889. intelliSchool got 770 votes and SpecKart 48.For the Judge’s Choice award—based on the judges’ scores alone—intelliSchool was awarded a total score of 60.6 points. Facemark earned 60, Redisafe earned 52, and Voluntree earned 49.5.Combining the points each team earned in the two categories, the final overall standings are:Congratulations to all the winners! intelliSchool wins $5,000 for the Grand Prize, and all the runner-ups also get a cash prize. FaceMark also takes home a $5,000 prize for winning the People’s Choice award. Plus, all the teams receive our popular Redis swag.Grand Prize and Judges’ Choice winnerintelliSchool, built by Priyadarshini Murugan and Sumanth Muni, is a web application and browser plugin that generates quizzes, notes, and flash cards automatically for use in educational videos. In addition to natural language processing and machine learning, the app uses RedisGears, Redis Pub/Sub, Redis Streams, RedisJSON, Sorted Sets, Hashes, and TypeScript.Priyadarshini, a masters student at Penn State University, said participating in the hackathon gave her the opportunity to explore how Redis can be used beyond caching. “With the usage of several Redis modules, we were able to convert our application into an event-driven architecture,” she says. “It helped us improve the performance of our application. Overall, the hackathon was a highly insightful and fun experience.”Sumanth, a masters student at New York University, said this was his first venture on an event-driven application. “It was quite an inspiration to learn how Redis Streams, Pub/Sub, RedisGears, and other Redis modules can be used to achieve the performance we were expecting from our application,” he said.People’s Choice winner and First Runner-UpFaceMark, built by Anshuman Agarwal, is a real-time video solution that can take student attendance in a classroom using contactless face recognition, so no one has to handle the same sign-in sheet. It was built with RedisAI, RedisGears, RedisTimeSeries, and Tensorflow.Anshuman said this hackathon was his first time using Redis, and that he began the process thinking that advanced Redis features, like Redis Streams and Redis modules, would be extremely complicated. To his surprise and delight, that was not the case. “I built the first prototype of my architecture using Redis Streams and RedisGears within two to three hours of reading for the first time what they are,” he says. “I truly believe Redis has come a long way from what it’s known for, and the submissions in this hackathon are a true demonstration of the super powers Redis has.”Second Runner-UpBuilt by Vi Ly, Sagar Bansal, and Moksh Nirvaan, Redisafe is an AI-based tool that checks for PPE (personal protective equipment) on medical staff to meet safety protocols to prevent the spread of infections. It uses RedisAI, Redis Streams, TensorFlow, and Express.js.Third Runner-UpVoluntree makes it easy for nonprofits to connect with volunteers via social media platforms. It was built by Mehedi Hasan Masum, Tanvir Hasan, and Shakil Ahmed and uses Redis queues, Redis Streams, RedisGears, and RedisAI.Honorable MentionRounding out the finalists is SpecKart, a blockchain-based decentralized e-commerce platform. Created by Dennis Sam and Mekha Krishnan, it uses RediSearch, Redis Pub/Sub, Redis as a cache, Socket.io, and Ethereum.Thank you to everyone who participated, judged, and voted! We hope to host more Redis hackathons. If you’d like to share feedback on this hackathon or be the first to know about future ones, please email us at community@redis.com."
379,https://redis.com/blog/diving-into-crdts/,Diving into Conflict-Free Replicated Data Types (CRDTs),"March 17, 2022",Redis,"Active-Active Geo-Distribution allows you to place your Redis database cluster instances and data centers close to your users, no matter where they are. Placing read replicas closer to your users is the right thing to enable real-time response. For write-heavy applications, that’s not enough. So how do you develop an Active-Active geo-distributed cluster?Conflict-free replicated data types (CRDTs) (aka convergent replicated data type or commutative replicated data type) are a family of replicated data types with a common set of properties that enable operations to always converge to a final state consistent among all replicas. To ensure that conflicts never happen (there is no concept of conflict resolution) and cause problems in your applications, operations on CRDT data types have to respect a specific set of algebraic properties.CRDTs work in Redis Enterprise when you create a CRDT-enabled database because standard commands get swapped with an equivalent CRDT implementation. Let’s look at a Redis data structure with a CRDT equivalent and the nuances that Active-Active geo-distributed replication adds.Redis Sets are similar to what your favorite programming language offers in the standard library, offering specialized operations to add and remove elements, check if an element is part of the set, and perform set intersection, union, and difference. Here’s how it looks like when calling actual Redis commands:// Create a set:
> SADD fruits apple pear banana
3// Test if it contains apple:
> SISMEMBER fruits apple
1// Remove a fruit:> SREM fruits banana1// Test if it’s still present:
> SISMEMBER fruits banana
0Running the same commands in a CRDT-enabled Redis Enterprise database would display the same behavior, but what’s happening behind the scenes is completely different.In a CRDT database, all replica nodes can independently apply changes to the “fruits” key. This enables you to write genuinely geo-distributed applications. The downside is that the “replication lag” is also experienced when writing to a value. This approach offers significant advantages for geo-distributed applications and achieving data consistency.CRDT Sets & Redis EnterpriseIn a Redis Enterprise CRDT-enabled database, sets operations work under a few additional rules, the two most important of which get applied when merging operations coming from different nodes:The second rule is sometimes referred to as the “observed remove” rule, meaning that you can delete only items that you were able to observe when the command was issued.A note on replication lag and consistency modelWhile eventually all replica nodes will converge to the same final state, in the short term a command sent to ReplicaEU may not have yet been propagated to ReplicaUS, for example. This situation, albeit normally brief, is the reason why CRDT data structures must hardcode conflict-resolution strategies. All replicas in the same CRDT database will constantly sync their state to provide as consistent a view of the dataset as possible, but keep in mind that CRDTs are also useful for ensuring high availability in case of network partitions.This means that to implement better resilience in your system you will need to account for the possibility of prolonged moments where the system has not yet been not fully synchronized. This makes CRDTs a form of eventual consistency. This is usually described as strong eventual consistency because it’s much more efficient than the more common types of replication based on quorum quotas. (For more information, see the Redis page on Active-Active Geo-Distribution.)CRDTs perform replication as commutative operations. This has the desirable quality (for a distributed system) that the order of replication does not matter. Replication in an arbitrary order fundamentally reframes many distributed system race conditions, and its usefulness increases as asynchrony (e.g., distribution) increases. In many systems, copies of some data need to be stored on replicas. Examples of such systems includeMore CRDT data structuresFor a complete list of the CRDT data structures that Redis Enterprise supports, take a look at the official documentation.What are the practical requirements for developing an Active-Active application?Let’s look at the main development aspects that could raise questions.Client libraries for CRDT databasesOne question is whether you need a special type of Redis client to interact with a CRDT database. The answer is no—any normal Redis client can connect to a CRDT database and execute commands directly. As mentioned earlier, the commands don’t change, it’s the underlying mechanics that do. The only thing that clients don’t offer out-of-the-box is the ability to connect to a different geo-distributed replica in case the one closest to the service instance becomes unavailable. We’re working on this issue, but in the meantime, in the event of a network split, you will have to decide at the application level when it’s appropriate to connect to a replica in a different region.CRDT Application architectureThe advantage of Active-Active geo-distribution is the ability to share some state across service instances distributed globally, all while experiencing local latencies when manipulating that state. To fully exploit this capability, your services need to rely on CRDTs’ semantics as much as possible and, as such, avoid keeping an internal state that would get lost (or become un-mergeable) in the event of failures. CRDT-based databases are available even when the distributed database replicas cannot exchange the data.CRDTs can’t solve all problems because they hardcode specific merge rules that might not be appropriate for your particular problem. Counters are a typical example: CRDT counters are great, but they can’t be used to model a bank account balance because merges could allow the counter to go negative-and there is no way at the application level to prevent this from happening. In other words, CRDTs are an efficient but nuanced form of eventual consistency that doesn’t apply properly to inherently transactional problems.Testing Active-Active Geo-distributed CRDT ApplicationsIt might seem that testing an Active-Active geo-distributed application must be much more complicated than testing normal single-master ones. While CRDTs certainly are a complex component of your data model, their behavior is fully deterministic in terms of results. You have to account for the primary situation when the cluster is partitioned. As mentioned above, it’s OK to continue sending updates to a replica that has been disconnected from the rest of the cluster because the updates will eventually be merged successfully when the connection is reestablished. You need to ensure that your service can still operate correctly when disconnected from the whole Active-Active geo-distributed CRDT database cluster.In other words, the only non-obvious extra testing required on your part should be about how the application behaves in the event of a network partition.CRDTs allow you to create geo-distributed applications that can offer local latencies to your entire user base, all the while rendering your entire application more resilient to failure. While not all problems can be solved by CRDTs, there is a vast space of improvements that most companies can benefit from. As an example, please take a look at this recent blog post by Kyle Davis, where he shows how to implement a leaderboard using Sorted Sets—and hints at the benefits of a CRDT version.To learn more about writing Active-Active applications with Redis Enterprise, take a look at our Under the Hood: CRDTs whitepaper and the official documentation.Strong consistency is propagating any update to all copies of data. In the most simple case, a series of updates to the data come from a single source, and all other entities that hold a replica of the data are guaranteed to receive those updates in the same order.When multiple copies of the data need to be updated simultaneously, there needs to be a way to agree upon the correct version of the data. With strong consistency, once one entity makes any updates, all other replicas are locked to eliminate the need for conflict resolution until they’ve been updated to the same new version. The source of truth pushes updates to all entities in the same order to keep them ‘in step.’The locking mechanism used in strong consistency is inconsistent with the need for real-time performance. This is where eventual consistency and CRDTs come into play. Every cluster or node can potentially make and receive updates, so there is no way to ensure that each replica gets the same sequence of updates. Eventual consistency is the property where the state of the data is eventually reconciled, regardless of the order of update events that reach each replica. CRDTs can resolve replication conflicts that arise from concurrent updates or concurrent inserts on replicas existing in a decentralized environment."
380,https://redis.com/blog/stunnel-secure-redis-ssl/,Using Stunnel to Secure Redis,"May 13, 2014",Itamar Haber,"What do you do if you want to secure access to your Redis? Plain password authentication (i.e. the AUTH command) only gets you so far and in some cases you need something a little stronger. There are several ways you could go about that, such as firewalling your Redis or using spiped, but (post-Heartbleed) SSL is still one of my favorites. The following article explains how to set up a secure (read authenticated and encrypted) communication channel between your Redis client and server using stunnel. Naturally, this is the Do-It-Yourself hardcore approach, so if you want to have Redis and SSL without the heavy lifting, check out our plans and Redis clients that natively support SSL.— ItamarThe following post had originally appeared on Benjamin Cane’s Blog on February 18th, 2014 at Sending redis traffic through an SSL tunnel with stunnel.Lately if you have been paying attention to tech or even mainstream media you might have seen a few stories about data breaches. Sometimes these data breaches have allowed attackers to gather unencrypted passwords or credit card numbers. In the past these types of attacks still happened, but there was not as many attacks as today and when they happened they were kept secret. With more and more internet based services becoming part of peoples lives, there is even more targets for attackers who are looking to get sensitive data.These attackers can often be quite crafty on the ways they get this data, many times they do it by gaining access to a database but another common place to capture and steal data is through unencrypted network traffic. There are many commonly used services that either do not support SSL encryption or that option is rarely used. Redis a distributed memory cache is a newer service that at this time does not support SSL connections. I’ve been using Redis lately on one of my side projects, but I keep finding myself limited by the lack of SSL encryption.Redis has been designed for use within a trusted private network, and does not support SSL encrypted connections. While that is ok for many implementations, it does not lend well to cloud based implementations. While some cloud providers offer private networks, not all of them do. So if you want to run a Redis master on one server and your application on another, you have no choice but to leave that connection unencrypted. Leaving that sensitive traffic to be sent across the cloud providers network or even the general internet with no protection from someone with a network sniffer.In this article I am going to show you how to secure your Redis connections with stunnel. This article should handle the SSL part of securing a connection but you should also follow the other recommendations in Redis Security.The stunnel application is a SSL encryption wrapper that can tunnel unencrypted traffic (like redis) through a SSL encrypted tunnel to another server. While stunnel adds SSL encryption it does not guarantee 100% that the traffic will never be captured unencrypted. If an attacker was able to compromise either the server or client server they could capture unencrypted local traffic as it is being sent to stunnel.In today’s article we will use stunnel to encrypt traffic from a client host to a server host. We will install stunnel on both the client andserver hosts and establish a tunnel that redirects localhost:6379 on client to the redis instance running on server.We will first install redis and then setup stunnel to forward connections from external sources to the local redis instance.To install redis we will use apt-getAfter installation we only need to make one change to the redis configuration. For better security we will enable requirepass which requires all clients to authenticate before being able to pull or put data from the redis instance.Find:Replace With:Example:Upon installation redis-server is started automatically, in order for our configuration changes to take effect we will need to restart the instance.Now that redis is installed and running we will install stunnel. For ease we will install stunnel with apt-get as well.Unlike redis, stunnel doesn’t start on boot automatically. To start stunnel on boot we will need to edit the /etc/default/stunnel file.Find:Replace With:Like any other SSL protocol stunnel requires a certificate to use for client to server communication. While you could get a signed certificate from a certificate authority such as Verisign, since we are using this for internal purposes only we can create a self signed certificate.First we will create a private key, I am using openssl to create a 4096 bit RSA key. In my example I am using 4096 bit key as it adds more security than a 1024 or 2048 bit key.Now that we generated a key we will now create a certificate. When generating the certificate we will be asked a series of questions; the answers provided are used to prove the validity of the certificate. The -days flag specifies the number of days this certificate is valid for, you can modify this if you need to but 5 years should be good enough.Since this is really only being used for internal communications there isn’t a right or wrong answer to these, but the below example can be used as a guide for answering the certificates question.Example:We will combine both the key and certificate into a single file for stunnel to use. We will also change the file permissions to restrict who has access to read these key files.By default stunnel reads all *.conf files in/etc/stunnel/We will create a file named redis-server.conf and place our configuration within it.Add:Example:By default redis listens to the localhost IP 127.0.0.1 on port 6379Our configuration has stunnel accept connections on the external IP and forward the connections to the redis instance listening on 127.0.0.1:6379After the configuration file is in place we will start stunnel.At this point the server host is setup and ready, and we need to setup the client server to initiate the SSL tunnel with stunnel.For this example we will install redis-server on the client as well; though this step is only to install the redis-cli tool. Most likely you would not need to install redis-server if you are using this setup for an application running on the client host; so keep in mind this part is optional.By default redis starts on boot, however we do not want redis to be running on the client host. To disable redis from starting on boot we can use the update-rc.d command.Installing stunnel on the client is similar to the server installation, minus some configuration differences.To have stunnel start on boot we will need to edit the /etc/default/stunnel4 file.Find:Replace With:In order to establish an SSL connection we will need the private.pem file that we generated on the server host. You should always practice good certificate management with this key, if it was to fall into the wrong hands then the attacker could decrypt any SSL traffic that was previously capturedThe stunnel client configuration is very similar to the server configuration, to specify this stunnel instance is a client we will add client = yes to the configuration.Add:Example:On a client instance the accept and connect settings are reversed from the server configuration. This tells stunnel to listen locally on port 6379 and forward connections to the server host IP with port 6379Once the configuration is in place we can start the stunnel service.Now that both the server and client hosts have stunnel installed and a SSL tunnel established we can test this connection by using the redis-cli command to connect to localhost on the client.The way this configuration works is when a client on the client host connects to port 6379 locally it will be forwarded through the SSL tunnel that stunnel has created with the server host and redirected to the redis instance running on server. To setup an application to call this instance you would simply install the application on the client host and have it connect to redis on localhost the same way my example showed.This same setup could also be used for setting up Master/Slave replication in redis, however the Slave instance would need to listen to a port other than the default 6379—Benjamin Cane, a.k.a. @madflogo, is the founder of CloudRoutes and a Solutions Architect working on High & Continuous Availability systems in the financial services industry. He has been Administrating Linux and Unix systems for about 10+ years. You might find him writing and sharing stories about Linux, Unix, Python, DevOps, and other various Sysadmin-like things.Visit Redis for more on Redis technologies."
381,https://redis.com/blog/announcing-private-preview-program-upcoming-redis-enterprise-pack-5-0/,Announcing the Private Preview Program for the Upcoming Redis Enterprise Pack 5.0,"August 31, 2017",Cihan B,"Greetings, I am very excited to announce the preview program for our upcoming release of Redis Enterprise Pack version 5.0. As part of the program, we will give you a chance to directly provide feedback on the new capabilities and be part of the inner circle with engineering teams that deliver Redis open source bits and Redis Enterprise services and products.5.0 is a huge release with improved high availability, better scaling performance for Redis applications, advanced security and real-time search + query with indexing at Redis speeds. Here are the highlights;Figure.1: Geo-distributed CRDBsBesides the top features, Redis Enterprise Pack 5.0 also bring other improvements in core performance, scale and high availability for general Redis applications.Trying Redis Enterprise Pack 5.0 is simple. You can get the preview product on Ubuntu, RHEL, Oracle Linux or get the docker image for MacOS or Windows.The program will start around Sept 11th, run for a few weeks and is expected to complete around late Oct 2017.To sign up, please send an email to pm.group@redis.com and we will get you started with preview documentation and preview builds for you to try."
382,https://redis.com/blog/automated-cluster-recovery-redis-enterprise-kubernetes-operator/,Automated Cluster Recovery Comes to the Redis Enterprise Kubernetes Operator,"November 12, 2019",Amiram Mizne,"Released in October 2018, the Redis Enterprise Kubernetes Operator is only about a year old, but multiple customers are already using it to run Redis Enterprise clusters in production. Now, as we celebrate our Kubernetes Operator’s first birthday, we are adding another powerful Day-2 operations enabler to its toolset: Automated cluster recovery.For the first time, a Kubernetes Operator can manage a stateful service as if it were stateless, transforming how system operators and developers test, deploy, and manage Redis across environments.For some background, the Redis Enterprise cluster is a platform for managing multiple databases of various configurations in a multi-isolated-tenant architecture. We like to think of it as an orchestration platform for Redis database instances, also known as shards.Each of these databases can be set up in one of the following ways:In a typical deployment scenario, the cluster can manage hundreds of shards, scale to tens of terabytes of data, and run tens of millions of operations per second.Redis Enterprise cluster employs multiple mechanisms to guarantee the availability of the cluster even in cases of multiple shards failure or node failure events, as long as the majority of the cluster nodes remain alive.In the unlikely event of what is known as quorum loss, when more than half of the cluster nodes are down, many bad things occur. Caches no longer store complex operations or return cached results, extending application-response time. Web sessions are lost, and e-commerce shopping carts are emptied.Any such incident can have far-reaching negative business consequences to organizations, anything from poor customer experience to loss of multiple transactions, and those effects are magnified during busy hour, high season, or other special events like product launches.In all of these scenarios, restoring a cluster and its data to the original, fully operational state is crucial for business-critical Redis use cases where even a second of downtime could translate to the loss of thousands or even millions of operations.Beyond recovering from uncommon disasters and errors, there are other operational scenarios where automated cluster recovery is essential. Redis operators often relocate or replicate a cluster to a new environment in order to provide services in new regions or serve an expanding population. They also want to recycle the underlying infrastructure of a cluster for maintenance, patching, and scaling with minimal impact to production services. To improve resiliency, they need a precise, repeatable, and expeditious recovery model to facilitate chaos engineering in production environments.Kubernetes natively orchestrates the lifecycle of stateless services, whereas, in the case of a stateful service such as our cluster, that responsibility would fall on the system operator. This is where the Redis Enterprise Kubernetes Operator comes in.Our new cluster recovery mechanism lets users address all of these scenarios and more, by enabling recovery from an entire cluster failure event in just minutes. Kubernetes Operator cluster recovery executes a fully automated, predictable, and consistent process to recover the Redis Enterprise cluster. These operations, which previously could have taken hours to complete with increased vulnerability to human error, are now fast, reliable, and fully automated.Empowered by years of operational knowledge in which we manage more than a million Redis instances in production across all major public clouds and hundreds of on-premises environments, we have introduced a dedicated process to choreograph cluster bootstrapping. Under normal operating conditions, the bootstrapper creates a new cluster upon the instantiation of the deployment. When cluster recovery is required, the system operator can easily update the cluster’s declarative spec to initiate the recovery process.For auto-recovery, the bootstrapper recreates the cluster and automatically mounts the persistent volume claims of the previous cluster, which contain persistence data. It then recovers the original cluster configuration, joins the remaining nodes, and recreates all the databases provisioned on the original cluster to the new, recovered cluster. Next, recovery loads the datasets, balancing the data across the cluster nodes, and associating the endpoints previously used with each database.In most scenarios, the cluster recovers in a matter of minutes with no human intervention.The new Kubernetes Operator-powered recovery experience is available now, with our latest version of the Kubernetes deployment and Redis Enterprise cluster.If you’d like to experience how the Redis Operator performs automated cluster recovery, you can set up a Redis Enterprise cluster on any one of multiple Kubernetes distributions, on-premises on in the cloud, by following the instructions on our Kubernetes documentation or GitHub. Then, visit the Redis Enterprise Cluster Recovery for Kubernetes page for a walkthrough."
383,https://redis.com/blog/bug-fix-lua-messagepack/,Bug Fix: Lua and MessagePack,"June 13, 2018",Redis,"Apple Vulnerability Research made us aware of a potentially exploitable bug in Redis during May 2018. We took this disclosure very seriously and devoted resources to properly fixing this issue. We are happy to tell you that Salvatore Sanfilippo (the creator and Lead Developer of open source Redis) made the patch for open source users and contacted all major Redis providers. We have pushed the fix to all our Redis Enterprise offerings: Cloud, VPC and Software.Background: Redis embeds Lua for database-level scripting. The Lua scripting engine has a number of libraries included that are relevant to Redis use-cases. The original bug found was related to the MsgPack library, an implementation of the MessagePack serialization format. MessagePack is similar to JSON, but is more space efficient because it relies on binary encoding for control and, where possible, values themselves.A function that accepts a variable number of arguments (variadic) is available in Lua to “pack” a message using the MessagePack format. The implementation behind this variadic function does not properly check stack availability. Without this check, a very large number of arguments can be passed into this function and cause an overflow. Because of the nature of Redis, it’s possible that that data coming directly from an untrusted user is being accepted and processed in this way.There are no known ‘in-the-wild’ exploits of this bug. Because of the nature of this bug, as with any overflow situation, it is theoretically possible to perform remote code execution, however this would have to target very specific versions of Redis in very specific scenarios. We have not been made aware of any proof-of-concept of this behavior. More likely, this bug could be exploited to cause instability or a Redis server crash.During the review process it is customary to review other parts of the codebase for related errors. During this process, Salvatore assisted by the OSS community found similar bugs in the Lua implementation of packing and unpacking other structures. Fixes have also been implemented to address these issues.Apple Vulnerability Research has also raised an issue regarding Sentinel remote configuration as a security risk. We do not agree with this finding as Redis specifically and explicitly enables remote configuration with the understanding that Sentinel should not be accessible by a public facing address.Furthermore, Redis Enterprise doesn’t use Sentinel for high availability but rather other internal mechanisms, so this finding has no effect on Redis Enterprise users.All un-patched versions of Redis after 2.8.18 are potentially affected. To be at risk, you would have to be using both Lua and directly allowing unrestricted user inputs to be passed into scripts in very specific ways. While we believe it is a very small fraction of users using these features together in this way, we still suggest everyone apply the patch to prevent future vectors of attack on unpatched systems.The patch adds the proper checking of stack availability and returns an error if a Lua script tries to pack a message that is too large. This guarantees that, at run time, this particular attack will be impossible.We have checked other parts of Redis for similar bugs and so far have not found any further bugs with the same risk profile.In general, as long as you use one of our access control (Cloud Security Group) and authentication (SSL, Password)/access control (SIP) mechanisms, your Redis should be secured. In addition, If your application doesn’t use Lua scripting, you should not experience any issue whatsoever. And even if your application uses Lua, this bug happens in extreme conditions and only if your application calls Lua with a very large number of arguments. As this bug existed since Lua support in Redis was announced (in 2014) we believe you probably haven’t experienced the issue before and unless your application is changed dramatically, the likelihood for this to affect your Redis deployment is extremely low. Furthermore, Redis Enterprise comes with built-in replication, auto-failover and data-persistence mechanisms that ensure high-availability and durability of your data in cases of a Redis failure.That said, we have patched our Redis Cloud and VPC deployments, and if you have further questions, please contact support@redis.com to verify whether your deployment is not at risk. If you use Redis Enterprise software in an on-premises deployment you can download the latest Redis Enterprise version that includes the vulnerability fix from here.Last but not least, make sure your Redis Enterprise database is protected by password and the access control and authentication mechanisms.We suggest open source users first ensure that any Redis instances are not unrestrictedly open to the internet and then to upgrade to the latest version of Redis (3.2.12, 4.0.10, or 5.0-rc2, at time of writing) which can be found on the redis.io downloads page."
384,https://redis.com/blog/customer-qa-redis-enterprise-action-smartwaiver/,Customer Q&A: Redis Enterprise in Action at Smartwaiver,"April 26, 2017",Saman,"As one of the most-loved databases by developers, the list of popular applications that rely on Redis today is mind-boggling, and we always appreciate first-person perspectives from our customers who are on the front lines of implementing Redis in their application stack. So, we recently sat down with Ted Knudsen, CTO at Smartwaiver (and a long time fan of Redis databases). Smartwaiver is an online waiver solution that converts release-of-liability waivers into interactive, digital documents. Thousands of businesses rely on Smartwaiver to streamline their operations and make the waiver signing process secure, simple, fast and as painless as possible. Ted is a pragmatic technologist who’s savvy enough to use the best tools at his disposal and practical enough to pick the ones that make the most business sense. He is also happy to share his ideas on how to use Redis as a primary data store and as a system of engagement for his use case.Here are some excerpts from our conversation:Can you summarize how Smartwaiver uses Redis Enterprise in its stack today?Ted: At Smartwaiver, we use the Redis database in a variety of different ways throughout our application infrastructure. Thousands of kiosks call home into redis, to see if they have any special commands to run locally (such as autofill information). Redis also fills a variety of other roles for us — from underpinning interactive reporting to maintaining daily statistics and dashboards.What are your favorite Redis Enterprise database capabilities?We’ve found that Redis’ data structures (like List, Hash, Sets and Sorted Sets) are great for implementing complex functionality with simplicity. We use Redis to power command and control functionality within our kiosk application, which allows it to accomplish things like autofill, registration and more with lightning fast speeds and with very few resources. There’s really no other database that beats Redis’ pure ease of use.Can you elaborate on how Smartwaiver uses Redis Enterprise to power complex analytics?We run several internal dashboards out of our Redis database. These use Redis Lists to maintain and display the most recent waivers, and Hashes and Sorted Sets to display more complex graphs that compare “counts per hour” to “top customer counts per hour.” We also leverage Redis to maintain counters on transactions saved to our backend MySQL database, such as “completed waivers” per minute, per hour and to-date (and many more).Does Redis also support any caching use cases for your application?Yes, we also use Redis Enterprise to cache information about Smartwaiver kiosks and implement webhooks to integrate it with other services.In addition, our interactive reporting relies on the Redis cache for rapid load times. It would just be too slow to use data out of MySQL for our millions of waivers across thousands of customers.Have you compared Redis with other in-memory databases?We tried ElastiCache before but found it really has no viable high availability. Now, we run Redis Enterprise Pack databases within our VPC to maximize performance, availability and minimize our resource costs. Because network bandwidth and sharding vs. not sharding are big considerations for us, Redis Enterprise Pack provides the cost-efficiency and high performance we need within our VPC, making it a perfect choice!Read more testimonials from Redis customers."
385,https://redis.com/blog/everything-you-missed-at-redis-day-london/,Everything You Missed at Redis Day London,"November 20, 2019",Paul Bushell,"London may be infamous for its rain and gloominess in November, but the sun shined for two rare days as hundreds of developers, engineers, software architects, programmers, and business professionals gathered for Redis Day London.Held from 11–12 November at the Park Plaza London Riverbank Hotel, the second annual event brought together the Redis community for training, thought-leadership sessions, and networking. During day one, Kyle Davis and Loris Cro of Redis hosted a series of training sessions for 115 delegates who were new to Redis or wanted to learn about the latest updates in the Redis world. Salvatore Sanfilippo, Redis Creator, and Yiftach Shoolman, Redis CTO and Co-founder, kicked off the following day with two keynotes on the future of Redis, followed by presentations from community members and customers who shared how they have used Redis in their applications.Both days were filled with engaging questions and lively discussions that spilled out into the pubs well after official programming ended. Redis Day London provided a special opportunity for people who had traveled near and far to meet others who have used Redis for their own problem-solving.In the morning, some delegates in the packed room flocked to the Redis Streaming Architecture session. Our Developer Advocate Loris Cro shared an overview of how to use the Streaming commands, and how to use them to develop high performance, resilient architectures. Others attended the Redis 101 session, led by Head of Developer Advocacy Kyle Davis, which quickly brought people up to speed on the basics of Redis.The room came back together for two sessions, Redis Data Modeling & Patterns and Redis Clustering. In Redis Data Modeling, Kyle discussed multiple techniques and patterns to model data using module and built-in Redis data types. The clustering session, led by Redis’ Technical Enablement Architect Elena Kolevska, was planned with a Q&A-style panel discussion built in.The day wrapped up with Probabilistic Data Structures, RedisTimesSeries, and RediSearch sessions. Whether brand-new to Redis or an experienced user, these sessions provided an opportunity for people to ask questions and challenge their assumptions about data.Redis for a dating website, an online fantasy game, and moreOn day two, Redis Creator Salvatore Sanfilippo kicked off the day with a discussion on the forthcoming Redis 6.0, and Yiftach Shoolman, Redis CTO and Co-founder, followed with a few product announcements: RedisInsight, automated cluster recovery for Redis Enterprise Kubernetes Operator, and the first official Redis Developer Certification program.Customers shared creative and innovative ways they were able to use Redis in their businesses. Mugunthan Soundararajan, SVP of Technology at Matrimony.com, shared how using Redis Streams resulted in a 72% higher observed throughput compared to Apache Kafka for the matchmaking and marriage services site. Thomas Schedler, CEO and Head of Development at Sulu CMS, led a session on how the Symfony Messenger Component uses Redis Streams for event notification across bounded contexts.Engineers from Play Games 24×7 shared how they translated the rules of cricket into a popular online fantasy game using Redis Clusters. At ZEIT, Head of Operations Marcos Lilljedahl and Principal Engineer Matheus Fernandes explained how Redis Streams has helped scale out their serverless hosting platform.The original Redis Day London venue, CodeNode, fell through only a week before the event. Luckily, the Redis team was able to secure the beautiful Park Plaza London Riverbank, located along the River Thames, for a last-minute location swap. It was almost a parallel to the Redis product: just like Redis Enterprise has been able to withstand problems and perform under pressure—like during the AWS eu-central-1 outage last week—the Redis team was able to overcome adversity and still produce an insightful and fun conference.Both evenings ended with lively and insightful conversations at The Black Dog and The Rose, two neighborhood pubs within walking distance of the venue. Redis Day London was a rare opportunity to connect and learn—to bring together Redis users from all corners of the world to share and discuss their applications of Redis.Interested in attending a future Redis Day? Save the date for our future programming in Seattle, Bangalore, and San Francisco:Redis Day SeattleWhen: January 13–14, 2020Where: Hyatt Regency, 808 Howell St, Seattle, WA 98101Redis Day BangaloreWhen: January 21–22, 2020Where: Taj Yeshwantpur, 2275, Tumkur Road, Yeshwantpur, Bengaluru, 560022RedisConf 2020When: May 12–14, 2020Where: SVN West, 10 S Van Ness Ave, San Francisco, CA 94103"
386,https://redis.com/blog/extending-redis-cloud-support-new-google-cloud-platforms-regions/,Extending Redis Cloud support to new Google Cloud Platforms regions,"July 17, 2017",Aviad Abutbul,"We, at Redis, are happy to share that we have recently extended our Redise Cloud (Redis Enterprise Cloud) services to 2 more regions on Google Cloud Platform. You can now use Redise Cloud service on the following GCP regions: Eastern US (us-east1), Central US (us-central1) and Northeast Asia (asia-northeast1)Redise Cloud is an integrated Redis Service on Google Cloud Platform that is a fully managed database-as-a-service solution for hosting your Redis or Memcached databases. Redise Cloud provides stable high performance, effortless scaling and robust high availability features including persistence, cross zone/region/datacenter replication and instant automatic failover, with no data loss and is fully compatible with existing Redis applications.The steps here are super simple and go as follows:You can sign up for our free tier plan today to get started.For detailed information on Redise Cloud please visit our technical documentation"
387,https://redis.com/blog/hood-redis-enterprise-flash-database-architecture/,Under the Hood: Redis Enterprise Flash Database Architecture,"October 16, 2017",Cihan B,"For many interactive applications, responsiveness is key to engaging and fluid experiences. However, RAM is expensive. Keeping a large amount of data with Redis can be costly. Given the infrastructure cost, you end up choosing between the following 2 options;#1: Pay a premium for storing sizable datasets in RAM with Redis OR#2: Limit Redis database use to the most valuable data and augment Redis with disk-based relational or NoSQL databases.Redis Enterprise Flash provides a better option #3: Redis Enterprise Flash technology combines RAM and flash to store large data sets in Redis with much lower cost per GB. With Redis Enterprise Flash, you can extend RAM onto Flash devices like NVMe and SATA based SSD drives and keep larger data sets in Redis, all without losing Redis’ performance advantage.Let’s dig deeper into Redis Enterprise architecture to see how combining flash based SSD drives into the mix work in practice.Redis Enterprise Architecture OverviewLet’s start with an overview of Redis Enterprise before we drill into the Flash architecture.A Redis Enterprise cluster is composed of identical nodes that are deployed within a data center or stretched across local availability zones. Redise architecture is made up of a management path (depicted in the blue layer in Figure 1 below) and data access path (depicted in the red layer in Figure 1 below).– The Management path is composed of proxy which helps scale connections and cluster manager which is responsible for orchestrating the cluster and the placement of database shards, as well as detecting and mitigating failures.– The Data Access path is composed of master and slave Redis shards. Clients perform data operations on the master shard. Master shards maintain slave shards using the in-memory replication.Figure 1 Redis Enterprise nodes, with blue tiles representing the management path and red tiles representing the data access path with Redis as the shards.High Availability with Replication: Redis Enterprise uses in-memory replication to maintain master and slave replicas stretched across nodes, racks and zones. Redis Enterprise comes with various watchdogs that detect and protect against many failure types. In node, network and process failures that render the master replica inaccessible, Redis Enterprise automatically promotes the slave replica to be a master replica and redirects the client connection transparently to the new master replica.Besides the intra-cluster replication, Redis Enterprise also has built-in WAN-based replication for Redis deployments across multiple data centers. You can find additional details in the references section.Scaling & Performance with Sharding: Each Redis Enterprise cluster can contain multiple databases. In Redis, databases represent data that belong to a single application, tenant or microservice. Redis Enterprise is built to scale to hundreds of databases per cluster to provide flexible and efficient multi-tenancy models.Each database can contain few or many Redis shards. Sharding is transparent to Redis applications. Master shards in the database process data operations for a given subset of keys. The number of shards per database is configurable and depend on the throughput needs of the applications. Databases in Redis Enterprise can be resharded into more Redis shards to scale throughput while maintaining sub-millisecond latencies. Re-sharding is performed without downtime.Figure 2 Redis Enterprise places master (M) and slave (S) replicas in separate nodes, racks and zones and use in-memory replication to protect data against failures.In Redis Enterprise, each database has a quota of RAM. The quota cannot exceed the limits of the RAM available on the node. However, with Redis Enterprise Flash, RAM is extended to the local flash drive (SATA, NVMe SSDs etc). The total quota of the database can take advantage of both RAM and flash drive. The administrator can choose the RAM vs Flash ratio using the slide seen in figure 3. This ratio can be updated at any moment in the lifetime of the database without downtime.Figure 3 Create Database dialog in Redis Enterprise Pack with the view of the RAM and Flash configuration.Redis Enterprise Flash ArchitectureWith Redis Enterprise Flash, you get an enhanced version of Redis as a shard. Besides other modifications, with this shard, instead of storing all keys and data in RAM, less frequently accessed values are pushed to flash. In figure 4, you can see the RAM and Flash combined together for storing data as 2 separate shades of gray.Figure 4 Redis Enterprise Flash shards with process, memory and disk storage components. Redis Enterprise Flash uses both RAM and Flash for keeping data. RAM store all keys and some values. As the RAM fills up, less frequently used values are moved to flash (NVMe or SATA based SSDs).If applications need to access a value that is in flash, Redis Enterprise automatically brings the value into RAM. Depending on the flash hardware in use, applications experience slightly higher latency when bringing values back into RAM from flash. However subsequent accesses to the same value is fast, once the value is in RAM.Using smart placement techniques, Redis Enterprise Flash adapts to changes in the workload over time. Redis Enterprise Flash has a background task that ejects less frequently used values to flash in order to adapt and maintain a healthy dose of free space for new incoming operations.It is important to note that even though values get ejected to flash, all keys and metadata stay in RAM. Keys are typically smaller in size than values. Many Redis commands require access to keys without requiring access to the value. Keeping the full list of keys in RAM ensures many operations can be executed without any penalty of value retrieval from flash. Background services managing expiry, ensuring uniqueness of keys in the database are frequent operations in the database. With all keys stored in RAM, it is easy to check if the key already exists before inserting the new key or to run expiry checks.Durability: Redis Enterprise Flash uses a flash drive as a RAM extension. At bootstrap of the database, Redis Enterprise Flash expects and ensures that both RAM and Flash drive are completely empty. Once the engine is started, RAM+flash is populated from the durable copy of data (disk or another replica). When using Redis Enterprise Flash, you can use either of Redis Enterprise’s two durability options:– Disk-based durability: Redis Enterprise still maintains a durable copy on disk. Just like disk-based systems, this IO path is placed on a slower and more durable network-attached storage device. Redis databases provide tunable options to maintain this durable copy. You can read more about the durability options here.– Replication-based durability: Redis Enterprise also maintain a replica–a slave shard–for durability. Replication-based durability protects against node, rack or zone failures and provides better write performance than network-attached storage writes. This means that in the event of an unplanned interruption, it is likely that your replica is more up to date than your durable copy on disk. To take full advantage of the replicated-durability, Redis provides the WAIT command. WAIT ensures that a write can wait for acknowledgement until multiple replicas confirm that write. This ensures that a write confirmed with WAIT on replicas will be durable even if a node catches on fire and never comes back to the cluster.“Buffer Cache” vs The “RAM Extension” ApproachThe smart data placement in Redis Enterprise Flash, which brings values from flash into RAM based on working set, is similar to disk-based database systems and the “Cache miss” on the buffer cache of the database. However, similarities between disk-based databases and the RAM extension method used in Redis Enterprise Flash end there. The IO patterns used in Redis Enterprise Flash are much more efficient than those of a disk-based system. Here are some of the differences between the two:– Hot Value Handling: Many application workloads perform repeated writes to a set of “hot” keys in a short period of time, such as when the keys belong to an active piece of data. For example, imagine repeated updates by an app to its database, tracking a current shopper’s state on a site as the shopper views various products. Disk-based databases perform these writes both in RAM and on disk to persist the changes each time. However, updates to data in RAM are not sent to flash in Redis Enterprise Flash. The RAM extension approach used by Redis Enterprise Flash does not require any writes to flash under repeated writes to “hot” keys, unless the value gets ejected to flash. Remember that active values don’t get ejected to flash and mostly stay in RAM, so the repeated updates to the active shopper’s keys simply happen in RAM and do not require flash writes. This ensure that the IO bandwidth of the Flash drive is only used for ejections to Flash.– Exploiting Ephemeral Storage: The cloud architecture in public or private clouds typically comes with two types of storage, faster ephemeral storage and slower durable network-attached storage. Disk-based databases require their writes to persist all the way to disk for every write. Thus you are required to use the persisted network-attached storage. However, Redis Enterprise Flash treats flash memory as a RAM extension, thus it can fully take advantage of local, fast ephemeral storage.– Write Amplification: Disk-based databases depend on disk writes for durability. Each write to disk in disk-based databases is typically done through a redo-log (RL) or a write-ahead-log (WAL) before the actual values are updated on storage. Redis Enterprise Flash uses RocksDB to manage the flash drive access. The call sequence to RocksDB with Redis Enterprise Flash does not need to maintain these additional WALs. Write amplification measures the number of IO operations that any single read/write causes. Due to the logged writes, disk-based databases end up with much higher write amplification. You can read more about RocksDB and various IO amplification effects here.– Advances in HW with Persistent Memory: The techniques used in creating Redis Enterprise Flash are based on the new direction in memory technology. As persistent memory is introduced into the compute architecture such as Intel’s 3DXPoint, the idea behind these technologies is to allow the application to decide which part of the data will be kept in RAM and which will use Flash/Nand in-order to maximize performance at the optimal cost. Redis Enterprise Flash was designed to exploit these benefits.It is easy to get started with Redis Enterprise Flash with Docker on Windows, Linux or Mac machine. You can find the steps here: Redis Enterprise Flash Quick Start."
388,https://redis.com/blog/innovating-at-enterprise-pace/,Innovating at Enterprise Pace,"October 12, 2021",Adi Shtatfeld,"Enterprises today face a fast-changing market where the time-to-market keeps shrinking. This hyper-competitive market requires organizations to implement changes to applications and infrastructure much faster than they did in the past.These organizations are additionally challenged by critical legacy systems with operational procedures built to maintain maximum availability and sustainability as a vital business advantage. These operational procedures are designed to regulate the implementation of changes to prevent outage and data loss. In practice, they set a structured path for upgrades, restricting frequent changes so, in case of a problem, it’s easier to identify the change that caused the problem and revert the system to the pre-changed state.Organizations must balance stability and innovation. They have to stay competitive while maintaining maximum uptime. A recommended way to do so is to follow the “divide and conquer” strategy: By identifying and prioritizing the different system components, you can gain more control over the process.A system comprises several layers and modules, not necessarily coupled or sharing the same requirements and limitations. Dependencies could exist between different components of the system, but usually a different upgrade strategy can be applied per each. An operating system can be upgraded due to reaching end-of-life, for example, separately from upgrading the software and applications it runs.During the course of the system life cycle, there could be many reasons to upgrade: compliance, security exposures, infrastructure end-of-life, new functionality, better performance, and more. Ideally, we would have it all, but outside forces usually dictate a specific order. Which infrastructure or application should be updated first and which one can be delayed are decisions you need to make. For example, a major driver for update are external non-negotiable requirements such as security regulation compliance. Regulations often set a deadline which cannot be changed.In order to gain control over changes introduced to your systems, you should:In enterprise grade software it’s critical to have flexible adoption paths. Flexible adoption paths enable you to upgrade and modify your system in smaller units of work, based on the organization’s goals and requirements. Each component should be upgraded to the level that serves the business needs best.Starting with Redis Enterprise v6.2.4 you can choose the Redis server version you want to upgrade to according to your upgrade cadence tolerance. Choose the latest Redis if you can tolerate frequent upgrades and want to leverage the newest enhancements, or stick with major releases if you want to minimize the number of times you upgrade. This flexibility is designed to support the release of more than one open source Redis version a year, while maintaining a longer product life cycle for Redis Enterprise Software. Specifically, organizations that want to utilize the 18-month long product life cycle can take advantage of the seamless upgrade path by adhering to major Redis releases.Redis recognizes that many organization’s environments consist of both rigid, legacy-based systems and flexible, cloud-based systems. Redis has designed an upgrade process that can easily conform to whatever your operational procedures require. Redis Enterprise upgrade process is a non-disruptive upgrade (NDU), meaning you can upgrade your systems with no impact on availability. Redis maintains support for three levels back of open source Redis so that you can determine the upgrade process that’s optimal for your organization. This NDU upgrade process and backward compatibility are designed to ensure availability and performance while giving you the latest in Redis Enterprise innovation. Get the latest Redis Enterprise data platform functionality when you need it while adhering to all your operational procedures by identifying what needs to be upgraded and prioritizing based on your needs.——Redis Enterprise Software is a real-time data platform based on open source Redis, delivering enterprise capabilities for a high-performance cache and primary database. Redis Enterprise is often used in a multi-tenant cluster configuration."
389,https://redis.com/blog/inovonics-choosing-right-sensor-work-redisedge-iot-applications/,Inovonics: Choosing the Right Sensor to Work with RedisEdge for IoT Applications,"June 19, 2019",Lalit Pandit,"In my previous post, I gave a high-level overview of our Redis-based, industrial Internet of Things (IoT) technology at Inovonics, which is the leader in wireless networks. I will give more details on each of our system components in this and subsequent posts, and introduce you to some of the more technical features of Redis (and Redis Enterprise in particular) that we use at Inovonics.To recap my previous post, the following components comprise a full IoT system:Be aware that gateways, cloud data stores and network engineering can be configured in several different ways, and in some cases can be skipped altogether. At a minimum, an IoT system needs sensors, a network and some way of managing data from the sensors. Redis offers a portfolio of technologies and products for data storage, event triggering and analytics across all of these components. For this post, let us discuss the eyes and ears of the system, namely its sensors.In choosing or designing sensors for a specific application, consider the following list of criteria:Last but not least, when selecting a sensor, the cost of the sensor itself is a major consideration. This includes both the sensor’s material cost and the installation cost. Keep in mind that each application may require other, more specific criteria for sensor selection. For example, when choosing sensor electronics, data treatment implications also influence its performance (speed) and the cost of your sensors. A sensor might work with its data in any of the following modes:Sensor without separate explicit data storage: In this model, the sensor simply sends and receives messages when triggered. For example, a temperature sensor may send a notification when the temperature rises above a certain threshold. In addition, the sensor may be equipped with some electronics in order to perform calculations (e.g., counters). Some pulse-encoded sensors send a message when the number of pulses exceeds a certain value since the last message. Typically, hardware electronics manage the state of the sensor without requiring a separate data store.Sensor with short-term data storage: Here, the sensor is equipped with storage—such as internal flash or local storage—to temporarily buffer the data. The sensor may process and send only information of interest, or it may simply choose to send data in batches. Buffering can help synchronize the speed of sensing and transmitting data. A buffer can also guard against wireless network outages because the sensor simply resends the data. A sensor may choose to store its state as well, in which case a power cycle on the sensor will not cause any loss of state or data. A sensor fitted with a camera for pictures or videos will usually have a temporary data store.Sensor with long-term data storage: One scenario for longer-term storage is when a sensor may not have any means of communicating on a wired or wireless network. In these cases, a technician visits the sensor periodically to retrieve its data. If you use these types of sensors for applications with stringent demands, it is very important to ensure that the data does not get lost due to any hardware or software issues.If your application demands high volumes of data storage for either the short or long term, RedisEdge from Redis can be an excellent complement to your IoT environment. It is a multi-model database that was purpose-built for the demanding conditions at the IoT edge. Since RedisEdge can ingest millions of writes per second with <1ms latency and a very small footprint (<5MB), it works great for constrained compute environments.  And even though RedisEdge is best known for its blazing-fast performance, its reliability in storing data also adds value to applications where performance may not be the driving factor. RedisEdge gracefully meets the diverse data services needs of IoT edge environments that might require multiple data models (e.g., time-series or graph data) to support video streaming analytics, image recognition or other complex computing requirements.Regardless of the data store you choose, there are several trade-offs you may decide to make in your sensor selection. For example, a more “intelligent” sensor may require more powerful processing at the edge, which would drive up costs and impact battery life (due to higher power draw). In addition, harsh environmental conditions and tamper proofing requirements to bolster physical security may necessitate sturdier devices, which would increase costs as well. Given all of this, it is always important to analyze your short-term and longer-term application requirements and determine the ideal sensor for your use case. In my next blog, I will discuss network implications to consider for designing your IoT system."
390,https://redis.com/blog/inovonics-uses-redis-enterprise-drive-real-time-iot-data-analytics/,How Inovonics Uses Redis Enterprise to Drive Real-time IoT Data Analytics,"April 17, 2019",Lalit Pandit,"Over the past 30 years, Inovonics has risen to become a leading provider of industrial wireless IoT technology. We’ve deployed tens of millions of devices in challenging commercial environments across a wide variety of critical applications—from life safety to intrusion alerting to multi-family submetering.And while we work tirelessly every single day to improve the reliability and ingenuity of our wireless infrastructure and IoT solutions, we’ve come to realize that we have an equally compelling product to offer… data!The massive—and unique—sets of data collected by our wireless devices and sensors have tremendous application. For example, we can:Our previous generation system was based on desktop applications connecting to a gateway directly at the customer site (i.e. no cloud). This gateway also served as a data repository based on a relational database for the IoT devices. Customers needed a desktop computer to access the data and it was very challenging for us as a vendor to detect issues with the end point(remote devices), and maintain and troubleshoot the gateway remotely.When we designed the new generation of our gateway and application, our goal was to consolidate all data from IoT devices in a central repository—and to future proof the architecture with respect to performance and reliability. Providing ubiquitous access to the data and insights in markets such as life safety was also a key requirement. These non-negotiables, along with the desire to keep operational footprint costs as low as possible, led us to build our new generation system in the cloud.Today, we’re running Redis Enterprise Cloud Pro, a fully-automated database-as-a-service, on Google Cloud Platform (GCP). Redis Enterprise acts as a data ingest, storing the millions of daily messages coming from Inovonics’ sensor networks and providing a central view from which data can be analyzed in the aggregate. Redis Enterprise also stores the application data model so that incoming messages can be correlated with representational information such as sensor location. (Our on-premises gateways have open-source Redis repositories that buffer the messages coming from IoT endpoints before they are sent to the cloud.)Our original intent was to use Redis Enterprise Cloud Pro as a cache only for incoming IoT device data and systematically age out the data to other, slower data stores. But due to the programmatic ease of interfacing with Redis Enterprise and Redis’ invaluable managed services which ensure uptime, we’ve been able to extend Redis Enterprise to serve as a primary repository of our data. We still intend to implement data archiving as data volume grows, but can do so at our own pace.The high reliability and performance of Redis Enterprise have enabled us to focus on building end user solutions for our target domains rather than optimizing database access. Our initial launch of the application built on top of Redis Enterprise was in the submetering market. We are now expanding the application in other markets, and, as more data emanating from our IoT devices gets consolidated in the cloud, we see opportunities for providing additional value based on predictive data analysis and aggregations.I hope you’ve enjoyed learning a little about Inovonics’ application of Redis Enterprise. In my next blog post, I’m looking forward to sharing more details on our IoT and Redis use case."
391,https://redis.com/blog/instant-customer-experiences-in-financial-services/,Creating Instant Customer Experiences in Financial Services with a Real-Time Database,"March 10, 2022",Henry Tam,"Accelerate Data Innovation with Real-Time Financial Services, an essential white paper companion highlighting the challenges of legacy infrastructures and opportunities made available by real-time data, is now available. Download for free below.Banks and financial institutions face exceptional challenges in today’s fast-paced digi-sphere. The meteoric rise of digital communications and evolving customer behaviors have propelled digital products from the periphery to the center. Success hinges on the quality of the customer experience.To compete, financial institutions must provide an array of real-time services to meet an expectation that’s been developed by the digital era and accelerated by the pandemic: customers demand a seamless omnichannel banking experience.The curtain may not have fully closed on in-person interactions, for they still remain a crucial aspect of banking, but today’s market demands revolve around speed and accessibility, requiring financial institutions to pivot towards digital banking.This requires stark transformations in many areas of the customer experience, including how users interact with the bank, how digital products are purchased, how portfolios are accessed, and how customer support is provided.But for banks, this poses one serious challenge: the rigidity of their legacy IT systems means they have problems with scalability, flexibility, reliability, and complexity. Fintech start-ups created in the cloud have a modern architecture that gives them the agility to adapt to market trends, the flexibility to innovate, and the opportunity to maximize the user experience.In contrast, banks are bearing the weight of a cumbersome architecture that leaves them heavy-footed as opportunities pass them by. This is problematic because today’s tech-sphere has shaped customer expectations in almost every industry, making them accustomed to fast, easy, and accessible digital products.Meeting these demands is synonymous with the adoption of digital products, a reality that highlights an advantage that fintechs have over banks – banks have only dipped their toes into digital waters, fintechs were born in them.As a result, creative fintechs are turning heads in today’s marketplace because of their ability to provide a fast, seamless, personalized, and easily accessible financial service. Banks and financial institutions must embrace cloud and microservices architectures that use real-time data to maximize the user experience and ultimately meet customer expectations.Below we’ll reveal how real-time data can accelerate the modernization of banks and surpass consumer and business expectations.Customers across all demographics want fast, easy, and simple access to their finances through digital doorways. Being the first digital generation, millennials are more intertwined with digital technology, demanding a firmer grip on their finances with a few taps on their smartphone.Older generations have the same expectations but for different reasons. Digital banking is a safer way for the most vulnerable to access their finances in the pandemic. Equally, people with mobility issues can circumvent the challenges of in-person visits just by going online or via an application.Yet the greatest of these expectations is for digital banking to be absolutely seamless. This means that real-time data isn’t expected, it’s demanded, and users won’t tolerate an omnichannel experience that’s decimated by lags or inconsistent interactions due to the lack of a standardized data layer.The reality is that even though customers favor banking through digital channels, they often prefer face-face-to-face interactions when committing to more complex financial products. Mortgages, loans, and investments – these are all examples of when the human side needs to be a part of the banking experience.Research shows that banks are more likely to maximize sales when combining the human element with digital channels to produce a more complete banking solution. One European bank experienced consistent sales growth of up to 20% over two to three years. Around 60% of active customers leverage digital channels (online and mobile) and 80% of all touchpoints occur on digital.However, only 25% of sales are made online or via mobile (20% online, 5% mobile). Now, although digital channels have revamped the banking experience, and customers have welcomed and adapted to this, many of the conversions still happen in-branch or on the telephone.Customers expect to use different channels depending on their objectives. They do research, gather information on investments or loans on the website, discuss with a banker or advisor the different options, and finalize the transaction over the phone, email, or in-person. Often, they then hop onto the mobile app to check status or balances and then chat with support if they encounter any issues.But banks may have ended up with disparate databases for each channel that was developed. Without a way to integrate the siloed data or use a unified data layer to present the 360 views of customer data in real-time, customers will face an incoherent experience when moving between the different channels.Moreover, providing a seamless user experience isn’t just table stakes for banks anymore – deficiencies in any channel are enough to create friction and encourage customers to go to competitors.Research suggests the bar has been set and it’s sky-high:Transitioning from legacy IT systems to modern cloud architectures won’t happen overnight and will be a long process for many traditional banks. But the adoption of a unified real-time data layer that acts as a superhighway from the backend systems to customer-facing applications will give banks the speed and flexibility to innovate with digitized products and services. It also provides an omnichannel experience, and ultimately meets customer expectations all while this transition takes place.But creating real-time customer experiences requires a real-time multi-model in-memory database. Using one that can provide the necessary data with low latency and high read/write throughput will revamp the customer experience from top to bottom; lags will be eliminated, all interaction channels will be cohesive, promoting a seamless experience. Users will be able to instantly access their account information on their mobile devices wherever they are.In addition, users can instantly search through their transaction history or understand where they are spending too much on their credit cards with an in-memory database that supports secondary indexing of their datasets and can query and aggregate the data in a fully distributed manner in real-time.For traditional banks, the returns can be astronomical. The adoption of a real-time data platform paves the way for an omnichannel platform that will transform the customer experience in a way that builds their trust, engagement, and loyalty. This results in better customer retention, acquisition, and enhanced brand reputation, which in turn lead to growth and fending off those fintech disruptors.Today’s tech environment is moving at such a pace that yesterday’s data is old enough to be stale and inaccurate. Acting on these insights is enough for an opportunity to slip by or for a hidden inhibitor to hamper the user experience. All it takes is a lag to create friction, two to create frustration, and three to kill the experience altogether.This places banks in a very uncomfortable position. IT legacy systems are inherently rigid and lack the scale and flexibility to quickly unearth, filter, and streamline unstructured data in a way that’s optimal for analysis. Cloud-based architectures, on the other hand, provide fintechs with instant access to accurate data.The accessibility of these cloud-based architectures gives fintech the upper hand over banks when it comes to data analytics. When fintechs are analyzing, banks are still gathering. When fintechs have spotted a new opportunity, banks are still searching for it. When fintechs are innovating, banks are still guessing.To level the playing field, banks must integrate a database that can power real-time analytics. This is an indispensable asset that allows analysts to respond instantly to customer preferences, negative feedback, and fix errors before they linger long enough to contaminate the user experience. With real-time analytics, banks can discover which products users are interested in or how users interact with their products at that precise moment in time so they can recommend new products or services.This takes a lot of the guesswork out of the personalization process and allows banks to create digitized products that are more closely aligned with customer needs and preferences. But this is only the tip of the iceberg; trends can be detected instantly, important performance indicators can be monitored, and points of engagement and disengagement can be identified as well as areas of strength, mediocrity, and weakness.Fintechs are basking in a hotpot of data innovation, partly because of their ability to micro-measure experiments when innovating new ideas. Leveraging real-time analytics sets up the perfect conditions for experimentation because analysts can instantly gauge what’s working and what’s not.New ideas can quickly be refined, tweaked, and tested in shallow waters before being thrown into the deep end. Banks will then have the ability to tailor the banking experience based on user behavior and create a product that resonates with their target market.Financial markets are volatile and always susceptible to unexpected fluctuations. A political event, a pandemic, or even a tweet from Elon Musk is enough to give them a shake.As these markets ebb and flow with international events, portfolio managers are closely analyzing which investment opportunities will generate the best returns for their clients. New investment assets like cryptocurrency (Bitcoin for example) are set to be mainstream. Sudden fluctuations in the markets open just as many doors as they close. For a trader to capitalize on these investment opportunities, they need to make decisions that are based on reliable and accurate market data.But in today’s fast-paced environment, the speed at which data is accrued has a large bearing on the quality of data – decisions made on minutes-old market or pricing data can lead to severe losses. To stay ahead of the curve, portfolio managers must leverage real-time insights and risk calculations to move with the market’s rapid movements instead of playing catch-up.Using real-time analytics, portfolio managers can see the potential gain or loss made from an investment decision at that precise moment of time. These potential gains or losses will recalibrate within each notch in an investment asset (stocks, options, commodities, futures, etc.) price. This minimizes risk by allowing traders to know when to cut their losses, reacquire stock or even capitalize on a window of opportunity before it closes.We also can’t ignore the unprecedented influx of new users due to fintech apps like Robinhood and others entering the market in recent years. The industry has become more cutthroat, more competitive, and more unpredictable. And given that market values fluctuate on a dime, traders must leverage real-time data to make quick, calculated decisions on price moves to maximize returns for their clients.Banks and financial institutions need to modernize to meet modern-day expectations. Customers demand the seamless user experience that’s provided by innovations from the fintech disruptors. Legacy IT systems and Relational Database Management Systems (RDBMS) are still the backbone and system of records for many banks and will take time to update. A solution is needed which enables them to innovate and respond to customer expectations without disrupting these legacy back-office systems.A real-time modern data layer provides a fast and powerful solution that many banks can pivot to while making the transition to cloud-based architectures. Redis Enterprise can accelerate this process by supplementing banks with a real-time database that guarantees real-time customer/client experiences."
392,https://redis.com/blog/introducing-redis-enterprise/,Introducing Redis Enterprise,"February 7, 2017",Manish Gupta,"Redis was founded in 2011 to deliver highly resilient and scalable Redis databases for enterprises worldwide. Over time we developed and fine tuned our technology to deliver its benefits in a robust, reliable automated manner both in a variety of cloud environments as well as downloadable software. Today, Redis unveiled Redis Enterprise (Redise), a unified brand representing an exponential increase to the power of Redis with the highest levels of availability in geographically distributed environments, effortless scaling to any dataset sizes, and significant cost reduction to enterprise customers, all the while retaining support for versatile open-source Redis, beloved for its data structures and modules, that deliver the fastest performance with the lowest compute resources and unmatched simplicity.Redis Enterprise technology can be deployed in multiple ways including on any public cloud (Redise Cloud), in corporate VPCs (Redise Cloud Private), as self-managed software (Redise Pack), or as fully managed on-premises software (Redise Pack Managed). This provides maximum flexibility to enterprise application developers, architects, devops, and operations teams, who can choose their mode of Redis consumption depending on where they need the high performance and resilience of Redis Enterprise.The success of Redis Enterprise is a testament to the innovation that Redis continues to foster, supplementing the efforts of the Redis open source community. Over 60,000 customers benefit from the investments that have delivered pioneering capabilities in the database world including database-as-a-service, Redise Flash, Modules,  Spark-Redis integration, Redis Search module, Redis-Machine Learning module, etc.The introduction of Redise comes at a very important moment in the industry. Real-time experience or insight is not just a moniker or a vision statement anymore. Enterprises need and demand it today. Most databases that advertise this tend to either offer this level of performance in a very limited manner or simply market their roadmap. Redise by Redis is making real-time real today. Enterprises are leveraging Redise for mission critical e-Commerce, Personalization, Social, Fraud Detection, Metering, IoT, and other real-time applications.Explore Redise on our brand new website at redis.com and experience the fastest and most prolific database in the world!"
393,https://redis.com/blog/introduction-redis-enterprise-openshift/,Introduction to Redis Enterprise on OpenShift,"August 21, 2019",Redis,"Here’s a brief introduction to Red Hat OpenShift, including questions we often get asked and our thoughts on why it’s a great choice for deploying and running your Redis Enterprise Cluster.There are a few pieces of background you should have before I can properly answer that question, so let me start with a simple definition and then progressively fill in the details.OpenShift is Red Hat’s Platform-as-a-Service (PaaS) offering for running containerized applications based on their Kubernetes distribution.PaaS is a category of cloud computing services that allows customers to deploy and run applications without having to worry about many operational details. With an Infrastructure-as-a-Service (IaaS) offering, you pay for computing units (be it bare-metal servers or virtual machines), and then it’s up to you to properly install all the necessary software and operate it over time. With PaaS, you don’t need to spend resources or expertise on operating your infrastructure. To support this level of automation, OpenShift makes use of Kubernetes.Kubernetes is an open source container-orchestration system for automated provisioning, scaling and management of your apps and services. In other words, Kubernetes runs clusters of services for you. It was open sourced by Google and is now part of the Cloud Native Computing Foundation (CNCF). You can think of OpenShift as a distribution of Kubernetes, however keep in mind that OpenShift is a product, while Kubernetes and its various distributions are just projects. When you install Kubernetes, it’s up to you to figure out how to set it up correctly, and if something doesn’t work, you can count on community support at best. OpenShift, being a product, bundles access to Red Hat’s support. This might seem like a small difference to some, but it’s hugely important for enterprises. OpenShift is based on a Kubernetes distribution called OKD.OKD is Red Hat’s open source distribution of Kubernetes that powers OpenShift. As mentioned above, OKD is the open source project, while OpenShift is the product. A distribution of Kubernetes includes core Kubernetes components plus an opinionated choice of tooling that surrounds the core engine. In many ways, it’s the same thing that happens with Linux distributions: the kernel is Linux, but all the components around it differ depending on the choices of the respective maintainers. OKD, for example, has a different way of specifying the desired layout of a service cluster compared to vanilla Kubernetes (OKD templates vs. Helm charts).First of all, we must talk about the benefits of using Kubernetes in general. With Kubernetes, developers gain a self-service interface for orchestrating resources. When a developer writes code for their application, they also write any infrastructural requirements (e.g., databases, caches, load balancers) into a configuration file. This file is then checked into a source-control repository with the rest of the code.Developers that use this method don’t need deep operational knowledge because Kubernetes handles the details for them. After that, running the infrastructure is just as easy. Any change applied to the aforementioned configuration file will then be used by Kubernetes to infer the desired cluster state. Kubernetes knows how to assess the current cluster state and determine which operations to apply in order to transition towards the desired state. This also applies, for example, to horizontal scaling when the developers can simply changes the number of desired replicas of a given service.The benefits of using OpenShift relate to Red Hat’s expertise running open source services and infrastructure. OpenShift is purpose-built for a great Kubernetes experience, starting from the operating system (OS) itself. OpenShift 4 (the latest version to date) is optimized to run on CoreOS, a Linux distribution specialized to run containerized applications (acquired by Red Hat in January 2018). OpenShift also features tools that are missing from vanilla Kubernetes, such as a web control panel for service deployment and monitoring, an integrated CI/CD system and many others.Redis Enterprise Cluster helps you scale linearly to millions of operations per second. We can manage it for you, so you don’t need Kubernetes for that. That said, if you are already investing in the Kubernetes ecosystem and want your developers to have the same self-service access to Redis Enterprise as any other resource, we have great support for OpenShift through the Operator Framework.Kubernetes needs to manage many different kinds of resources in a highly automated way, but it can’t know how to run everything right out of the box. The Operator Framework is a way of extending Kubernetes by adding custom logic specifically designed to manage a single type of resource. At Redis, we have created and maintain an operator for Redis Enterprise that you can easily get from the OperatorHub, OpenShift’s marketplace for operators.Once you set it up, you will then be able to focus on what you really care about: developing your applications. Redis Enterprise can help you consolidate your existing architecture and expand to new possibilities like deploying a geo-distributed multi-master database cluster. For more information on design patterns for replication, take a look at Sheryl Sage’s guest post on the official OpenShift blog.To learn how to use the Redis Enterprise Cluster operator on OpenShift, read this how-to by Amiram Mizne, checkout our documentation or sign up for a free OpenShift trial."
394,https://redis.com/blog/introduction-redis-ml-part-five/,An Introduction to Redis-ML (Part Five),"September 5, 2017",Tague Griffith,"This post is part five of a series of posts examining the features of the Redis-ML module.  The first post in the series can be found here.  The sample code included in this post requires several Python libraries and a Redis instance with the Redis-ML module loaded.  Detailed setup instructions for the runtime environment are provided in both part one and part two of the series.Decision TreesDecision trees are a predictive model used for classification and regression problems in machine learning.  Decision trees model a sequence of rules as a binary tree.  The interior nodes of the tree represent a split or a rule and the leaves represent a classification or value.Each rule in the tree operates on a single feature of the data set. If the condition of the rule is met, move to the left child; otherwise move to the right.  For a categorical feature (enumerations), the test the rule uses is membership in a particular category.For features with continuous values the test is “less than” or “equal to.”  To evaluate a data point, start at the root note and traverse the tree by evaluating the rules in the interior node, until a leaf node is reached.  The leaf node is labeled with the decision to return.  An example decision tree is shown below:Many different algorithms (recursive partitioning, top-down induction, etc.) can be used to build a decision tree, but the evaluation procedure is always the same.   To improve the accuracy of decision trees, they are often aggregated into random forests which use multiple trees to classify a datapoint and take the majority decision across the trees as a final classification.To demonstrate how decision trees work and how a decision tree can be represented in Redis, we will build a Titanic survival predictor using the scikit-learn Python package and Redis.Titanic DatasetOn April 15, 1912, the Titanic sank in the North Atlantic Ocean after colliding with an iceberg.  More than 1500 passengers died as a result of the collision, making it one of the most deadly commercial maritime disasters in modern history.  While there was some element of luck in surviving the disaster, looking at the data shows biases that made some groups of passengers more likely to survive than others.The Titanic Dataset, a copy of which is available here, is a classic dataset used in machine learning.  The copy of the dataset, from the Vanderbilt archives, which we used for this post contains records for 1309 of the passengers on the Titanic.  The records consist of 14 different fields: passenger class, survived, name, sex, age, number of siblings/spouses, number of parents/children aboard, ticket number, fare, cabin, port of embarkation, life boat, body number and destination.A cursory scan of our data in Excel shows lots of missing data in our dataset.  The missing fields will impact our results, so we need to do some cleanup on our data before building our decision tree.  We will use the pandas library to preprocess our data.  You can install the pandas library using pip, the Python package manager:or your prefered package manager.Using pandas, we can get a quick breakdown of the count of values for each of the record classes in our data:Since the cabin, boat, body and home.dest records have a large number of missing records, we are simply going to drop them from our dataset.  We’re also going to drop the ticket field, since it has little predictive value.  For our predictor, we end up building a feature set with the passenger class (pclass), survival status (survived), sex, age, number of siblings/spouses (sibsp), number of parents/children aboard (parch),  fare and port of embarkation (“embarked”) records.  Even after removing the sparsely populated columns, there are still several rows missing data, so for simplicity, we will remove those passenger records from our dataset.The initial stage of cleaning the data is done using the following code:The final preprocessing we need to perform on our data is to encode categorical data using integer constants.  The pclass and survived columns are already encoded as integer constants, but the sex column records the string values male or female and the embarked column uses letter codes to represent each port.  The scikit package provides utilities in the preprocessing subpackage to perform the data encoding.The second stage of cleaning the data, transforming non-integer encoded categorical features,  is accomplished with the following code:Now that we have cleaned our data, we can compute the mean value for several of our feature columns grouped by passenger class (pclass) and sex.Notice the significant differences in the survival rate between men and women based on passenger class.  Our algorithm for building a decision tree will discover these statistical differences and use them to choose features to split on.Building a Decision TreeWe will use scikit-learn to build a decision tree classifier over our data.  We start by splitting our cleaned data into a training and a test set.  Using the following code, we split out the label column of our data (survived) from the feature set and reserve the last 20 records of our data for a test set.Once we have our training and test sets, we can create a decision tree with a maximum depth of 10.Our depth-10 decision tree is difficult to visualize in a blog post, so to visualize the structure of the decision tree, we created a second tree and limited the tree’s depth to 3.  The image below shows the structure of the decision tree, learned by the classifier:Loading the Redis PredictorThe Redis-ML module provides two commands for working with random forests: ML.FOREST.ADD to create a decision tree within the context of a forest and ML.FOREST.RUN to evaluate a data point using a random forest. The ML.FOREST commands have the following syntax:Each decision tree in Redis-ML must be loaded using a single ML.FOREST.ADD command.  The ML.FOREST.ADD command consists of a Redis key, followed by an integer tree id, followed by node specifications.  Node specifications consist of a path, a sequence of  .  (root), l and r, representing the path to the node in a tree.  Interior nodes are splitter or rule nodes and use either the NUMERIC or CATEGORIC keyword to specify the rule type, the attribute to test against and the value of threshold to split.  For NUMERIC nodes, the attribute is tested against the threshold and if it is less than or equal to it, the left path is taken; otherwise the right path is taken.  For CATEGORIC nodes, the test is equality.  Equal values take the left path and unequal values take the right path.The decision tree algorithm in scikit-learn treats categoric attributes as numeric, so when we represent the tree in Redis, we will only use NUMERIC node types.  To load the scikit tree into Redis, we will need to implement a routine that traverses the tree.  The following code performs a pre-order traversal of the scikit decision tree to generate a ML.FOREST.ADD command (since we only have a single tree, we generate a simple forest with only a single tree).Comparing ResultsWith the decision tree loaded into Redis, we can create two vectors to compare the predictions of Redis with the predictions from scikit-learn:To use the ML.FOREST.RUN command, we have to generate a feature vector consisting of a list of comma separated <feature>:<value> pairs.  The <feature> portion of the vector is a string feature name that must correspond to the feature names used in the ML.FOREST.ADD command.Comparing the r_pred and s_pred prediction values against the actual label values:Redis’ predictions are identical to those of the scikit-learn package, including the misclassification of test items 0 and 14.An passenger’s chance of survival was strongly correlated to class and gender, so there are several surprising cases of individuals with a high probability of survival who actually perished.  Investigating some of these outliers leads to fascinating stories from that fateful voyage.  There are many online resources that tell the stories of the Titanic passengers and crew, showing us the people behind the data.  I’d encourage you to investigate some of the misclassified people and learn their stories.In the next and final post we’ll tie everything together and wrap up this introduction to Redis-ML.  In the meantime, if you have any questions about this or previous posts, please connect with me (@tague) on Twitter."
395,https://redis.com/blog/introduction-redis-ml-part-four/,An Introduction to Redis-ML (Part Four),"August 28, 2017",Tague Griffith,"This post is part four of a series of posts introducing the Redis-ML module.  The first article in the series can be found here.In this post, we’re going to look at the matrix operations provided by the Redis-ML module and show some examples of how to process Matrix data using the Redis database.Technical RequirementsThe example code in this post is written in Python and requires a Redis instance running Redis-ML.  The instructions for setting up Redis can be found in either part one or part two of this series.Matrices in RedisMatrices are common in machine learning, statistics, finance and a host of other domains, so they were a natural addition to Redis.  The Redis-ML module adds matrices as a native Redis data type.  It also provides mathematical operations that combine matrices to create new values.Reading and writing matrix values is performed through the ML.MATRIX.SET and the ML.MATRIX.GET commands which have the following syntax:When working with the Redis-ML module, remember that commands use row-major format.  Multiplication and addition are supported by the ML.MATRIX.MULTIPLY and the ML.MATRIX.ADD commands.These commands combine two matrices that are already in Redis and store the result in a new key.If I wanted to compute a basic Matrix equation such as y = Ax + b using the Redis-ML module, I would enter the following commands into the Redis CLI:Redis returns the result to our client with the shape of the matrix (in this case 3 rows and 1 column) followed by each of the elements of the matrix in row-major order.I could also compute the Matrix equation A’ = cA (where c is a scalar value) using the following code:Matrices are used for a wide range of applications, from linear transforms to representing multivariate probability distributions.  In the next post, we’ll look at decision trees and random forests, two additional classification models supported by Redis.Please connect with me on twitter (@tague) if you have questions regarding this or previous posts in the series."
396,https://redis.com/blog/introduction-redis-ml-part-six/,An Introduction to Redis-ML (Part Six),"September 20, 2017",Tague Griffith,"This post is the final post in a series delving into the features of the Redis-ML module.  The first post in the series can be found here.In previous posts we learned how to use Redis and scikit-learn to build a real-time classification and regression engine, how to use linear regression to predict housing prices and how to use decision trees to predict survival rates.  We even took a small detour into R to demonstrate ML toolkit independence, but one question we haven’t focused on is, “Why?”  Why would we want to use Redis for a real-time predictive engine?If we look at the landscape of machine-learning toolkits, most focus on the learning side of ML, leaving the problem of building a predictive engine to the reader.  This is where Redis fills a gap; instead of trying to build a custom server, developers can rely on a familiar, full-featured data store to build their applications.  Simple and powerful data management, reduced overhead, and minimal latency are just three of the major advantages of building your machine-learning models with Redis.Data Management FunctionalityAlthough Redis-ML data types like linear regression object and random forests are very different from the built-in 4.0 types like sets and hashes, it’s important to understand that the Redis-ML keys are still Redis keys.  All of the Redis features for managing, persisting and replicating keys work equally well with Redis-ML keys.Redis already provides developers with a managed keyspace for storing data.  Additional statistical models can be added to an application with a simple SET command, allowing developers to maintain multiple versions of models for cases in which data needs to be reprocessed.  A Redis-ML key, like any Redis key, can be maintained using the Redis key management commands.To scale up a Redis-based predictive engine, you simply deploy more Redis nodes and create a replication topology with a single master node and multiple replica nodes.  Updates to your statistical models are written to the master node and automatically distributed to the replicas, so you don’t have to write any additional code (as you would with a custom application).Reduced Operational ComplexityRedis is already a part of most companies’ tech stack.  Your operations staff already understand how to scale, manage and monitor Redis instances–they may even have automated many deployment tasks.  So employing Redis for your ML needs requires considerably less overhead than adding operational support for a new, homegrown service that will take a while to implement and address the operational issues of.Fast ThroughputFinally, Redis maintains all data in memory, which makes it extremely fast.  It also has a highly tuned, optimized networking stack and sophisticated memory and buffer management,  all of which would need to be replicated in order for a home grown service to match the performance of Redis.  In benchmarks, we’ve seen Redis perform thirteen times faster than homegrown Java applications in predictive operations.Redis is a great way to accelerate the performance of your existing data pipelines and with the Redis-ML module you can speed up prediction operations.  Hopefully you’ve enjoyed this series on the Redis-ML module.  If you’re still curious and want to learn more, remember that the module is open-source software and you can find the source here on Github.  The Redis-ML team is actively soliciting contributions."
397,https://redis.com/blog/introduction-redis-ml-part-three/,An Introduction to Redis-ML (Part Three),"August 18, 2017",Tague Griffith,"This post is part three of a series of posts introducing the Redis-ML module.  The first article in the series can be found here.  The sample code for this post requires several Python libraries and a Redis instance running Redis-ML.  Detailed setup instructions to run the code can be found in either part one or part two of the series.Logistic RegressionLogistic regression is another linear model for building predictive models from observed data.  Unlike linear regression, which is used to predict a value, logistic regression is used to predict binary values (pass/fail, win/lose, healthy/sick).  This makes logistic regression a form of classification.  The basic logistic regression can be augmented to solve multiclass classification problems.The example above, taken from the Wikipedia article on Logistic Regression, shows a plot of the probability of passing an exam relative to the hours spent studying.  Logistic regression is a good technique for solving this problem because we are attempting to determine pass/fail, a binary selector.  If we wanted to determine a grade or percentage on the test, simple regression would be a better technique.To demonstrate logistic regression and how it can be used in conjunction with Redis, we will explore another classic data set, the Fisher Iris Plant Data Set.Data SetThe Fisher Iris database consists of 150 data points labeled with one of 3 different species of Iris: Iris setosa, Iris versicolor, and Iris Virginica.  Each data point consists of four attributes (features) of the plant.  Using logistic regression, we can use the attributes to classify an Iris into one of the three species.The Fisher Iris database is one of the data sets included in the Python scikit learn package.  To load the data set, use the following code:We can print out the data in a table and see that our data consists of sepal length, sepal width, petal length and petal width, all in centimeters.Our target classification is encoded as integer values 0, 1, and 2.  A 0 corresponds to Iris Setosa, a 1 corresponds to Iris Versicolor and a 2 corresponds to an Iris Virginica.To get a better sense of the relationship between various measurements and the flower type, we generated two plots: one of sepal width versus length and another of petal width versus length. Each graph shows the classification boundaries (determined through logistic regression) of the three classes and overlays it with the points from our data set.  Blue represents the area classified as Iris setosa, green represents Iris versicolor and grey represents Iris Virginica:We can see in both plots that there are a few outliers that get misclassified, but most of our Iris types cluster together in distinct groups.Performing a Logistic RegressionThe code to perform a logistic regression in scikit is similar to the code we used previously to perform a linear regression.  We first need to create our training and test sets, then we fit a logistic regression.To split the training and test sets, we use the following code:For this example, we split our data into blocks of 10 elements, put the first element into the test set and put the remaining 9 elements into the training set.  To ensure our data contains selections from all three classes, we’ll need to use a more involved process in this example than previous examples.Once we construct our training and test sets, fitting the logistic regression requires two lines of code:The final line of code uses our trained logistic regression to predict the Iris types of our test set.Redis PredictorAs with our linear regression example, we can build a logistic regression predictor using Redis.The Redis-ML module provides ML.LOGREG.SET and ML.LOGREG.PREDICT functions to create logistic regression keys.To add a logistic regression model to Redis, you need to use the ML.LOGREG.SET command to add the key to the database.  The  ML.LOGREG.SET command has the following form:and the ML.LOGREG.PREDICT function is used to evaluate the logistic regression from the feature values and has the form:The order of the feature values in the PREDICT command must correspond to the coefficients.  The result of the  PREDICT command is the probability that an observation belongs to a particular class.To use Redis to construct a multiclass classifier, we have to emulate the One vs. Rest procedure used for multiclass classification.  In the One vs. Rest procedure, multiple classifiers are created, each used to determine the probability of an observation being in a particular class.  The observation is then labeled with the class it is most likely to be a member of.For our three-class Iris problem, we will need to create three separate classifiers, each determining the probability of a data point being in that particular class.  The scikit LogisticRegression object defaults to One vs. Rest (ovr in the scikit API)  and fits the coefficients for three separate classifiers.To emulate this procedure in Redis, we first create three logistic regression keys corresponding to the coefficients fit by scikit:We emulate the One vs. Rest prediction procedure that takes place in the  LogisticRegression.predict function by iterating over our three keys and then taking the class with the highest probability.  The following code executes the One vs. Rest procedure over our test data and stores the resulting labels in a vector:We compare the final classifications by printing out the three result vectors:The output vectors show the actual Iris species (y_test) and the predictions made by scikit (y_pred) and Redis (r_pred).  Each vector stores the output as an ordered sequence of labels, encoded as integers.Redis and scikit made identical predictions, including the mislabeling of one Virginica as a Versicolor.You may not have a need for a highly-available, real-time Iris classifier, but by leveraging this classic data set you’ve learned how to use the Redis-ML module to implement a highly available, real-time classifier for your own data.In the next post, we’ll continue our examination of the features of the Redis-ML module by looking at the matrix operations supported by Redis-ML and how they can be used to solve ML problems.  Until then, if you have any questions regarding these posts, connect with the author on twitter (@tague)."
398,https://redis.com/blog/introduction-redis-ml-part-two/,An Introduction to Redis-ML (Part Two),"August 10, 2017",Tague Griffith,"This post is part two of a series of posts introducing the Redis-ML module.In the previous post of the series, we used the Python scikit-learn package and Redis to build a system that predicts the median house price in the Boston area.  Using linear regression, a powerful tool from statistics, we constructed a pricing model that predicted the median home price for a neighborhood using the average number of rooms in a house.At the end of the post we said that the next post would cover classification, a machine learning process of identifying the category that something belongs to based on prior examples of items in the category. But rather than move on to that topic prematurely, this post is going to tie up some loose ends on linear regression.The sample code in this post requires the same Redis modules and Python packages as the code in the previous post.  If you already set up an environment, you can skip the section on technical requirements.  Any additional requirements will be covered as the sample code is introduced.Redis RequirementsThe sample code in this post requires Redis 4.0 or later with the Redis-ML module loaded.  To run the samples in this post, start Redis and load the Redis-ML module using the loadmodule directive.You can verify that the Redis-ML module was loaded by running the MODULE LIST command from the Redis command line interface (redis-cli) and ensuring that redis-ml appears in the list of loaded modules:Conversely, you can run the Redis-ML Docker container provided by Shay Nativ, which is preconfigured with the dependencies you will need.  To deploy the redis-ml container locally, launch it with the following command:The Docker run command will automatically download the container if necessary and start up a Redis 4.0 server listening on port 6379.  The -p option maps the port 6379 on the host machine to port 6379 in the container, so take the appropriate security precautions for your environment.The sample code in this post has the same Redis requirements as the sample code in Part One, so if you set up an environment to experiment with the code in that post, you can continue to use it for this and subsequent posts.Python RequirementsTo run the sample Python code in this post, you will need to install the following packages:You can install all of the packages using pip3, or your preferred package manager.Linear Regression ModelsLinear regression represents the relationship between two variables as a line with the standard equation  y=b + ax.  The line’s parameters, the slope and the intercept, differ by dataset, but the structure of the model is the same.Any toolkit capable of performing a linear regression on a data set to discover the parameters of the line can be used in conjunction with Redis.  To illustrate the independence of the model, we are going to rebuild our housing price predictor using R, a statistical language and environment used to analyze data and perform experiments.  R provides a variety of features for data analysis, including a package for performing linear regression.  R is usually used as an interactive data exploration tool rather than a batch processing system.The following code was tested using version 3.4.1 of the R.app on mac OS.  If you are interested in running the code, you can download the environment from the R Project website.  The code depends on the MASS package; if your version of R doesn’t include the MASS package, download it with R’s built-in package manager.The housing price predictor built in the previous post used Python to run a linear regression over sample data using the scikit-learn package.  Once the linear regression process discovered the best line to represent the relationship between the room count and the median neighborhood house price, we re-created the line in Redis and used Redis to predict unknown housing prices from observed features.In this post we are going to demonstrate how different toolkits can be used in conjunction with Redis, by reimplementing our linear regression program in R.  The following R code will replicate the linear regression performed by our Python code in the previous post:Even if you have never worked with R before, the code should be easy to follow.  The first part of the program loads our data.  The Boston Housing Dataset is a classic data set used in machine learning instruction.  R, like many statistical toolkits, include several classic data sets.  In the case of R, the Boston Housing Dataset is included in the MASS package, so we must load the MASS package before we can access the data.The second section of our code splits the housing data into a training set and a test set.  Following the methodology used in the Python version, we create a training set from the first 400 data points and reserve the remaining 106 data points for the test set.  Since R stores our data as a data frame and not in an array (as scikit does), we can skip the slicing and column extraction we performed in Python that made our room data column more accessible.The linear regression is performed in R using the built-in linear regression function (lm).The first parameter to lm is used to describe the predictor and predicted values in the data frame.  For this problem, we use medv ~ rm as we are predicting the median housing price data from the average rooms data.In the final step of the script, the summary method is used to display the coefficients of the fit line to the user.Plotting our results and comparing them with the results from the previous code:The summary method shows the coefficients determined by the linear regression procedure as:Like the scikit-learn package, R determined the slope of our line to be 9.4055 and the intercept to be -35.2609.  Our R code provides identical results to scikit up to the fourth decimal place.From this point on, the procedure for setting up Redis to predict housing prices is the same as before. First, create a linear regression key using the ML.LINREG.SET command:And once the key is created, use the ML.LINREG.PREDICT command to predict housing values:Once we round the results to four decimal places, we still get an estimated median house price of $23,053 (remember our housing prices are in thousands) for this particular neighborhood.This may be the first and only time you run R, but the important thing to take away is that the model–not the toolkit–is what is important when using Redis to serve your models.The other important topic in linear regression that we didn’t cover in our last post is multiple linear regression.Multiple Linear RegressionIn all the examples we have shown so far, we’ve used a single variable to predict a value.  Our housing predictor used only the average room size to predict the median housing value, but the data sets have multiple data values (referred to as “features”) associated with a particular median home price.Linear regression is often performed with multiple variables being used to predict a single value.  This gives us a model that looks like y=0+1x1+ … + nxn.  In a multiple linear regression we need to solve for an intercept and coefficients for each variable.The following code implements multiple linear regression, trying to fit a prediction line using all of the available data columns from our Boston Housing Data:The table below shows the coefficient scikit determined for the best predictor line.  Each coefficient corresponds to a particular variable (feature).  For instance, in our multiple linear regression, the constant for average rooms is now 4.887730, instead of the 9.4055 that resulted when only considering the room data.From there, we create a Redis key boston_house_price:full to store our represented multiple linear regression.  Keep in mind that Redis doesn’t use named parameters for the arguments to the ML.LINREG commands.  The order of the coefficients in the ML.LINREG.SET must match the order of the variable values in the ML.LINREG.PREDICT call.  As an example, using the linear regression code above, we would need to use the following order of parameters to our ML.LINREG.PREDICT call:This post demonstrated toolkit independence and explained multiple linear regression, to round out our first post on linear regression.  We revisited our housing price predictor and “rebuilt” it using R instead of Python to implement the linear regression phase.R applied the same mathematical models to the housing problem as scikit and learned the same parameters for the line.  From there, it was a nearly identical procedure to set up the key in Redis and deploy Redis as a prediction system.In the next post, we’ll look at logistic regression and how Redis can be used to build a robust classification engine.  We promise.  In the meantime, connect with the author on twitter (@tague) if you have questions regarding this or previous posts."
399,https://redis.com/blog/introduction-redis-ml/,An Introduction to Redis-ML.  Part One,"July 24, 2017",Tague Griffith,"Despite widespread interest in machine learning (ML), using it effectively in a real-time environment is a complex problem that hasn’t been given enough attention by framework developers. Nearly every language has a framework to implement the “learning” part of machine learning, but very few frameworks support the “predict” side of machine learning.Once you’ve trained an ML model, how can you build a real-time application based on that model? With many toolkits, you have to build your own application. We are just starting to see frameworks focused on the prediction side of machine learning.An earlier post provided an overview of the features available in the Redis-ML module, so in this post, we’re going to dive deeper into machine learning – explain some of the techniques in an accessible way and show how you can augment a machine learning pipeline with Redis. As an example, we will walk through the code for a sample program for predicting median housing prices from various features of a neighborhood.The sample code in this post is written in Python 3 using a variety of freely available packages for machine learning. You will need to install the following packages, using pip3 or your preferred package manager, to run the samples:You will also need a Redis 4.0.0 instance and the Redis-ML module. Shay Nativ, the developer behind the Redis-ML module, created a Docker container with Redis 4.0.0 and the Redis-ML module preloaded. To use that container in conjunction with the code in this post, launch the container using the command:Docker will automatically download and run the container, mapping the default Redis port (6379) from the container to your computer.To build our housing price predictor, we will use a machine learning technique known as linear regression.Linear regression was part of the statistician’s toolbox long before algorithmic machine learning was invented. With linear regression, we attempt to predict a result (sometimes called the dependent value) from one or more known quantities (explanatory variables). For linear regression to work, we must be able to accurately estimate our results with a straight line.In the graph above, taken from the Wikipedia article on Linear Regression, we can see how our data points cluster around an idealized line. This data set is a good candidate for linear regression. In practice, linear regression is used to model a variety of real-world problems where a linear relationship from observations can accurately predict a result, such as the price of a house based on square footage, or college GPA from high school GPA and SAT scores.From algebra we know that a line is represented by an equation of the form y = b + ax, so to “learn” a model of this form, we need to apply an algorithm to discover the parameters of the line – the slope and intercept. Nothing incredibly fancy, in fact prior to algorithmic machine learning, most statistician would “fit” these models by hand. These days, it’s far more common to use a computer to find the line’s parameters and a variety of tool kits (TensorFlow, Scikit, Apache Spark) are available to solve a linear regression problem. The important thing to remember is that once we’ve learned a linear regression model, we have a mathematical formula for predicting results that could be implemented by any system.Let’s work through an example of performing a linear regression and discovering the model parameters using the popular Python scikit-learn package and the Boston Housing dataset.The Boston Housing Dataset is a classic data set used in teaching statistics and machine learning. The dataset predicts the median housing price for a neighborhood in the Boston area using neighborhood features like the average number of rooms in a house, the distance from main Boston employment centers, or crime rate. To make it easier to visualize the linear regression process, we’re going to work with a single feature of the data, the average rooms per dwelling (RM) column.The Boston Housing Dataset is provided as part of the scikit-learning package, so, let’s start by plotting our data to visualize the relationship between room count (RM) and median price (MEDV):While not a perfect line, we can see a pretty strong linear relationship between the average number of rooms and the median house price in a neighborhood. We can even draw an idealized representation of the relationship and see how the data points cluster around it.The following code demonstrates how to load the Boston Housing dataset using scikit. The Boston Housing dataset consists of twelve different features used to predict housing prices, so after loading the dataset, we extract data from the fifth column (the RM column) from the data for our sample.Now we split our data into two sets, a training set and a test set. For our example, we create our training set from the first 400 samples and the test set from the remaining 106 samples.This method of splitting ensure ensures we always run with the same sets for reproducible results.Now that we’ve constructed our training and test sets, we can use the LinearRegression model supplied by scikit to fit a line to our data:After running our code, we find that scikit has fit a line to our data with a coefficient of 9.40550212 and an intercept of -35.26094818316348.Now that we have these parameters, we can implement a linear model to predict housing prices in the Boston area based on the average number of rooms in a house in a neighborhood of interest. Now that I have this model, how can I build an application to make real-time predictions and use the functionality an app or a website?The Scikit package provides a predict function to evaluate a trained model, but using a function within an application requires implementing a host of other services to make it fast and reliable. This is where Redis can augment your machine learning systems.The Redis-ML module takes advantage of the new Modules API to add a standard linear regression as a native datatype. The module can create linear regressions as well as use them to predict values.To add a linear regression to Redis, you need to use the ML.LINREG.SET command to add a linear regression to the database. The ML.LINGREG.SET command has the following form:By convention, all of the commands in the Redis-ML module begin with the module’s identifier, ML. All linear regression commands are prefixed with LINGREG.To setup Redis to be a predictive engine for Boston housing prices using the line we fit in scikit, we need to first load the Redis-ML module using the loadmodule directive.Then we set a key to represent our linear regression using the constants from scikit by executing the ML.LINGREG.SET command. Remember that the intercept is the first value supplied and the coefficients are provided in feature order. From our scikit code to fit a regression line to the housing data, we determined our line had a coefficient of 9.40550212 for the RM variable and an intercept of -35.26094818316348. We can use the ML.LINGREG.SET command to set a Redis key to compute this linear relationship:Once our boston_house_price:rm-only key is created, we can repeatedly predict the median house price in a neighborhood by using the ML.LINGREG.PREDICT command. To predict the median house price in a neighborhood that averages 6.2 rooms per house we would run the command:Redis predicts a median house price of $23,053 (remember our housing prices are in thousands) for this neighborhood.It’s helpful to understand how to work with the ML.LINREG commands from the redis-cli, but it is far more likely we would be doing this from an application. We can extend our Python code which fits the regression line to automatically create the boston_house_price:rm-only key in Redis. Once we’ve created the key in Redis, we implement a test that generates predictions from Redis using our test data.We can also generate scikit’s predictions for the same set of data using the predict routine:For comparison, we’ve plotted the results. In the graph below, the black circles represent the actual prices for the test data in our dataset. The blue markers (+) represent the values predicted by Scikit and the magenta markers (x) represent the values predicted by Redis.As you can see the Redis and scikit make the same predictions for median house price given the average number of rooms. While linear regression may not correctly predict the exact price of every data point, it provides a useful means of estimating an unknown price based on some observable features of a neighborhood.In this post, we dug deeper into the linear regression feature of the Redis-ML. We looked at how to use the popular scikit Python package to fit a linear regression line to some housing data and then create a housing price prediction engine using Redis 4.0.0 and the Redis-ML module.In the next part of the series we will look at how Redis-ML can be used to implement an engine for classification, another kind of machine learning problem that attempts to determine the class of unknown data from previous examples."
400,https://redis.com/blog/join-us-at-redis-day-london-2019/,Join Us at Redis Day London 2019,"November 5, 2019",Paul Bushell,"The first Redis Day London was so much fun that we’re coming back to do it all over again. Join us on Tuesday, 12 November, for Redis Day London 2019—it’s your chance to enjoy speakers, networking, and informed discussion about everything and anything Redis.If you’re a Redis-oriented developer, engineer, software architect, programmer, or business professional, you won’t want to miss this free one-day event. (Want to get even more out of the event? Attend our special pre-conference training day on Monday, 11 November.)Register for Redis Day London 2019 now!At Redis Day London 2019 you’ll hear about new Redis features—and see how Redis streams and the latest Redis modules are already being used in development and production.  You’ll also hear from fellow Redis users about best practices and uses of Redis, and get a sneak peak of the future of Redis and what’s coming in the next big release.On Tuesday, 12 November, some 19 Redis experts will grace the stage at the Park Plaza London Riverbank Hotel (note the new venue!). The headliner, as always, will be Salvatore Sanfilippo, the creator of Redis. Salvatore will kick off the start of Redis Day with a discussion of Redis 6.0, followed by Yiftach Shoolman, CTO and Co-Founder of Redis, who will give an update on Redis Enterprise.We’re also excited to hear from Elena Kolevska, who recently left her position as CTO of culture app InvisibleCity to become a Technical Enablement Architect at Redis, who will talk about using Bloom Filters in Redis; and Parin Turakhia, Senior Engineering Manager at BookMyShow, who’ll share how the Indian entertainment company uses Redis to book more than a million tickets per day. Plus, Konfio.mx Senior Architect Pipe Gutierrez will introduce a Redis module for cancellable timeout queues.In the afternoon, Alfred Biehler, Customer Engineer at Google Cloud, will speak about how Google uses Redis, and Mugunthan Soundararajan will share how Matrimony.com uses Redis Streams. You also don’t want to miss the launch of RedisInsight and Ogma Graph Visualization, and hear how the engineers at Play Games 24×7 built a robust game engine service with a high level of concurrency. And that’s only a fraction of the speaker lineup.We’ll serve a complimentary breakfast and lunch, and you’ll want to stick around for the happy hour/reception at the end of the day to network with fellow Redis users. Oh yeah, we’ll also be handing out plenty of cool Redis swag, including t-shirts, laptop stickers, and more.If you’re new to Redis—or just want to dive deeper or brush up on your skills—come to the Park Plaza London Riverbank Hotel on Monday, 11 November for a day of hands-on training. Kyle Davis and Loris Cro of Redis will host a full day of sessions on getting started, Redis streaming architectures, Redis clustering, RediSearch, and more.Where: Park Plaza London Riverbank Hotel, 18 Albert Embankment, London, SE1 7TJ (Note location change—this event is no longer being held at CodeNode.)When: Tuesday, 12 November, 2019 8:30 a.m. – 6 p.m. (Training day is Monday, 11 November 2019, 9:30 a.m. – 5 p.m.)This is just a taste of what’s on tap at Redis Day London 2019—get more information and check out the full agenda and complete list of speakers here.Register now before we sell out!Photo credit: Heidi Sandstrom, Unsplash"
401,https://redis.com/blog/looking-back-redisconf-2017/,Looking back at RedisConf 2017,"June 14, 2017",Tague Griffith,"In the last week of May, over 900 Redis enthusiasts gathered in downtown San Francisco for two days at the Third annual RedisConf.  Featuring over 50 speakers and 42 sessions, RedisConf is the premiere event for developers to learn from the experts and share their own experiences with the Redis community.Training DayOne of the biggest additions to this year’s conference was an entire day of tutorials preceding the  conference.  A host of Redis developers, from curious beginners to Redis experts joined us for a full-day of hands on training in Redis development.  The tutorials covered a range of topics from basic Redis data structures through the latest developments in Redis modules.The highlight of training day was an appearance by Salvatore, who volunteered his time to teach attendees about advanced Redis data structures, transactions, and pipelining.Day OneThe first day of RedisConf17 kicked off with a warm welcome from Redis’ CEO, Ofer Bengal.  Following Ofer, Salvatore Sanfilippo, creator of Redis took the stage to provide an overview of the current state of the Redis 4.0 release and to talk about the design of the upcoming Redis Streams data type.  The general session included keynotes from Joshua McKenty of Pivotal Labs, Kelsey Hightower of Google.  Kelsey blew the audience away with a live deployment of a Redis cluster using Kubernetes via voice command.  Salil Deshpande of Bain Capital and Sam Ramji of Google closed out the general session with a fire-side chat about open source and cloud infrastructure.Salvatore returned to the stage later in the day with a deep technical breakout session Rax, Listpack and Safe Contexts covering recent improvements to Redis internals.  Other day one sessions covered a wide range of interests – DevOps best practices, Redis cluster,  scaling, and the new reJSON module.  In a second, unexpected fire-side chat, one of the presenters described the small desk fire that he started when building a home brew digital temperature sensor running Redis.After a full schedule of keynotes and sessions, The Spazmatics closed out the first day of the conference playing 80’s hits.Day TwoOfer Bengal and Yiftach Schoolman kicked off the day two general session with a joint talk on doing more with Redis.  Reynold Xin from Databricks demonstrated how to process large datasets using Apache Spark, a technology that integrates well with Redis, and Chris Richardson of Eventuate.io explained his pattern language for designing microservices.  Charity Majors, of Honeycomb.io, closed out the general session with a keynote on the limitations of monitoring.Following the keynote, EMACS user and future Redis module contributor, Tom Middleditch entertained attendees with jokes, stories and advice from his experience running Pied Piper on ‘Silicon Valley’, the hit HBO show.Many of the keynote themes were echoed in sessions throughout the day.  Redis employees expanded on Do More with Redis in presentations covering real-time machine learning, Multi-Master Redis, and the Redis Graph module.  The patterns for developing microservices introduced in the keynote, made an encore appearance in sessions covering containerization, Kubernetes, Mesosphere, and microservices development.The day ended with lucky conference attendees winning new RED iPhones and gift cards.  The luckiest attendees boarded a bus to see Game 1 of the Golden State Warriors – Cavaliers NBA finals.Looking ahead to RedisConf18Even though we just finished this year’s conference, the Redis team has started planning RedisConf18.  The growth of the conference over the past 3 years has been astounding and next year we want to bring you another awesome event.We’re already working on the details for RedisConf18; the date and location of next year’s conference will be announced towards the end of summer.  Keep an eye on redisconf.com or sign up for our RedisConf 2018 mailing list to get the latest updates.We want to thank all of our sponsors, speakers and attendees for contributing to a fantastic conference; we look forward to seeing you all back in San Francisco next year for RedisConf18.The RedisConf Team"
402,https://redis.com/blog/machine-learning-steroids-new-redis-ml-module/,Machine Learning on Steroids with the New Redis-ML Module,"November 9, 2016",Cihan B,"Redis’ in-memory architecture is well known. When you need “fast at high throughput,” Redis is the obvious choice.Redis is also well known as a “structure store.” While other databases model data in tables/columns, documents or key/value pairs, Redis accommodates all of these structures at once. You can combine tables, documents, key/value pairs and other data types into a single database and assign each entity the data type that best suits it. If you “lift the hood” and look into the other available engines, you will see that documents you save in the database are typically shredded and stored in some underlying native data type that the system understands. Redis has no such abstractions! Each data type is stored natively in the engine, and come with its own “verbs” to perform the actions natively on the data type. With modules (introduced in May 2016), you can now embed your own complex data types in Redis without sacrificing performance.Here at Redis, we have been working with modules for a while. A month ago, we unveiled the RediSearch module, which delivers the fastest search engine by combining the inverted full-text index with an in-memory architecture. This week, we unveiled another ground-breaking module: Redis-ML, which works with the popular Apache Spark MLlib and other machine learning libraries.With Spark + Redis-ML, you can save Spark-generated ML models directly in Redis and generate predictions for interactive, real-time applications. We developed this approach with the Databricks folks because we were impressed by Spark’s ability to train the ML models. However when it comes to generating predictions in real-time, Redis is the engine! With Redis’ Enterprise Cluster you can:The typical machine learning flow for Apache Spark is shown below:Spark trains the model using your historical data. The model is typically saved to disk and loaded later for generating predictions. However Spark is not particularly ideal for end user applications.By combining the strengths of both Spark and Redis, you can greatly simplify the flow: (1) after using Spark for training, you save the ML models directly in Redis using Redis-ML, then (2) y use the Redis-ML “verbs” to generate predictions in real time for interactive applications.Redis’ team ran a benchmark with about 1000 models, each with about 15,000 trees generated by Spark ML using the Random Forest algorithm. . The usage of Redis ML with Spark ML reduces model classification times by a factor of 13X, without using any additional memory and while reducing model save and load times!There are a few reasons why Redis-ML can do this. Redis’ in-memory architecture is one of the reasons. Redis is built for real-time, interactive applications. Modules like Redis-ML also avoid layers of abstraction that other databases are encumbered by. ML structures are served with their native “verbs” directly from memory, without going through additional layers of translation.As machine learning is becoming a key component of user experience in most next-generation applications (including recommendation engines, fraud detection, risk evaluation, demand forecasting, sentiment analyses, robotics and self driving cars), you will need a machine-learning-capable database that can deliver these models reliably to your applications. Redis-ML will be a critical component in putting your ML models into production.For more details:"
403,https://redis.com/blog/november-redis-events/,November Redis Events,"November 3, 2017",Tague Griffith,"Fall has finally hit the Bay Area. There’s a definite chill in the late afternoon when Karl starts rolling over the hills. Even with the holidays fast approaching, the team here at Redis will be at a bunch of different events all over the US and Europe.ConferencesAs part of a whirlwind tour of London, Redis will be a part of Big Data London on Nov. 15th and 16th. Our CMO, Manish Gupta and our Head of Developer Advocacy, Tague Griffith will be speaking at the conference. Stateside, you can catch our team at QCon in San Francisco Nov. 13th through the 15th, Spark Live: Chicago on the 16th, and AWS:reInvent 2017 at the end of the month in Las Vegas. We will also be speaking at a couple of different camps in this year’s OpenCamps event in Midtown Manhattan.TrainingThere are two training events you can take advantage of this month. We will be hosting two Redis Enterprise Developer Workshops. These free workshops explain the architecture of Redis Enterprise, demonstrate how to use several Redis data structures, and show how to manage concurrency with transactions. For those of you in the San Francisco Bay area, RSVP for our Sunnyvale event at the Plug and Play Tech Center on Thursday, November 9th. Londoners can RSVP for our London event at CodeNode on Monday, November 13th. Pre-registration is required and space is limited, so sign up today.Community EventsDue to the holidays, many of our meetups are taking this month off. Both the San Francisco and Seattle Redis meetups are taking a break this month, but the Silicon Valley Redis Meetup will have its regular meeting on Wednesday, November 8th at Hacker Dojo in Sunnyvale. Roland Lee of Heimdall Data will be talking about the performance benefits of using Redis for SQL auto-caching.After a hiatus, the London NoSQL meetup is holding an event on Tuesday, November 14th to learn more about the Redis 4.0 release and the upcoming Redis Enterprise 5.0 release. We are also hosting a joint meetup with the Los Angeles Cloud Foundry group at the Pivotal offices in Santa Monica on Tuesday, November 7th.Whether you just stop by one of our conference booths or you join us for a whole day of training, we hope to see you this month!Photo courtesy of Bhautik Joshi."
404,https://redis.com/blog/query-caching-redis/,Query Caching with Redis,"April 30, 2019",Redis,"Find out if your cache is enterprise-grade, and learn how to: Scale globally while maintaining low latency and cache more efficiently to lower costsWhen I started out building websites, I didn’t focus much on performance. Performance is a luxury you worry about once you’ve mastered the basics, like HTML and CSS or whatever programming language you’re using on the backend. A beginner’s goal is to stand a site up, be able to get from page-to-page and make sure it looks good on a bunch of devices. Caching and performance are things we figure out later.There are many great reasons to look into caching solutions. From an SEO perspective, Google penalizes slow websites. If your site has a long load time, your ranking could be impacted, which of course could have a serious revenue impact. John Mueller of Google says that page load times over two seconds “impact crawling” of your site and that he looks for “[less than] 2-3 seconds.” Another factor is the usability impact of slow loading sites. When I started coding, around eight seconds was acceptable to completely load a page. Only if it took longer than that, did you worry about people abandoning the page. Today that time is probably around two seconds and may be even less depending on your industry.The bottom line: the faster your site, the better it ranks and the more visitors will stay and interact. But still, this issue didn’t come to a head for me until my client called and said their new site was “too slow.” To be fair, it had some heavy images on it and they were using a terrible hosting company. This was also back in the day when $10 hosting meant your site was on a shared server. Their idea of server optimization was to install the latest version of CPanel (yikes). After some digging, I built a basic page caching system and it worked great. The client was happy, I started learning a new skill, and things were good. But soon I realized that although page-level caching was a great tool, it was only one tool.Page caching works like this: a request comes in, the server processes it and then stores the resulting HTML. For the purposes of this blog post, let’s use an example with Redis. The next time someone requests our page, it will check the cache first. If the page is there, the system will use that instead of processing the request at the server level.This scenario works well for pages that don’t change too much, e.g. the terms and conditions or privacy policy. These pages aren’t trafficked the same as the main application, which probably has user-based dynamic data.Let’s imagine we’ve got an application that acts as a user directory and allows filtering based on some sort of activity. Users can view a list of people and see their email address and phone number. Our app uses a SQL database, so to get that data out we need a simple select query:SELECT username, email, phone_number FROM users WHERE activity='baseball';In our app, let’s say that activity is a list users set up when they sign up. So, the types of activities people can see vary by user. We’ll store the activity list in another table and would require that information on this page too.SELECT name, id FROM activity_list WHERE user_id=1;In this situation, we’ve got a query that needs to run against the current user. The data this user will see is a subset of the whole list that’s unique to them. Given this, we can’t cache the page, so we need to cache each individual query instead.Now we need to think about when to cache the queries and how. One easy solution is to use Redis Hashes to store our result set. We can store the data as a JSON-encoded string, and when we’re ready, just pull that out and use it in our codebase.So what does caching look like? Here’s some pseudo-code:1. > HGET user-activity-list cache
2. if the result is not nil return the result set else go to step 3
3. run the query and save the DB result set to a variable
4. if the DB result size is > 0
5. transform the result set data to a JSON string
6. > HSET user-activity-list cache JSON string result set
7. else result set <= 0 throw errorIn regards to the above pseudo-code, I should point out that of course we’re ignoring the fact that this is actually a session store pattern. In the real world, you’re not going to want to just cache these queries forever in Redis, you’d create and store them only while the user is logged into and active on your site. I’ve created this generic user-activity-list key in Redis only as an example to demonstrate the theory and spark ideas for how you could do this in your own code.Along with that, our key name isn’t the best. The name user-activity-list is again just for this very generic and very narrow example. For an actual application, you’d want to name your keys in a more predictable way. If I was doing this for real, the key would have the username in it somewhere and this would all be part of the user session so it’d be easy to retrieve and use.Moving on, there are a couple of problems we should address. First, we’re getting the user-activity-list, but after we run this query once, we should probably expire the query so we don’t risk showing our user some rather stale data. There are a couple of different ways we could solve this issue.Assume this list of activities is something the user herself updates. She would go to the settings page, fiddle with some stuff, and save the change. She may only do this once every week or once a month. In this case, we could keep the cache around until the user changes something:1. user adds ""sportsball"" their list and saves
2. in response to the user save we UNLINK user-activity-listYou could do the first and second part any way you like. For this system, I’m pretending we have a way to register/push events. That part doesn’t matter much, but what does matter is that our system deletes the cache every time we update. We don’t need to worry about re-caching the data, since that’s the job of the original function.Expire on write may not work for every scenario. There are cases where we’ll want to cache the query for a set amount of time only. For these, we can use Redis to expire our keys.Let’s take our first caching solution and see what that looks like with a Redis EXPIRE in it:1. > HGET user-activity-list cache
2. if the result is not nil return the cached value else go to step 3
3. run the query and save DB result set to a variable
4. if the DB results size is > 0
5. transform the result set data to a JSON string
HSET user-activity-list cache JSON string results
EXPIRE user-activity-list 100The trick here is in step five. After setting the key, we use Redis to set up an expire time (in seconds) for that key. Redis will delete the key for us so we don’t have to worry about managing that in our code.In situations where page level caching doesn’t work or won’t be effective, caching database queries is a great alternative. We can use Redis to set and get a hash value with saved query values, and return those values instead of hitting the database. This would speed up our site incredibly. Think about this: the standard time to access and return information from a database to a user is 100 milliseconds, while the average time from Redis is only two milliseconds. That’s a massive performance gain.In our fictional app, if users hit this activity page 1000 times an hour and we have to hit the database each time, that’s going to add up (and this is for only one query). Imagine if this page has 2-5 queries? What if those queries required complex joins with multiple tables? Three synchronous queries on a page could easily take upwards of 300 milliseconds just to return data from the database. In a world where we only have 2-3 seconds to capture our user’s attention, why put ourselves at that disadvantage?Now, imagine that we can retrieve the data from our cache 90% of the time.Caching queries in Redis could turn that 300 milliseconds into just six milliseconds on a single page. Across the entire site, this drops time spent grabbing data from many minutes in an hour to just a few seconds. That’s a performance boost that should make for very happy clients.In our fictional app, if users hit this activity page 1000 times an hour and we have to hit the database each time, that’s going to add up (and this is for only one query). Imagine if this page has 2-5 queries? What if those queries required complex joins with multiple tables? Three synchronous queries on a page could easily take upwards of 300 milliseconds just to return data from the database. In a world where we only have 2-3 seconds to capture our user’s attention, why put ourselves at that disadvantage?Now, imagine that we can retrieve the data from our cache 90% of the time.Caching queries in Redis could turn that 300 milliseconds into just six milliseconds on a single page. Across the entire site, this drops time spent grabbing data from many minutes in an hour to just a few seconds. That’s a performance boost that should make for very happy clients."
405,https://redis.com/blog/redis-4-0-availability-redis-enterprise/,Redis 4.0 Now Available on Redis Enterprise,"September 26, 2017",Aviad Abutbul,"The much-awaited Redis version 4.0 is here! Here at Redis we make an ongoing effort to ensure that our customers can enjoy all the latest and greatest features of Redis.We are excited to announce that our latest version of Redis Enterprise, our core engine that powers Redise Cloud and Redise Pack products, now supports the latest release of Redis – version 4.0.Redis 4.0 release adds several significant enhancements including:You can learn more about the release in this blog post and if you really want the full blown list you can always visit the release notes.Support for Redis 4.0 is available on Redise Cloud. You can start exploring the new version immediately using our 30MB free subscription, just sign up here and you are good to go.You can also experience Redis 4.0 and new certified modules with the preview of Redise Pack 5.0. Redise Pack 5.0 delivers rich functionality combined with the new capabilities in Redis 4.0 engine. Here it is a high level:You can find detailed information on Redise Pack version 5.0 preview program here and how you can signup for the program."
406,https://redis.com/blog/redis-caching-assessment-tool/,Can Your Cache Power Modern Applications? Find Out With Redis’ New Caching Assessment,"September 16, 2022",John Noonan and Ryan Powers,"Introducing the Redis Caching Assessment, a fast and simple tool that quickly analyzes the state of your cache and delivers actionable solutions in less than 5 minutes. We explain how we built this exciting new framework and how it can help you get started with boosting your cache’s performance to build modern applications.Try the Redis Caching Assessment toolFor decades, databases have been working behind the scenes to power businesses and their customer-facing applications. However, there’s been a shift, and the old world’s systems of records are no longer the prevailing design pattern for modern applications. As pointed out in a tweet by David McJannet (CEO of HashiCorp), modern applications are now, by default, ‘systems of engagement’ that require caching.We couldn’t agree more. Modern applications require an enterprise-grade cache to power them to meet today’s real-time consumer demands.This is not to say the system of records will go away; they will always be there to store your critical business data. Caching takes the data stored in a database on your server’s hard disk and moves it to a temporary place where it can be accessed far more quickly and efficiently by users as they interact with your applications. As a result, the complex and time-consuming operation of acquiring data only needs to be performed once. This allows the data to fuel systems of engagement for your customers or employees at the frontlines of your business.If caching is a strategic requirement for all modern applications, it’s critical to employ a cache that can meet today’s needs.Does your cache help you effectively meet the needs of modern applications? Our new Redis Caching Assessment was built to help you answer this critical question.Today’s modern software development teams are leveraging cloud-native technologies, distributed architectures, and DevOps practices, all with the goal of creating flexible and highly performant modern applications that drive consumer growth (whether internal by employees or customers). Let’s step through the most critical application criteria based on our customer deployments.Real-time performance: Modern businesses operate in a digital-first world. The user experience of your applications has never been more important and should always be a strategic priority. Speed is one of the most critical components of the user experience. Your customers expect applications that respond instantly, at all times, or they will abandon your applications and potentially your business entirely.Flexibility: Building responsive applications that meet customer needs should be simple and streamlined. Teams shouldn’t be operating complex arrays of disparate niche data services across multiple deployment environments. A piecemeal approach inevitably leads to degraded performance, data consistency issues, and operational headaches.Resilience: Because so much business is conducted digitally, organizations must ensure that their applications and data are highly resilient. Even small or infrequent cache outages can have a devastating impact on application performance, resulting in damage to brand reputation, loss of customers and vital business data, and the opportunity cost of spending resources troubleshooting application outages.Cost-efficiency: Building responsive modern applications opens new areas of opportunity for businesses. It’s why companies continue to pursue digital transformation, cloud migration, and application modernization. However, for these initiatives to be worth it, the cost of these new applications can’t be greater than the value they produce. What good is an investment in a new AI/ML application if the costs to build and operate it are greater than the added revenue it brings in?Understanding these modern application requirements and how an enterprise cache can accomplish them is critical. Luckily, there is no shortage of material that can help you leverage caching for your own personal needs (Caching at Scale with Redis, Caching for Microservices, Caching for Hybrid Cloud). But you need a plan to help you get started.That’s exactly why we built the Redis Caching Assessment tool. Many businesses believe they are caching efficiently without realizing there is significant room for improvement, while others know they have issues with their cache but are unsure how to solve them.We established the Redis Caching Assessment framework by leveraging data from 8,900 of our caching customers. In analyzing this data, we asked, “What obstacles do most businesses face when caching at enterprise scale? What have been the most common solutions?” What we learned is now the basis of the Redis Caching Assessment Tool, created to help you understand your current state and the logical next steps to prime your cache to meet modern application requirements.To provide an assessment and prescriptive approach to enhance your cache, we created an assessment that is only 12 questions long and takes less than 5 minutes to complete. Once submitted, we analyze your answers and not only tell you the current stage of your cache but create a personalized report with details on each question, its answers, and actionable suggestions.Together, the framework and self-assessment tool will help you quickly improve your caching strategies to better equip your teams with real-time enterprise-grade data and your customers with an instant experience.Can your cache power today’s modern applications? Find out now."
407,https://redis.com/blog/redis-cell-rate-limiting-redis-module/,redis-cell: a Rate Limiting Redis Module,"January 24, 2017",Itamar Haber,"It is my absolute pleasure to present you with this guest post by Brandur Leach, redis-cell‘s author and winner of the 1st place in the Redis Modules Hackathon. The module is an efficient implementation of a rate limiter that can be used, for example, to protect from activity spikes. In this post, Brandur explains why the module came into being, how it works and the reasons behind his choice of programming language (spoiler: Rust).When I first noticed the Redis module hackathon and started to brainstorm project ideas, I settled on one quite quickly. I’ve been in industry long enough to have seen Redis put to a huge variety of different uses–indeed it’s the Swiss Army knife of the modern production stack–but there’s one place in particular that I see it being brought to bear over and over again.Web services that are exposed to a network tend to need various layers of protection. The most common form of protection is of course authentication, which ensures that the users accessing your resources are the ones that you expect to be accessing them. Another very common one is controlling the rate at which users allow to access those resources. This is obviously quite useful for public services that need protection against (both intentionally and unintentionally) malicious actors, but also for internal services to protect against certain types of accidental use — the thundering herd problem for example, where many consumers wake up simultaneously and contend for access to the same resource (like an API) simultaneously. Even companies like Google, who are widely known for the excellence in technical competency, have admitted to occasionally making this sort of mistake.If you look at almost any commonly used APIs that you can find online, you’ll notice that the vast majority of them are controlling access using rate limiters. GitHub, Spotify, Heroku, and Uber are all good examples.A naive rate limiter implementation might simply track the number of operations taken in the expected period of time and expire buckets as that period comes to an end. Most real world rate limiters use a slightly more sophisticated algorithm called “drip bucket”. It’s an easy metaphor that models rate limit capacity as a bucket that has a fixed-size hole in its bottom. As a user consumes operations, water is added to the bucket and its water level rises. If the bucket becomes full, no more operations are allowed, but luckily, the hole in the bucket is allowing water to escape at a constant rate. As long as the rate of water in and the rate of water out stay roughly equal, the system stays at equilibrium and operations are never limited.Drip bucket is useful because its implementation is both computationally and storage efficient, but also because it offers a good user experience by providing a “rolling” time period. Even if a user accidentally burns through their entire limit in a single burst, they’ll have more limit available almost immediately instead of having to wait for the next window to start.redis-cell implements a variation of drip bucket called “generic cell rate algorithm” (GCRA). It’s funtionally identical, but uses some clever logic so that each users being tracked needs only a single backend key to track their entire state.The module exposes a single command: CL.THROTTLE. It’s invoked with parameters that describe the rate limit that should be enforced. For example:CL.THROTTLE user123 15 30 60 1▲     ▲  ▲  ▲ ▲|     |  |  | └───── apply 1 operation (default if omitted)|     |  └──┴─────── 30 operations / 60 seconds|     └───────────── 15 max_burst└─────────────────── key “user123”It responds with an array where the foremost element indicates whether the request should be limited. The other elements contain quota and timing metadata that’s often returned from HTTP services as informative headers along with the response. For example:127.0.0.1:6379> CL.THROTTLE user123 15 30 601) (integer) 0   # 0 means allowed; 1 means denied2) (integer) 16  # total quota (`X-RateLimit-Limit` header)3) (integer) 15  # remaining quota (`X-RateLimit-Remaining`)4) (integer) -1  # if denied, time until user should retry (`Retry-After`)5) (integer) 2   # time until limit resets to maximum capacity (`X-RateLimit-Reset`)Rolling your own rate limiting module is quite possible of course, but redis-cell aims to provide a general and widely-useful implementation that can be integrated into a project built with any programming language or framework, just as long as its stack includes Redis.redis-cell’s other notable feature is the language that it’s written in. Although the Redis module API was originally intended to be consumed by another C program (it’s exposed as a C header file in redismodule.h), the project is implemented in pure Rust. This is achieved through the use of the Rust FFI (foreign function interface) module which allows the program to break out of its normal safety rails and call directly into a systems level API. It’s also made possible because Rust programs are bootstrapped using only a tiny runtime, and much like C programs, have no garbage collector.So why bother? Well, although I could probably be considered to be nominally literate in C, I don’t have anywhere near the expertise to be confident that I wouldn’t write a program that contained a memory overrun or some other unsafe operation that would manifest as a program-killing segmentation fault. As evidenced by widespread issues like Heartbleed, even highly competent C developers are not beyond this class of mistake.The rust compilers guarantees that all my memory accesses are safe, and its strong type system goes a long way towards ensuring that I’m not accidentally misusing code in a way that could cause a runtime problem. This is good for me, but even better for would-be contributors the project; even someone who’s never written Rust before has only a miniscule chance of introducing a low-level problem as long as they can get the program to compile.Let’s look at a simple example of this safety in action. The Redis module API provides certain functions to allocate memory inside of Redis, and in the default mode of operation, modules are tasked with freeing any memory that they allocate in this way. So if RedisModule_CreateString is invoked to create a new string, a call to RedisModule_FreeString is expected to eventually free it.In Rust, I wrap theses string with a higher level type so that I don’t have to work with them directly:pub struct RedisString {ctx: *mut raw::RedisModuleCtx,str_inner: *mut raw::RedisModuleString,}Now the trouble with manual memory management is that it can be dangerous. Say I have a function that allocates a string, performs an operation with it, and then frees the string before returning:fn run_operation( )-> i64 {let s = RedisString::create(…);…s.free();return 0;}Even if it works perfectly fine at first, it’s easy for a bug to be introduced somewhere down the line by someone not intimately familiar with the original code. Say for example that a new return directive is introduced midway through the function:fn run_operation() -> i64 {let s = RedisString::create(…);…if error_occurred {// s is leaked!return 1;}…s.free();return 0;}The new conditional branch doesn’t free the string before leaving the function, so the program now has a memory leak. This is a very easy mistake to make in C.With Rust, we can do things a little differently. By implementing the language’s built-in Drop trait (think of a trait like an interface in most languages), we can guarantee the memory safety of RedisString:impl Drop for RedisString {fn drop(&mut self) {self.free();}}Drop is like a destructor in C++; it’s called when an instance of the type goes out of scope. So in this case we ensure that our low-level free function always gets called. There are no gotchas or failure conditions to worry about.We don’t even have to manually call free anymore. No matter how many new conditional branches are introduced, memory safety is always guaranteed:fn run_operation() -> i64 {let s = RedisString::create(…);…if error_occurred {// s is freed here automaticallyreturn 1;}…// and here too!return 0;}Foremost I’d like to thank Redis for hosting the Redis module hackathon and giving me the opportunity to work on this. I also appreciated the help of Daniel Farina in detangling some of the Redis internals and figuring out how its module system was laying out memory, and Itamar Haber for giving me some instruction on the correct use of some of the Redis module APIs."
408,https://redis.com/blog/redis-cloud-beta-ibm-bluemix/,Redis Cloud Beta on IBM BlueMix,"June 18, 2014",Itamar Haber,"I’m delighted to announce the official beta launch of our Redis Cloud service on IBM’s BlueMix. Big Blue’s BlueMix is an implementation of IBM’s Open Cloud Architecture, which leverages Cloud Foundry to enable developers to rapidly build, deploy, and manage their cloud applications, while tapping a growing ecosystem of available services and runtime frameworks.As the leading managed Redis service available, it is only natural that we’d offer our Redis Cloud service on this new and exciting platform. Our beta offer consists of a free 1GB subscription for Redis Cloud off IBM’s SoftLayer. To start using it immediately, follow these steps:This is all you need to do to start using Redis Cloud with your BlueMix application. For more information on how to use Redis Cloud with your app, please refer to our documentation. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
409,https://redis.com/blog/redis-cloud-private/,Redis Cloud Private,"June 3, 2017",Aviad Abutbul,"We are very excited to announce the preview release of the new simplified Redise Cloud Private (RCP) managed DBaaS.Redise Cloud Private delivers a fully managed, cost effective, stable high performance Redis databases in dedicated  clusters within your cloud account, using your own instances, inside your VPC, with the option to run Redis databases on RAM or RAM+Flash (Redise Flash)  as an extension of memory, using High IOPS NVMe-based SSD instances.While Redise Cloud Private (RCP) is a long standing option for customers running production clusters, there were manual aspects to the setup. Today we released the capability to automatically deploy dedicated RCP clusters into your cloud account in just a few clicks. To provision RCP, users provide dataset size and throughput requirements with the credentials for RCP to operate in your own VPC. RCP automatically plans and provisions your cluster and databases in an instant. Users can scale RCP throughput and dataset size up and down at any time with a simple click as well. After your cluster is deployed, our team of Redis experts, monitor and manage your RCP cluster just like any fully managed service for you.With Redise Cloud Private you get reduced operational complexity, with the effortless scaling, and high resilience of Redise, delivered securely inside your private cloud environment.Redise Cloud Private allows you to easily implement our Redise Flash technology that will allows you to slash operational expenses when hosting large datasets in Redis by utilizing Flash memory(SSDs) in combination with RAM. In addition since RCP is running within your cloud account you can benefit from the special rates you have with your cloud provider.With RCP you can forget about instances, clusters, scaling, data persistence/high availability settings and failure recovery. Redise Cloud Private comes with zero operational hassle!Efficiently replicate Redis databases between Redise Cloud Private and Redise Pack or across cloud regions, with built-in compression and WAN optimization technologies. Create multi-region, multi-cloud or hybrid (on-premises and cloud) Redise deployments that accelerate your applications and require zero time to DR.You can read more about about RCP benefits here.The steps for creating a simple Redis Cloud Private (RCP) deployment are as follows:You can read more about it at following link.When you enter your database requirements – specifically memory limit and max throughput, we plan both the infrastructure and the number of shards (shard is any type of provisioned Redis instance, such as a master copy, slave copy of data etc.) needed to support your need.Redise Cloud Private is pricing is based on the number of shards you asked for. Every time there will be a change needed to the number of shards you will prompted with the new number and will be asked to approve that change.With RCP you get reduced operational complexity, with the effortless scaling, and high resilience of Redise, delivered securely inside your private cloud environment.We are currently only supporting AWS for the zero-touch deployments, other cloud platforms will come soon. Please contact us if you need an RCP deployments on other cloud providers.This is still a preview and we would very much love to get your inputs and feedback. Feel free to reach out to us at pm.group@redislab.com."
410,https://redis.com/blog/redis-enterprise-flash-intel-optane/,Redis Enterprise Flash on Intel Optane,"August 8, 2017",Redis,"Our partners at Intel have recently announced the availability of Optane, an NVMe-based SSD device built on top of the new persistent memory technology from Intel and Micron, 3D XPoint (3DXP).One of the main advantages of 3DXP is that it gives the application a way to decide which part of the dataset should be stored on fast memory, e.g. DRAM, and which part on a slower memory, e.g. Flash/Nand. Our Redis Enterprise Flash (RF) product was built from the ground-up with the same approach in mind.We therefore decided to conduct an RF benchmark using Optane drives and compare the results with the benchmark we ran using the previous generation of Intel’s NVMe-based SSD, the P3700. We knew that achieving better results with Optane would be challenging, as RF had performed impressively with P3700.Before I get into the results of our benchmark, I’ll first cover some quick background about Redis and Redis Enterprise Flash. Redis is known for its extremely fast performance, mainly because it serves datasets entirely from RAM. However, RAM prices have remained flat during recent years, and even deploying a dataset with only a few hundreds gigabytes can be very expensive (not to mention the cases wherein one or more replicas are needed for high-availability).RF solves this problem by storing Redis’ keys, dictionary (the main data structure behind the keys) and “hot” values (the most accessible values) in RAM, while “cold” values (the less accessed part of the dataset) identified by the LRU algorithm are kept on Flash (the technology behind SSDs). Distributing the data this way guarantees that ongoing operations are executed almost as quickly as Redis on RAM.This architecture is mainly designed for use cases in which the working dataset is smaller than the total dataset (which is the most common scenario), as it allows RF to maintain performance similar to that of RAM, while dramatically reducing the server’s infrastructure costs. RF is fully compatible with open-source Redis and incorporates the entire set of Redis commands and features. Flash is treated as a RAM extender and does not replace the existing data-persistence mechanism of Redis. With all of this in mind, let’s look into our latest performance tests on AWS.We benchmarked the performance of Redis(e) Flash using the following setup:The graph below show the 1000B results respectively for 50%, 85% and 95% RAM hit ratio.The table below summarizes the average improvement factor of the Intel Optane SSD compared to the Intel P3700 across all tests:The new Intel Optane SSD is a major improvement over the P3700 (especially for items larger than 1000B), delivering more than nine times the throughput using our RF product.Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz2-sockets, 88 hardware threads, 44 cores (22 cores/socket)128GB of memory (8GB DDR4 DIMMs at 2133 MHz)4x Intel® OptaneNetwork: 10Gbps EthernetOS – RHEL 7.0memtier_benchmark version 1.2.6Redis Enterprise version 4.5Memtier command:./memtier_benchmark -s 192.168.22.14 -p 12320 –pipeline=20 -c 10 -t 20 -d 1000 –key-maximum=42949673 –key-pattern=G:G –key-stddev=1177484 –ratio=1:1 –distinct-client-seed –randomize –test-time=120 –run-count=1 –hide-histogram"
411,https://redis.com/blog/redis-enterprise-rising/,Redis Enterprise Rising!,"February 6, 2018",Leena Joshi,"The story of Redis has been an unusual and exciting one! With our roots in the open source, we started as a cloud service for Redis and Memcached many years ago and over time developed robust automation, high availability and scaling techniques that attracted hundreds and thousands of users.  We made the powerful software that underpins our service, available as downloadable software for the customers to use on premises and in any private environments. During this time, we added critical features to our technology that gave it a broader appeal such as:At the same time, we expanded our deployment capabilities – not only did we add many more regions and clouds, we also made our robust, reliable and cost-effective service available inside VPCs – much to the delight of some of our customers.Since the technology underpinning was common to all the ways in which our products surfaces,  we intuitively called it Redis Enterprise. ( sometimes you will see it as redis to the power e – like redise).To make it obvious and simple that our secret sauce technology is what serves our 8000+ enterprise and our tens of thousands of free users, we are unifying our product names under this umbrella name,  Redis Enterprise. Our deployment options – public clouds, VPCs and software are simply monikers to help customers choose the deployment environment and level of service they desire.Our public cloud service, Redis Enterprise Cloud is not just fully -managed, it is also hosted in the cloud regions of your choice on our instances. You simply pick a plan, move a slider to scale up or down – we do all the rest.Our private cloud service, Redis Enterprise VPC runs inside your VPCs as a fully-managed service, and brings you all the advantages of Redis Enterprise (Active-Active, Redis on Flash, Modules, automation, seamless scaling, cost savings and many many more) with the security and privacy of your VPCs.Redis Enterprise software ( called Redis Pack or Redis Enterprise Cluster in previous incarnations) contains the power of our Redis Enterprise technology and can be deployed in several operating environments including Linux, MacOs, Windows,  inside Docker containers , in Pivotal Cloud Foundry and in OpenShift environments orchestrated by Kubernetes.If you encounter our old product names on content created in prior years, please pardon us. “A rose by any other name would smell as sweet” , argues Juliet in “Romeo and Juliet”, but we think Redis Enterprise is sweeter!"
412,https://redis.com/blog/redis-enterprise-susceptible-rediswannamine/,Is Redis Enterprise susceptible to RedisWannaMine?,"March 22, 2018",Redis,"On March 8th, Imperva published a blog post about a new cryptojacking attack that incorporates Redis. From an engineering perspective, this attack isn’t so much a marvel as a complicated Rube Goldberg machine that happens to have Redis as one of the cogs. Good news though:The Imperva blog post does a good job of explaining the exploit, so I won’t “reinvent the wheel” here, but I will briefly outline how Redis is involved:Experienced Redis geeks are probably screaming right now. Let’s go over all the poor decisions related to Redis that need to be made to be vulnerable for this exploit:In short, this attack targets those who manually altered settings to do dangerous things.Redis Enterprise, on the other hand, is protected out-of-the-box against these types of attacks by providing a pure separation between the management / administrative plane and the data-path plane. For example, all CONFIG commands of the standard Redis API are disabled. Additionally, Redis Enterprise comes with a built-in multi-layer security control that includes an access control layer, authentication layer, authorization, forensics layer, encryption layer and available protection layer. A detailed description of all of these features can be found on the Redis Enterprise Security Technology page.So, there you have it—a very ill-advised configuration is at the heart of this exploit. Don’t jump through hoops to make your open source Redis server less secure… or just use Redis Enterprise and you’re golden."
413,https://redis.com/blog/redis-enters-billion-downloads-club/,A Billion Thank Yous to our Redis Community,"September 26, 2018",Madhukar Kumar,"If you attended RedisConf earlier this year you may have been mildly surprised to hear – “Redis is Everywhere.” From web applications to mobile apps to IoT devices, Redis is one of those silent but efficient elements of our day-to-day lives that is rarely seen but is present everywhere..Not buying it?Well, last night we reached a milestone that shouldn’t have surprised anyone.We hit 1 Billion Redis container launches on Docker.One Billion is a significant number by any standards but consider this: Running up to this milestone we were seeing about 4 million launches in a day. To put this in perspective, this is roughly 70,000 launches in a minute or about 11,574 launches in a second.No other database has been launched as many times on Docker at the time of writing this blog.And all of this is a testament to the Redis community. The community of developers and database users that had humble beginnings and a journey leading up to yesterday’s milestone.We are humbled to be part of this journey. A journey that started off with one developer and one dream.The vision is still something that guides us all at Redis – to help developers across the globe create applications that can process and manage data blindingly fast in the simplest possible way.In today’s world more and more users demand database platforms that offer the versatility and resource efficiency necessary to power the next generation of applications-functionality that legacy databases lack. The Redis platform provides that advanced functionality, enabling users to power a broad range of solutions in a variety of real-time use cases.It is this maniacal focus on performance and simplicity that also gained Redis the title of being the most loved database by 100,000 developers from 183 countries in Stack Overflow survey two years in a row.A billion is a big number but in the end it is about the developers who continue to use Redis to solve for problems they face every day. At Redis we call them Redis Stars. These are the stars who often join the developer community with a clear eyed vision of making a difference and are not daunted by the challenges that they are thrown into day after day.The biggest of all those challenges are often hearing the word “No.” “No, you cannot have more time.” “No you cannot have more resources.” “No you cannot throw away the old technology to create new ones.”To those developers and dreamers who quietly break away the barriers one hurdle at a time, we say thanks for reaching this milestone with us. We are humbled and we hope this journey takes the community to newer heights and milestones that are yet unseen.Redis will be hosting Redis Day London, November 15 at Code Node."
414,https://redis.com/blog/redis-hacking-hshacks-iii-hackathon/,Redis Hacking at the HSHacks III Hackathon,"April 13, 2017",Tague Griffith,"Last month, Redis was a sponsor of the third HSHacks Hackathon.     Teams came from all over the San Francisco Bay Area to spend 24 hours turning their ideas into working applications.  Although the teams worked late into the night, there was still time for some fun and games and even a few tutorials on some of the latest technology.Our developer relations team had a blast spending the day working with various teams on everything from debugging projects to helping the students find better algorithms to solve their problems.  We even got a chance to help a couple of first time hackers get up and running.  Several teams took a break from development to attend our introductory workshop on Redis.As part of our involvement with the hackathon, we sponsored a best Redis hack contest for students interested in using Redis as part of their project.  Several teams, with a wide range of ideas, submitted applications to our content – including a first person shooter with a 3D spatial controller to an app for connecting congress with constituents.We want to congratulate Team Vitruvian from Cupertino High School in Cupertino, CA for winning our content.  Team Vitruvian won not only the best Redis Hack competition but also the competition for the best use of of a healthcare dataset.The team spent their 24 hours designing and building an app for improving the quality of health care provided patients in rural areas and developing countries.  The Vitruvian app centralizes the storage of patient records online, so that machine learning and AI can be applied to patient records.  For the hackathon the team prototyped identifying patients at risk for heart disease from their medical records.Our budding hackers ran into a challenge while building their app – how could they process SMS messages received from the app.  Team Vitruvian turned to Redis and the built in pub/sub functionality to route messages to the proper system and send an appropriate response.We are proud to be supporting the next generation of software engineers and Redis hackers."
415,https://redis.com/blog/redis-labs-adds-two-factor-authentication-enhance-account-security/,Redis Labs Adds Two-Factor authentication to enhance account security,"September 4, 2018",Aviad Abutbul,"Today we’re introducing two-factor authentication (2FA) for Redis Enterprise, aimed at strengthening overall security and preventing unauthorized access. This enhancement can help protect your Redis account and eliminate unauthorized access. While this setting is optional, we strongly recommend implementing it.Here are the step-by-step instructions on how to enable 2-factor authentication:2) In your user profile, click on the ‘Multi Factor Authentication’ button.3) Next, click ‘Activate Now.’4) Start the process by entering your mobile phone number and click ‘Continue’:Wait a few moments and check your phone. You should receive an SMS with your six-digit code for verification. Enter it and click ‘Verify.’That’s it. You are all set and ready to go.If you prefer, you can also configure your Google Authenticator app (iOS, Android) by clicking on the ‘Configure’ button next to that option.Simply scan the barcode into your authenticator app and provide your verification code.Q: I don’t have a mobile phone. Can I add 2FA to my account?A: Currently, this is not an option. You will need a mobile device to receive your 2FA codes via SMS.Q: I enabled 2FA but now I want to disable it. How can I do that?A: 2FA can be disabled through your Redis profile.Q: I added 2FA to my account, but forgot my mobile phone today and don’t have it with me. Can I log in?A: We highly recommend configuring with both SMS and the Google authenticator app so you will always have a fallback option. If you still cannot log in, please contact our support team at support@redis.com.Q: My phone was stolen. What should I do?A: Disable 2FA so that someone else is not be able to log in as you, and then enable it with your new mobile phone.We highly recommend you spend a few moments to enhance your account security by enabling 2FA on your Redis account.For further information, feedback or suggestions, please contact us at pm.group@redis.com."
416,https://redis.com/blog/redis-on-windows-8-1-and-previous-versions/,Running Redis on Windows 8.1 (and previous versions) – Part II of III,"July 31, 2018",Redis,"It’s an open secret that more developers code on Windows than any other OS. Every year Stack Overflow shares its developer survey, and every year Windows is the most popular OS for development, and Visual Studio is the most popular IDE. Of course, there are many ways to slice the data, but it suffices to say A LOT of you reading this post are using Windows right now. What isn’t obvious, however, is how to install Redis on Windows so you can develop applications that use Redis. Even if you access Redis remotely (ex: Redis Enterprise Cloud), you still need a Redis client compiled for your local Windows machine.Today there is one way to develop with Redis natively on Windows 8.1 (and earlier versions of Windows), and that is with an unsupported port of Redis 3.2.1 for Windows.In part 1 of this “Redis on Windows” series, I explain how to run Redis on Windows 10 via the Windows Subsystem for Linux (WSL). In this post, I explain how to run Redis on earlier versions. In a future post, I will explain how to run Redis in a Docker container.Officially, Redis is not supported on Windows. There is, however, a 3.2.1 version of Redis that was ported to Windows by MSOpenTech. It’s over two years old and has some drawbacks, so the link from the redis.io Downloads page has been removed. That said, newer versions of Redis are backward compatible, so if you don’t need new commands, then this 3.2.1 version of Redis might work for your development purposes.Note: There have been many security fixes and other improvements since version 3.2.1, so I highly recommend against running older versions of Redis in production. This blog post expects that you want to run Redis on your developer machine for development purposes only. As always, you should develop your code on a non-open, trusted network that is behind a firewall.Connect to Redis for WindowsSummaryNow that you have Redis running on your developer machine, you’ll want your application to talk to Redis. Check out the Redis clients section on redis.io for a list of Redis client libraries. In particular, look at the C# clients ServiceStack.Redis and StackExchange.Redis."
417,https://redis.com/blog/redis-labs-customers-award-accolades/,Redis Labs’ Customers Award Accolades!,"December 13, 2016",Manish Gupta,"Over October and November Redis conducted two surveys on Redis usage. One of the surveys was of Redis customers, conducted by TechValidate, who provides third party validation of the results. We are more than honored and encouraged by the sentiments expressed by the respondents!The TechValidate survey was emailed to 1600 out of 6700+ Redis’ customers and had 191 respondents. Not all questions were mandatory, so many of the charts will show different numbers of responses. All results were validated by TechValidate; no responses were excluded.We were of course brimming over with pride when we saw the response to the first question, “ What challenges does Redis help you solve that you value the most?” 78% of respondents cited the high availability provided by Redis as our most valuable feature! Redis’ stable high performance and seamless scaling and clustering closely followed in the top three most valued benefits of Redis.We also asked customers to quantify the benefits they see with Redis. Unsurprisingly, a majority of them saw 70-90% reduced downtime with Redis, a 50-70% improvement in performance, 30-50% cost savings and 30-50% faster time to market.We also asked customers if they were using Redis for data that wasn’t stored in other databases, i.e. as a primary datastore…and 71% said they were!The survey revealed that a prime driver of increased usage by customers is Redis’ ability to scale with application use, number of users, and processing needs.The really eye-opening responses were to questions around Redis usage: what were their Redis use cases and what types of solutions did they power. Redis use cases span the gamut of possibilities – job & queue, high speed transactions, content caching, user-session storage, real-time analytics, data ingest and more.The range of solutions that employ Redis was also impressive – mobile applications and e-commerce applications are leading favorites, while real-time analytics, personalization and customer engagement are also common applications. Fraud detection and IoT are emerging uses-stay tuned for customer case studies from these use cases!We also asked our customers where they would like to move additional data into Redis from. Not surprisingly, over 56% would like to move additional data into Redis from RDBMSs like Oracle, Microsoft SQL Server, MySQL, supporting the overall trend of data moving away from fixed schema databases.Many of our fortune 500 customers are moving data into Redis from everywhere ☺We also got a range of fantastic customer quotes like:All the results are public and can be found on the TechValidate page here.A second parallel survey done mostly with input from OSS Redis Customers was conducted as well, the results of which can be found here."
418,https://redis.com/blog/redis-labs-experience-spark-summit-east-february-2017/,"The Redis Labs Experience at Spark Summit East, February 2017","February 17, 2017",Rod Hamlin,"As Gold sponsors of this year’s Spark Summit East, the team from Redis was excited to spend time in one our country’s oldest cities, Boston. Flying out from our headquarters in sunny Mountain View, CA, we had no idea what we were in for. The diversion mid-flight to Chicago for a medical emergency should have been a sign that this would be an interesting journey.Spark Summit Day One went off without a hitch. With just under 2,000 attendees, the conference and expo areas where filled with Spark developers anxious to network and learn.Along with our partner, Databricks, the Redis team stayed busy with prospects and customers demonstrating how Databrick’s cloud-based Spark workspace integrates with Redise Cloud. This integration enables Databricks users to serve Spark processes and SQL queries with Redise Cloud while allowing RediseCloud users to instantly run analytics processing using Databricks’ cloud-based Spark clusters. (see diagram below)Joint customers benefit from being able to access the power of Spark integrated with the real-time high performance of Redis. Analytics can shift from being post-facto to inline and instantaneous with the combination of these two powerful big data engines.We also showed off our Redis-ml module which includes native datatypes for storing and serving machine learning models generated by frameworks such as Apache Spark. As companies make machine learning an integral part of their business, Redis and Redis-ml play a critical role in production implementations.Day One of Spark Summit wrapped up with an energetic cocktail reception in the Expo Hall. The Redis northeast sales team headed out to dinner with customers, and the partner team ventured out to experience one of Boston’s many great restaurants with another Redis’ partner, Accenture. And while the weather on Wednesday felt cold to us Californians, the locals said 50 degrees was down right balmy for February.Enter a blizzard. A little snow never hurt anyone. Around 6:00 am on Thursday, I was awakened by strong winds and snow pounding against my window on the 9th floor of the Back Bay Hilton. By mid-afternoon, we were in the middle of a powerful blizzard that brought several feet of new snow up and down the coast.Fortunately, the Hilton is right across the street from the Hynes Convention Center so we only had to venture a few hundred feet outside. Surprisingly, Day Two of the Summit was only slightly less busy than Day One.Our team remained busy throughout Day Two of the summit. The Redis staff was excited to walk several Fortune 500 prospects through the impressive results of the latest Redis-Spark Time Series benchmark. This Redis’ benchmark using time-series data shows that running Spark on Redis as a data store results in 135 times faster processing compared to Spark using HDFS, and 45 times faster processing compared to Spark using Tachyon as an off-heap data store or Spark storing the data on-heap.Day Two ended with another evening of superb dinners with prospects. A few of us decided to brave the weather, jump on the subway, and head over to TD Garden to see our San Jose Sharks take on the Boston Bruins. After a quick stop at Four’s Sports Bar, we found ourselves in the Garden, admiring all of the championship banners hanging from the rafters. The hockey game did not turn out well for the Sharks in the end, but we had a great time and made some new friends along the way.By Friday morning, the sun was back out and we headed off to the airport. Fortunately, the folks at Logan International are some of the best in the world in dealing with snow and ice, and our flights back to SFO departed safely, and on-time.Thanks to all of the great folks at Databricks for organizing a fantastic event. We are already looking forward to RedisConf 2017 in May, and Spark Summit in June – both in San Francisco. And safe to say, there is no snow in the forecast."
419,https://redis.com/blog/redis-labs-fastest-growing-enterprise-software-company/,Redis Labs – The Fastest Growing Enterprise Software Company,"November 8, 2017",Manish Gupta,"Whoa! We are proud to be recognized as the fastest growing Enterprise Software company in Silicon Valley and to be ranked 50th overall in North America in Deloitte’s 2017 Technology Fast 500™ awards. Deloitte’s Technology Fast 500™is a widely recognized ranking of the 500 fastest growing technology, media, telecommunications, life sciences and energy tech companies in North America.Redis’ momentum continues to accelerate and is validated by becoming the most deployed database on Docker, the most popular cloud database, and the most loved database. 2017 has been a very rewarding year for Redis team and the overall Redis community. This year started with InfoWorld’s Technology of the Year award and now the Deloitte recognition forms a wonderful bookend.The success can be fully attributed to the loyal Redis community, innovative hard work of Redis employees and to the very happy customers that have deployed Redis Enterprise in diverse mission-critical real-time applications.Thank You."
420,https://redis.com/blog/redis-license-bsd-will-remain-bsd/,Redis’ License is BSD and will remain BSD,"August 22, 2018",Yiftach Shoolman,"Since the recent licensing change for our Redis modules, there’s been a lot of confusion and misinformation circulating about the implications of those changes. We want to address your questions and be crystal clear: the license for open source Redis was never changed. It is BSD and will always remain BSD.We recently did change the license for Redis modules developed by Redis from AGPL to Apache v2.0 modified with Commons Clause. For those of you who are not familiar with Redis modules – these are add-ons on top of Redis core like RediSearch, RedisGraph, ReJSON, ReBloom and Redis-ML.Cloud providers have been taking advantage of the open source community for years by selling (for hundreds of millions of dollars) cloud services based on open source code they didn’t develop (e.g. Docker, Spark, Hadoop, Redis, Elasticsearch and others). This discourages the community from investing in developing open source code, because any potential benefit goes to cloud providers rather than the code developer or their sponsor.There are two key reasons we decided to make the switch from AGPL:This new license allows full use of our Redis modules under the popular, liberal Apache v2.0 terms, but restricts the selling of the modules themselves. That means you can build internal, external and commercial products on top of our modules and sell those, but cannot directly sell the original modules. We believe this licensing supports the open and free use of modules, while still maintaining our rights over commercializing our assets.Commons Clause was created by a coalition of several open source infrastructure companies, some of which use different open source licenses. In order to maintain a standard framework, we decided to piggyback the restriction (on cloud providers creating managed services) on an existing open source license.We know some members in the community have had questions about this change, and we are happy to clarify any of the license details further to put your minds at ease.  Please feel free to tweet me @yiftachsh."
421,https://redis.com/blog/redis-meetups-around-world/,Redis Meetups around the World,"June 16, 2017",Tague Griffith,"This week turned into International Redis Meetup Week with three different meetups in the US and Europe.  The developer communities in Paris, London and Seattle got together to talk Redis.LondonThe London Redis community gathered at CodeNode London for an evening of technical talks and networking.  Tague Griffith from Redis  warmed up the crowd with his talk on Redis based solutions to the problems of using predictive models in production systems and Rui Gu from the Reddison team followed up with a talk about using Redis as an in-memory data grid.The event was a great opportunity for the community to get together and plan for future events in the London area.  Get involved with the Redis London Meetup if you’d like to help out.ParisRedis’ own François Cerbelle gave a talk, Scalabilité et Performance, covering LUA scripting, modules, and some of the differences between OSS Redis and Redis Enterprise at the Paris Redis Meetup.  François talk included a great demo showing multiple ways to implement transactions in Redis and how modules can be used to add all sorts of new functionality not available in Redis today.SeattleThe Seattle Redis Meetup brought in local developers to talk about two very popular Redis applications for their monthly meeting.  Dmitry Polyakovsky of SnapRaise talked to the crowd about scaling Redis for caching and background tasks with a small detour through rate limiting with Redis.  Daniel Hochman of Lyft gave an encore presentation of his RedisConf17 talk Geospatial Indexing at Scale: The 10 Million QPS Redis Architecture Powering Lyft.  If you’re a Redis Geek in the Seattle area and would be interested in speaking at or hosting a meetup, contact the organizers.There are many great community events happening around the world, sign up for your local meetup to join in the fun!"
422,https://redis.com/blog/redis-recommends/,Redis Recommends…,"September 25, 2016",Leena Joshi,"You just finished watching Narcos, season 2 and are about to slip into the post-binge-watching doldrums, when up pops a recommendation from Netflix. “Because you liked Narcos, you should watch..El Patron De Mal.”Its not just Netflix. The New York Times does it, Amazon does it, Google does it, even your teenager’s school’s application has recommendations.Applications are getting smarter every day and it is no secret that they rely on extremely smart developers and technologies to pull out the insights to make these recommendations. What makes for an effective recommendation? Something you are likely to act on!How do you get such a recommendation? While there are many algorithms that can crunch hundreds of different variables to get you the right article to read on a Sunday morning, there are some very simple ones too. People who share the same characteristics – geography, demographic, past preferences, past dislikes- make for a rich pool of potential recommenders. Some very simple approaches include just plain old comparisons. Computing which users have the most in common with you, who have done things in the same order as you, which items have they liked that you haven’t looked at yet, gives us a very simple path to generating the next set of recommendations for you.In developer terms, implementing comparisons can be done in many different ways – but why take the trouble to write complex code to perform these comparisons, when you can use a database with built in set operations, that does it for you?Redis is one of the few databases in the world that has sophisticated set operations. Redis Sorted Sets are beloved, powerful and adept at solving a multitude of problems. Built-in set operations are good for many things like time-series data, bid management, scoring, ranking and more. They are also a really simple way of implementing recommendation algorithms.Conceptually, it is quite simple – you have hundreds and thousands of users- to find the most similar users, you compare users. Simple set intersection operations will get you users who rated the same item, simple set union operations to find all users who rated a group of items. Based on who rated common items similarly, you generate a similarity score and based on this score (Sorted set operations like ZRANGE to extract top similar by range of score), you find the top users similar to the user in question. Performing similar steps to find items they ranked highly, but that have not been purchased/seen by your current user, should give you the top recommendation to make to your user!The cool thing about Redis Sorted Sets is that they maintain an ordered view of each member by score automatically and in-memory. This, combined with set operations makes for blazing fast retrieval times, which means you can implement a simple, extremely high performance recommendation engine with very little effort. We did just this! We recently published (on github) the code for a simple Recommendations engine written in Go, and using Redis. The other cool thing about this engine is that it is easily extensible. It is implemented with user scores as its only criterion for calculating similarity, but it is easy to add location, demographic and other characteristics too! This whitepaper describes how it works – so download it along with the code!"
423,https://redis.com/blog/redis-streams-apache-spark-structured-streaming/,Redis Streams + Apache Spark Structured Streaming,"June 3, 2019",Roshan Kumar,"Recently, I had the honor of presenting my talk, “Redis + Structured Streaming: A Perfect Combination to Scale-out Your Continuous Applications” at the Spark+AI Summit.My interest in this topic was fueled by new features introduced in Apache Spark and Redis over the last couple months. Based on my previous use of Apache Spark, I appreciate how elegantly it runs batch processes, and the introduction of Structured Streaming in version 2.0 is further progress in that direction.Redis, meanwhile, recently announced its new data structure, called “Streams,” for managing streaming data. Redis Streams offers asynchronous communication between producers and consumers, with additional features such as persistence, look-back queries, and scale-out options – similar to Apache Kafka. In essence, with Streams, Redis provides a light, fast, easy-to-manage streaming database that benefits data engineers.Additionally, the Spark-Redis library was developed to support Redis data structures as resilient distributed data sets (RDD). Now, with Structured Streaming and Redis Streams available, we decided to extend the Spark-Redis library to integrate Redis Streams as a data source for Apache Spark Structured Streaming.During my talk last month, I demonstrated how you can collect user activity data in Redis Streams and sink it to Apache Spark for real-time data analysis. I developed a small, mobile-friendly Node.js app where people can click on the dog they love most, and I used it to run a fun contest through my session. It was a tough fight, and a couple of folks in the audience even got creative with hacking my app. They changed the HTML button name using the “page inspect” option and tried to mess with my demo. But in the end, Redis Streams, Apache Spark, the Spark-Redis library, and my code were all robust enough to handle those changes effectively.The audience also asked some interesting questions during and after my presentation, such as:My main takeaway from the summit was that there’s growing interest in continuous processing and data streaming. Owing to the demand, we published a more detailed article on this topic over at InfoQ, which offers a detailed recipe for how to set up Redis Streams and Apache Spark and connect both using the Spark-Redis library. Or feel free to check out the full video of my presentation here."
424,https://redis.com/blog/redisai-ai-serving-engine-for-real-time-applications/,Announcing RedisAI 1.0: AI Serving Engine for Real-Time Applications,"May 19, 2020",Pieter Cailliau and Filipe Oliveira,"As modern mission-critical applications incorporate more and more machine learning techniques, they face some surprisingly complex challenges. Those challenges center around coping with real-time model serving requirements and monitoring the impact of these new features on end users.We’re happy to announce that RedisAI, an AI serving engine for Redis, is now generally available. RedisAI was built with two core goals:RedisAI is designed to run where the data lives, decreasing latency and increasing simplicity, in use cases ranging from transaction scoring and fraud detection to recommendation engines personalization, among many others. For more information on common use cases and when to apply it, see the RedisAI product page.This blog is intended to help developers and architects see under the hood of RedisAI, and learn how it addresses the goals listed above. We will dive into architecture and share the benchmarks we’ve established for a prominent real-world AI problem—real-time fraud detection on financial transactions. These benchmarks reveal that RedisAI increases speed by up to 81x compared to other model serving platforms, when the overall end-to-end time is not dominated by the inference itself.Most artificial intelligence (AI) frameworks ship with a runtime for executing the models developed with it, and the common practice for serving these is building a simple server around them. Because RedisAI is implemented as a Redis module, it automatically benefits from the server’s capabilities, including Redis’ native data types and robust ecosystem of clients, as well as its persistence and clustering, not to mention the flexibility provided by Redis modules and the peace of mind of proven Redis Enterprise support. Of course, there’s also Redis’ high availability (99.999%) and infinite linear scalability.Because Redis is an extendable in-memory data structures server, RedisAI uses it for storing its machine learning (ML) native data types. The main data type supported by RedisAI is the tensor, which is the common representation of ML data.Additionally, RedisAI adds two data structures—models and scripts—for model runtime features. Models represent a computation graph by one of the supported deep learning (DL) or machine learning framework backends and are set with information about which device they should run on (CPU or GPU) and backend-specific parameters. RedisAI has several integrated backends, including TensorFlow, Pytorch, and ONNXRuntime.Scripts can run both on CPUs and GPUs, and let you manipulate tensors via TorchScript, a Python-like domain specific language (DSL) for tensor operations. This lets you pre-process your input data before you execute your model, and post-process the results, such as for ensembling different models to improve performance.Because tensors are stored in the memory space of the Redis server, they are readily accessible to DL/ML backend libraries and scripts, with minimal latency. This locality of data allows RedisAI to provide optimal performance when serving models. It also makes it a perfect choice for deploying DL/ML models in production and allowing them to be used by any application.Importantly, this architecture is not tied to a single backend, letting you decouple your backend choice (a decision typically made by data scientists) from the application services using these models to provide functionality. Switching models (even when the model is created in a different backend) is as easy as setting a new key into Redis.Other important RedisAI features include its auto-batching support and the DAG (as in direct acyclic graph) command. With auto-batching, requests from multiple clients can be automatically and transparently batched into a single request to increase CPU/GPU efficiency during serving. The new AI.DAGRUN command supports the prescription of combinations of other RedisAI commands in a single execution pass, where intermediate keys are never materialized to Redis.To benchmark RedisAI, we created a benchmark suite consisting of a fraud-detection use case based on a Kaggle dataset with the extension of reference data. This use case aims to detect a fraudulent transaction based on anonymized credit card transactions and reference data.We used this benchmark to compare four different AI serving solutions:We wanted to cover all solutions in an unbiased manner, helping prospective users make an informed decision on the solution that best suits their case, both with and without data locality.Read more about the optimization in the addendum.See AddendumOn top of that, we wanted to reduce the impact of reference data in this benchmark so that it sets the lower bound of what is possible with RedisAI, by:Baseline latency comparing different solutionsThe first test compares the solutions without any reference data for single client performance. All serving solutions in this benchmark are essentially a serving wrapper around their core libraries. The setup and data flow is shown here:The table below shows the end-to-end inference time measured on the benchmark client. This test sets the baseline. It shows that RedisAI does not introduce any overhead on running a model compared to other serving solutions. In certain cases it is even more optimized, largely due to the programming languages that were chosen (RedisAI is written in C/C++, TensorFlow Serving in C++, TorchServe is written in Java, and the common REST API servers are written in Python).Impact of latency with reference dataNow that we have set the baseline, let’s look at how the latency is impacted when the model requires 1KB of reference data. For TensorFlow Serving, TorchServe, and Gunicorn, the reference data will reside in a different host.As explained above, for RedisAI, the reference data resides within Redis, already as a tensor. That’s why the setup is simpler:The table below captures the results of this second test, and shows that RedisAI reduces end-to-end inference latency up to 8x compared to other model serving solutions when there is reference data at play for single-client performance. The q99 numbers, meanwhile, show that RedisAI delivers a much more stable solution than any others:How do these solutions handle scale?After analyzing the single-client performance, the next question is how do these solutions scale on highly concurrent scenarios? We designed a third test in which we varied the number of clients from 16 to 160, increasing 16 clients at a time.For a dataset consisting of 1 million distinct credit-card transactions, the common HTTP server solution was limited at around 21K full inference cycles per second, TensorFlow Serving at around 40K full inferences cycles per second, TorchServe at around 50K full inference cycles per second, and RedisAI at around 192K full inference cycles per second.On the same hardware and serving based on the same model, RedisAI handles 4.8 times more inferences than TensorFlow serving, 4 times more inferences than TorchServe, and 9 times more inferences than common web APIs, as shown here:Considering the best results for each distinct model serving solution, note that while other model servers are overloaded at around 50K inferences per second, RedisAI is performing at steady and stable sub-millisecond latencies, without requiring additional virtual machines to be added to the cluster, at up to 190K inferences per second, as seen in these charts:If you relate the speedup factors on throughput and inference latency, RedisAI presents an overall speedup of 16x vs. TorchServe, 25x vs. TensorFlow Serving, and 81x vs. the common HTTP server. This means that on the same underlying hardware RedisAI can be 81x more efficient on serving the total 1 million inferences, as illustrated here:Benchmark analysisThis initial benchmark shows that data locality makes a tremendous difference in the benchmark and in any real-life AI solution.Note that the impact of reference data in this benchmark has been greatly reduced, and, as mentioned earlier, only sets the lower bound of what is possible.We plan to enhance our benchmark and create more setups in line with many modern and legacy deployments so that you can easily understand the potential speedup of your application architecture.Lastly, as seen on the previous charts, for high-load/high-concurrency use cases, there is no match for RedisAI, given that Redis is the only model server retaining sub-millisecond latencies and steady and state results as we increase concurrency.RedisAI achieves these impressive results mainly because it was built from the ground up for high performance. Seamlessly plugged into ​Redis, it uses Redis’ core characteristics to scale while avoiding the usual high-load/high-concurrency workload bottlenecks that Redis has already solved.RedisAI is fast, stable, and supports multiple backends—and we’re constantly working to enhance its capabilities. (In upcoming blogs we plan to focus on how you can deploy models with the support for ML-flow and how you can monitor your models in production.) RedisAI shows that we can create a feature-rich, high-performance model server on top of Redis. We’re looking forward to hearing your feedback, so please get in touch or comment on GitHub, or at the new RedisAI community forum."
425,https://redis.com/blog/redise-cloud-private-now-generally-available/,Redise Cloud Private is Now Generally Available,"December 6, 2017",Aviad Abutbul,"We are excited to announce that Redis Enterprise Cloud Private (RCP) is now generally available, with simplified signup and automated management. This release brings customers the power to deploy fully managed Redis Enterprise within their VPC in a zero touch and seamless manner.Redis Enterprise Cloud Private delivers fully managed, cost-effective, stable high-performance Redis databases in dedicated clusters within your cloud account, using your own instances inside your VPC. You can deploy Redis databases on RAM or on RAM + Flash (Redis Enterprise Flash). With Redis Enterprise Flash, SSDs acts as an extension of RAM for hosting large datasets at lower cost with Redis Enterprise Cloud Private!Over the last few months during the preview period, we have been working hard to improve the product and add new features based on the feedback we received. Here are a few important improvements in this generally available edition:Free Trial – No Credit Card Required!Take RCP for a test drive with our unlimited 14-day trial option, no credit card required.Simplified Sign-upTo simplify subscriptions and make them more flexible, we have added the following options:Data-at-Rest EncryptionFor better security and compliance, you can now enable data-at-rest encryption with a click of a button for all your databases on Redis Enterprise Cloud Private.Price CalculatorFor convenience, we have created a price calculator that uses your cloud and region preferences, along with throughput and data size needs, in order to estimate your bill.Get Started Today!Getting started is very simple with Redis Enterprise Cloud Private.Step #1 – Sign up for a Redis Enterprise Cloud Private accountSign up for an account on redis.com. This takes seconds.Step #2 – Create an AWS account for RCPDelegate an account under your AWS account for RCP. RCP will use it for remote provisioning and management. You only have to do this once!Step #3 – Create a new subscriptionChoose your data center and subscription options.Step #4 – Create your Redis databaseCreate a database or databases using “database templates.” Remember to choose the “Free Trial” option if you want to get started without a credit card.Step #5 – Connect to your Redis databaseUse redis-cli or a simple python app to connect to your deployment.We are continuing to improve Redis Enterprise Cloud Private. For any questions and feedback, please contact pm.group@redis.com."
426,https://redis.com/blog/redisinsight-gui/,RedisInsight—The Redis GUI You’ve Been Looking For,"November 12, 2019",Pieter Cailliau,"When it comes to databases, there are two kinds of people in the world. Those who love to type commands and those who like to interact with their data visually.OK, maybe the world doesn’t always need to be divided into two categories. Sometimes you want the best of both worlds. And now Redis gives you exactly that—you can continue to use Redis’ familiar command-line interface (CLI) and also choose to work with your data visually.At RedisConf 2019 earlier this year, we announced that we were acquiring RDBTools—a popular graphical user interface (GUI) tool for manipulating and visualizing Redis data—from HashedIn. We are happy to announce that after months of tinkering and adding features into the product, the brand new version of the tool—now called RedisInsight—is available to download for Microsoft Windows, Apple macOS, and Ubuntu Linux for free.Wait, did you say “FREE?”Yes, I did! RedisInsight is 100% complimentary! We want every Redis user to be able to take advantage of RedisInsight. We hope it will make Redis easier to use, with better visibility of your data. Eventually, you’ll be able to use RedisInsight as a single place for both GUI- and CLI-based interactions with your Redis database.RedisInsight is designed to educate and “give insights” to Redis users in several ways:So what exactly can you do with this powerful new tool? Turns out, quite a few things:If you ever thought it would be nice to have a GUI for Redis or to visually view the data inside your Redis instance, you now have the choice to download RedisInsight and make it part of your development and operational toolkit.Learn more and give it a try at /redis-enterprise/redis-insight/.Note: For the visualization component of RedisIngraph in RedisInsight, we partnered with Linkurious. RedisInsight leverages Linkurious’ powerful graph-visualization library Ogma, which enables you to interactively explore data within RedisGraph."
427,https://redis.com/blog/redistimeseries-ga-making-4th-dimension-truly-immersive/,RedisTimeSeries GA: Making the 4th Dimension Truly Immersive,"June 27, 2019",Pieter Cailliau,"Today we are happy to announce the general availability (GA) of RedisTimeSeries v1.0. RedisTimeSeries is a Redis module developed by Redis to enhance your experience managing time series data with Redis. We released RedisTimeSeries in preview/beta mode over six months ago, and appreciate all the great feedback and suggestions we received from the community and our customers as we worked together on this first GA version. To mark this release, we performed a benchmark, which achieved 125K queries per second with RedisTimeSeries as compared to other time series approaches in Redis. Skip ahead for the full results, or take a moment to first learn about what led us to build this new module.Many Redis users have been using Redis for time series data for almost a decade and have been happy and successful doing so. As we will explain later, these developers are using the generic native data structures of Redis. So let’s first take a step back to explain why we decided to build a module with a dedicated time series data structure.In the DB-engines trend chart below, you can see that time series databases have gained the most in popularity recently. In addition to the ever-growing amounts of data and new time series use cases for self-driving cars, algorithmic trading, smart homes, online retail and more, we believe there are two main technological reasons for this trend.The first reason is that the query pattern and scale of time series data differs from what existing database technologies were built for. While most databases were designed to serve more reads than writes, time series use cases have a high ingestion rate of large volumes of data, and a lower number of read queries. In a root cause analysis use case, reads are sporadic and only touch upon random parts of the data set. In the use case of training AI models (e.g., for anomaly detection in sensor data), typically reads would span a larger part of the data set but still occur significantly less frequently than writes. Because of Redis’ scalable architecture, which delivers high write throughput with low latency, Redis is natural fit for current time series query patterns.The second reason for this trend is that the toolset required for working with time series is not present in traditional database technologies. Efficient use of resources requires several structural changes, such as automatic downsampling of historical time series data and double delta encoding, as well as features for intuitively querying and aggregating time series data. Traditional database technologies introduce a lot of effort on the application side to address features like retention, downsampling and aggregations.At Redis, we are strong believers in eating our own dog food. For our cloud offering (which manages over 1 million Redis databases running on thousands of Redis Enterprise clusters), we collect metrics from each cluster inside an internal Redis database. During an internal project to enhance our own infrastructure metrics, we experienced first-hand the limitations and development effort of using core Redis data structures for time series use cases. We figured there must be a better and more efficient way. In addition to the toolset-specific features above, we also wanted out-of-the-box secondary indexing so we could query time series sets efficiently.There are two ways to use time series in Redis while reusing existing data structures: Sorted Sets and Streams. Many articles explain how to model time series with Redis core data structures. Below are some of the key principles we’ll use later in our benchmark.A Sorted Set stores values by their scores. In the case of time series data, the score is the timestamp on which an event was observed. The value repeats the timestamp followed by a separator and the actual measurement, e.g. “<timestamp>:<measurement>”. This is done because each value in a sorted set should be unique. Alternatively, the value holds the name of a unique key, which stores a hash where more data or measurements can be kept for the given timestamp.Downsides of this approach:Redis Streams, the most recently added data structure (and hence currently less frequently used for time series), consumes less memory than Sorted Sets and is implemented using Rax (a separate implementation of Radix trees). In general, Redis Streams enhances the performance of insertion and reads compared to Sorted Sets, but still misses a toolset specific to time series, since it was designed as a generic data structure.Downsides of this approach:In RedisTimeSeries, we are introducing a new data type that uses chunks of memory of fixed size for time series samples, indexed by the same Radix Tree implementation as Redis Streams. With Streams, you can create a capped stream, effectively limiting the number of messages by count. In RedisTimeSeries, you can apply a retention policy in milliseconds. This is better for time series use cases, because they are typically interested in the data during a given time window, rather than a fixed number of samples.If you want to keep all of your raw data points indefinitely, your data set will grow linearly over time. However, if your use case allows you to have less fine-grained data further back in time, downsampling can be applied. This allows you to keep fewer historical data points by aggregating raw data for a given time window using a given aggregation function. RedisTimeSeries supports downsampling with the following aggregations: avg, sum, min, max, range, count, first and last.When using Redis’ core data structures, you can only retrieve a time series by knowing the exact key holding the time series. Unfortunately, for many time series use cases (such as root cause analysis or monitoring), your application won’t know the exact key it’s looking for. These use cases typically want to query a set of time series that relate to each other in a couple of dimensions to extract the insight you need. You could create your own secondary index with core Redis data structures to help with this, but it would come with a high development cost and require you to manage edge cases to make sure the index is correct.RedisTimeSeries does this indexing for you based on `field value` pairs (a.k.a labels) you can add to each time series, and use to filter at query time (a full list of these filters is available in our documentation). Here’s an example of creating a time series with two labels (sensor_id and area_id are the fields with values 2 and 32 respectively) and a retention window of 60,000 milliseconds:TS.CREATE temperature RETENTION 60000 LABELS sensor_id 2 area_id 32When you need to query a time series, it’s cumbersome to stream all raw data points if you’re only interested in, say, an average over a given time interval. RedisTimeSeries follows the Redis philosophy to only transfer the minimum required data to ensure lowest latency. Below is an example of aggregation query over time buckets of 5,000 milliseconds with an aggregation function:127.0.0.1:6379> TS.RANGE temperature:3:32 1548149180000 1548149210000 AGGREGATION avg 5000
1) 1) (integer) 1548149180000
2) ""26.199999999999999""
2) 1) (integer) 1548149185000
2) ""27.399999999999999""
3) 1) (integer) 1548149190000
2) ""24.800000000000001""
4) 1) (integer) 1548149195000
2) ""23.199999999999999""
5) 1) (integer) 1548149200000
2) ""25.199999999999999""
6) 1) (integer) 1548149205000
2) ""28""
7) 1) (integer) 1548149210000
2) ""20""RedisTimeSeries comes with several integrations into existing time series tools. One such integration is our RedisTimeSeries adapter for Prometheus, which keeps all your monitoring metrics inside RedisTimeSeries while leveraging the entire Prometheus ecosystem.Furthermore, we also created direct integrations for Grafana and Telegraph. This repository contains a docker-compose setup of RedisTimeSeries, its remote write adaptor, Prometheus and Grafana. It also comes with a set of data generators and pre-built Grafana dashboards.To demonstrate the full power of our newly GA RedisTimeSeries module, we benchmarked it against three common techniques for handling time series data. We used a client-server setup with two separate machines in order to compare the performance of Sorted Sets, Streams and RedisTimeSeries for ingestion, query time and memory consumption.Specifically, our setup included:Redis Streams allows you to add several field value pairs in a message for a given timestamp. For each device, we collected 10 metrics that were modelled as 10 separate fields in a single stream message.For Sorted Sets, we modeled the data in two different ways. For “Sorted Set per Device”, we concatenated the metrics and separated them out by colons, e.g. “<timestamp>:<metric1>:<metric2>: … :<metric10>”.Of course, this consumes less memory but needs more CPU cycles to get the correct metric at read time. It also implies that changing the number of metrics per device isn’t straightforward, which is why we also benchmarked a second Sorted Set approach. In “Sorted Set per Metric,” we kept each metric in its own Sorted Set and had 10 sorted sets per device. We logged values in the format “<timestamp>:<metric>”.Another alternative approach would be to normalize the data by creating a hash with a unique key to track all measurements for a given device for a given timestamp. This key would then be the value in the sorted set. However, having to access many hashes to read a time series would come at a huge cost during read time, so we abandoned this path.In RedisTimeSeries, each time series holds a single metric. We chose this design to maintain the Redis principle that a larger number of small keys is better than a fewer number of large keys.It is important to note that our benchmark did not utilize RedisTimeSeries’ out-of-the-box secondary indexing capabilities. The module keeps a partial secondary index in each shard, and since the index inherits the same hash-slot of the key it indices, it is always hosted on the same shard. This approach would make the setup for native data structures even more complex to model, so for the sake of simplicity, we decided not to include it in our benchmarks. Additionally, while Redis Enterprise can use the proxy to fan out requests for commands like TS.MGET and TS.MRANGE to all the shards and aggregate the results, we chose not to exploit this advantage in the benchmark either.For the data ingestion part of our benchmark, we compared the four approaches by measuring how many devices’ data we could ingest per second. Our client side had 8 worker threads with 50 connections each, and a pipeline of 50 commands per request.Table 1: Ingestion details of each approachAll our ingestion operations were executed at sub-millisecond latency and, although both used the same Rax data structure, the RedisTimeSeries approach has slightly higher throughput than Redis Streams.As can be seen, the two approaches of using Sorted Sets yield very different throughput. This shows the value of always prototyping an approach against a specific use case. As we will see on query performance, the Sorted Set per Device comes with improved write throughput but at the expense of query performance. It’s a trade off between ingestion, query performance and flexibility (remember the data modeling remark we made earlier) for your use case.The read query we used in this benchmark queried a single time series and aggregated it in one-hour time buckets by keeping the maximum observed CPU percentage in each bucket. The time range we considered in the query was exactly one hour, so a single maximum value was returned. For RedisTimeSeries, this is out of the box functionality (as discussed earlier).TS.RANGE cpu_usage_user{1340993056} 1451606390000 1451609990000 AGGREGATION max 3600000For the Redis Streams and Sorted Sets approaches, we created the following LUA scripts. The client once again had 8 threads and 50 connections each. Since we executed the same query, only a single shard was hit, and in all four cases this shard maxed out at 100% CPU.This is where you can see the real power of having dedicated data structure for a given use case with a toolbox that runs alongside it. RedisTimeSeries just blows all other approaches out of the water, and is the only one to achieve sub-millisecond response times.In both the Redis Streams and Sorted Set approaches, the samples were kept as a string, while in RedisTimeSeries it was a double. In this specific data set, we chose a CPU measurement with rounded integer values between 0-100, which thus consumes two bytes of memory as a string. In RedisTimeSeries, however, each metric had 64-bit precision.RedisTimeSeries can be seen to dramatically reduce the memory consumption when compared against both Sorted Set approaches. Given the unbounded nature of time series data, this is typically a critical criteria to evaluate – the overall data set size that needs to be retained in memory. Redis Streams reduces the memory consumption further but would be equal or higher than RedisTimeSeries when more digits for a higher precision would be required.When choosing your approach, you need to understand the ingestion rate, query workload, overall data set size and memory footprint of your time series use case. There are several approaches for modeling time series data in Redis as we have seen, each with different characteristics. RedisTimeSeries provides a new approach that treats time as a first class citizen and comes with an out-of-the-box time series toolkit as described earlier. It combines efficient memory usage with extraordinary query performance, with a small overhead during ingestion. This turns the desire for real-time analytics of time series data into a reality.We are happy with what we achieved in our 1.0 GA version of RedisTimeSeries, but this is only the beginning. We would love to hear your feedback, so we can incorporate it into our roadmap. In the meantime, here’s the list of things we’re planning next:We strongly believe that all Redis users with time series use cases will benefit from using RedisTimeSeries. If you’re still not convinced and want to try it out yourself, here’s a quickstart guide."
428,https://redis.com/blog/redis-enterprise-proxy/,The First Rule of Redis Enterprise Proxy: Developers Don’t Need to Know About Redis Enterprise Proxy,"May 25, 2023",Filipe Oliveira and Yoav Peled,"A lot happens behind the scenes of a Redis Enterprise cluster. The proxy masks all that activity from the database clients.Most developers start small when they build an application, using a simple Redis open source (Redis OSS) database, even when they know the software will become part of a complex system. In the beginning, using the database is pretty straightforward. It has a single endpoint, to which the application connects and starts sending requests. That’s about it.The challenge starts when the Redis application needs more, such as scaling and high availability. You can use Redis OSS Cluster and Redis Sentinel for that purpose. However, it requires the developer to maintain the database topology and handle the practicalities of scaling. In other words, you have to write more code. At the enterprise level, this can quickly become complex.Redis Enterprise addresses those complexity issues by removing extra work. Whether you start at the enterprise level or migrate from Redis OSS, we designed it to excel at a large scale while keeping it simple for applications to use the database.In this post, we cast light on Redis Enterprise Proxy. We show common Redis cluster scenario examples, demonstrating how the proxy mitigates topology changes. Finally, we share benchmark numbers that demonstrate the proxy’s efficiency.Redis Enterprise Proxy is an entity with negligible latency that mediates between applications and the database. It exposes the database endpoint to database clients while masking behind-the-scenes activities that the Redis Enterprise cluster performs. This allows developers to focus on how an application is using the data, instead of worrying about frequent changes in database topology.The proxy employs a multi-threaded architecture. It can easily scale up by using more available cores. It is designed to cope with high traffic by using multiplexing and pipelining. When thousands of clients are connected to Redis Enterprise simultaneously, the proxy consolidates all of the incoming requests into a set of inner pipelines and distributes them to the relevant database shard. Net result: Requests are processed much faster, allowing high throughput with low latency.What’s all this mean in practical terms? Let’s look at a few common cluster-level scenarios that result in topology changes. We show how such changes remain hidden behind the proxy, which keeps exposing the same database endpoint to users as before. From the developer’s perspective – yours – this means less coding and a smooth migration from Redis OSS to Redis Enterprise.Whenever a database shard reaches a certain (predefined) size, Redis Enterprise can scale it. Scaling is accomplished by launching a new Redis instance and moving half of the hash slots from the original shard to the new shard. This allows the throughput and performance of the database to increase linearly.There are two ways to scale a database in Redis Enterprise:Figure 2 shows an example of a single-shard database being scaled–up into a two-shard database. On the left side (before scaling), you can see a single node containing the single shard. On the right side (after scaling is complete), the database is re-sharded. Now Shard 1 and Shard 2 are located in the same node, each holding half of the hash slots.Does scaling-up change the way clients connect to the database? No, it doesn’t. Clients continue to send requests to the same database endpoint as before, letting the proxy take care of forwarding each request to the appropriate shard.Note that this is different from the Redis OSS cluster, in which the clients connect to each shard separately, and thus must be aware of the cluster topology.In contrast, consider what happens when we scale-out a database while using a multi-proxy policy. In this situation, we have multiple proxies running behind the same endpoint.(Note that with Redis Enterprise you can also scale-out a database while using the OSS Cluster API. In that case, however, each proxy has its own endpoint.)Figure 3 shows an example of a two-shard database being scaled-out into a four-shard database. A new node is added to the cluster on the left side, containing a still inactive proxy. After the scale-out is complete, Shard 1 and Shard 2 are located in Node 1, and Shard 3 and Shard 4 are located in Node 2. Both nodes now contain active proxies.However, scaling-out does not change the way clients connect to the database, as these changes are all transparent to the clients. The databases continue to send requests to the same database endpoint as before. The proxy that handles each request forward those requests to the relevant shard.A key aspect of Redis Enterprise’s high availability is automatic failover, which relies on data replication. When a failure is detected within a Redis Enterprise cluster – whether it’s a database shard outage or an entire node failing – the cluster is designed to self-heal in a matter of seconds.The healing process is performed by the cluster manager, and it usually requires database topology changes inside the cluster. The proxy is notified and adjusted according to the new topology.From the database clients’ perspective, nothing changes. Clients continue to use the same database endpoint as before since the topology changes are internal and are hidden behind the proxy.Let’s take a look at two failover examples.On the left side of Figure 4 is a master shard in Node 1, and its replica is in Node 2. The proxy sends all client requests to the master shard, which continuously synchronizes data changes with its replica. So far, so good. But what happens when things go wrong?In case of a master shard failure, the Redis Enterprise cluster manager promotes the replica shard to become the master shard. The proxy now redirects incoming requests to the new master shard, letting the clients continue as usual. The last step is to create a new replica shard (shown on the right side of Figure 4).In this example, the entire node fails, which includes both the master shard and the proxy. Database clients are disconnected.However, once the Redis Enterprise cluster manager completes the failover process, clients reconnect to the same database endpoint as before and continue as usual. From the developers’ and operations point of view, there is no need to make any changes, because the cluster failover mechanism assigns the same endpoint to a different proxy.Figure 5 illustrates the process when Node 1 fails. The proxy of Node 2 becomes active and Redis Enterprise promotes the replica to become the master. The database is now available again, so clients can reconnect without being aware of this topology change. The cluster manager also finds a healthy node (Node 3), in which Redis Enterprise creates a new replica shard.The proxy certainly simplifies things for database clients. But how fast does it happen? To examine its efficiency, let’s see some benchmark numbers.To benchmark latency, we used a single endpoint Redis Enterprise Cloud cluster. We executed a common scenario containing a mixture of 20% SET (write) and 80% GET (read) commands.We created one database with a memory limit of 5GB and we chose five throughput targets: 50K, 100K, 200K, 400K, and 800K operations per second (ops/sec). For each configuration, Redis Enterprise Cloud selects the appropriate cloud instance to use, making sure that the cluster has sufficient resources at a minimal cost.The following results demonstrate just how fast Redis Enterprise is. The benchmark maintains sub-millisecond median (p50) latency for all target throughputs. In some cases, it achieves sub-millisecond p99 latency.We at Redis believe in the power of simplicity. That’s why we designed Redis Enterprise as the best option when moving from Redis OSS to the enterprise level.Try Redis Enterprise Cloud or download the latest Redis Enterprise Software to start a free trial."
429,https://redis.com/blog/ringing-new-year-new-office-london/,Ringing in the New Year with a New Office in London,"January 9, 2017",David Maitland,"On New Year’s Eve, I was lucky enough to ring in 2017 from the top of Tower 42, one of the best spots in London to watch the city’s famed fireworks show. Even more exciting was the fact that, at the end of the traditional midnight countdown, this landmark London skyscraper officially became home to Redis’ newest global office!The opening of our brand new European headquarters in the heart of London caps off a phenomenal 2016 for Redis, in which we saw significant revenue, headcount, and customer growth. All this was enough to propel us into the position of fastest growing NoSQL provider in the marketplace.To support this unprecedented growth—and set the stage for an even more successful 2017—Redis found itself with an urgent need to extend its global footprint. London was chosen as the natural new office location not only because it makes an ideal geographic complement to current Redis’ offices, but also because this bustling capital is teeming with opportunity and energy. London is a major business and financial centre, a hub for trans- and intercontinental travel, and one of the most cosmopolitan cities in the world.For all the reasons above, I can’t think of a more strategic (and fun!) location for supporting the rapidly-growing number of enterprise Redis installations we’re deploying across Europe and the entire EMEA region as a whole. And if we manage to attract even more members to what is already one one of the world’s largest open source developer communities, all the better.As Redis’ vice president of sales for the EMEA region and someone who is beyond excited to be based out of our new European headquarters, I’d like to extend a formal invitation. If you’re passing through London on business—or holiday—and you think your organization’s modern applications can benefit from the world’s most powerful NoSQL database (trust me, they can), look us up!In the meantime, happy New Year and cheers to 2017!"
430,https://redis.com/blog/run-ai-model-close-possible-data/,Run Your AI Model as Close as Possible to Your Data,"April 2, 2019",Pieter Cailliau and Luca Antiga,"Today we are happy to announce a preview version of RedisAI, in collaboration with [tensor]werk. RedisAI is a Redis module for serving tensors and executing deep learning models. In this blog post, we will walk you through the capabilities of this new module, what it enables and why we believe it’s a game changer for machine learning and deep learning (ML/DL) models.RedisAI came to light for two core reasons. First, because moving your data to a host that executes your artificial intelligence (AI) model is expensive, and every millisecond counts in today’s instant experiences. Secondly, because serving models have historically been a DevOps challenge. We built RedisAI so you can run your model where the data lives, and easily serve, update and ensemble your models from multiple back ends inside Redis.To illustrate why we believe colocality (running your ML/DL model where your data lives) matters, let’s consider a chatbot application example. Chatbots typically use recurrent neural networks (RNN), often arranged in seq2seq architectures to present an answer to an input sentence. More advanced models preserve the context of the conversation in the form of a numerical intermediate state tensor, using two input tensors and two output tensors. As an input, the model takes the latest message by the user, and an intermediate state representing the history of the conversation. Its output is a response to the message and the new intermediate state.This intermediate state must be kept in a database to support user-specific interaction, just like a session, so Redis is a great choice here. The chatbot’s model could have been deployed in Spark, wrapped in a Flask application or any other DIY solution. Upon receiving a chat request from a user, the serving application needs to fetch the intermediate state from Redis. Since there is no native data type in Redis for a tensor, the database would have to first do some deserialization, and after the RNN model ran, it would have to make sure the latest intermediate state was serialized and sent back to Redis.Given the time complexity of a RNN, the wasted CPU cycles on serializing/deserializing and the expensive network overhead, we knew we needed a better solution that could ensure a great user experience.With RedisAI, we’re introducing a new data type called a Tensor. With a set of simple commands, you can get and set Tensors from your favorite client. We’re also introducing two more data types, Models and Scripts, for model runtime features.Models are set with information about which device they should run on (CPU or GPU) and backend-specific parameters. RedisAI has several integrated backends, such as TensorFlow and Pytorch, and we are working to support ONNXRuntime soon. This runtime for ONNX and ONNX-ML adds support for “traditional” machine learning models. What’s nice, however, is that the command for executing a Model is agnostic of its backend:This allows you to decouple your backend choice (a decision typically made by data scientists), from the application services using these Models to provide functionality. Switching Models is as easy as setting a new key into Redis. RedisAI manages all requests to run Models in processing queues and executes them on separate threads, while Redis stays responsive to all other commands.Scripts can run both on CPUs and GPUs, and allow you to manipulate Tensors via TorchScript, a Python-like Domain Specific Language for Tensor operations. This lets you pre-process your input data before you execute your Model, and post-process the results, e.g. for ensembling different Models to improve performance.One more great feature is the ability to run several commands via a directed acyclic graph (DAG) command, which we’ll be adding to RedisAI in the near future. This will allow you to combine several RedisAI commands in one atomic operation, such as running multiple instances of a Model on different devices and ensembling the results by averaging predictions with a script. Using the DAG engine, computations are executed in parallel and then joined. For a full and more in-depth feature list, visit redisai.io.The new architecture could be simplified like this:Taking code straight out of your Jupyter notebooks and placing it in a flask app might not provide the best guarantees in production. How can you be sure you’re using your resources optimally? What happens with the intermediate state of our chatbot example when your host goes down? You might end up reinventing the wheel of existing Redis functionality.On the other hand, dealing with very opinionated solutions might be challenging, as they tend to be less composable than you’d expect.The goal of RedisAI is to bring the Redis philosophy to the task of serving AI models. Since we’re implementing Tensors, Models and Scripts as full-blown Redis data structures, they inherit all the enterprise-grade features of Redis. If you need to scale model serving, you can simply scale your Redis cluster. You can also use Redis Enterprise’s high availability features to make sure your Model and Tensors are always available. Since Redis scales easily, you can add as many Models as you want or need to run in production, lowering your infrastructure cost and total cost of ownership.Lastly, RedisAI fits perfectly in the existing Redis ecosystem, allowing you to do everything from reusing Streams for input data and classified output data streams, using RedisGears to make correct translations between data structures, to adopting RedisGraph to keep your ontology up-to-date and more.In the short term, we want to make RedisAI generally available including its support for the three main ML/DL backends (Tensorflow, Pytorch and ONNXRuntime). In our next phase, we would like those backends to be loaded dynamically, which will allow you to load only the specific backends you need for the devices you want to run them on. For example, this would allow you to use Tensorflow Lite for your Edge use cases.Another feature we plan to implement in the near future is called autobatching, which will look into queues and automatically consolidate calls to the same Models to squeeze the last bits of efficiency. Furthermore, RedisAI will also expose metrics about how well your Models are behaving, and we’ll look into the DAG feature explained earlier.We’re looking forward to hearing all your feedback, so please get in touch or comment on github."
431,https://redis.com/blog/running-machine-learning-data-store-redis-labs/,Running a Machine Learning Data Store on Redis Labs,"April 6, 2017",Jay Johnson,"Managing large, pre-trained predictive models across an organization and ensuring the same version is in production can be a challenge with the rapid pace of changes in the AI/machine learning space. Here, we have an approach that demonstrates how to automate building, storing, and deploying predictive models from a Remote Machine Learning Data Store hosted on Redis. This approach is focused on showing how DevOps CI/CD artifact pipelines can be used to build and manage machine learning model artifacts with Jupyter IPython notebooks, accompanying command line automation versions, and administration tools to help manage artifacts across a team. By utilizing DevOps for your machine learning build workflows you can easily manage model deployments across intelligent environments.In general, machine learning workflows share these common steps to create a predictive model:1. Define a dataset2. Slice the dataset up into train and test sets3. Build your machine learning algorithm model4. Train the model5. Test the modelWe wanted to share how to automate these common steps within a machine learning pipeline under a server API that creates model artifacts on completion. Artifacts are dictionaries containing the models’ analysis, accuracy, predictions, and binary model objects. Once the artifact is created, it can be compressed as a pickle serialized object and uploaded to a configurable S3 location or in another persistent storage location. This post is also a demonstration for designing a machine learning API with a pseudo-factory to abstract how each step works and the underlying machine learning model implementation. This approach lets a team focus on improving model predictive accuracy, improving a dataset’s features, sharing models across an organization, helps with model evaluation, and deploying pre-trained models to new environments for automation and live intelligent service layers that need to make predictions or forecasts in real-time.Here is the workflow for using a machine learning data store powered by Redis and an S3 artifact backbone:This workflow is built to help find highly predictive models because it uses an API that can scale out expensive tasks (like building, learning, training and testing models) and natively manages machine learning models with Redis caching with an S3 backbone for archiving. Just like DevOps in the enterprise software world, automating build workflows enables your organization to focus on stuff that matters like: finding the most predictive models, defining quality datasets, and testing newly engineered features.To continue reading the Jay Johnson’s complete blog, please visit the Levvel site."
432,https://redis.com/blog/search-benchmarking-redisearch-vs-elasticsearch/,Search Benchmarking:  RediSearch vs. Elasticsearch,"April 18, 2019",Redis,"Click to learn more about RediSearch: RediSearch: A High Performance Search Engine as a Redis Module white paperRediSearch is a distributed full-text search and aggregation engine built as a module on top of Redis. It enables  users to execute complex search queries on their Redis dataset in an extremely fast manner. The unique architecture of RediSearch, which was written in C and built from the ground up on optimized data structures, makes it a true alternative to other search engines in the market. It works great as a standalone search engine for indexing and for retrieval of searchable data.When we first launched RediSearch, we benchmarked it against popular search engines like Elasticsearch and Solr to test how powerful the engine is. This time, we decided to try a slightly different benchmark in order to (a) give you a clear, reproducible setup, with all search engines optimized to provide their best performance and (b) simulate multiple real life scenarios based on what we see from our RediSearch users.In this Search benchmark, we compared RediSearch to Elasticsearch over two use cases:We first indexed 5.6 million docs (5.3GB) from Wikipedia and then performed two-word search queries over the indexed dataset.As you can see in the figure below, RediSearch built its index in 221 seconds versus 349 seconds for Elasticsearch, or 58% faster.Once the dataset was indexed, we launched two-word search queries using 32 clients running on a dedicated load-generator server. As you can see in the figure below, RediSearch throughput reached 12.5K ops/sec compared to 3.1K ops/sec with Elasticsearch, or x4 faster. Furthermore, RediSearch  latency was slightly better, at 8msec on average compared to 10msec with Elasticsearch.Here, we simulated a multi-tenant e-commerce application where each tenant represented a product category and maintained its own index. For this benchmark, we built 50K indices (or products), which each stored up to 500 documents (or items), for a total of 25 million docs. RediSearch built the indices in just 201 seconds, while running an average of 125K indices/sec. However, Elasticsearch crashed after 921 indices and clearly was not designed to cope with this load.We benchmarked RediSearch and Elasticsearch for the following use cases:Elasticsearch is a great feature-rich search product created by the great people at Elastic.co, but when it comes to performance, it has inherent architecture deficiencies, as summarized by the table below:Read more about RediSearch here and the technology behind it. To get started with RediSearch – try our Redis Cloud Pro here or download Redis Enterprise Software here.Following feedback from readers we updated the reference to the wikipedia dataset and added a link to the benchmark source code for reproduction purposes. We would be happy to get more feedback if any."
433,https://redis.com/blog/see-austin-oscon-2017/,See you in Austin for OSCON 2017,"May 2, 2017",Tague Griffith,"The Redis team is getting ready to head out to this year’s Open Source Convention (OSCON) and the Community Leadership Summit which precedes it.  OSCON runs from Monday, May 8th through Thursday the 11th, with the leadership summit on May 6th and 7th.Stop by booth #332 and meet with our experts about using Redis to solve a variety of technical challenges in personalization, fraud detection, IoT, metering, and social apps.We have organized two  Birds of a Feather (BoF) sessions.  On Monday, May 8th from 7:00 – 8:00 PM in Meeting Room 9 we will be talking about using Redis in domains outside of caching and key-value storage.  Our second BoF session is on Wednesday, May 10th from 7:00 – 8:00 PM in Meeting Room 9C.  We will cover the new module system that is a part of the upcoming Redis 4.0 release.  We hope you can join us and add to the conversation.See you in Austin!"
434,https://redis.com/blog/session-store-patterns/,Session Store Patterns,"March 12, 2019",Redis,"On a typical day, you might open your web browser and rapidly access social media, do some shopping, newspapers and more. You likely have numerous notifications from various events you may or may not actually attend, and you might expect your preferred shopping site to tailor search results to your order history. Perhaps your media platform of choice presents you with topics in which you’ve previously demonstrated an interest, in order to encourage you to continue to consume. All of these phenomena are indicative of an intelligent session store — that is, a session store that stores data beyond your username or basic preferences.When traffic to any given server increases, a session store is often the ideal mechanism to hold user data. But in order for a session store to be able to store and process intelligent — or more complex — data, it might require the assistance of a microservice. By decoupling session storage from the server, microservices enable companies to handle more complex use cases and process more advanced data without compromising on speed or performance.Microservices can make it possible for social media applications to send you a group notification, for example, or for an eCommerce site to recommend products for you based on previous purchases you’ve made. And if your company would like to use microservices in this way — in order to facilitate advanced data processing and more effective use of infrastructure— Redis is the ideal choice. By serving as both a database and a transport mechanism, Redis allows companies to add on a limitless number of servers, and makes it possible for a microservice and a server to communicate with one another.When you compare Redis to other popular databases, you’ll find that Redis has true highly availability, and is able to perform more writes and reads at much lower latencies. Perhaps more importantly, a Redis powered session store microservice is able to implement a patterns that optimize user experience: content surfacing, activity pattern monitoring and group notifications with minimal resourcesFor news sites and e-commerce platforms, it is of the utmost importance that users are consistently shown new and personalized content. This can be complicated to execute, as it would involve storing every piece of content a user has ever interacted with. However, Redis surmounts these challenges by using a session store contained Bloom filter to cross-check whether or not a certain piece of content is new to a particular user. Bloom filters are incapable of returning false negatives, so content that does not appear in a certain user’s Bloom filter is guaranteed to be “fresh” content for that user, yet they don’t need to store each itemActivity pattern monitoring, meanwhile, is one of the best tools in a company’s arsenal to ensure that their clients’ experience on a site is expertly personalized. Redis’ microservice can help your company collect users’ behavioral data, and then use that data to better customize any given user’s experience. This can be a difficult process, especially given the limitations of a microservice, but Redis makes it possible by recording site activity with bit counting, tracking the unique pages a user visits in a HyperLogLog and combining these data points with specific insights, such as the number of times a user has interacted with a particular page or whether or not a user is new to a site. And because Redis employs a HyperLogLog, it is nearly impossible for third parties to extract user information, which can help reassure users about their privacy.Redis Enterprise can also improve a website’s performance by making group notifications much more efficient. By storing group notifications in a simple List data structure and creating a Bloom filter for each user in their Redis-backed session, Redis has created a very lightweight process for sending group notifications and assessing when they have been read and received. Redis Enterprise is particularly well-designed for this operation because it enables persistence through in-memory replication.In order to create a site that is engaging to each and every user, it is important to go beyond the basics. A session store is a useful way to store user data, but microservices backed by Redis can help your company excel by processing intelligent data and making sure that your user retention rate is high. If you have any questions about using microservices to make your session store more intelligent, or you’re curious about the patterns and modules that Redis offers, please don’t hesitate to reach out to us at product@redis.com."
435,https://redis.com/blog/set-command-strange-beast/,The SET command is a strange beast,"May 13, 2019",Redis,"The Helicoprion is a now extinct but strange animal that roamed the seas of the early Permian. It looks more or less similar in both size and shape to a contemporary Great White Shark. Most likely, it was a formidable predatory of the seas. The thing that set it apart was that it had a “Tooth-whorl,” which is somewhat akin to having a shark-toothed circular saw located inside the lower jaw. Seems like it would be a good idea, but evolution had a different idea and we don’t have any existent animals like that. An evolutionary dead end.In some ways the Redis SET command is like the Helicoprion, but it still roams the waters of Redis servers all over the globe. It is a very early command with some unusual features that seem like a good idea but can prove to be dangerous yet deeply useful when correctly used.On the other hand, the SET command seems to be as plain and ordinary as anything. We use it as one of the first commands when learning and we use it for doing a simple test to make sure Redis is working properly. I can’t tell you how many times I’ve typed the command:> SET foo barSo, that is nothing exotic on the surface. But is it hiding something?Coming back to our simple SET example. Let’s add some more context:> UNLINK foo
(integer) 1
> HSET foo bar 123
(integer) 1
> SET foo bar
OKDid you catch the weirdness here with SET? The existing key foo was of type hash (due to the HSET), but when I ran SET immediately afterwards it still accepted the command. This is actually deeply weird compared to other Redis commands. Let’s take the same commands but flip the order of the last two:> UNLINK foo
(integer) 1
> SET foo bar
OK
> HSET foo bar 123
(error) WRONGTYPE Operation against a key holding the wrong kind of valueYou can see that SET disregards the existence or type of key and always writes. Hashes, on the other hand, throw an error when confronted with a non-empty key of a different type. The same holds true for all data types except for Strings and specifically the SET command and a few variants (PSETEX, SETEX, MSET). Take this, for example:> HSET foo bar 123
(integer) 1
> APPEND foo bar
(error) WRONGTYPE Operation against a key holding the wrong kind of value
> INCR foo
(error) WRONGTYPE Operation against a key holding the wrong kind of value
> SETBIT foo 1 1
(error) WRONGTYPE Operation against a key holding the wrong kind of value
> BITFIELD foo SET u8 0 1
(error) WRONGTYPE Operation against a key holding the wrong kind of value
> INCRBY foo 1
(error) WRONGTYPE Operation against a key holding the wrong kind of value
> INCRBYFLOAT foo 1
(error) WRONGTYPE Operation against a key holding the wrong kind of value
> SETRANGE foo 1 barbar
(error) WRONGTYPE Operation against a key holding the wrong kind of valueSETNX and SET…NX (more on this later) are an interesting side note, they will SET if the key does not exist, returning a 1 if it was set and 0 if not. So, it doesn’t type check, but rather presence checks.All in all, SET just doesn’t care about types. It always writes, very little can stand in its path.If you’ve been around the block in Redis a few times you know that you can retrieve the type of data stored at a key by using the TYPE command. So, for example, let’s return to foo:> SET foo bar
OK
> TYPE foo
stringEasy enough. You set the value “bar” at the key foo. Now, let’s see something else:> SET foo 1234
OK
> TYPE foo
String
> GETRANGE foo 2 3
""34""So, you might think that the number 1234 is being stored as characters, and you’d be more/less correct. However, there is more to this story:> INCR foo
(integer) 1235
> GETRANGE foo 2 3
""35""This illustrates that Redis understands characters as text and numbers – you can think of it as a form of loose typing. But it gets weird:> SET foo ""hello world""
OK
> INCR foo
(error) ERR value is not an integer or out of rangeObviously, Redis cannot increment a non-number. But there are more details to cover. Redis also understands float values. Take this example:> SET foo 1.2
OK
> INCR foo
(error) ERR value is not an integer or out of range
> INCRBYFLOAT foo 0.8
""2""
> INCR foo
(integer) 3You can see the initial value is put in as a float and therefore the INCR (which is for integers) would not work. However, INCRBYFLOAT does work. This changes the value to a whole number that can be used by the previous not allowed INCR command.The other thing that is distinctive about the command is the ability to supply two categories of optional arguments: one category for expiration and the other for existence checking. Let’s take a look at the first category: expiration arguments.For most commands, if you want to expire a key immediately, you need to issue the EXPIRE or PEXPIRE immediately afterwards, most often in a MULTI / EXEC transaction. For example:> MULTI
OK
> SADD baz alpha beta gamma
QUEUED
> EXPIRE baz 10
QUEUED
> EXEC
1) (integer) 3
2) (integer) 1This ensures that you cannot be interrupted between your SADD and your EXPIRE command. You know that immediately after the EXEC you’ll have a set that expires in 10 seconds. With SET, however, you can do this without a transaction.> SET foo bar EX 10
OKAlternately, you can use PX instead of EX to expire in units of milliseconds instead of seconds. It’s a handy shorthand that can also be expressed with SETEX and PSETEX. I think of these commands as shortcuts only – they exchange a few keystrokes in your application and a few bytes to and from the server for a little less readability and flexibility.The other category of arguments, NX / XX, controls how SET works with existent or non-existent data. The NX key sets a value only if the key does not exist. So, take this example:> UNLINK foo
(integer) 0
> SET foo 1234 NX
OK
> GET foo
""1234""
> SET foo 5678 NX
(nil)
> GET foo
""1234""You can see that the 4th command actually doesn’t do anything because the key foo already exists. This has numerous uses: setting default values and without overwriting existing data, preventing accidental SETS when user input is part of a key, etc.The inverse of this is the XX command. This only sets a value when the key already exists.> UNLINK foo
(integer) 1
> SET foo 1234 XX
(nil)
> set foo 1234
OK
> SET foo 5678 XX
OKThis can be used to confine writes to expressly defined keys. One thing this doesn’t do is type check. So XX will still overwrite a key of another type, as long as it exists.Absolutely not. SET is fundamental to the operation of many excellent patterns in Redis. However, it has a number of features that are fundamentally different from the rest of Redis. It is important to know how these features work in order to make proper assumptions about how to structure your keyspace and operate Redis in your application."
436,https://redis.com/blog/strength-in-numbers-delivering-a-high-value-ecosystem-to-our-customers/,Strength in Numbers: Delivering a High Value Ecosystem to our Customers,"November 1, 2016",Rod Hamlin,"Consumers today expect – actually demand – information and services in near real time, at their fingertips. We have become a smartphone society. Get a ride, find a last minute hotel, book a table for dinner, get directions, have lunch delivered all in minutes, without doing anything more than tapping the screen on your phone.The shift in consumer behavior that has come with the availability and popularity of these new services is the new norm. Applications and companies that provide these services today are under tremendous pressure to stay competitive. This forces them to constantly differentiate and add better content or services, deliver it faster, and at lower prices. And while the quantity of information is exploding exponentially (making it harder for apps to sift through all the data), consumers’ patience for waiting is dropping dramatically.This is why digital economy companies today are turning to next-generation data solutions to meet user expectations. Older, relational databases are no longer equipped to handle the data ingestion volumes and sub-millisecond response times demanded by these new applications. With performance expectations increasing and the price of memory dropping, in-memory solutions such as Redis are rapidly becoming the databases of choice for today’s massively popular applications and services.At Redis, Salvatore Sanfilippo, the creator of Redis (and the company’s lead for open source development), steers the technology’s evolution together with a highly talented, dedicated team of Redis engineers. In order to adapt Redis for enterprise use, Redis developed an additional technology layer that encapsulates the open source software and provides an enhanced deployment architecture for Redis, while supporting the open source API.Along with growing a strong developer community, Redis has worked hard to create a wide and robust partner ecosystem. Global leaders from across several categories have partnered with Redis:Redis works closely with global SIs such as Accenture and Wipro, as well as smaller SIs like Levvel, Datafactz, Systech and Avalon who are well known experts on big data and NoSQL. We are bringing hardware-optimized solutions to market with IBM, Intel and Samsung. Thousands of Redis’ customers are successfully deployed today on popular public Clouds, including AWS, Azure, Google Cloud Platform and IBM SoftLayer. And customers can deploy the Redis enterprise solution on Cloud Foundry though our global partnership with Pivotal. Companies can enjoy Spark optimization and acceleration through our partnership with Databricks. Clearly, the ecosystem is robust, and growing every day.And now through Redis Modules, Redis has opened up the ecosystem even wider. Redis Modules are add-ons which extend Redis to cover most of the popular use cases for any industry. They seamlessly plug into open source Redis or enterprise-class Redis, are processed in-memory and enjoy Redis’ simplicity, high performance, infinite scalability and high availability. Modules can be created by anyone, and the Module Hub marketplace already includes modules created by Redis as well as others. All modules in the marketplace (open source or commercial) are certified by Redis for use with open source Redis, Redis Enterprise Cluster (RLEC) or Redis Cloud.Apple popularized this revolutionary approach of allowing external developers to build apps when the company opened its app store for iOS. This approach has been so successful that most vendors are moving to a similar model in order to keep up. Enterprises today seek out vendors and solutions that embrace this open ecosystem approach. It is no longer considered acceptable to be tied down to a heavy, expensive, inflexible contract and relationship with your technology provider.Redis is committed to customer success and we believe this collaborative ecosystem approach creates a win-win situation for our customers. New customers are able to leverage the entire ecosystem, getting the very best from many, and each new partner attracts more customers to the ecosystem. As customer, vendor and partner interactions and relationships grow, the entire ecosystem becomes increasingly more valuable for all members.Redis is working hard to create a community where partners, developers and customers draw value from each other – and we look forward to having you join with us on this journey.rod@redis.com@rodhamlin"
437,https://redis.com/blog/submit-successful-session-redis-day-new-york/,How to Submit a Successful Session for Redis Day New York,"April 25, 2019",Redis,"Redis Day New York is fast approaching (June 27th – get your tickets NOW). This will be a new event for New York, but not a new event for us. We’ve done it three times in Tel Aviv, once in London and now we’re adding New York. The Redis Day format is fairly unique and very different from RedisConf. At RedisDay, we have a single track, short talks and (generally) we don’t have a public Q&A time per session. What does that mean? A few things:So, I’m sure I’ve convinced you. Now how do you convince us? Well, we’re looking for talks that are about Redis in some way. I know that might seem obvious, but judging from submissions I’ve reviewed in the past, not everyone picks up on that. Beyond that here are a few pointers:From a practical standpoint, submit as early as possible. Now, this might surprise you but humans are really excellent at procrastination. To the point that if we say it’s due on May 10th, then we will get a flood on May 9th. I know everyone is busy and working towards more timelines that toes, but the truth of the matter is that we pour over the first few submissions, but those that come last minute are often given less review. Why? A couple of reasons. One, we end up accepting some talks early if we’re sure we want to have the talk. If you submit last minute then, literally, you have less of a chance just by nature of the number of slots free and occasionally we have to make tough decisions. The other reason is that we don’t have as much time to review the flood of entries that come in at the last minute.Now that you know why you should submit your talk for Redis Day New York, how do you actually submit it? Just head on over to the Redis Day New York CfP page and fill out the form. I would estimate this would take you less than 30 minutes as it’s just a handful of questions. If you’ve used Sessionize before to submit a talk anywhere, it’s even faster as it fills out your non-event specific details. What are you waiting for? Get CfPing!"
438,https://redis.com/blog/surfing-wave-redis-labs/,Surfing the Wave with Redis Labs,"March 9, 2017",Roshan Kumar,"The Forrester Wave™: In-Memory Databases, Q1 2017 report was published last week.  Redis received the highest ratings for key criteria such as performance, analytics support, and use cases.If you compare the current 2017 Wave with the previous report published in 2015, you will notice that in-memory databases have grown in importance! The 2017 report reads, “An in-memory database is not just a nice-to-have option anymore – it has become critical to support next-generation transactions, analytics, and operational insights.” The 2017 Wave also clarifies and expands the definition of an “in-memory database” as:The updated definition highlights how the in-memory database category is maturing in its offerings, expanding its use cases to handle mission critical data, and supporting cloud adoption.Although this is the first year Redis has been featured in the Forrester Wave, it was positioned ahead of all other pure-play vendors in the report!Though the vendor inclusion criterion calls for “a standalone in-memory database solution,” the definition calls for “not associating the database with an application” A more appropriate restriction would have been: “Only those databases that can be run standalone in-memory.” This would have eliminated some low-performing disk-based databases that use RAM as nothing more than cache.Forrester assigns significant weighting for “professional services” and “support” when rating vendors. Industry trends are moving in the opposite direction, however; customers are demanding technologies that are easy to implement and require limited professional support. We believe that such assessments should favor solutions that are forward looking, employ cloud-first architecture, and are designed to be “zero touch.”It’s notable that the Forrester Wave appreciates Open Source as a strategy to succeed. Open Source indeed adds a lot of value to its products, while broadening adoption, expanding the talent pool, and harnessing the innovation of the developer community.The 2017 Forrester Wave report confirms what our 6700+ customers and 60,000+ registered users already know: Redis is the leading provider of in-memory database technology, worldwide.  In fact, of all the vendors in the Wave’s popular DB-Engines Top Ten Solutions list, Redis is the only pure in-memory database provider. We are proud to offer Redis Enterprise – Redise, the most scalable, highly available, cost-effective offering of the Redis database, in every deployment scenario (whether in the cloud, on-premises, or hybrid).  We plan to build on our momentum and continue to deliver the best possible value to our customers. We carry on contributing  to the community,  expanding our use cases and spurring innovation through groundbreaking ideas such as Modules, Redise Flash, and more. The Redis Conf 2017 in May will showcase Redis uses, Redis experts and advanced Redis scenarios. You can register to attend the conference now, or submit a paper to conduct your own session at the conference!"
439,https://redis.com/blog/unveiling-new-redis-enterprise-cloud-ui/,Unveiling the New Redis Enterprise Cloud UI,"February 7, 2017",Aviad Abutbul,"I’m delighted to announce that we have commenced rollout of the new and improved user interface for Redise Cloud users.The gradual rollout process will be performed over the coming weeks, so some of you may still see our old interface for a little while—don’t you worry, we will get to you as well!This upgrade is the beginning of a series of upgrades that will align the user experiences across the entire Redise platform.With this new interface you get an enhanced and modernized user experience along with a huge aesthetic upgrade. We have simplified many of the user experiences with this new interface so you can expect to get things done much more quickly and easily.Under the hood you still get the same secure, highly available and fully-managed Redis, while enjoying the seamless, zero-downtime scaling and true high availability of Redise Cloud.Here’s a short video introduction to our new interface:https://youtube.com/watch?v=s0KgvjPUUBMIn addition to the new user interface, we have recently added support for payments through AWS SaaS Marketplace.This new integration simplifies the payment process for AWS users by consolidating all cloud-related service fees into one bill.Your feedback and opinion matters to us! Feel free to drop us a line with your feedback at pm.group@redis.com"
440,https://redis.com/blog/use-redis-kitura-server-side-swift-web-framework/,"How to Use Redis with Kitura, a Server-Side Swift Web Framework","March 28, 2019",Shabih Syed,"During a recent project, I needed to develop application services using varied technology stacks. One of my requirements was to pick a different programming language for each microservice in my application. While Java, Node and Python were easy choices, I wanted to try something new and obscure. During a conversation with my brother, who happens to be an active iOS developer, I learned for the first time about Swift and Kitura and decided to give it a try.Whenever I am experimenting with a new language, I try to use it with a database. In this example, I will show how easy it is to use Redis as a data store for Swift-based microservices.I’ll begin with a brief description of my technology stack, and then walk you through the steps to build an application with these tools:Swift is a general purpose, multi-paradigm, compiled programming language developed by Apple Inc. for iOS, macOS, watchOS, tvOS, Linux and z/OS.Kitura is a free open source web framework written in Swift, developed by IBM and licensed under Apache 2.0. It’s an HTTP server and web framework for writing Swift server applications.Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. With almost 1.5 billion docker pulls, it is one of the most popular NoSQL databases.First, download and install the latest version of Xcode from the App Store.Next, check if Swift is installed. In the terminal, type the ‘swift’ command and pass the ‘–version’ flag:swift --versionApple Swift version 4.2.1 (swiftlang-1000.0.42 clang-1000.10.45.1)
Target: x86_64-apple-darwin17.7.0If Swift is not installed, then run the following –install command:xcode-select --installInstalling the Kitura web server is pretty easy as well.1. Create a new directory to host your project.mkdir MyKituraApp && cd MyKituraApp2. Initialize the new directory as a Swift project.swift package init --type executableCreating executable package: MyKituraApp
Creating Package.swift
Creating README.md
Creating .gitignore
Creating Sources/
Creating Sources/MyKituraApp/main.swift
Creating Tests/
Creating Tests/LinuxMain.swift
Creating Tests/MyKituraAppTests/
Creating Tests/MyKituraAppTests/MyKituraAppTests.swift
Creating Tests/MyKituraAppTests/XCTestManifests.swift3. To add Kitura to your dependencies, edit `Package.swift`.Open `Package.swift` and edit it so it has the following text:4. Build the project to pull down your new dependency:swift buildGo under the sources folder and edit `main.swift` so it has the following text, which will initiate the Kitura web-server.5. Since we’ve added code to `main.swift`, you’ll need to recompile the project:swift build6. Now you are ready to run your Swift app.swift run7. Navigate to http://localhost:8080 in your browser, and it will return the following:I use Redis Cloud, a fully managed Redis database-as-a-service in this example. Creating a Redis instance with Redis Cloud is easy and free, and there are other options available as well (feel free to explore them in Get Started with Redis).1. Visit the Redis Get Started page, and click SIGN UP under the “Cloud Hosted” section. You will land at the following page:2. Login to create your subscription and select a free (30MB) Redis database.3. Name your database and activate it.4. Take note of your database endpoint and password.In this example, redis-15878.c91.us-east-1-3.ec2.cloud.redis.com is the URL of your Redis database and 15878 is the port.Kitura-Redis is a pure Swift client for interacting with a Redis database.1. To add Kitura-Redis to your dependencies, you’ll need to edit `Package.swift` again.Open `Package.swift` and edit it so it now has the following text:2. Now you’ll use Kitura-Redis to establish a connection with your Redis database* in the cloud and set a key called “Redis” with a value “On Swift”.This is a simple example, but you can, of course, use Redis in more complex ways.Go under your sources folder and edit `main.swift` so it has the following text:* Make sure to update , and with your Redis Cloud database configuration.3. Since you’ve added code to `main.swift`, you’ll need to recompile the project:swift build4. Now you are ready to run your Swift app and will see the following:swift runRedis Password AuthenticatedConnected to RedisRedis on Swift5. Using Redis CLI, you can check if the key value you set in ‘main.swift’ is successful.redis-15878.c91.us-east-1-3.ec2.cloud.redis.com:15878> Keys *
1) Redis
redis-15878.c91.us-east-1-3.ec2.cloud.redis.com:15878> GET Redis
on SwiftFor more information, visit the following pages:SwiftKituraIBM Swift/KituraIBM Swift/Kitura-RedisHappy Swifting with Redis!"
441,https://redis.com/blog/welcome-new-meetups/,Welcome New Meetups!,"July 28, 2017",Tague Griffith,"Meetups are a great opportunity for community members to learn more about unfamiliar features in Redis and new releases such as Redis 4.0.  Here at Redis we love to support burgeoning Redis community events and wanted to say welcome to the newest Redis meetups.SpainLast month, Carlos I. Peña launched the Barcelona Redis Meetup to bring local Redis enthusiasts together.  Although still in its early stages, the Barcelona Meetup has attracted 50 community members and is in the planning stages for their first meetup.Carlos is looking for other community members to help organize, host and speak at future meetups.  If you’re a Redis Geek in the Barcelona area, please join up and if you would be interested in helping with the meetup or speaking at a future event, please let Carlos know.FranceFrançois Cerbelle launched a set of meetups for French Redis Geeks across the country.  The Redis France Meetup is the main organizing vehicle for meetup activities, but François also wants to organize events for Redis enthusiasts outside of the Paris area.  He hopes to plan events in Lyon, Bordeaux, Lile and Toulouse initially.Like most meetups, the new France meetups are looking for speakers and hosts, so if you are available, please contact François and let him know.CanadaThis summer, Canada added its second meetup group.  The Toronto Redis Meetup launched in July to support the growing Redis community in Toronto.  The meetup is putting together a planning committee and looking for community members interested in organizing, hosting and speaking at future events.If you would like to find a local Redis meetup or users group, check out this list of world-wide Redis meetups on Meetup.com.  Organizers are always looking for speakers, so if you are interested in sharing your Redis experience, please volunteer."
442,https://redis.com/blog/what-to-choose-for-your-synchronous-and-asynchronous-communication-needs-redis-streams-redis-pub-sub-kafka-etc-best-approaches-synchronous-asynchronous-communication/,"What to Choose for Your Synchronous and Asynchronous Communication Needs—Redis Streams, Redis Pub/Sub, Kafka, etc.","May 3, 2019",Redis,"Let’s talk about communication tools and patterns. With the introduction of Streams in Redis, we now have another communication pattern to consider in addition to Redis Pub/Sub and other tools like Kafka and RabbitMQ. In this article, I will guide you through the defining characteristics of various communication patterns, and I’ll briefly introduce the most popular tools used to implement each. Finally, I’ll leave you with a small take-away that will hopefully help you build better solutions faster.In this context, synchronous means that all parties need to be active at the same time to be able to communicate. The simplest form is Service A and Service B doing direct remote procedure calls (RPC), by invoking Service B’s HTTP REST endpoint from Service A, for example. If Service B went offline, Service A would not be able to communicate with B, and so A would need to implement an internal failure recovery procedure, which most of the time means doing graceful degradation. For example, Netflix’s “watch next” section could display a random sample of shows if the recommendation service was unreachable.These services only do graceful degradation because for more sensitive use cases (e.g., a payment service asking an order service to start processing a paid order), other asynchronous mechanisms I’ll describe below are more common. And while the RPC paradigm works well for one-to-one communication, you will occasionally need to support one-to-many or many-to-many. At that point, you have two main options: brokerless or brokered tools.Brokerless means that participants are still connected directly but have the option of employing a different pattern than RPC. In this category, we have libraries such as ZeroMQ and the more recent nanoMsg. They are rightfully described as “TCP sockets on steroids.” In practice, you import the library into your code and use it to instantiate a connection that can employ various built-in message routing mechanisms, such as Pub/Sub, Push/Pull, Dealer/Router, etc.Brokered means that participants connect to the same service, which acts, as the name suggests, as a central broker to implement the whole message-routing mechanism. While this architecture is usually described as star-shaped, with the broker being the center of the star, the broker itself can be (and often is) a clustered system.In this category, Redis Pub/Sub stands alone as far as I know. You can still use tools with persistence like NATS or RabbitMQ for this use case, as they do allow you to turn off persistence, but the only pure synchronous messaging broker that I know of is Redis. The difference is not just in persistence, but in the general idea of reliable delivery (i.e., application level acks) vs. fire-and-forget. RabbitMQ defaults to the former behavior while Redis Pub/Sub focuses on just doing the bare minimum amount of work for fire-and-forget. As you can imagine, this has implications in performance (there’s no such thing as a free lunch after all), but reliable delivery does apply to a wider range of use cases.Since Streams was not available before Redis version 5, some people opted to use Pub/Sub in situations where they would have preferred better delivery guarantees, and are now making the switch. So, if you’re building a new application or unsatisfied with a current one that uses Pub/Sub, consider Redis Streams if what you need is “Pub/Sub, but with the ability to resume on disconnection without losing messages.”Brokerless tools are the fastest communication methodology you can think of, even faster than Redis Pub/Sub. They unfortunately cannot abstract away all complexity — such as the need for each participant to know the location of all others in order to connect to them, or complex failure scenarios that you don’t generally have to deal with in brokered systems (e.g., the case where a participant dies mid-fanout).The beauty of using Redis Pub/Sub, in this case, lies in not having to give up too much throughput and getting in return a simple, ubiquitous infrastructure with a small integration surface. You only need a Redis client for your language, and can use PUBLISH and (P)SUBSCRIBE to move messages around.Of course, asynchronous means communication can still happen even if not all participants are present at the same time. To enable this pattern, persisting messages is mandatory, otherwise there would be no way to guarantee delivery in the face of failures. Tools in this category mainly consist of queue-based or stream-based solutions.This is the “traditional” way of doing asynchronous communication and the base for most service oriented architectures (SOAs). The idea is that when a service needs to communicate with another, it leaves a message in a central system that the other service will pick up later. In practice, these message inboxes are like task queues.Another expectation for these systems is that tasks be independent from one another. This means that they can be (and almost always are) processed in parallel by multiple identical consumers, usually referred to as workers. This property also enables independent failure, which is a good feature for many workloads. As an example, being unable to process a payment from one user (maybe because of missing profile information or other trivial problems) would not stop the whole payment processing pipeline for all users.The most well-known tool in this category is RabbitMQ, followed by a plethora of other tools and cloud services that mostly speak AMQP (Rabbit’s native protocol) or MQTT (a similar open standard). It is common practice to use RabbitMQ through frameworks that offer an easy way to implement various retry policies (e.g., exponential backoff and dead-letter) plus a sugared interface that makes handling messages more idiomatic in specific client ecosystems. Some of these frameworks are “humble” task queues such as Sidekiq (Ruby), Celery (Python), Dramatiq (Python), etc. Others are “more serious” enterprise service buses (ESBs), like NServiceBus (C#), MassTransit (C#), Apache Synapse (Java) or Mulesoft (Java).The simpler version of this pattern (task queues) can also be implemented using Redis Lists directly. Redis has blocking and atomic operations that make building bespoke solutions very easy. A special mention goes to Kue, which uses Redis in a nifty implementation of task queues for JavaScript.First of all, it’s worth noting that the simplest way of using streams is just as a form of storage. Streams are an immutable, append-only series of time-directed entries, and many types of data fit naturally into that format. For example, sensor readings or logs contain values that by nature are indexed by creation time and are append-only (you can’t change the past). They also have fairly regular structure (since they tend to keep the same set of fields), a property that streams can exploit for better space efficiency. This type of data fits well in a stream because the most direct way of accessing the data is by retrieving a given time range, which streams can do in a very efficient way.Back to our communication use case, all streams implementations also allow clients to tail a stream, receiving live updates as new entries get added. This is sometimes called observing or subscribing to the stream. The simplest way to use Streams as a communication tool is to push to a stream what you would otherwise publish over Pub/Sub, basically creating a resumable Pub/Sub. Every subscriber just needs to remember the last entry-id it processed, so it can easily resume if there’s a crash or disconnection.It is also possible – and sometimes preferable – to implement service-to-service communication over streams, entering the realm of streaming architectures. The main concept here is that what we previously described as tasks/messages, would now be an event. With a queue-based design, tasks get pushed to a service’s queue by another service that wants it to do something, but in a streaming architecture, the inverse happens: every service pushes state updates to its own stream, which is in turn observed by other services.There are many subtle implications from this change in design. As an example, you can add new services later and have them go through the whole stream history. In queues, this is not possible because tasks get deleted once completed and the way communication is generally expressed in those systems does now allow for this (think imperative vs. functional).Streams have a dual nature: data structure and communication pattern. Some data fits into this naturally (e.g., logs) and communication between services doesn’t necessarily have to be based on task queues. The practice of fully embracing this dual nature is called event sourcing.With event sourcing, you define your business models as an endless stream of events and let the business logic and other services react to it. It’s not always easy to do this translation, but the benefits can be great when dealing with hard questions such as “what was the state of object X at time Y?”, which would otherwise be very hard to answer or straight impossible without proper audit logging.Maybe your run-of-the-mill mobile application doesn’t need event sourcing, but for enterprise software that has to deal with customers’ personal data, shipping, and other “messy” domains, it can be of great help.To implement these kinds of patterns, there are plenty of tools you can use. Of course, we should start with the elephant in the room: Apache Kafka, as well as alternatives like Apache Pulsar (from Yahoo) and re-implementations of Kafka in other languages, plus a few SaaS offerings. Finally, there’s also a newcomer: Redis Streams.First of all, note that what Redis calls a “stream,” Kafka calls a “topic partition,” and in Kafka, streams are a completely different concept that revolves around processing the contents of a Kafka topic.That said, in terms of expressiveness, both systems are equivalent: you can implement the same application on either without any substantial change in how you model your data. The differences start once you dive into the practical details, and they are many and substantial.Kafka has been around for a long time and people have successfully built reliable streaming architectures where it is the single source of truth. However, if you’re open to trying new technology, value simplicity in both development and operations, and need sub-millisecond latency, then Redis Streams can fill a very similar spot in your architecture.When I say simplicity, I mean it. If you never tried Redis Streams, even if you plan to go with Kafka in production, I suggest you try prototyping your application with Redis Streams, as it literally takes a couple of minutes to get up and running on your laptop.Let’s consider a few examples to see which problems are best solved by each pattern.If you’re trying to make a couple of client devices (e.g., phone, Arduino) talk in a LAN to each other or to a program that’s running on a computer, the shortest path to a working solution is probably a “TCP connection on steroids.”An IRC-style chat application (i.e., without history), or a plug-and-play real-time processing pipeline for volatile logs/events works well with a brokered approach. Benjamin Sergeant talked about this last use case at RedisConf19 in San Francisco (slides).Web crawlers very often rely on this pattern, as well as many web services with operations that can’t be completed immediately in response to a request. Think, for example, about video encoding in YouTube.This technique works best for log processing, Internet of Things (IoT) devices and microservices, in addition to Slack-style chat applications (i.e., with history).I want to leave you with one last consideration before concluding. The real super-power of Redis is that it’s not just a Pub/Sub messaging system, queue, nor stream service. It’s also not just a general-purpose database. Actually, with enough perseverance, you could implement every single pattern described above on top of a relational DBMS, but there are practical reasons why that would be a bad idea.Redis offers a real Pub/Sub fire-and-forget system, as well as a real Stream data type. Furthermore, with Redis modules, Redis also supports real implementations of many different data types. Let me map this assertion back to our persisted and non-persisted chat application use cases.Above, I concluded that Pub/Sub would have been the right choice since this type of chat application only needs to send messages to connected clients. However, to implement even a simple version of this application, you still have to think about a global channel list and a user presence list for each channel. Where do you store that state? How do you keep it up to date, especially when a service instance dies unexpectedly? In Redis, the answer is easy: sorted sets, expiring keys and atomic operations. If you were using RabbitMQ, you would need a DBMS.Conversations can be very naturally expressed as a stream of messages. We don’t even have to bring event sourcing into the mix here, as it is already the native structure of this data type. So why Redis over Kafka for this example? Just as before, your chat system is not going to be only a stream of messages. There’s channels and other state that is best represented in different ways. There’s also the “User X is typing…” feature: that information is volatile, you want to send it to all participants, but only when they’re connected. If you were using Kafka, you would need to spin up a Pub/Sub system regardless.In distributed systems, when you need coordination, you often need shared state, and vice versa. Ignoring this fact can quite often lead to over-complicated solutions. Redis understands this very well and it’s one of the reasons behind its unique design. If you embrace this principle, you will find that hard problems can occasionally be solved in few commands, given the right primitives, and that’s exactly what Redis gives you. For example, this is how you can transactionally append an entry to a stream, push a task to (the beginning of) a queue, and publish to Pub/Sub:MULTI
XADD logs:service1 * level error req-id 42 stack-trace ""...""
LPUSH actions-queue ""RESTART=service1""
PUBLISH live-notifs ""New error event in service1!""
EXECTo get to a working solution, you’ll need to defeat seven evil concurrency problems.Plot of Redis Pilgrim vs. The WorldI hope this gives you an understanding of the main patterns for communication that are commonly employed by distributed systems. Next time you need to connect two services together, this should help you navigate your options. If you enjoy talking about TCP connections on steroids and streaming architectures, feel free to reach me on Twitter @croloris.The Redis Streams data type is a great feature of Redis and will become a building block of many applications, especially now that Redis has a pool of modules that add new full-fledged capabilities for time-series, graph and search. To learn more about Redis Streams, check out this introductory blog post by Antirez, as well as the official documentation. But don’t forget that streams are not the right tool for every job: sometimes you need Pub/Sub, or simply humble blocking operations on Redis Lists (or Sorted Sets, Redis has that too)."
443,https://redis.com/blog/wrap-redisconf-19-recap/,It’s a Wrap—RedisConf 19 Recap,"April 4, 2019",Madhukar Kumar,"Editor’s note: This post recaps RedisConf 2019. Join us at RedisConf 2020 on May 12-14 in San Francisco! You won’t want to miss keynote speaker Julia Liuson, Corporate Vice President of Microsoft’s Developer Division, who headlines a roster of more than 50 speakers sharing their expertise on the world’s most-loved database.On any other day, Pier 27 in San Francisco is a relatively quiet yet picturesque part of town but on Monday, April 1, hundreds of Redis users also lovingly referred to as Redis Geeks, descended on the pier as part of Redisconf ‘19 and turned it into a vibrant venue of knowledge exchange. It was no April fools’ gag either because the conference officially started a day later. So what was going on?At Redis, the home of Redis and the team responsible for the largest annual Redis conference, we call this Day 0. This is the day where Redis Geeks can take part in Redis training. This year, in addition to the Redis training track, hundreds of developers also signed up for two half-day sessions around how to use Redis in microservices by none other than Chris Richardson, a notable figure in Microservices universe. Unlike last year, Redisconf 19 also featured the Silent Disco party inspired headphones that enabled attendees to sit in a large room, while a number of parallel sessions took place on different topics.The next day over 1,500 Redis Geeks took over the Pier and the day started with some significant announcements by Redis creator Salvatore Sanfilippo, the creator of Redis and Ofer Bengal and Yiftach Shoolman, the co-founders of Redis.Salvatore talked about the next big Redis release 6.0 and offered a preview of what is to come around multi-threading and Access Control Lists (ACLs). When Ofer and Yiftach took the stage they announced a slew of announcements that firmly positioned Redis Enterprise as the first true multi-model database.The first announcement was a new Redis module that turns Redis into a Time Series Database with built-in functionality to manage time-stamped data. You can learn more about RedisTimeSeries here.The second announcement was around RedisAI that enables users to serve their existing AI models including TensorFlow models against data within Redis. With the introduction of the two new models, Redis Enterprise is now a true multi-model database with support for all major data models. However, a true multi-model database doesn’t just have a polyglot persistence strategy, it should also allow interoperability between different data models and this is exactly what Yiftach announced with RedisGears, an in-database programmable framework that allows inter-model communication without the need for extracting and transforming data from different models.Day 1 also included a fireside chat between Ofer Bengal, CEO of Redis with Google Cloud CEO Thomas Kurian around Redis and it’s popularity among developers.Finally, the keynote included a discussion panel with Engineers from two gaming companies – Etermax and Scopely, who discussed the importance of Redis and speed of deployment to their platforms success in online gaming use cases.Day 2 was all about partnerships and the day started with a talk by Martin Ford, an NYT futurist on AI and its effects on society followed by Ofer and Yiftach taking the stage to highlight the power of our ecosystem with many product-level integrations that enhance the Redis experience.The first part of the keynote included partnerships around visualizations, a popular topic around all Redis users. Redis announced the acquisition of RDBTools, a GUI for Redis developed by HashedIn. RDBTools will now be maintained as part of the Redis portfolio of products. The next set of announcements included using Grafana for TimeSeries with an interactive demo. Subsequently, the audience got to see some pretty cool collaborations and demo of how to visualize Streams with Lenses.io and Linkurious for Graph data.Probably one of the biggest announcements came right after when Yiftach announced General Availability of Redis Enterprise on Intel’s newly unveiled persistent memory technology – Optane.Ofer then announced the availability of RedisEdge for IoT, a brand new solution that uses all of Redis’ multi-model capability to deliver sub-millisecond latency data processing on IoT devices.The next part of the Day 2 Keynote included talks from Redis partners Microsoft and Google and the audience was treated with a demo of a Redis operator by Aparna Sinha, Group Product Manager of Kubernetes.We had over 100 breakout sessions, with topics ranging from getting started with Redis to best practices for clustering, geo-replicated databases for DR using CRDT and using Redis as a primary datastore. These sessions were packed with information from many of our Redis experts and customers from Netflix, Fiserv, IBM, Twitch, Verizon, New Relic, Microsoft, Credit Karma, Kong and many more, briefly punctuated with lunch that was served in typical San Francisco style — food trucks.The three packed days seemed to pass in a blink and as much fun as it was to hear all the speakers and the new product announcements, RedisConf 19 came to an end on Wednesday evening. As one of the fellow Redis Geeks, I am now looking forward to the next RedisConf and can’t wait to go through this all over again.We know that this event wouldn’t be the success that it is without the support from our community, customers, and partners — so a big thank you goes out to everyone involved for innovating and growing with us. We sincerely hope you had as much a fun time attending this event, as we did hosting the event.If you’d like to try out the latest modules and connect it all with RedisGears, head over here to download the Redis Enterprise 5.5 Preview Edition."
444,https://redis.com/blog/redis-architecture-13-years-later/,13 Years Later – Does Redis Need a New Architecture?,"June 28, 2022",Yiftach Shoolman and Yossi Gottlieb and Filipe Oliveira,"Redis is a bedrock technology and, as such, we occasionally see people considering alternative architectures. A few years ago, this was brought up by KeyDB, and recently a new project, Dragonfly, claimed to be the fastest Redis-compatible in-memory datastore. We believe these projects bring many interesting technologies and ideas worth discussing and debating. Here at Redis, we like this kind of challenge, as it requires us to reaffirm the architectural principles that Redis was initially designed with (hat tip to Salvatore Sanfilippo aka antirez).While we are always looking for opportunities to innovate and advance the performance and capabilities of Redis, we want to share our perspective and some reflection on why the architecture of Redis remains the best in class for an in-memory, real-time datastore (cache, database, and everything in between).So in the next sections, we highlight our perspectives on speed and architectural differences as it relates to the comparisons being made. At the end of this post, we have also provided the details of the benchmarks and performance comparisons vs. the Dragonfly project, that we discuss below and invite you to review and reproduce these for yourself.The Dragonfly benchmark compares a standalone single process Redis instance (that can only utilize a single core) with a multithreaded Dragonfly instance (that can utilize all available cores on a VM/server). Unfortunately, this comparison does not represent how Redis is run in the real world. As technology builders, we strive to understand exactly how our technologies compare to others, so we did what we believe is a fair comparison and compared a 40-shard Redis 7.0 Cluster (that can utilize most of the instance cores) with Dragonfly, using a set of performance tests on the largest instance type used by the Dragonfly team in their benchmarks, AWS c6gn.16xlarge. In our trials, we saw Redis achieve 18% – 40% greater throughput than Dragonfly, even when utilizing only 40 out of the 64 vCores.We believe a lot of the architectural decisions made by the creators of these multithreaded projects were influenced by the pain points they experienced in their previous work. We agree that running a single Redis process on a multi-core machine, sometimes with dozens of cores and hundreds of GBs of memory, is not going to take advantage of the resources that are clearly available. But this is not how Redis was designed to be used; this is just how many of the Redis providers have chosen to run their services.Redis scales horizontally by running multi-processes (using Redis Cluster) even in the context of a single cloud instance. At Redis (the company) we further developed this concept and built Redis Enterprise that provides a management layer that allows our users to run Redis at scale, with high availability, instant failover, data persistence, and backup enabled by default.We decided to share some of the principles we use behind the scenes to help people understand what we believe are good engineering practices for running Redis in production environments.Running multiple Redis instances per VM lets us:We don’t allow a single Redis process to grow beyond 25 GB in size (and 50 GB when running Redis on Flash). This allows us:The flexibility to run your in-memory datastore with horizontal scaling is extremely important. Here are just a few reasons why:We appreciate fresh, interesting ideas and technologies from our community as offered by the new wave of multithreaded projects. It’s even possible that some of these concepts may make their way into Redis in the future (like io_uring which we have already started looking into, more modern dictionaries, more tactical use of threads, etc.). But for the foreseeable future, we will not abandon the basic principle of a shared-nothing, multi-process architecture that Redis provides. This design provides the best performance, scaling, and resiliency while also supporting the variety of deployment architectures required by an in-memory, real-time data platform.Last, we also found that both Redis and Dragonfly were not limited by the network PPS or bandwidth, given we’ve confirmed that between the 2 used VMs (for client and server, bot using c6gn.16xlarge) we can reach > 10M PPS and >30 Gbps for TCP with ~300B payload.memtier_benchmark commands used for each variation:We used the same VM type for both client (for running memtier_benchmark) and the server (for running Redis and Dragonfly), here is the spec:"
445,https://redis.com/blog/using-redis-vss-in-llm-chain/,Using Redis VSS as a Retrieval Step in an LLM Chain,"May 30, 2023",Daniel Vassilev and Dan Palmer,"In this short tutorial, we create a chain with Relevance AI, Redis VSS, OpenAI GPT, and Cohere Wikipedia embeddings.Working with large language models (LLM) often requires retrieving the correct data to inject as context into a prompt. This is to give the LLM understanding of your own custom data without having to finetune the model. A popular strategy for retrieving the data is using vector search, as it’s exceptionally good at matching similar data without having an exact match. Redis natively supports vector similarity search and is built for speed.In this example, we show how to build and deploy a chain, or a sequence of operations, in Relevance AI that allows us to ask questions of Wikipedia using Redis vector search to extract the best article based on our question. For embeddings, which are lists of numbers that can represent many types of data, we use Cohere’s multilingual model.To follow along, you need a Redis database that supports JSON document data structures and built-in real-time Search and Query features. You can create one on Redis Enterprise Cloud or with Docker using Redis Stack.Once you have Redis running, we import data from Cohere’s multilingual Wikipedia embeddings dataset on HuggingFace. This requires a few simple steps. You can see the full code in this jupyter notebook.Step 1. Install the Python libraries for redis and datasets.Step 2. Create a client.Step 3. Download the sample dataset.Step 4. Ingest each document into Redis using JSON.Step 5. Create a vector search index.This command specifies the index, Wikipedia, which stores data in JSON, where all keys are indexed with the prefix wiki:. In the schema, we reference each field in JSON as $.field_name and give it a friendly label using as name and its data type. The vector field, emb, is of type vector. and uses HNSW as the index type with L2 as the distance metric.Once the command runs, you’ll have a Redis index supporting vector similarity search.Consult the documentation for more details on how to set up an index in Redis.Now it’s time to jump into a Relevance AI notebook to start building our chain.Step 1. If necessary, sign up for a free account with Relevance AI. Once you’re logged in, choose “Build AI chains” and click “Create new chain.” This takes you to a notebook.Step 2. You need to configure an OpenAI API key and the Redis connection string before we can execute the chain. To do this, select “API keys” from the sidebar, then provide your Redis connection string and OpenAI API key.Step 3. Choose “Start with prompt.” Add a new transformation for “Vector search (Redis).” Fill out the form with the following details:Step 4. Configure the LLM prompt to inject the context from the vector search and to ask our question. You can customize the prompt to fit your needs.The intent here is to help people query vast swathes of information. Say you’re mulling over your favorite Bob Dylan song, “Stuck Inside of Mobile,” and you wonder, “Who strummed those infectious guitar riffs?” With our LLM chain, you could effortlessly pose that question to an AI-infused Wikipedia search, which would swiftly return with your answer. (We leave the non-AI-infused search to the casual observer, which may demonstrate the usefulness of such a tool.)This is more than just an exercise in technological wizardry; it’s about enriching your understanding and satiating your curiosity at lightning speed. Like a knowledgeable friend eager to share trivia over a cup of coffee, our LLM chain stands ready to engage in a dialogue with you, adding that personal touch to your search for knowledge.A chain can be deployed in two ways with Relevance AI: as an embeddable application or as an API endpoint. An application can also be shared directly with a link where a user can see the form, fill it out, and run the chain. These are now ready to be used in production.Want to experiment more or try out the SDK? You can view a technical tutorial on building a business analyst agent that can query SQL, create charts, and answer questions.Get started today with your own vector-search powered retrieval system by signing up for Redis Enterprise Cloud and Relevance AI.Join us on June 3, 2023, in person in San Francisco for a 12-hour LLM hackathon with the MLOps community featuring Redis and Relevance AI. Collaborate with other machine learning practitioners for a chance to win prizes."
446,https://redis.com/blog/managing-microservices/,Managing Microservices,"May 31, 2023",Redis,"Understand your microservice deployment options, including automations to save your team precious time and other practical advice for saving systems from unexpected failures.After kicking things off with Microservice Architecture Key Concepts, followed by The Principles of Designing Microservices, we continue our series on working with containerized microservice applications. Herein: a microservice management overview, covering deployment strategies, the importance of versioning, and configuration methods that keep teams from launching full redeployments for simple changes outside the codebase.You’re already sold on the benefits of converting your applications to use a microservices approach. However, questions arise as to how to make the transition. Consider these elements as you design the structure you expect to use for managing microservices and familiarize yourself with the concepts. It’ll save you a lot of confusion in the long run.Containers are pre-packaged software bundles comprised of all the components necessary to run software – which may mean anything from a standalone application to a database in an orchestration system environment. Containers support infrastructure as code and are popular among DevOps teams. Containers’ operational efficiency features help by automating frequent commands, extending databases to include Active-Active configuration, and establishing automatic failover protocols for clusters or nodes that go offline. If each microservice operates in its own container, teams can launch their own releases and eliminate workflow dependencies on other teams.> See how teams are using operators to get the most out of Kubernetes.A/B testing is a means of testing multiple versions of a microservices application or service by infusing different variables into each version in a controlled manner – features, user interface differences, server configuration, whatever – to determine which elements generate the most success in performance terms – whatever success looks like in the appropriate domain. To gauge multiple approaches, web traffic is split between Test A and Test B to monitor how users respond and interact with new implementations or removed features of each isolated product being tested, using logs, traces, and monitoring.> Learn How to Build a Real-Time A/B Testing Tool Using Redis.Blue-green deployment is a helpful strategy for migrating data, testing it, reacting to the changes with contained exposure, and reducing downtime. As explained in Data Ingestion: 6 Ways to Speed Up Your Application, blue-green deployment is a way to perform data migrations. With this premise, you ingest data in parallel. The application continues to use a “blue” legacy database while a new “green” cloud-native database is deployed in parallel for live-production testing and cut-over of this new data pipeline with the assurance of a rollback.> There are lots of ways for DevOps teams to migrate data. Redis Enterprise Cloud services are ready to help you unify hybrid and multicloud data layers.Historically, miners used canaries as an early-warning system to let them know of low oxygen levels. A canary release has the same idea – to help identify problems before they become critical. The incremental deployment strategy tests a microservice release by distributing it to a small subset of users before unveiling it to an entire user base. This way, a development team can test user experience issues, hone in on defective code, and respond to honest user feedback that can then be implemented into a final product.Microservices and their respective applications rarely stay stagnant. Updates, refreshes, and general coding tweaks have to be implemented occasionally.As its name suggests, continuous integration (CI) is an automated process that deploys new code into an existing deployment environment. For CI to benefit developers and DevOps teams, it needs a solid testing and deployment automation strategy to ensure a fast turnaround of production-quality releases.Continuous deployment (CD) typically promotes new deployments into a production environment after undergoing automated tests.> Learn how the Redis Developer Hub Expands to Support the Needs of DevOps Teams by making databases a part of the CI/CD pipeline.Configuration management ensures that each microservice’s corresponding configuration files are correct and immediately accessible. Configuration management keeps developers and IT from changing the application code when a microservice needs modifications. Keeping the code intact and merely updating configuration files also saves teams from having to initiate a rebuild of the application.Microservice architecture is a dynamic framework that encapsulates components of a workflow or process, typically focusing on specific business areas. For example, a retail application encompasses essential functionalities such as shopping cart management, order processing, payment handling, checkout processes, product catalogs, seamless integration with back-end finance and accounting systems, and robust customer service support.In setting up microservices architecture, DevOps is tasked with applying the guidelines that make a microservice function a certain way in the overall application infrastructure. These settings can include database connection details, API endpoints, logging levels, runtime configurations, and feature toggles, also known as “feature flags.”The proper configuration values for a microservice, often stored ins JSON or YAML files, depend on the application’s security requirements, the database it runs on, its state in the development cycle, and its deployment environment, among other factors. Besides JSON and YAML files, configuration sources are also found within the parameters set in the command-line arguments.A centralized configuration store is a server or repository from which one can manage all microservices’ configurations, regardless of their environments. The configuration features from one microservice to the other can vary greatly, and juggling such a disparate set of parameters and secrets without a globally available single record “can make things over-complicated very quickly.” Apart from being a dedicated hub for configuration, it simplifies and accelerates the uptime process for new releases.Dynamic configuration management (DCM) addresses configuration changes in an application without initiating a redeployment. Touching the application code triggers the automatic need for running a full regression suite with all functional and integration tests.These pre-banked tests are typically executed via a centralized configuration store.Secrets share sensitive data such as passwords, keys, credentials, and authentication tokens. Secrets keep secret data separated from an application’s code, allowing for the management of secrets without needing code changes.In Kubernetes, secrets are “by default, stored unencrypted in the API server’s underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.”To maintain secure user authentication, development teams may use an external secrets operator, such as the HashiCorp Vault, to maintain secure user authentication. Secrets operators authorize all access before sharing sensitive information.Versioning is a way for DevOps teams to track the historical configuration changes made to a microservice’s attributes, including “key-value pairs, software bill of material (SBOMs), common vulnerabilities and exposures (CVEs), licenses, swagger details, consuming applications, and deployment metadata.”That helps DevOps to verify that all namespaces and clusters have the versioned release they’re supposed to be operating with and allows quick implementation of configuration rollbacks when previously used functions or settings are required. As a result, anyone working on an organization’s microservices can accommodate new features, bug fixes, and other changes.Service mesh, service discovery, and load balancing are all related concepts that work together to enhance microservices’ observability and reliability..A service mesh is a pattern inserted in the infrastructure layer that controls the delivery of service-to-service messaging. In the case of large-scale applications with many growing microservices, a service mesh keeps requests clear and streamlined and routes pertinent information to the corresponding service while keeping the application performance robust.A service mesh consists of a data plane and a control plane. The difference between both planes is that “the control plane decides how data is managed, routed, and processed, while the data plane is responsible for the actual moving of data.” Ethernet, Wi-Fi, cellular, and satellite networks are examples of data planes. The control plane that tracks the multiple services is also known as “service discovery.”Service discovery facilitates service-to-service communication by highlighting available instances ready for communication without configuration changes. A service registry storing the IP addresses, port, and health status of available services becomes a centralized hub for discovering which services are ready for communication.In any networking scenario, load balancing is the process of distributing workloads and computing resources across servers, splitting up the work to enable better resource utilization, and system response time.A load balancer is one of the built-in capabilities of a service mesh. It uses algorithms to decide where to route traffic, dynamically distributing traffic without the need for external load-balancing devices on other networks. When a request reaches a microservice, that request is intercepted and relegated to the appropriate instance. Which instance handles the traffic is essentially decided by the service discovery or control plane, using tests and health checks.Autoscaling uses predefined system thresholds to scale a service up or scale down. Suppose a site is expecting a massive surge in traffic because of a sale, a popular sporting event, or due to a highly-anticipated new product release. In that case, autoscaling includes as many instances as necessary to fulfill the demand and to keep the microservices based application running at peak performance.System failures sometimes occur, and it’s vitally important to prepare for the possibility of a failed node or cluster.Anticipate these issues at the implementation level. In The Principles of Designing Microservices, we briefly cover the importance of designing microservices for resiliency with tactics such as the Circuit Breaker pattern, introducing fault-tolerant patterns, and implementing asynchronous communication with an event-driven message broker to promote eventual consistency.For more on eventual consistency, read Database Consistency Explained.Want to find more information on microservices? Discover our dedicated microservices playlist on our Youtube channel."
447,https://redis.com/blog/redis-assisted-client-side-caching-in-python/,Redis Server-Assisted Client-Side Caching in Python,"January 9, 2020",Itamar Haber,"Everybody knows that there are only two hard problems in computer science: cache invalidation and naming things. This post, as you may have guessed from its name, is about dealing with the first one: cache invalidation problem. I wrote it because a new feature in Redis 6 makes it easier for clients to manage a local cache. This mini-project’s source files can be found in the rsacsc-py repository.I am not going to address the need for caching per se. We cache data practically in every aspect of computer engineering to reduce the time it takes us to access it subsequently. We cache on oh-so-many levels and will continue to do so until someone finally cracks instantaneous communication, and probably afterward.Redis is a remote server, meaning any data in it is accessible over the network. From the perspective of a Redis client, network-induced latency is usually the biggest contributor to overall latency. Avoiding repetitive calls by caching previous replies can mitigate that.Redis uses a key-value store data model, an abstraction that maps a unique identifier to each data structure and the data in it. Because data access is always by key name, it’s easy for a Redis client to store its respective data locally for caching purposes. But that brings up the hard problem of invalidating that cache.The aptly-named server-assisted client-side caching is a new capability added in Redis version 6. It is intended to assist the management of a local cache by having the server send invalidation notifications. The server tracks the keys accessed by a client and notifies the client when these change.Because Redis server-assisted client-side caching (RSACSC for short) is admittedly somewhat immature in the first release candidate, I wanted to take it out for a real-world test drive. The idea was to make a proof of concept to get a better feeling for what’s there and what’s still missing.I was leaning towards prototyping this in Python, and a short informal poll supported this approach. I had a three-part setup in mind:To make the connection, I chose redis-py. It provides the Redis class that is a straight-forward zero-fuss client, and Python’s nature makes extending it easy.The requirements from the cache component are basic, so I was perfectly happy adapting the LRU cache example in Python’s OrderedDict documentation.For this experiment, I chose to implement caching for a single Redis command, namely GET. The premise is to make the client use the read through pattern: that is, to attempt a read from the local cache and defer to Redis in case of a miss. Subclassing the Redis client and overriding its get() method gives us the following:The new Redis class initialization isn’t fancy. It begins by calling its base class’ __init__() and setting a couple of properties. It ends with a call to Redis’ CLIENT TRACKING command, which enables the server’s assistance for the connection.The class’ get() method is where the magic happens. We try to read the key’s value, by name, from the cache that’s available through the connection’s manager. In case of KeyError, or a cache miss, we revert to the base class’ get() to fetch the data and store it in the cache.Once the Redis client has opted-in to being tracked, the server maintains a record of keys that the client had read from. Instead of tracking every individual key, Redis uses a hashing function on the keys’ names to assign them to slots. Specifically, it uses the 24 least-significant bits of the key name’s CRC64 digest, resulting in roughly 16 million possible slots.This reduces the server resources required to track multiple keys for multiple clients. The invalidation messages sent by Redis, therefore, consist of slots that need to be invalidated rather than key names. It is up to the client to infer the relevant key names that need to be removed from its cache given the slot.That means the client needs to employ the same hashing function to track how the keys in the local cache map to slots. That lets us perform slot-based invalidation of the client’s cache when an invalidation notification arrives. For that, we’ll use the add() and discard() methods when keys are added and discarded from the local cache, respectively.How an invalidation message is sent to a tracked client depends on the Redis Serialization Protocol (RESP) that the client is using. Earlier versions of Redis use RESP2, but its successor, RESP3, is already present in Redis 6 and will deprecate the older protocol completely in Redis 7.RESP3 packs in many new features, including the ability for the server to “push” additional information on an existing connection to a client alongside the actual replies. This channel is employed for delivering invalidation notifications when using the server-assisted client-side caching ability.However, because RESP3 is so new, only a few clients currently support it, so RSACSC also works with RESP2. Because RESP2 lacks the “push” ability, RSACSC broadcasts invalidation messages to interested parties using the existing support for PubSub in Redis.Handling the invalidations and the keys-to-slots mapping is what the manager is for. Here’s what it looks like:The manager is initialized with a connection pool, from which it creates its own client for PubSub as well as any subsequent cached connections requested by the using application. It also maintains a dictionary called slots that maps a slot number to the set of key names that it holds. Lastly, it maintains the Cache class that is the LRU cache’s implementation.The start() method, not surprisingly, starts the manager by beginning to listen to the __redis__:invalidate PubSub channel in a separate thread. The messages intercepted on that channel are handled by the _handler() method. It, in turn, calls the invalidate() method to invalidate the requested slot:Invalidation is just a matter of popping keys, one by one, from the respective slot’s set and deleting them from the cache. Lastly, the manager exposes a factory method, get_connection() that’s used by the code for getting new cached connections:This post isn’t about benchmarking or Python’s performance per se, but it’s important to understand the mechanism’s impact. For that purpose, I’ve used the benchmark.py script on a 2019 MacBook Pro with a Redis instance running locally using defaults (except that I turned off snapshotting).Before performing the tests, the benchmark script populates the database with 1000 keys and sets up the cache manager with a capacity of 100. It then runs several timed tests to measure performance. Each test is repeated five times for both types of connections: regular and cached.The result of the first test actually demonstrates one of caching’s disadvantages: cache misses. In this test, single_read, we read every key from the entire database just once, so every access to the local cache results in a miss:Note that the averages are computed only for the last runs, so every first run in the series is considered a warmup. The averages above show that the missed cache reads add almost 13ms for every 1,000 reads, roughly an 18% latency increase.However, repeating the test on a dataset that fits into the cache—that is, only 100 keys—shows more encouraging results. While the first cached run shows an increase in latency, subsequent ones reduce latency by two orders of magnitude:The next test is named eleven_reads because it reads every key in the database once along with 10 other keys that are always the same ones. This highly synthetic use case provides even more dramatic proof of the cache’s benefit (despite that not being the purpose, per se).The last test extends eleven_reads with an additional write request to one of the 10 constant keys, which triggers the invalidation of a part of the cache. Latency of cached runs increases slightly, both because of the extra write command but also due to the need to re-fetch the contents of the cache:This was a good way to spend some time caching. You may already be familiar with Redis’ keyspace notifications, which are events about the keyspace—such as modifications to keys—sent on PubSub channels. Keyspace notifications can, in fact, be used in much the same manner as Redis server-assisted client-side caching to achieve similar results.Because PubSub is not a reliable means of transmitting messages, using either keyspace notifications or RESP2-based RSACSC can result in lost invalidation notifications and stale content. With the advent of RESP3, however, RSACSC notifications will be delivered as long as the connection is alive. Any disconnects can then be easily dealt with by a local cache reset.The move in RESP3 from PubSub broadcasting to connection-specific notifications also means that clients will get invalidation notifications only for slots that interest them. That means fewer resources are spent on communication, and less is better.Regardless of the RESP version used, client authors can use RSACSC for caching much more than just the GETting of entire strings. The mechanism is agnostic of the actual data structure used for storing the key’s value, so all core Redis types and any custom ones declared by modules can be used with it.Furthermore, instead of just caching key-value tuples, the client can cache requests and their replies (while keeping track of the keys involved). Doing so enables caching of substrings returned by GETRANGE, list elements obtained via LRANGE, or virtually any other type of query.The one thing I knew I didn’t want to implement in this exercise was a CRC function. I assumed that Python would already have the right one for me.To find which CRC Redis is using, you could just look at its source code—the file src/crc64.c tells us right at the beginning that:I did a quick search for “Python CRC64 jones” and after skimming the docs I chose to pip install crcmod so I can use its predefined crc-64-jones digest.Sometime later, and much more than I’d care to admit, I found the reason why my stuff wasn’t working. A closer inspection of the docs revealed that crcmod uses a different (off-by-one, but that’s a different joke) polynomial. Here they are together, can you spot the difference?What’s more, crcmod had adamantly refused to use Redis’ polynomial pettily claiming:I then, of course, just gave up and ported the Redis CRC64 implementation. Not exactly a hard task: one copy-paste, a few searches replace, and one line of actual code to rewrite. If you’re going to give RSACSC a go, make sure that the CRC64 implementation you’re using checks out to 0xe9c6d914c4b8d9ca."
448,https://redis.com/blog/what-is-multicloud/,"What Is Multicloud Infrastructure? Its Benefits, Drawbacks, and Strategies","February 22, 2023",Redis,"Why has the multicloud strategy caught on like wildfire in the cloud space? Cloud teams have plenty of tools at their disposal to help scale operations, so what makes multicloud any different? In this post, we explore its rise in popularity among architects and IT decision-makers, how multicloud differs from other computing approaches, along with advantages and disadvantages, and what relevant supplementary tools boost the power of a multicloud architecture.Multicloud is a cloud deployment strategy that involves more than one cloud provider to facilitate hosting an enterprise architecture. This is not to be confused with hybrid cloud, which typically involves the mixed use of a private cloud and public cloud provider.In order to understand why 81% of tech practitioners plan to adopt a multicloud strategy in 2023, first, let’s clarify the distinctions between multicloud and other cloud infrastructures, such as hybrid cloud, public, and private clouds, as well as how physical on-prem servers fit into the mix.First off, having an on-premise (on-prem) physical infrastructure is not inherently old-fashioned. Local data centers are still viable options for many use cases and, as you’ll see, work perfectly in conjunction with cloud deployments as well.On-prem has its benefits and drawbacks. The main benefit being total control of the infrastructure.Cloud services often have a pay-as-you-go model. So the more complex and data-intensive an application is, the higher the cost. When a company has complete control over on-premises equipment, it is responsible for managing its infrastructure. This eliminates the need to rely on a third-party team and allows the company’s applications to be as involved as they need to be.  This also helps keep server usage predictable.The safeguarding of sensitive data can be a factor that is beneficial in certain situations and detrimental in others. Onsite data security requires its own internal headcounts and extra financial resources to deliver the same level of security that leading cloud vendors provide. Monitoring for attacks on a physical data center typically requires 24/7 staffing, and attacks can come from anywhere, from outside the organization or within.Private clouds are often vendor-provided cloud services that provide a dedicated infrastructure and services for a single organization. With a private cloud, a cloud vendor often establishes an on-premises cloud infrastructure to suit its client by installing software (and sometimes hardware) that meets the paying customer’s requirements. It may or may not be provider-managed.Sometimes, it helps to keep data siloed, to provide an extra layer of security or privacy, or if the client wishes to withhold sensitive data from a certain provider, for instance. These are real-world use cases for adopting a private cloud strategy. Also, some organizations prefer to host their own data and applications in-house, keeping all data off third-party servers. In that case, they can opt for a private cloud, not just for reasons of control but also to expedite the setup process, with cloud provider assistance in hardware maintenance.But unlike an on-prem server, a private cloud increases security, not just by keeping sensitive data out of a public cloud setting, to begin with. This approach also strengthens data segregation. With an on-prem server, sensitive data can still be accessed by unauthorized personnel within the organization. Private cloud enables data security teams to fully orchestrate the paths data can take to make sure data is seen and handled by those with the proper permissions.In essence, a public cloud environment means that you are renting a company’s physical infrastructure, which consists of physical servers. Someone else maintains the data center. You do not need to interact with or maintain this infrastructure on your own. The cloud provider takes care of everything from initial setup to maintenance tasks. The public cloud option enables teams to dedicate more time and financial resources to their applications instead of the infrastructure.Scalability is another big plus for a public cloud strategy. If a company’s traffic is season-specific or centers around one specific event one would add more servers to meet traffic demand.But what if there is a substantial amount of sensitive data, and compliance rules dictate that this data should not live in a public cloud setting? You can opt for an in-house private cloud setup, or you can adopt a hybrid cloud strategy to take advantage of your existing hardware.As its name suggests, a hybrid cloud uses a combination of public and private clouds.The core concept of a hybrid cloud is that all systems must work in conjunction with one another. The cloud resources (whether public or private) and on-prem components (if any) have to be fully interoperable. In other words, they need to exchange data between one and the other when a use case calls for it.A multicloud infrastructure uses two or more public cloud providers and allows for a combination of public and private environments. While a hybrid cloud infrastructure has interoperability, the primary cloud providers are rallying around Kubernetes to bring that same interoperability to multicloud.Multiple clouds let organizations reduce latency by ensuring that an end user in Berlin can access a server that’s reasonably nearby (say, Frankfurt) rather than waiting for the round-trip to-and-from a server in Atlanta. Multiclouds also permit organizations to take advantage of prepackaged tools from a favored cloud vendor. And… that’s just two of countless reasons for choosing a multicloud infrastructure. We unpack the benefits of a multicloud adoption and its drawbacks and include some hypotheticals to illustrate why multicloud is becoming a more attractive choice for architects and IT professionals.The rise of microservices from monolithic applications has accelerated multicloud adoption, though that’s not the only factor.Establishing a multicloud infrastructure can help your IT operations stay nimble and adaptable and enable linear scalability. You can change things around more easily when circumstances warrant, and the more resources you put into it, the stronger its throughput.Why would you want to run one single application across multiple clouds? Besides breaking a monolithic architecture into smaller, manageable pieces, a multicloud architecture lets you pick and choose the features you want from multiple cloud services.Think of multicloud’s flexibility in the way a savvy consumer uses credit cards. Perhaps they use their Delta Airline credit card for large purchases to gain airline miles, an Amazon card for the money-back rewards, and a Shell card to rack up points and save on gas. Each provider offers its specific incentives.Multicloud works much in the same way. Perhaps the latest Google Cloud update has a machine learning component from which your application can benefit, something that the other cloud vendors lack.There’s also an infinite amount of third-party resources that you can use to make your application more robust. Kubernetes, for example, is vendor agnostic; it lets you deploy containerized applications across multiple clouds without the need for a physical server.Multicloud lets you design an infrastructure piecemeal, upping the scalability with a function from this cloud and a little bit from another.Today, even a second of latency may be unacceptable for some applications. Some applications genuinely require real-time responsiveness, such as medical devices or geolocational systems.Uptime, too, is a critical issue in many situations. Nobody wants a system to go down or to operate so slowly that it cannot do its job. Keeping availability at 99.999% is a goal for many organizations.In both these cases, one way to meet the necessary thresholds is data replication. Spreading database replicas around the globe can alleviate constraints on a region’s availability, and if one of the far-flung cloud providers offers Active-Active replication, all the better. Active-Active syncs a database with all the nodes in the network, guaranteeing real-time data consistency across all replicas, wherever in the world they may be.Another way to improve availability is to split the overall workload among several cloud services. Some businesses have steady heavy traffic all year long, while others see traffic spikes seasonally. In the latter instance, it wouldn’t make sense to provide cloud support all year round if that huge influx of traffic only occurs for a two-week period every year.Take Wimbledon, for instance. It’s arguably the most celebrated event in professional tennis and has ardent fans in every corner of the globe. When ticket sales open, the Wimbledon site would expect a significant increase in web demand. A keen cloud services team would make sure that all regions are prepared for the impending traffic surge. But what if you already know that your current cloud service providers don’t service, say, Argentina? That would be the right moment to ensure the Wimbledon application is distributed to a cloud provider that does service the lower South American region.IT organizations often have data from previous spikes. The tennis web team might know from experience that in the three months between ticket sales opening and the start of the tournament, traffic doesn’t just plateau – it plummets. In that case, you’d cease supplemental cloud support and ramp it back up again when the tournament begins.Outages do happen so you need a disaster recovery plan. If an entire region experiences a major outage, disaster recovery replication, cluster recovery, and other disaster recovery mechanisms offered by multiple cloud providers can step in to keep operations afloat.With multicloud, costs can easily spiral. The more vendors you work with, the more money it costs – if only to administer the relationships. Having a full replica backup in the cloud as a “just in case” comes with a price.IT teams need to consider the appropriate multicloud architecture and establish a system to guarantee that workloads aren’t set to an autopilot set-it-and-forget-it trajectory. Keep close tabs on workload lifts and what cloud does what. Know when to pull the plug on a supporting cloud system – and have a process in place to let you know when the situation occurs. If managed deftly, the multicloud approach could pay off in major dividends in the form of major savings.Establishing a sound multicloud security strategy can help you manage IT costs. Finding a cloud vendor that provides essential resources, such as threat detection, can avert bad actors from causing serious financial damage to your business through cyber attacks. Data loss, compromised assets, and waning consumer trust have serious financial consequences.Multicloud options also have a stumbling block or two. Take these five considerations into account when considering a multicloud environment.According to Forrester’s Unlocking Multiclouds Operation Potential, security, and operational complexity are major organizational challenges. “Eighty-nine percent of respondents agree that security is the most critical determinant of success for a cloud strategy,” the white paper says, “and this is exacerbated by multicloud environment complexity. Thirty-one percent ranked complexity as the most significant internal challenge to cloud security.”Not all cloud environments are made the same, and thus, their controls are vastly different. For one, each cloud has a different nomenclature, and your personnel needs to stay up to date. That is: You have to train people to work with new-to-them services and technologies.Cloud providers, including Google Cloud and Amazon Web Services, offer their own certifications. Do you want to invest resources to certify your internal team on all major cloud services, or to hire independent teams that specialize in each of these services? Both options increase overhead and expenses. But either way, upskill your staff. As Sir Richard Branson suggests, “Train people well enough so they can leave; treat them well enough, so they don’t want to.”Remember those specialized functions each cloud brings to the table? Unfortunately, that level of specialization can be a double-edged sword. Choosing a cloud provider for one proprietary function can create vendor lock-in. It can be dangerous to tie your company’s success to a single vendor or cloud service provider.Any cloud adoption strategy, multicloud or not, is a big decision. Hopefully, this post gave you lots to consider or at least shed some light on the particularities of this growing cloud computing strategy. It’s just a start, though. For more information on how multicloud factors into the ever-shifting digital landscape, download our white paper, The Future is Here: Multicloud for the Distributed World.Looking for definitions to other related concepts?  Check out our glossary section."
449,https://redis.com/blog/redis-7-geographic-commands/,Speeding Up Geographic Commands in Redis 7,"February 21, 2023",Filipe Oliveira and Martin Dimitrov,"Intel and Redis have made significant performance enhancements! Here we share the optimization techniques we used to evaluate and maximize Redis GEO command performance, such as reducing wasteful computation and simplifying algorithms.All the work has paid off. Altogether, we improved Redis performance by up to four times the previous speed!Redis’s GEO commands (formally geospatial indices) are a powerful tool for working with geographic data. Redis can store an object’s geographic coordinates, such as store locations or delivery trucks on the move. Then Redis can perform math and coordinate-based operations on that data, such as determining the distance between two points (How far away is the delivery car with my lunch order?) or finding all registered points within a given radius of another point (Where is the closest pet store to my current location?).Ultimately, those longitudes and latitudes are just data for your applications to access and update. And as with any database that is part of your critical business logic, you want the best possible performance – particularly when the systems accessing the data are moving around physically (that delivery truck) and thus need real-time responses. To accomplish that, it’s important to optimize the queries and algorithms you use to interact with GEO data.To maximize the Redis GEO commands’ execution performance, we delved into performance analysis and workload characterization techniques. The goals were to identify bottlenecks, optimize resource utilization, and improve overall performance.In this post, we share those optimization techniques, such as reducing wasteful computation and simplifying algorithms, as well as the results of our analysis. The takeaway: We can reach four times the throughput on GEOSEARCH queries, and that enhancement has already been released as part of Redis 7.Performance analysis and workload characterization are important techniques for understanding applications’ performance characteristics. These techniques involve collecting and analyzing data about the application’s behavior and its resource usage to identify bottlenecks, optimize resource utilization, and improve overall performance.Whatever application we’re trying to speed up, we use deterministic, concise, and methodical ways to monitor and analyze performance. To do so yourself, you can adopt any of a number of performance methodologies, some more suited than others, depending on the class of issues and analysis you want to make.In broad terms, when you want to improve a program’s efficiency, this generic checklist applies to most situations:1. Determine the performance goals: Before you start, you need a clear understanding of what you want to achieve. The goals might include improving response time, reducing resource utilization, or increasing throughput. In this case, we want to both improve the response time of each individual operation and consequently to increase the achievable throughput.2. Identify the workload: Next, determine the workload that you will analyze. This might involve identifying the specific tasks or actions that the application will perform, as well as the expected load and usage patterns. For the GEO commands use case, we used a dataset with 60 million data points based on OpenStreetMap (PlanetOSM) data and a set of geodistance queries for the benchmark.3. Collect data: Once you identify the workload, collect data about the application’s performance and its resource utilization for each use case. You might use profilers, debuggers, or performance analysis tools. Gather data about the program’s memory usage, CPU utilization, and other metrics. Profiling CPU usage by sampling stack traces at a timed interval is a fast and easy way to identify performance-critical code sections (also called hot spots). Our findings are precisely based upon using the Linux perf tool for that goal. Our documentation about identifying performance regressions and potential on-CPU performance improvements includes a full list of tools and deeper methodologies.4. Analyze the data: Once you collect the data, you can use various techniques to analyze it and identify areas of the application that may affect performance. Some of those techniques are analyzing performance over time, comparing different workloads, and looking for patterns in the data. We find it easy to improve on CPU-related use cases by analyzing the recorded profile information using perf report, when you have the coding context of the application you’re profiling. The more experience you have with the application’s code, the easier it is to optimize it.5. Optimize the application: Based on your analysis, you can then identify areas of the application that may be causing performance issues. Then take steps to optimize the code, such as changing to a more efficient algorithm, reducing the amount of memory used, or making other code adjustments. In the scenarios we showcase next, we focus on the efficiency of the algorithms and reducing the amount of duplicate computation.6. Repeat the process: Performance analysis and workload characterization are ongoing processes, so it’s important to regularly review and analyze the application’s performance. This allows you to identify and address new bottlenecks or issues that may arise in updates or usage situations.You can’t make software run faster if you don’t know what it’s doing. In this case, it means a short introduction to the process of working with geospatial data.Most commands to query Redis Geospatial Indices calculate the distance between two coordinates, so it makes sense to examine how that’s done algorithmically. The Haversine distance is a useful measure, as it takes into account the curvature of the Earth, with more accurate results than a Euclidean distance calculation.To calculate the Haversine distance on a sphere (such as the Earth), you need the latitude and longitude of the two points, as well as the radius of the sphere. The Haversine formula calculates the distance between the points in a straight line, following the shortest path across the surface of the sphere.These calculations rely heavily on trigonometric functions to compute the Harvesive distance, and calling trigonometric functions is expensive in terms of CPU cycles. In a bottleneck analysis of a simple GEOSEARCH command, around 55% of the CPU cycles are spent within those functions, as shown in Figure 1. That means it’s worth the time to optimize those code blocks, or as Amhdal would say, “The overall performance improvement gained by optimizing a single part of a system is limited by the fraction of time that the improved part is actually used.”So, let’s focus on the largest possible optimization.As the profile data above demonstrates,  54.78% of CPU cycles are generated by geohashGetDistanceIfInRectangle. Within it, we call geohashGetDistance three times. The first two times, we call the routine to produce intermediate results (such as to check if a point is out of range in longitude or latitude). This avoids CPU-expensive distance calculations if the conditions are false. It makes sense to check that data is in range, but we don’t need to do so repeatedly.Our first optimization attempt to reduce the wasteful computation of intermediate results in two steps (and described in detail in the PR #11535):That optimization made a significant difference. For that GEOSEARCH query, using a single benchmark client with no pipeline, we decreased the average latency (including round trip time) from 93.598ms to 73.046ms, representing a latency drop of approximately 22%.The same performance analysis of GEOSEARCH queries also showed that, in some cases, Redis performs spurious calls to sdsdup and sdsfree. These commands allocate and deallocate strings’ memory, respectively. This happens with large datasets, where many elements are outside the search’s range.The performance improvement we proposed in PR 11522 was simple: Instead of pre-allocating the string’s memory, we changed the role of geoAppendIfWithinShape to let the caller create the string only when needed. This resulted in approximately 14% latency drop and 25% improvement in the achievable ops/sec.To optimize geographic index querying, we focused on data pattern information. The intent is to simplify the number of calculations required for the Haversine distance. This is purely a mathematics exercise.When the longitude difference is 0:The PR #11579 describes the optimization and obtained performance improvement of this simplification. The bottom line: It resulted in 55% increase in the achievable operations per sec of the Redis GEOSEARCH command.All use cases that heavily rely on converting a double to a string representation (such as the conversion of a double-precision floating point number (1.5) to a string (“1.5”), can benefit by replacing the call to snprintf(buf,len,”%.4f“,value) with an equivalent fixed-point double to string optimized version.The GEODIST command, shown in Figure 3, is a good example. About a quarter of the CPU time is devoted to the type conversion.PR 11552 describes in detail the optimization we suggested, which resulted in a 35% increase in the achievable operations per second of the GEODIST command.A primary reason for Redis’s popularity as a database is its performance. We generally measure queries in sub-millisecond response time – and we want to keep improving it!The cumulative effect of the optimizations we describe in this blog post: We increased the performance of Redis geospatial commands by up to four times their previous speed. You can already benefit from these enhancements, as they are part of Redis 7.0.7. And that should get you where you’re going, faster than you did before.Note that this is not an isolated performance improvement – quite to the contrary. This set of improvements is a real-life example of the impact of the performance efforts that we call the “Redis benchmarks specifications.” We are constantly pushing for greater visibility and better performance across the entire Redis API.Want to learn more about geospatial computing? This five-minute video gives you the lay of the land.Find out more about in the performance blogs in this series, most of them co-written with Intel:*Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries."
450,https://redis.com/blog/data-durability-vs-availability/,Data Durability vs Data Availability: Streaming Video Helps Explain,"December 28, 2022",Talon Miller,"Need to understand the difference between data durability and data availability? Your favorite binge-watch can help illuminate the distinctions.Data has a shelf life. Data is vulnerable to bit rot – also known as data decay, bit decay, and data rot. That’s true of software and media, such as websites that disappear, taking with them years of news articles, movies, or other information, only a fragment of which is captured by the Wayback Machine.However, bit decay also applies to the physical media of a storage device and the resulting deterioration in the data integrity of the information stored there. Losing a bit here and a byte there over time adds up to data that can’t be trusted. Data loss means, well, data is gone – possibly important documents, such as historical e-commerce records or tax information. Worse, a drive failure can mean that data is stored inaccurately, which means wrong information.It isn’t merely a technical problem. In industries like financial services, the consequences of vulnerable or slow data could cost a trusted brand name its customer loyalty, tarnish its reputation, and lead to lost revenue.Now, raise your hand if you have a favorite show on a streaming service.According to Statista, 83% of American consumers used a video-on-demand streaming service in 2022. Typically, customers want to watch their favorite TV shows when they want them, wherever in the world they may be. The media of that TV show – let’s say it’s Andor – is the same set of bits wherever it’s viewed, so it serves as a good example of persistent data. It illuminates the importance of data durability and availability, as well as the implications of working with bad data.No, durability and availability are not one and the same! However, to work with trusted data, it’s important that both durability and availability work in tandem. It’s not possible to deliver true data quality with one and not the other.Using your go-to binge-worthy show, let’s explore why that is.Data durability is a means of safeguarding data from loss or corruption in the event of an outage or failure.Data durability is the process by which one ensures data is (and remains) intact, devoid of any degradation.  In essence, durable data means uncompromised data.In streaming media terms, imagine you’re sitting down to watch the latest Star Wars spin-off series. Your 4K TV is set. You’ve heard the effects are mind-boggling. Yet, for some reason, the show starts up, and the quality leaves a lot to be desired.In this instance something may have interfered with the data durability of the streaming, i.e. the quality has degraded but the the information is still available.Learn how Redis improves with data durability through AOF and snapshot persistence options.Data durability relies on persistent data.When data is persistent, it is accessible and ready to be used at the start of the next application session. The expectation is to suffer no data loss between the last session and the next.To maintain data durability, it’s critical to persist the data to disk. Data is persisted to disk at set intervals in one of two different ways: append-only files (AoF) and snapshots.AoF provides two choices for appending write operations to disk:AoF generally is the wisest choice for any organization that needs uncompromised data in real-time.A snapshot is a kind of data replication that captures the state of the database at a specific point in time. Unlike AoF, snapshots are typically written to disk in intervals of 1, 6, or 12 hours.The criteria for database persistence are not universal. The persistence configuration depends on the database type (whether it’s NoSQL or a relational database) and database size, and other matters. Therefore, it’s best if you make configuration decisions when establishing your architecture.The other option is to edit an existing database’s configuration. Note that the change is not automatic. Changing a database’s persistence model can take time to complete.In other words, how do you measure a database’s resiliency against corruption from bit decay or loss?Durability is usually illustrated in percentages, such as 11 nines (99.9999999%) or 99.9999999% durability, which according to Google Cloud, means that “even with one billion objects, you would likely go a hundred years without losing a single one!”The math behind all these nines requires some dense statistics, so know that the main statistical measurements used to measure data durability are typically the Poisson distribution and binomial distribution. Whereas the Poisson distribution measures the probability of an event occurring (k) times in a given interval of time, the binomial distribution assesses the probability of one of two results in a repeatable test, namely “success” or “failure.”Data availability means the data is operational. When the data is requested, it is delivered. In other words, data availability is a synonym for system uptime.That’s why data durability and availability, together, create the perfect equation: uncompromised data plus uptime and accessibility or immediate access to uncorrupted files.Oh yes! Back to the new Star Wars series for a second.You reach a cliffhanger in the show, but you have to make it to the airport for an international business trip. You can start a new session once you get to your hotel. When you restart the show, like magic, you pick up where you left off, even if the location is halfway across the planet. That’s the beauty of both availability and high availability. It gives you the power to say, “I want my Baby Yoda crisp, clear, and on the attack when I’m in Italy, even though I paused him mid-action back in Kansas.”Despite your significant location change, what makes this following session so smooth and possible is Active-Active Geo-Distribution – not “The Force,” alas, however cool that might be. Active-Active Geo-Distribution takes a database’s replicated data and makes it available to nodes distributed across various available zones. By establishing local nodes with a replica, latency is vastly reduced, enriching the viewing experience.Baby Yoda moves slowly enough. Your database shouldn’t make matters worse.Data availability is measured by taking total uptime and dividing it by the sum of that uptime plus downtime: Availability = Uptime ÷ (Uptime + Downtime).Availability is usually phrased as “five nines” or “eleven nines” but is typically expressed as a percentage, such as 99.999%. When it’s all said and done, “five nines” averages out to about five minutes of total downtime in a year.Put into context, suppose we know that an instance was only available for 145 hours in a week (168 hours). That’s a 13-hour difference. In that case, the calculation would look something like this: 145 ÷ (145 + 13) = 91.772% availability.High-availability architecture is really interesting, and (for techies) it is fun to unpack what’s possible when used to its optimum potential. Check out High-Availability Architecture Demystified and What Is Data Replication for more information on clustering and disaster recovery.In 2022, Uptime Institute’s Outage Analysis found that “over 60% of failures result in at least $100,000 in total losses, up substantially from 39% in 2019. The share of outages that cost upwards of $1 million increased from 11% to 15% over that same period.”That’s a real-world result. The metaverse and gaming platform Roblox suffered a 73-hour outage in October 2021. It left the company with an estimated $25 million in lost bookings, according to Data Center Frontier.In the financial industry, for example, where billion-dollar deals are dependent on who gets there first – every millisecond that data isn’t available could translate to lost revenue, partnership opportunities, and a potential exodus of customers.Netflix had an outage for 1.5 hours in July 2022 that affected customers in the United States, France, and India. That elicited negative responses like, “Friday Night and @netflix is down! How’s your Friday Night going?” reported Reuters.Hopefully, your Friday night with Baby Yoda is going just great!Curious to learn more about Redis Enterprise’s built-in disaster recovery capabilities? Watch our Tech Talk."
451,https://redis.com/blog/top-redis-youtube-videos-2022/,The 5 Most Popular Redis YouTube Videos of 2022,"December 22, 2022",Alex Patino,"Here they are – the five most popular Redis videos of 2022. The most viewed videos of the year offer practical advice, and they cover everything from caching and storing on Flash, and how Redis has grown to provide caching and enterprise-grade solutions under one system to make application development smoother and simpler.There is plenty to cover, so let’s get right to it. The countdown starts… here!Redis as a cache is well known. But since its start, it has become an in-memory cache and primary database, all in a single system: Redis Enterprise.In this clip, Redis’s Will Johnston provides a high-level overview of how Redis Enterprise powers applications. He gets into more granular detail in his blog post, Redis as a Cache vs Redis as a Primary Database in 90 Seconds.In this video, Will Johnston once again focuses on Redis Enterprise. Here he describes one key feature: Redis on Flash (RoF). RoF enables DRAM to expand its capacity by storing terabytes of data on solid-state drives (SSDs), greatly minimizing latency issues. Other Redis providers have the speed necessary to power robust applications but lack the software, such as RoF, that keeps data expenditures from rising. His post, “Lowering Costs with Redis Enterprise in 90 Seconds,” explores this further.If you work with containerized applications and are curious how RoF serves those deployments, Senior Group Product Manager Brad Ascar explains the concepts in further detail in Redis Enterprise for Kubernetes Now Supports Flash Memory.The internet clearly has a soft spot for Will Johnston! His rundown on the benefits of Active-Active Geo-Distribution, including its 99.999% uptime, sub-millisecond latency, and data persistence, struck a chord with many.How do large-scale operations manage real-time consistency across geographic zones? Will explains the basic concepts in just 80 seconds above and expands a bit further in “Learn How to Scale Redis Across the Globe in 90 Seconds.”Not all applications are built the same; they all have different needs. So if an application stack suddenly calls for a new architecture, one that calls for cloud migration, multicloud, or hybrid cloud, you had better hope you’re not stuck in a cloud lock-in.Flexibility is key. Just as monolithic applications are giving way to modern microservice architectures, so too is the cloud data space adapting to the needs of a digital-first culture. The one and only Will Johnston provides more insight into multicloud and hybrid cloud capabilities here and in his supplementary post, Understanding Redis for Cloud and Multicloud in 90 Seconds.The most viewed Redis video of 2022 packs a lot of information in just 90 seconds!Once again, Will Johnston manages to show the breadth of Redis Enterprise’s modern components, tools, and modules.What other Redis topics generated YouTube attention in 2022? A distinct range makes up this list. And even though not all of the videos below feature the great Will Johnston, we promise they’re still well worth the time."
452,https://redis.com/blog/kubernetes-adoption-inception-to-cloud/,"Tracing Kubernetes Adoption, From Inception to the Cloud","December 14, 2022",Redis,"Among the factors contributing to developers’ growing reliance on Kubernetes are its hybrid cloud capabilities, team-friendly portability, and cost-consciousness for cloud deployments.Kubernetes has come a long way since 2014 when Google first introduced it as an open source answer to Borg, Google’s internal container orchestration solution.Since then, the deployment system and containerized application tool has had many technical upgrades. After releasing Kubernetes v1.0 in 2015, it began supporting OpenAPI in December 2016, which enabled API providers to define their operations and opened the path for developers to automate their tools. By 2018, Kubernetes had become so mainstream that Google dedicated an entire podcast to it.Flash forward to the present day. Kubernetes’ popularity skyrocketed in tandem with the adoption of cloud-native services. According to the recent Splunk State of Kubernetes report, there’s been a “300% increase in container production usage in the past five years,” with large organizations being the predominant driving force behind Kubernetes’ mainstream adoption.According to the Cloud Native Computing Foundation (CNCF) 2020 Annual Survey, organizations using or evaluating Kubernetes ballooned from 78% in 2019 to 96% in 2022, making it the go-to platform for building platforms.That’s not to say that Kubernetes was immediately accepted by the development community. For a while, container orchestration required a technology choice among Kubernetes, Docker, and Mesosphere. Eventually, Docker and Kubernetes made friends with one another, and developers are comfortable using both. Mesosphere, however, lost their attention.Kubernetes has stayed open source, and one mark of its prevalence is the number of contributions to its codebase. There have been over 2.8 million contributions to Kubernetes made by companies.Kubernetes, as we know it today, comes with many automations already baked in, making it easier for development teams to get started. It’s easy to tick off the advantages on your fingers: Everything is managed through code and is easily portable if running on cloud. Kubernetes’ flexibility makes scaling applications a simple process. Containers are lean by design; they store only the must-have resources an application needs to run. That makes applications significantly faster and lighter and Kubernetes all the more appealing to developers.With each passing year, cloud computing has continued to gain traction while on-prem Kubernetes deployments have taken a 3% hit year over year, according to VMware Tanzu’s The State of Kubernetes 2022 report. Most Kubernetes deployments are in multicloud or hybrid clouds. “When we asked people about growth plans in the coming year,” the report states, “almost half (48%) expect the number of Kubernetes clusters they operate to grow by more than 50%; an additional 28% expect the number of clusters to increase notably (20% to 50%).Kubernetes’ growth is thanks in part to its continued investment in software development and infrastructure efficiency. Development teams have come to rely on the flexibility that orchestrating from on-prem, single-cloud, hybrid cloud, or multicloud provides. These container-based hybrid cloud and multicloud environments allow teams to handle massive workloads with minimal refactoring or replatforming.This developer efficiency is particularly true when building microservice-based applications. Since they’re comprised of different units, teams can more readily put their resources to the right tasks while working in one platform. That’s a considerably tougher request when working with a monolithic architecture. It’s a useful way of keeping too many proverbial cooks out of the proverbial kitchen, so to speak.Kubernetes’ bread and butter are its automations. By taking highly repeatable commands and setting them to automate, teams can eliminate unnecessary headcounts and sidestep any time-consuming IT tinkering. When surveyed, 45% of registrants in VMware’s report indicated they rely on Kubernetes because of its product capabilities and roadmap.The cloud infrastructure market grew 24% year over year in Q3 2022, according to Synergy Research. With more large enterprises including containerized cloud deployments in their stack and looking to freely move their data from cloud to cloud with no lock-ins, the ability to work in a hybrid cloud environment is becoming mission-critical for most large organizations; that ongoing trend is reflected in the 41% of respondents that choose Kubernetes for this very reason, as per VMware’s report.Most major cloud service platforms have a Kubernetes distribution tailored to their respective services, points out a 2022 Evans Data Cloud Development Survey. Foremost, 37% of developers use dedicated Kubernetes-based cloud services, such as AWS’s Amazon EKS or Microsoft’s Azure Kubernetes Service. “These implementations of Kubernetes, with varying levels of service, use these companies’ internal cloud back-ends for container orchestration,” the report explains. In the Evans Data report, 32% of developers use a commercial Kubernetes distribution, and 28% own a managed “vanilla” Kubernetes.The pandemic drove digital services and experiences into overdrive, which encouraged the already-in-progress transition to businesses moving services online. One result was that cloud deployments escalated – along with application load. Back in June 2018, Kubernetes 1.11 was released, which introduced IPVS-based in-cluster load balancing, an advancement that greatly increased application production scalability.Load balancing is key, as it’s a hands-free means of maintaining infrastructure efficiency, which in turn, keeps cloud costs from skyrocketing. According to Cast.ai, companies spent an exorbitant $16.2B in cloud waste in 2022. This overprovisioning is blood-letting dev teams of their available resources. Automations that save money, time, and manpower are the goal, though that has required teams to implement their own tools for automatic failover through the commercial support of an operator.As mentioned before, the core of Kubernetes has many built-in automations, but thanks to the Kubernetes operator pattern, it’s possible to automate tasks beyond what’s in the Kubernetes software.Kubernetes is great at keeping costs low by load-balancing cluster nodes in order to reach their ideal state. An operator goes a step further by adding an extra layer of orchestration, one that automatically steps in in the event of an automatic failover, monitors resources via a reconciliation loop, or even scales a cluster automatically.And if you use Redis – or you are considering doing so – we make it even easier. The Redis Enterprise Operator for Kubernetes streamlines and automates the management of the Kubernetes layer. It’s the result of everything learned, millions of cluster deployments later.For anyone running Redis on Kubernetes and interested in comparing the Redis operator and Helm charts, read An Introduction to the Helm Tool and Helm Charts."
453,https://redis.com/blog/redis-enterprise-cloud-terraform-1-0-0/,Redis Enterprise Cloud Releases Terraform Version 1.0.0,"August 31, 2022",Noam Stern,"Redis is investing in its Terraform registry, helping developers manage their databases across multiple modules easily. Let’s see what new updates Redis Enterprise Cloud’s Terraform version 1.0.0 brings, how to implement it, and what to consider before upgrading.Terraform is an infrastructure as code (IaC) tool that enables users to manage, build, change, and version their infrastructure safely and efficiently. Redis Enterprise Cloud solutions support multiple Terraform resources, such as accounts, subscriptions, and databases for a different cloud provider. Let’s see what changes you can expect with Terraform 1.0.0.Create a Terraform subscription.The most significant difference between prior versions and version 1.0.0 is that the database resource is extracted from the subscription resource. Until now, the database was a nested block inside the subscription resource.Now, we are introducing a new resource called ‘rediscloud_subscription_database,’ which will allow users to manage their databases on a separate logical resource and even by different configuration files if required. It can decrease provisioning time, improve the user experience, and even increase security in scenarios where other team members have different permissions on the databases within a subscription.Redis has also improved some of the functionalities in its resources. One critical implementation is users can create or modify a database with multiple modules. You can read about the changes in the Terraform Redis Cloud Provider CHANGELOG.The new version contains breaking changes. If you’re already using Terraform for Redis resources with a version prior to version 1.0.0, you must import your existing configuration file to a new directory before upgrading the version.The process is straightforward. It usually won’t take more than 10 minutes (depending on the number of resources in your TF configuration file). Watch the video above for a quick walkthrough of how the importing process works.Follow the four steps guide in the Redis registry in Terraform."
454,https://redis.com/blog/redis-om-net/,Redis OM .NET Update,"August 4, 2022",Steve Lorello,"Embedded documents, indexed arrays, and other awesome additions to Redis OM .NET!In November 2021, we released the v0.1.0 version of Redis OM .NET (an object mapping library for Redis). Naturally, as with any early release software, there’s more to be done. Since then, we worked out a few of the library’s quirks. We also added several exciting new features to .NET to get developers productive quickly.Before we look at what’s new, if you need a general overview of Redis OM, you can check out the announcement post or watch the video below.Now, let’s check out the new features added to Redis OM .NET!Redis OM .NET now allows you to index the fields in embedded objects within your model. For example, suppose you have a Customer model with an embedded Address model:Previously, you would not have been able to index the embedded Address within Customer. Now, Redis OM .NET provides two ways to do this.Let’s say you want to share your embedded model between other models and don’t want to over-index fields within the embedded model.You can leave the embedded model alone and index it from your model using JSON paths relative to the root of your model. To index City and State within the Address of the Customer:Another way to accomplish this is to declare what you want indexed within the Address class, and cascade into it using the IndexedAttribute’s “CascadeDepth” property. So with the following Address Model:You can Index from your customer model like so:CascadeDepth refers to how deep into the object’s graph you want to index. Setting the Cascade depth will cause Redis OM to follow all of IndexedAttribute‘s and SearchableAttribute‘s for fields to index within your model’s object graph.With one of these two modes followed, when you create the index, the embedded document will be indexed as you would expect, and when you then run LINQ queries like this:… Redis OM knows how to properly build the query based on the provided model.Another awesome feature we’ve added to Redis OM since v0.1.0 is the ability to index and query arrays of strings within JSON objects. Decorate the array with the IndexedAttribute.With this done, you can query these arrays using the Array’s Contains method:Redis OM previously only supported the indexing of numerics, strings, and GeoLocs. Since version 0.2.0, you can also index Enums, ULIDs, GUIDs, and Booleans. To do so, use the IndexedAttribute to decorate those fields within your model.One pain point for users of aggregations is that not all fields can be marked as or necessarily are marked as aggregatable. As a result, those fields were previously ineligible to be used in Aggregation within Redis OM. NET. With the new Load and LoadAll methods within the AggregationSet in Redis OM, it is now possible to Load the fields not marked as aggregatable.Let’s take the Customer model we used earlier and add an age field to show how it works.Age is indexed, but not marked as aggregatable. Previously, we could not use Age in our aggregation pipeline. If we ran something like this, we’d get an error because Age is not loaded in the pipeline:Now, in Redis OM .NET, you can call Load on the AggregationSet Loading either a single field or many (by initializing an anonymous object):To just load the Age Or:To load multiple fields out of the model, notice the anonymous type.Another quirk of aggregations is that they do not return well-formed objects. The reason for this is that the aggregation returns with the result of its pipeline, which is usually exactly what it needed to complete all the operations requested of it, and the results of its operations. That’s fixed now.With the new Hydration API, you can cobble these bits into a more usable format, allowing you to fully or partially hydrate your model from a RedisAggreagtionResult.Let’s revisit the examples from the Load section. What if we wanted to bring back our entire customer model along with the results of the apply function? We can do so by calling LoadAll instead of Load. Then we call Hydrate on each RedisAggregationResult, as it is enumerated. Hydrate attempts to bind anything in the AggregationResult to your model. Therefore, may have empty fields that were not present in the result.In previous iterations of Redis OM, if you wanted to update an item, you needed to enumerate it, update it, and then save the whole RedisCollection. This can be burdensome. We added a new API to allow you to easily update the objects you stored in Redis.Similarly, we made it easier to delete objects you inserted into Redis. To delete objects, execute:One requested feature is a passed ConnectionMultiplexer to initialize the RedisConnectionProvider. This constructor allows you to use whatever multiplexer you are already injecting into your other services with Redis OM.Redis OM now supports configurable chunk sizes for automatic pagination under the hood. Previously, a fixed chunk size could have performance implications with huge query results. Small, non-configurable chunk sizes, combined with large result sets, can lead to many round trips that Redis OM has to make to materialize the entire result set. Now, you can pass in the desired chunk size to Redis OM.Note that this is subject to the MAXSEARCHRESULT configuration parameter in RediSearch.We’ve been up to quite a lot over the past few months, with many exciting new features added to Redis OM .NET and the inevitable bug-bashing that comes with any piece of software. You can keep an eye out for new features by watching the GitHub Repository. If there are features that you are interested in seeing in the library, please open an issue on GitHub."
455,https://redis.com/blog/high-availability-architecture/,High Availability Architecture Demystified,"July 1, 2022",John Noonan,"A high available architecture is when there are a number of different components, modules, or services that work together to maintain optimal performance, irrespective of peak-time loads.In its purest sense, this system allows businesses to work continuously without failure over a given period of time. Many businesses can’t afford even a minute of downtime. Considering that data is the lifeblood of many businesses, even just a short period of downtime can be incredibly costly.In certain real-life scenarios, lives may depend on a database built for high availability. When a patient arrives in the emergency room, medical professionals need instant access to their medical health records to understand what treatment decisions are best. Any delay in accessing this information could have a devastating impact.Note: High availability is often measured in the percentage of time that a service is available to users. According to the Microsoft Network Developer Glossary, for a server to be considered “highly available”, it needs to achieve 99.999% network uptime.High availability clusters are a group of hosts that merge as a single system to prevent downtime. If one server in a high availability cluster goes down, the mission-critical app is immediately transferred to another server as soon as the fault has been detected.No system is immune to failure, and high availability clusters ensure that optimal performance levels are maintained regardless of inevitable failures. As a result, these tend to be used for the most mission-critical applications, websites, and transaction processing systems.A high availability cluster will utilize multiple systems that are already integrated, so should a failure cause one system to fail, another can be efficiently leveraged to maintain the continuity of the service or application being used.The high availability load balancing cluster plays a crucial role in preventing system failures. Having a load balancer in place essentially distributes traffic across different web nodes that are serving the same website or application users. This reduces the pressure on any one server, allowing each cluster to work more optimally while allowing traffic only to be sent to healthy servers.The active/passive cluster is made up of at least two nodes. As the name implies, not all of the nodes will be active. If one node is active, the second is a read-only on standby. The passive server acts as a backup and will be utilized should the active server fail to work.This type of cluster typically uses at least two nodes that execute the same service at the same time. In an active-active cluster, both nodes act as primary nodes, meaning either can accept reads or writes. Should one node fail, the user will automatically be connected to the other to ensure continuity of service. Once the first node has been replaced, users will then be split between the two original nodes.The overarching benefit of the active/active cluster is that it allows you to accomplish node-network balance. If server failure instances are detected a load balancer will transmit user requests to the servers that are readily available and then analyze node-network activity. The load balancer will then push traffic to the nodes that are capable of serving that traffic allowing for greater levels of fault toleranceThis strategy follows a cyclical process, similar to the round-robin model, whereby users are spread randomly across available nodes, or conversely, may adhere to a weighing scheme where one node is prioritized over another based on a percentage.A general rule that’s followed in distributed computing is to avoid single points of failure at all costs. This requires resources to be actively replicated or replaceable, without a single factor being disrupted should the full service go down.Imagine if you had fifty running nodes that were powered by one database. If one node fails, it will not have an impact on the persistent state of others, irrespective of the number of running nodes.But should the database fail, the entire cluster will go down, making the database a single point of failure? This is referred to as a shared disk cluster.On the other hand, should each node maintain its database, a node failure will not impact the entire cluster. This is referred to as a shared nothing cluster.Note: If you want to discover more about high availability clustering technology then make sure to watch this webinar. With over 20 years of experience in the software industry, George Carbonnel will unpack everything you need to know about how clustering technology with Redis Enterprise delivers high performance as well as high availability.There are a number of different requirements that you’ll need to maximize durability and high availability. These include:Load balancing is crucial to any highly available architecture. Its primary function is to distribute traffic across backend servers to transmit data more efficiently as well as prevent server overloads. A prerequisite of any load balancing system is to identify what failover process should be carried out when there’s a node failure.The ability to scale databases or disk storage units must be taken into account by all highly available architectures. There are two solutions you can pick between to achieve scalability:We live in a fast-paced digital world where being able to distribute highly available clusters across the globe is now mandatory. Doing so will ensure that if a natural disaster strikes a single location, the impact made will not hinder their ability to provide the service.For all its consistency, highly available architectures will always be susceptible to some sort of malfunction that can disrupt service. Therefore, should a service go down, businesses must have a recovery strategy available to get the entire system running again as quickly as possible.This is often referred to as disaster recovery – a set of policies and procedures designed to return a service to full functionality in the event of a disruptive event.High availability is often measured in the percentage of time that a service is available to users. This is done by dividing the total uptime by the system period, which is then multiplied by 100 to get a percentage. According to the Microsoft Network Developer Glossary, for a server to be considered “highly available”, it needs to achieve 99.999% network uptime.Quite often the percentage availability is referred to as the number of nines in the digits. So four nines would be 99.99%.Note: 99.99% availability is considered the industry standard.There are a number of steps you can take to maximize high availability, ranging from the number of components you have to check through to replacing failed servers. Here are some practices that you can use to achieve high availability.Geo-redundancy is a crucial line of defense against the outbreak of natural disasters that can lead to service failures. This practice involves deploying numerous servers across different geographical locations, thereby spreading the risk and allowing the architecture to fall back on a different server should a natural disaster strike one region.Note: You can easily achieve this with a database that has Active-Active Geo-Distribution.High availability architectures usually involve numerous loosely coupled servers that provide failover capabilities. A failover is seen as a backup operational mode that is automatically utilized when the functions of a primary system go down.As mentioned previously, a load balancer will spread incoming traffic across different servers to mitigate the risk of any downtime. Be sure to configure your load balancer to utilize an algorithm that’s tailored to your needs to fully optimize this solution.RPO is a marker for the maximum amount of data you can lose without causing harm to your organization. This highlights the data-loss tolerance of your business as a whole and it tends to be measured in time units, e.g. 1 minute or 1 day.Setting your RPO to less or equal to 60 seconds will help you maintain maximum availability. Doing so will ensure that if there is a primary source failure, you won’t lose more than 60 seconds worth of data.Redis Enterprise is a powerful solution for any large corporation looking to achieve maximum availability. It’s a real-time data platform that ensures five-nines availability that provides elite automated database resilience while mitigating hardware failure and cloud outages risks.Redis Enterprise meets the high availability needs of the most mission-critical enterprise applications. It offers industry-leading functionality to provide 99.999% availability using: Active-Active Geo Distribution, automatic failover, intelligent clustering, a shared-nothing architecture, and global distribution.Want to learn more about how to achieve high availability?"
456,https://redis.com/blog/data-economy-customer-experience-with-data-driven-approach/,The Data Economy: Taking a Data-Driven Approach to Customer Experience,"May 31, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.You can often tell how serious a company is about new strategic objectives based on the roles they create to fulfill them.In recent years, we’ve seen myriad new titles created to reflect the importance of customer experience, machine learning, and digital transformation. At Citizens Financial Group, which is among the top 25 largest banks in the United States, this is evidenced in a role created two years ago: Chief Experience Officer. It’s a customer-focused role designed to ensure Citizens Bank builds capabilities that will continuously allow it to serve customers better while leveraging data to differentiate itself from competitors.Beth Johnson took on this role in January 2022 and has since been dedicated to improving customer experience while building a data-centric culture that embraces digital transformation. In this episode of the Data Economy Podcast, Johnson discusses the importance of using data strategically to improve the customer experience and the roles talent and culture play.In describing her role, Johnson discusses the importance of understanding customers – who they are, what they care about, and what’s important to them. She explains that you can’t always glean those answers through conversations and surveys.“I think the mistake people make is sometimes we think customers know what they care about and what’s important to them,” Johnson says. “But you can’t just ask them. You have to observe the behavior of our customers and how we interact to make sure we understand those needs that we each feel – and I use the word ‘feel’ intentionally – as well as rationally think about it so that Citizens can support them in their lives, and in the banking partnerships that they have.”She hits on an important point that I believe can be easy for many data leaders to lose sight of when they’re in the weeds working with data: The need to understand the emotional side of what they’re working on and not just the technical side.“I think the biggest mistake data and analytics organizations make is making it all about the data, or all about the math, or all about the technical components, versus tapping into those emotional components,” Johnson says, later adding: “It’s really that emotional end state for the customer that matters if we’re going to grow our business and continue to be relevant to our clients.”In the podcast, Johnson cites an example related to customers seeking home equity loans for a kitchen renovation. From an emotional perspective, customers will be happier if they can get their loans approved faster and start their renovations sooner. From a technical perspective, that means using data and analytics to streamline the process for generating a home equity loan with Citizens.“It’s that emotional end state for the customer that matters if we’re going to grow our business and continue to be relevant to our clients,” Johnson said.For technologists and data scientists, the lesson here is to learn the customer outcomes and pain points by partnering with business leaders. Then, work backward on what’s optimal, from data structures to cloud infrastructure, that meet short and longer-term objectives.As the Chief Experience Officer, Johnson has a laser focus on improving the customer experience, and one key way to do that is to use real-time data. Data is critical in financial services, from fraud protection to using data-driven insights to personalize experiences for customers. And the ability to tap real-time data to deliver instant experiences can be another way banks differentiate themselves.In the interview, Johnson discusses a couple of areas where real-time data provides improved services to customers in the form of overdraft protection and real-time fraud prevention. She suggests leaders ask themselves these questions when thinking about the power of real-time data and its role in building new products and services.“Does the availability of this real-time data change the way that you think about interacting with the customer and the design of your products and services? How does that ripple through your thought process?” Johnson asks.As customer experience and data becomes more inextricably linked, Johnson says Citizens is being very deliberate about solidifying the connection between the two. The 194-year-old banking institution recently created a new Chief Data and Analytics Officer role that will report to Johnson.“We’re being very intentional about combining the data role with the analytics role and the insights roles so that we can make sure we’re focused not only on the technology and the scaling but also being very use case and customer-driven,” Johnson said.For those of you who like hearing how other organizations are structuring and using their data, you won’t want to miss the discussion Johnson and Krigsman have on Citizens’ setup. They touch on the role of the cloud, data marts, and Citizens’ data intelligence platform and how Johnson sees the difference (or lack thereof) of data for operations versus innovation.Having a data-driven company-wide culture is key when it comes to making the most of data for customer experience and beyond. That can be especially difficult for long-standing companies and requires commitment and consistency. In the interview, Johnson discusses the steps she has personally taken to imbue a data and digital-centric culture at Citizens, including her partnership with human resources and the organization’s head of learning and development. And she shares a welcomed reminder that you can’t expect change to happen quickly.“You’ve got to stay at it,” she says. “It’s a constant drumbeat, and it’s one we’re going to continue to make progress on over the years.”Indeed, the race is on for organizations of all sizes to transform how they provide valuable services and personalized experiences. I believe that those companies that take steps like Citizens, focus on customer experience, and develop differentiating capabilities with real-time data will outpace their competitors."
457,https://redis.com/blog/data-economy-podcast-real-time-data-for-modern-business/,The Data Economy: Evolving Your Business at the Speed of Real-Time Data,"May 24, 2022",Isaac Sacolick,"The Data Economyis a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.What will be different about IT and data in the years ahead? The simple answer is speed.Business leaders are challenging their IT and technology teams to release mission-critical applications and data service MVPs faster than ever before. And the pace and expectations are only going to accelerate.Shawn Bice, President of Products and Technology at Splunk, believes this accelerated pace will drive businesses to respond to their customer base in near real-time. In this episode of the Data Economy podcast, Shawn explains why he thinks real-time data will play a more prominent role for organizations that want to serve their customers better and compete in the years ahead.Here’s how Shawn contextualizes just how far data technology has come in the past 50 years: “From pretty much 1970 almost all the way up to the year 2000, most people would have built an application on a relational database system. Today, there are over 350 different systems tracked by DB engines. What’s happened is we used to have this one-size-fits-all database for almost three decades, and today it’s really these purpose-built systems that are very specialized for a given access pattern. That’s the world we’re in now.”It is quite remarkable to think about this shift and how huge the technology aspects of this change are, especially in competitive industries. We’re in a time when real-time data platforms are a source of differentiation for organizations of all sizes, and latency is the new outage. In the podcast, Shawn cites how consumers are growing accustomed to real-time updates thanks to everyday applications like rideshare and food delivery applications.“If you think of every application that you have on your phone where there’s some type of real-time activity happening, that just gives you an idea of how common it is becoming.”Shawn’s right. Just looking at the home screen on my iPhone, I see that the most useful and engaging applications use real-time data. The implications of this are that it’s only a matter of time before your customers demand real-time feedback from you, or before your competitors figure out a way to use real-time data to outfox you.And even if you can’t see a need for customer-facing real-time data, Shawn points out internal use cases, such as reporting, as one area where real-time data can make a difference.“You look at a productivity worker today and they’re looking at multiple dimensions of data on the fly, touching screens, in full context moving through entire business workflows by the touch of a finger. So yeah, the world has changed quite a bit,” Shawn says.IT leaders must recognize the importance of reinventing their data architecture, scaling beyond apps that don’t support the fast-growing digital demand or getting trapped with infrastructure that’s a business limiter. That means looking at ways to modernize data structures and leverage real-time databases and caching.Shawn recommends that technology innovators “go figure out a new graph database, a time-series database, or explore a ledger and compose it into the data architecture.” But he also guides technologists to understand the use case, “because if you start with the tech first, whatever the limitations of that tech are could constrain your application ideas.”CIOs who want to tap the full potential of their data will likely find that their biggest challenge isn’t the technology itself but rather the human side of the business. And you might be thinking this means building data literacy across your entire organization—and it certainly does—but it also means making sure that you don’t modernize your technology without modernizing your workforce.“A lot of folks have had people on their DBA teams or in their IT departments that have been there for 10, 15, 20 years. And sometimes those people have so much institutional knowledge, it can be great. But if somebody like that doesn’t really want to embrace some of this new technology, you really could find yourself stuck in the past,” says Shawn. He goes on to describe the importance of identifying change agents who are excited and eager to say “yes” to new data challenges.“Don’t let familiarity become a blind spot that stifles innovation,” Shawn warns.Shawn also discusses the importance of a strong data foundation for organizations that want to reinvent themselves or create new customer experiences. Take stock of your data foundation by reviewing today’s limitations and forecasting future business needs. Start by asking basic questions: Are you slow to provision database instances, or are your users experiencing performance degradations during peak periods?Shawn’s litmus test for determining the strength of your data foundation comes down to whether or not you often find yourself saying “no” to ideas from business leaders, as in “no, we can’t do it; we can’t get that data, or it’s not possible.” If the dreaded “no” is coming out of your department more often than not, then it’s time to take a good hard look at why.Another data challenge CIOs often come up against is business leaders who would rather rely on their gut instinct instead of their presented data. Shawn cites a heart-pounding example from Formula One, where ignoring data and going with a driver’s gut literally cost the race. Overcoming this gut versus data challenge often comes down to strengthening your data governance, which Shawn discusses in more detail in the podcast.Shawn leaves listeners with crucial advice on how to approach modernizing their technology, and it all comes down to taking full advantage of the cloud, fully managed APIs, and exploring new purpose-built systems. It’s sound advice for organizations driving digital transformation and recognizing the competitive advantages of establishing smarter, faster, and more innovative data-driven organizations.And that all points to modernizing real-time data capabilities!Please tune in to the podcast to hear more of Shawn’s insights on modernizing data fabrics, selecting database technologies, and establishing data governance."
458,https://redis.com/blog/data-economy-improving-data-literacy/,The Data Economy: Improving Data Literacy,"May 17, 2022",Isaac Sacolick,"The Data Economyis a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.A strong data-driven culture is the latest non-negotiable for organizations that want to put their data to work and stop relying on gut decisions and guesswork.In the year ahead, you’re going to hear more about the steps many organizations are taking to make data-driven decision-making core to executing company plans. Shy Chalakudi, Head of Enterprise Data Analytics and Digital Technology at Hewlett Packard Enterprise (HPE), is ahead of the pack, helping instill the data literacy and data focus needed to make data-led decisions ubiquitous at the 59,000-person company.Shy was a recent guest on The Data Economy CXO podcast, presented by Redis and hosted by Michael Krigsman of CXOTalk. She discusses the importance of embedding data into every aspect of an organization, the imperative of managing data effectively, and the critical role data is playing in HPE’s digital transformation.In the conversation, Shy’s passion for a data-driven future shines through when she declares that “data is going to become a universal language,” with data being what’s “going to let us communicate with each other and the machines, together in one common language.” For HPE, this means collecting zettabytes of telemetry data from the systems and devices it sells and using this data to improve customer experiences and its products.Shy has a clear message for CIOs tasked with digital transformation: Becoming a data-driven organization is essential to creating new business opportunities, improving customer satisfaction, and addressing new marketing opportunities. For HPE, as in many large companies, operating in a data-driven way starts by knowing the customer and customer 360 architectures that help centralize data and create a single version of the customer’s journey and activities. Companies in financial services and retail should have customer data initiatives at the top of their priority list.Shy oversees all aspects of global data initiatives at HPE, a Fortune 150 business information technology company known for its cloud and infrastructure services. Her team’s charter is to drive a data-first modernization for the company, which includes creating a unified data source so that all aspects of digital transformation are rooted in data.“My team members live and breathe to ensure that we create a sophisticated, automated, simplified view of data that can be consumed by both internal and external customers,” Shy explains.Among the focal points of Shy’s organization is data management—handling the governance of everything from data lineage, data profiling, data quality, and beyond. Her team’s work supports HPE’s efforts of changing its billing pattern from a monthly or yearly subscription to usage-based billing. Shy notes that “Data is at the front and center because you must know your customer and know what solution they are looking for in order to provide a customized, personalized service.”Real-time data is important when you want to respond to your customers quickly, Shy says. Indeed, when you have access to real-time customer data, you can make smarter, faster decisions that can allow you to stand out from your competitors. When you can make decisions more quickly, you can also make more accurate predictions through real-time analytics and decision capabilities.Any IT leader who has ever led any aspect of data transformation knows that collecting and delivering the data is only half the battle. If stakeholders aren’t using the data, then the effort falls flat. Shy’s team is working to avoid this adoption pitfall by embedding a data culture in HPE.In the podcast, Shy describes the model HPE is using to spread data literacy throughout the massive enterprise, noting that “every consumer of data has to be data literate in order for it to be successful.” At HPE that means every business organization has a data leader who is a senior leader in the organization responsible for supplying answers to key questions such as “What data do I need to run my business? What data am I missing that would help me push my business further?”Two things don’t surprise me about Shy’s story: First, she emphasizes getting the architecture, DataOps, and data governance in place before seeking machine learning and AI results. IT must be able to scale the data layer in hybrid cloud architectures automatically. “We think that machine learning and artificial intelligence are going to solve all of our problems,” Shy says.  “Trust me, I am a doctoral student and wrote a thesis on AI/ML. I value it very much, but I think it’s important to understand that it is garbage in, garbage out.”Second, Shy has the benefit of having HPE’s CEO champion the company’s data-driven efforts as it pivots its business model to deliver solutions, personalize services, and transform to a subscription business model. The CEO’s direction sets a strategy, mission, and culture change that helps Shy drive new ways of working.And while the top-down support provides air cover for Shy’s team to bring data literacy to each business unit, it doesn’t stop there. In the podcast, Shy discusses the important roles that “data stewards” and “data custodians” play in governing and managing data and ensuring it meets the needs of business partners.Developing a data literacy program is critical for building a data-driven culture. You can collect and distribute all the data you want, but if your colleagues don’t have the right skills to use data in their roles and draw insights from it, then your efforts will only take you so far. As Shy puts it: “We all have to become data literate in order to survive today—in any industry.”Please tune in to the podcast to hear more of Shy’s insights on the architectures and culture that enable HPE to process zettabytes of data while centralizing data and knowledge about its customers.Watch more episodes of The Data Economy podcast."
459,https://redis.com/blog/data-economy-electronic-arts-gaming-experiences-with-real-time-data/,The Data Economy: EA Unlocks Immersive Gaming Experiences with Real-Time Data and Emerging Tech,"May 10, 2022",Isaac Sacolick,"The Data Economyis a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.It can be easy for technology leaders to get shiny object syndrome—to be dazzled by the promise of NFTs, cryptocurrencies, and the metaverse. and yet, falling behind emerging tech can lead to digital disruption. At Electronic Arts (EA), Farah Ali’s job is to focus on the emerging technologies that will impact her business and customers over the next several years.Farah is Vice President of Technology Growth Strategy at EA, which is one of the world’s largest video game companies known for a portfolio of well-known titles, including The Sims, Madden, and FIFA. However, EA’s mission goes far beyond developing games. The company focuses on building delightful player experiences by using gaming, as Farah says, “to build connection and meaning while keeping curiosity alive, and doing so regardless of a player’s age or background.”Farah, a recent guest of The Data Economy podcast, discusses the important role data plays in improving customer experience, how the cloud is helping EA to scale, and many of the challenges companies targeting real-time customer experiences are likely to face.When a company fully embraces customer experience as part of its mission like EA has, you can count on technology and data to play a critical role in every aspect of the business.And when it comes to data, EA has an abundance of it. The company receives petabytes of data daily, Farah says, with more than 300 games and 500 million players in the EA ecosystem. There’s also historical data dating back to the company’s founding in 1982, some of which still plays a role in decision-making, Ali says.Like many CIOs and CTOs, Farah is focused on ensuring EA stays on top of technology trends. At EA, that means making full use of data and planning for the emerging metaverse, along with keeping up with other emerging technologies, such as NFTs, in-car gaming, and more.Many CIOs and CTOs covet a single-pane-of-glass view into all of their company’s data. It’s a worthy goal, yet Farah says that technology leaders should strive to do more than provide data—where possible, they should seek to build tools that provide actionable insights.“People don’t like looking at graphs. They don’t like looking at dashboards. They want to understand the data in the context of what they’re doing,” says Farah.Real-time data can be especially helpful for certain roles and functions, empowering leaders to make decisions faster and ensure optimal customer experiences. Farah has identified where real-time data is most powerful at EA. She describes the four key personas her teams consider when determining who benefits from real-time data access. While serving business functions is important, she highlights how real-time data is important to the product managers, developers, and data scientists charged with building and enhancing games.“Are the players actually interacting with the game the way it was intended? It’s a constant loop of playtesting, getting the data, going back to the drawing board, and iterating. There’s a lot of real-time telemetry, but it’s not always just for live games, it’s also for games that are currently under production,” she says.CTOs who are focused on improving real-time customer experiences, developing digitally enabled products, or experimenting with emerging technologies should note that the product, DevOps, and data science organizations will be the first customers of real-time data platforms.Real-time data and experience are often associated with preventing lag. In the podcast, Farah explains how her team takes it a step further, using it to personalize at scale, create new worlds, cultivate realism, and improve overall game quality.Flexibility and agility are critical when hundreds of millions of global users access your systems. Several years ago, EA made the deliberate decision to move nearly all of its workloads to the cloud, citing scale and efficiency as the primary reasons for the move. “It really helps us plan,” says Farah.Any CIO who has ever tried to guess what traffic will look like during a new product launch or holiday sale will appreciate the challenges she describes in the podcast. EA taps years of historical data to estimate how many peak users they’ll have during key events. Having real-time data collection and monitoring tools ensures that adjustments can be made in real-time when traffic spikes or dips unexpectedly.“Even if we’re off by a big magnitude, from one to twenty, we can still course correct if we are monitoring,” Farah says.The world is no longer static or slow-moving, so technology organizations in most industries should develop their data processing and platforms with the future in mind. IT leaders and architects may benefit from the data stream and telemetry analytics for processing time-series data, especially with IoT, financial transactions, and anytime there are large volume and velocity data sources.While the cloud is now commonplace, virtual worlds may be the next frontier in gaming and other customer experiences. As Farah thinks about the possibilities presented by the convergence of the elements that make up the metaverse—social media, augmented reality (AR), virtual reality (VR), online gaming, crypto currencies, and more—she notes the importance of gameplay experiences for players and making sure they have the best user experience.While some CIOs and CTOs may be wondering if they should even be paying attention to the metaverse, Farah is already asking questions about how players will be able to move their online identities between platforms.“There’s a lot of content that people will create. Who’s responsible for governing that?” she asks. “It’s the collective community that will decide what those experiences look like, whether something should be personalized or not, so I’m very curious how that will evolve.”Farah’s role requires her to be on the lookout for emerging trends to turn into incubation and exploration projects that could someday fuel the future of EA. And at the core of it all is processing, storing, and developing analytics on real-time and aggregated data.It’s challenging to forecast when technologies will become mainstream and impact customer experiences. Technology and data leaders can future-proof their architecture by standardizing on multicloud data layers and optimizing with flexible data structures. CIOs, CDOs, architects, and data scientists should blend internal and external data sources to support real-time decision-making, middle-term road mapping, and longer-term forecasting.Tune in to the podcast to hear more of Farah’s insights on improving customer experiences, scaling real-time data platforms, and planning for emerging technologies.Watch more episodes of The Data Economy podcast."
460,https://redis.com/blog/data-economy-podcast-digital-twin-for-real-time-predictive-analytics/,The Data Economy: How a Digital Twin Drives Real-Time Predictive Analytics,"April 19, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.“We are an engineering company at heart and hide all the complex things we do in the back so that it looks very simple.”That’s how Satyan Parameswaran, President of UPS Information Technology, describes the mission and culture at UPS. Satyan was a recent guest on The Data Economy, a podcast presented by Redis and hosted by Michael Krigsman of CXOTalk. He shares several fascinating insights into how a logistics company drives customer satisfaction, addresses new market opportunities, and improves its reputation using data.Behind the scenes are several impressive technologies, including a digital twin, advanced real-time experience technologies, internet of things (IoT) devices, and predictive analytics capabilities that help UPS manage a complex logistics network.Their operation certainly looks simple when a smiling driver delivers your important packages every morning, but behind the scenes, it is a complex logistics function operating in 220 countries and over 2,000 facilities. The company delivers 26 million packages and documents per day, and on any given day, there can be 50-60 million packages in transit, generating over 1 billion event data points.And what keeps Satyan up at night? The team behind the scenes at UPS knows they must scale and serve global customers while keeping costs intact. When the global demand for shipping fluctuates daily, and when there are all kinds of logistics issues, including extreme weather, flight delays, and power outages, UPS has the incredible challenge of predicting what resources they need across their facilities and network.Satyan and a multidisciplinary team of developers, process engineers, and data scientists turned up the heat—quite literally—and developed a harmonized enterprise analytical tool (HEAT), a digital twin that operates in real-time and predicts the resourcing needs at its facilities.A digital twin is a real-time model of a physical system or process. Developing digital twins requires modeling the underlying systems and sharing real-time data with them to create predictions. Digital twins are often used to simulate real-world conditions for prototypes, such as jet engines and building architectures, but UPS uses its digital twin in a production process to predict demand and provide facility directors with recommendations on resourcing needs.Satyan says, “To gain confidence, we have to predict at the mid-to-high 90%. It helps us manage the network, which is the living, breathing thing that allows us to move packages so that we do not disappoint the customers. Our multidisciplinary team started with the process engineers and created a product that actually works; otherwise, it would look like a science project.”Satyan provides many insights and advice to CIOs, CTOs, architects, and technology leaders on bringing real-time experience platforms like digital twins and machine learning models from the lab to production. That’s key because a recent study on bringing AI from experiments to production found that 88% of decision-makers expect increases in AI/ML use cases, but 40% believe their current data architectures won’t meet their future model-inferencing requirements.According to Satyan, “If you are not capturing and interpreting data quickly, you might lose the context of what the data is trying to tell you.” He recommends having a strong focus on data quality because having “tons of data does not mean that you have high-quality [data].”Real-time experiences and data processing capabilities used to be goals of only the most advanced financial services, digital advertising, gaming, and other tech-savvy companies. Today, more companies recognize the importance of developing advanced real-time experiences that use search, recommendation engines, time-series analysis, and graph databases. For example, retailers are developing real-time inventory systems, crop insurers are collecting real-time data from drones, and hospitals are transforming the patient experience.The world needed UPS’s technology and logistics capabilities to deliver vaccines to over 100 countries at over 99.99% reliability. UPS’ smart labels, a real-time IoT built on RFID technology that UPS has been iteratively improving since the mid-1990s, were fundamental to building a self-declaring package required for healthcare. “We didn’t anticipate that the world would go through COVID, and we were just getting prepared to deliver healthcare products at scale.”And isn’t that the point of developing scalable, real-time experiences and accelerating innovations? Because as technology leaders, we just don’t know what the next business opportunity will require, but we do know that high-quality, reliable, and real-time analytics are the baseline engineering capabilities. Tune in to the podcast to hear more of Satyan’s insights on creating an engineering and analytics culture that brings emerging technologies like digital twins and IoT RFID devices to deliver significant business outcomes.Watch more episodes of The Data Economy podcast."
461,https://redis.com/blog/get-sql-like-experience-redis/,How To Run a Redis SQL Query Without Disruption,"April 14, 2022",Roshan Kumar,"Related E-Book download: The Importance of In-Memory NoSQL DatabasesRunning a Redis SQL query doesn’t have to be difficult. I actually raised this point a few years ago when speaking to a friend who manages data warehousing solutions at a retail company. We began talking about Redis’ queries once he explained a problem that he was facing.“We have a pain point with our data warehousing solutions. We have use cases where we need to record data and perform analytical operations in real-time. However, sometimes it takes minutes to get the results. Can Redis help here? Keep in mind that we can’t rip and replace our SQL-based solution all at once. We can only take a baby step at a time.”Now, if you’re in the same situation as my friend, we have good news for you. There are a number of ways you can run a Redis query and introduce Redis into your architecture without disrupting your current SQL-based solution.Let’s explore how you can do this. But before we go any further, we have a Redis Hackathon contestant who created his own app that allows you to query data in Redis with SQL.Watch the video below.It’s quite straightforward to map your table to Redis data structures. The most useful data structures to follow are:One way that you can do this is by storing every row as a Hash with a key that’s based on the table’s primary key and have the key stored in a Set or a Sorted Set.Figure 1 shows an example of how you could map a table into Redis data structures. In this example, we have a table called Products. Every row is mapped to a Hash data structure.The row with primary id, 10001 will go in as a Hash with the key: product:10001. We have two Sorted Sets in this example: the first to iterate through the data set by the primary key and the second to query based on price.With this option, you need to make changes to your code to use Redis queries instead of SQL commands. Below are some examples of SQL and Redis equivalent commands:A. Insert dataB. Query by product idC. Query by priceThis returns the keys: product:10001, product:10002, product:10003. Now run HGETALL for each key.Now, if you want to maintain the SQL interface in your solutions and only change the underlying data store to Redis to make it faster, then you can do so by using Apache Spark and Spark-Redis library.Spark-Redis library allows you to use the DataFrame APIs to store and access Redis data. In other words, you can insert, update and query data using SQL commands, but the data is internally mapped to Redis data structures.First, you need to download spark-redis and build the library to get the jar file. For example, with spark-redis 2.3.1, you get spark-redis-2.3.1-SNAPSHOT-jar-with- dependencies.jar.You then next have to make sure that you have your Redis instance running. In our example, we’ll run Redis on localhost and the default port 6379.You can also run your queries on Apache Spark engine. Here’s an example of how you can do this:Now you can also use your Redis client to access this data as Redis data structures:A more effective approach would be to use the scan command because it allows you to paginate as you navigate through the data.And there we have it – two simple ways you can run a Redis SQL query without disruption. Going one step further, you may want to find out Why Your SQL Server Needs Redis in our new whitepaper.But regarding real-time data with Redis, this is only one of many ways you can use it to provide real-time experiences.If you want to discover how Redis can guarantee you real-time data transmission, then make sure to contact us."
462,https://redis.com/blog/reinvent-tesla-sweepstakes-winner/,Meet Our re:Invent Tesla Sweepstakes Winner,"April 13, 2022",Redis,"“Being an enterprise architect, I’ve always been looking out for something radically different. I’m very passionate about startups because they break the traditional model. I’ve been following Redis for a long time,” says Rameshwar Balanagu, the lucky winner of our re:Invent Tesla sweepstakes.Rameshwar Balanagu is a Director of Digital Strategy & Enterprise Architecture. To celebrate his win, Redis announced him as the recipient of his very own Tesla at the end of our RedisDays London virtual event.Just before RedisDays London, Rameshwar sat down with Redis CMO Mike Anand to chat about his fateful win (“the sweepstakes was an accidental thing!”), what he plans to do with his new vehicle with Autopilot features (“My wife is actually scared of freeways, so I was telling her, ‘This could be a way for you to get on the freeway!’”), and his thoughts on Redis as a real-time data layer. “Coming from a traditional database world, we had these key-value, NoSQL databases, but if you ask me as an enterprise architect, there’s a long way to go. You need ways to accelerate and the fact is that not every company is ready for modernization.”Meet our lucky Tesla winner and catch highlights from his conversation with Redis CMO Mike Anand below. Congratulations Rameshwar!"
463,https://redis.com/blog/data-economy-podcast-improving-security-with-digital-twins/,The Data Economy: Improving Security with Real-Time Data and Digital Twins,"April 12, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.Kurt John, Chief Cybersecurity Officer at Siemens USA, has the tough assignment of being smarter and faster with real-time security practices because, as he acknowledges, “the bad guys just need to find one way in.”Siemens is big, really big. In the United States alone, it is a $25 billion enterprise with over 50 thousand employees providing technology and software on some of the largest industrial projects, including smart infrastructure, power grids, and building automation. Kurt and a team of 1,200 global security experts protect petabytes of data, and their cybersecurity team has been in place for a long time, going back to 1984.Kurt is a guest on The Data Economy, a podcast presented by Redis and hosted by Michael Krigsman of CXOTalk. In this episode, he shares insights on how Siemens manages security risks and leads its transformation into a data-first, cloud-first company. Siemens’ innovations include HVAC systems, industrial motors, electrical switchgear, and software systems for factory automation, embedded systems, and digital twins.Kurt’s security charter spans everything from the devices that Siemens’s employees use, the software systems they sell, and the best practices they share with their partners. He stresses, “With cyber defense, strength lies in the volume of data and how quickly we can raise the flag of an anomaly, or that something’s just not right, as quickly as possible.” Kurt’s team watches for an indicator of compromise (IoC), which requires collecting streaming data, understanding usage patterns, developing models, and enabling real-time decisions on whether an application or group of them is showing anomalous behaviors. Because of Siemens’ size and complexity, it has a mix of commercial, open source, and proprietary security systems and anomaly-detection technologies. Detecting anomalies is one aspect of fraud detection, a form of risk mitigation for businesses processing purchases, trades, insurance claims, and other financial transactions. These algorithms use historical transactions to create a decision model, and real-time activities are then tested for irregularities, including fraud and other conditions requiring flagging or intervention.What can CIOs, chief data officers (CDOs), and technology leaders learn from the head of cybersecurity at one of the largest industrial software companies?An enterprise’s efforts to collect real-time data often start internally with efforts to reduce costs, drive efficiencies, improve quality, and address risks. Some business areas may not be competitive due to the cost of delivery, and many operations require added resiliency in this post-peak pandemic world of volatile supply chains and changing customer needs. Siemens knows this all too well, as its cybersecurity program extends to the 24,000 suppliers in its network.One bad breach can undermine the brand’s reputation. But adding real-time data and analytics can be a cyber defense and operational investment that evolves into a strategic differentiator.“The value of data shifts from cost and efficiency to an absolute treasure trove of insights into operations, products, and market feedback,” says Kurt. “We can deliver an improved product, in less time, with fewer resources, and with more intentionality.”Capturing market feedback and looking for insights on customer opportunities and pain points are essential disciplines for transforming organizations. Fast data ingestion processes real-time feedback, such as when customers struggle to complete a transaction on a newly released mobile site, but analysis of the aggregated data can help justify investments in new digital products and services.Kurt speaks about how digital twins technology is transforming Siemens’ business model. He says, “Digital twins have been absolutely transformational on two fronts: The first is efficiency in engineering. Instead of having to build and rebuild and then rebuild again, and create multiple prototypes to test how a system interacts, you build a digital twin, run all of your tasks, and simulate all of your tests.”That speaks to the internal efficiency possible with a digital twin system and is an opportunity for every manufacturing, construction, and energy company to review their operations and consider where digital twins can test and validate new products and services.Kurt explains how digital twins are used in production operations to identify anomalies, including operational issues and security events. The digital twin should receive real-time data and events similar to what’s occurring in the physical world. Its outputs can then be compared to real-world conditions, and when there are significant deviations, the operations and field engineers know there’s a problem.That’s Kurt’s second front, and then he adds two more. Digital twins are a training tool for technicians who can learn operations on a model before working on a physical system. And when technicians are in the field, they can connect their virtual reality headsets to the digital twin and validate their services while working on the physical system.These innovations start with IT leaders scaling a real-time data layer, modernizing applications, and optimizing hybrid clouds. IT leaders can find short-term benefits through cost, efficiency, and other operational improvements, but the longer-term benefits come from learning customer needs, deploying real-time experiences,  and developing customer-facing solutions.Tune in to the podcast to hear more of Kurt’s insights on how Siemens provides digital twins as part of its service offerings, a true transformation to an industrial software company.Watch more episodes of The Data Economy podcast."
464,https://redis.com/blog/data-economy-podcast-driving-sustainability-with-machine-learning/,The Data Economy: Driving Efficiencies and Sustainability Through Machine Learning,"April 5, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.Over the last several months, we’ve all learned about how hard it is to ship goods, about the backups at the Los Angeles commercial ports, and the trucking bottlenecks all over the country. Convoy is one company using real-time data, machine learning, mobile applications, the internet of things (IoT), smart trailers, and other technologies to drive supply chain efficiency, sustainability, and trucker happiness.Dorothy Li, CTO of Convoy, explains the trucking industry’s challenges in The Data Economy, a podcast presented by Redis and hosted by Michael Krigsman of CXOTalk. She says, “We are at a real pivotal moment, and Convoy’s digital freight network aims to solve the toughest problems in this massive $800 billion industry.”Dorothy describes the challenges of modernizing the trucking industry, which meant going from the old days of operating with very little data and still using carbon paper forms to today’s transformation into a modern digital freight network.Convoy’s goals include providing meaningful services to shippers, carriers, brokers, truckers, and the entire supply chain that depends on delivering goods efficiently, sustainably, and safely.For CIOs and CTOs driving digital transformation, Convoy’s mission and accomplishments illustrate how focusing on customer satisfaction and end-user experiences helps create new market opportunities.Dorothy describes several industry pain points, including the 61 billion “empty” miles—35% of the 187 billion miles driven annually—meaning when truckers drive between jobs without carrying a haul. Convoy’s mobile application and IoT-outfitted smart trailers are two ways Convoy collects real-time data to help better connect truckers with nearby job opportunities. “One of the key ways we solve the empty mile problem is with automated reload, where we’re able to batch multiple shipments together,” she says.Saving empty miles is not just an operational efficiency, and it’s one of Convoy’s sustainability efforts that helps the environment and makes the industry more enticing for truckers. Convoy also helps truckers get paid faster, reduce their time waiting at the dock or for appointments, and learn which roadside facilities have restrooms.Improving the trucker experience is core to Convoy’s mission. “Trucking shortage is really at the center of why we’re seeing a lot of the supply chain issues today,” says Dorothy. “And to make that profession much more sustainable, we need to improve efficiencies and make truckers’ lives better.”There are several key ingredients to transforming an industry. Identifying the pain points, improving the experiences, defining broader sustainability goals, and seeking to address inefficiencies are all important starting points. But having a good business strategy needs to lead to an intelligently sequenced technology strategy.A key technology capability that underpins industry transformations is identifying how collecting real-time data and leveraging machine learning capabilities helps address issues for all sides of the marketplace. For Convoy, that means making life easier for truckers, providing services to the 300 thousand truck drivers in its network, and enabling flexibilities for shippers.Here’s one example. As technologists, we all understand elastic capacity, and it’s one reason CIOs and IT leaders modernize large-scale databases for hybrid applications and target multicloud architectures. In trucking, elastic capacity means a shipper can seek services from carriers in Convey’s network when there is peak demand and not get locked into constraining long-term contracts during soft markets.Dorothy explains how providing value to truckers and shippers enables them to collect the necessary data, develop machine learning algorithms, and create multisided marketplaces. “For truckers, we use machine learning to recommend the best loads for them, taking a lot of the truckers’ past preferences into consideration. And for the shippers, we provide real-time pricing and bidding information again, using real-time market conditions.”Creating the marketplace requires a real-time mindset on what data you’re collecting, what value you’re providing to customers, and how you’re using the data to provide real-time value. That strategy gets backed into the architecture, and Convoy’s technology includes a mobile application, integrations with transportation management systems (TMS), IoT streams from smart trailers, relational databases, a cloud data warehouse, and an event-based platform that does streaming.And the implementation is only possible by recruiting the top data science talent away from some of Silicon Valley’s most advanced technology and data companies. You might wonder how a company in the trucking industry—historically not bleeding edge when it comes to technology—succeeds in building a platform and recruiting amazing talent.My answer is that it starts with great leadership, a worldly mission around sustainability, a focus on improving people’s lives, and demonstrating a commitment to investing in people, technology, and data science.Tune in to the podcast to hear more of Dorothy’s insights on how Convoy is transforming the trucking industry, innovating a sustainable approach to improve working conditions, and using machine learning to develop its digital freight network.Watch more episodes of The Data Economy podcast."
465,https://redis.com/blog/data-economy-podcast-data-diversity/,The Data Economy: Data Diversity and Pioneering Real-Time Experiences,"March 29, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.CIOs and CDOs (both digital and data) must execute in the present and transform their organization by developing a competitive edge with technology, data, and real-time experiences. But to succeed in a rapidly changing world, tech and data leaders must innovate a few steps ahead of the business need because of the time required to select target markets, align people, define capabilities, understand customer needs, and pilot capabilities.Now innovation leaders are certainly thinking this way around emerging capabilities in artificial intelligence, machine learning, and IoT – but what fuels and differentiates these capabilities is working with diverse data sources and enabling real-time experiences.You’ll learn a lot about this mindset—how to think, plan, align diverse teams, and partner with the business—from Ron Guerrier, HP’s CIO and guest of The Data Economy, a podcast presented by Redis and hosted by Michael Krigsman of CXOTalk.During the podcast, Ron covers a ton of ground on CIO priorities, such as creating business opportunities through digital channels and addressing new market opportunities. He guides CIOs who are concerned that their existing apps don’t support the fast-growing digital demand. Most important, it’s his views on data diversity and enabling real-time experiences that stand out, and I picked several key statements from his conversation with industry analyst Michael Krigsman from CXOTalk to share with you.I do more than my fair share of writing and speaking on the transformational CIO and becoming a digital and data-driven organization. But Ron nails some key and easy-to-understand directives for those CIOs and CDOs who are still learning the growth sides of their businesses.“Innovation requires finding new markets and identifying different ways that we work and how we service our customers,” says Ron. “How do we personalize the relationship with the customers? How do we get ahead of what their wants and needs are? The best way to do that is to really understand the customer, whether it’s through metadata or other means, and develop solutions that cater to them.”So innovation is not just about what customers need today. Innovation requires CIOs to partner with business leaders and develop a perspective on how to serve target markets differently.But what’s really interesting is that Ron points out that collecting metadata is part of understanding and developing solutions for the customer. And since customer needs change quickly, developing customer solutions and real-time experiences requires capturing and processing real-time data.Ron elaborates, “When it comes to customer experience, everyone expects that you know what I need. How do I get ahead of that, in a non-intrusive way, and use data to understand what the customer needs from us?”Here’s how Ron explains data diversity in simple terms: “If you keep getting the same data points from the same sources over and over again, you get that same result. We have to ensure that we’re taking data from different constructs, different locations, and bring it together.”In the podcast, Ron tells a story of a NASCAR fan’s changing interests and deciding to switch to Formula One.And my example is a social media site showing me an ad for hiking pants two days after I completed my kayaking trip to Vermont. That site lacks data diversity and isn’t providing a real-time experience.Ron goes on to describe the importance of real-time information. He asks, “How does the data flow, and [how do you] make sure that data goes from A to B at the right time frame? How old is the data? Because you don’t want to make decisions on latent data. What is the best-known information at this time? But the most important thing is, how do we refresh the data and make sure it stays relevant for decisions? How do we democratize data and get data to the edge?“Real-time data is important for decision-making, but it’s also key to providing personalized, real-time experiences. “The customer doesn’t want just one cookie-cutter view,” he says. “They want differentiation. And so we have to understand that and provide differentiation in the data we provide.”So, CIOs driving digital transformation must partner with their business leaders on innovations targeting new markets, integrate data from diverse sources, and select a platform that enables personalized, real-time experiences.But CIOs and CDOs don’t work alone, and all of this requires building diverse teams. Ron compares the outdated approach to innovation (“It was thrown over the fence, wait for a while, get it back and pray that it’s what the customer wanted)” to a modern way of partnering with business leaders (“Let’s go to the beach together. Let’s find the rock we really want, and let’s try to find the right rock”).In practical terms, “going to the beach together” means creating diverse teams, a learning culture, and  tech leaders who are “able to sit with someone in marketing analytics, someone who is in sales or supply chain analytics, understand their business, and say, ‘You need this type of data because you’re trying to solve this problem.’”One of Ron’s closing remarks is a reminder to CIOs on how to nurture diverse teams and innovation. “When diverse teams are in the ideation phase, it might take a little longer because it forces natural friction and different points of view,” he says. “But the outcomes are usually ten to twenty times better because they’ve thought of the problem in different ways and thought through different approaches. The data definitely supports why diversity matters.”Tune in to the podcast episode to get more of Ron’s insights!Watch more episodes of The Data Economy podcast."
466,https://redis.com/blog/what-is-data-replication/,"Data Replication Explained: Examples, Types, and Use Cases","February 22, 2022",Paula Dallabetta,"Data replication also known as database replication, is a method of copying data to ensure that all information stays identical in real-time between all data resources. Think of database replication as the net that catches your information and keeps it falling through the cracks and getting lost. Data hardly stays stagnant. It’s ever-changing. It’s an ongoing process that ensures data from a primary database is mirrored in a replica, even one located on the other side of the planet.These days, “instantaneous” isn’t quick enough. Cutting latency down to sub-millisecond intervals is the universal objective. We’ve all seen this situation before – pressing the refresh button on a website, waiting for what feels like an eternity (seconds) to see your information updated. Latency decreases productivity for a user. Achieving near-real-time is the goal. Zero time lag is the new ideal for any use case.Data replication is copying data from one host to another, like, say, between two on-prem, or one on-prem to a cloud, and so forth. The point is to achieve real-time consistency with data for all users, wherever they’re accessing the data from. Data-Driven Business Models (DDBMs), like the gaming industry, rely heavily on the analytics acquired through real-time data. For a clearer understanding of just how necessary real-time accessibility is for a DDBM, watch this video below:In case of an emergency, should your primary instance be compromised, it’s vital to have mission-critical applications safeguarded with a replica that can be swapped in its place. Disaster recovery replication methods work similarly to a backup generator; imagine blowing a critical fuse or your power grid goes dark – you won’t have to worry because you have a backup generator to swoop in as a substitute and keep your lights running.Because replica instances are exact copies of primary instances, you can guarantee performance will not falter, regardless of what happens to your primary. Even if the link between a primary and a replica breaks, performance is still assured as the primary will enact a partial resynchronization, gathering the commands that were not delivered to the replica during the disconnection. If not possible, a full resynchronization will be initiated using a snapshot.By spreading the data across multiple instances, you’re helping to optimize read performance. Performance is also optimized by having your data accessible in multiple locations, thus minimizing any latency issues. Also, when replicas are directed to process most of your reads, that opens up space for your primary to tackle most of the heavy lifting of writes.Reduction in IT labor to manually replicate data.Full Database Replication occurs when an entire primary database is replicated within every replica instance available. This is a holistic approach that mirrors pre-existing, new, and updated data to all destinations. Though this approach is very comprehensive, it also calls for a considerable amount of processing power and encumbers the network load because of the large size of the data being copied.Unlike full replication, partial replication only mirrors some parts of the data, typically recently updated data. Partial replication isolates particular bits of data following the importance of the data at a specific location. For example, a large financial firm with headquarters in London could have many satellite offices operating around the world, with an office in Boston, another in Kuala Lumpur, and so on.Partial replication allows the analysts in London to have only UK-pertinent data at their site and have only that data be consistently replicated for their needs. The other satellite offices in the United States and Malaysia, respectively, can do the same and not bog down any one system, which improves performance and minimizes network traffic.This form of database replication sees data from a primary database replicating data in real-time to a replica instance by mirroring these changes in the order that they were made in the primary database. This optimizes consistency. The replication takes what is called a “snapshot” of the data in the primary and uses that snapshot as a blueprint of what needs to be replicated elsewhere. With transactional replication, you can track and distribute changes as needed.Given the incremental nature of this process, transactional replication isn’t the optimal choice when looking for a backup database option. Transactional replication is a useful choice in situations where you need real-time consistency across all data locations, where each minuscule change needs to be accounted for, not just the overall impact of the changes, and if data is changing regularly from one specific location.As its name suggests, snapshot replication takes a “snapshot” of the data from the primary as it appears at a specific moment and moves it along to the replica. Like a photograph, snapshot replication captures what data looks like at a point in time, as it looks when it moves from the primary to the replica, but doesn’t account for how it is later updated. Thus, don’t use snapshot replication to make a backup.In the event of a storage failure, snapshot replication will have no path to updated information. To keep your information consistent, you can start with a snapshot, but then ensure that all changes made to the primary are then passed on to every replica.On the other hand, this method is rather helpful for recoveries in the event of accidental deletion. Think of it like your Version History on Google Docs. Wish you could work on your presentation the way it looked four hours ago? If Google Docs takes a snapshot of your work at hourly intervals, you could click back on that version, or “snapshot,” from four hours ago and see what your information looked like then.This method typically begins with a snapshot of the data and distributes that data to its replicas, and maintains synchronization of data between the entire system. What makes merge replication different is that it allows each node to make changes to the data independently but merges all those updates into a unified whole.Merge replication also accounts for each change made at each node. To go back to our previous Google Docs example, if you’ve ever shared a document with coworkers who then leave comments and edits on your document, you’ll see who made what changes and at what time. Merge replication functions in a very similar way.Also known as key-based incremental data replication, this method leverages a replication key to identify, locate and alter only the specific data that has been changed since the last update. By isolating that information, it facilitates the backup process, working with only as much load as it has to. Though key-based replication makes for a speedy method of refreshing new data, it comes with the disadvantage of failing to replicate deleted data.Active-Active Geo-Distribution, also known as peer-to-peer replication, works somewhat like transactional replication, as it relies on constant transactional data via nodes. With active-active, all the nodes in the same network are constantly sending data to one another by synching the database with all the corresponding nodes. All the nodes are also writable, meaning anyone can change the data anywhere around the world, and it will reflect in all the other nodes. This guarantees real-time consistency, no matter where in the globe the change may occur.Conflict-free Replicated Data Types (CRDTs) define how this data is replicated. In the event of a network failure with one of the replicas or nodes, the other replicas will have all the necessary data ready to replicate once that node comes back online. This is a solid solution for enterprises that need several data centers located across the globe. Take a look at the video below for examples of Active-Active Geo-Distribution use cases.With synchronous replication, the data is written in both the primary and the replica at the same time, hence the name. Asynchronous replication, on the other hand, only copies data to the replica once the writes have already occurred in the primary.  Asynchronous replication doesn’t tend to happen in real-time, though it’s possible. Because of the scheduled write operations that tend to happen in batches with asynchronous, sometimes data gets lost, in most cases when a fail-over event transpires. Still, asynchronous is an apt solution when having to replicate data over long distances, as the real-time component isn’t a mission-critical factor.Maintaining data across multiple instances requires a consistent set of resources. The cost of having a primary with multiple replica instances can be quite high in many instances. Maintaining these operations and ensuring that no system failures occur requires a dedicated team of experts. And depending on the architecture, the network bandwidth could get overloaded when new processes are put in place, which could affect latencies, reads, and writes.Redis makes data replication expedient, cost-effective, and simple to implement. Are you ready for the Redis Enterprise treatment?"
467,https://redis.com/blog/how-to-eliminate-waste-and-promote-a-greener-planet-using-redis/,How to Eliminate Waste and Promote a Greener Planet Using Redis,"January 18, 2022",Redis Growth Team,"What generates 300 million tons of solid waste within a year? American consumers. Of this amount, only half goes straight into landfills to be stored. When one is at overcapacity, it’s sealed off to prevent leakage. Meanwhile, deep beneath the surface, the trash continues to decompose, creating many toxic byproducts such as leachate, which is contaminated water. Carbon monoxide is also produced, contributing to rising CO2 levels in our air.What’s just as harrowing is that 50% of this landfill trash is compostable, reusable, and recyclable. Recognizing that this is completely avoidable, Rajesh Ramamurthy has created an exceptional application, GreenEarth, that can reduce waste and promote a greener planet. The application creates a platform where people from all over the world can find projects that utilize common household trash to repurpose them into useful items or creative art projects.Redis was used to retrieve user information from the cloud database and for storing individual user attributes with hyper-efficiency. This was critical to Greener Earth’s functionality since data processing needs to be performed at speed to maximize the user’s experience.Let’s take a look at how Rajesh brought this innovative application to life. But before we dive in, we’d like to point out that we have 29 other exciting apps that are improving everyday lives on the Redis Launchpad. So make sure to have a browse after reading this post.An app. A community and a platform for people to help save the planet. This application connects environmentally conscious people with projects that utilize household items. If, for example, a user wants to make use of their empty tin cans, Green Earth will connect them with projects that demonstrate how these items can be used to create useful household items.Below we’ll show you exactly how to build this application by going through each step in chronological order. We’ll also highlight what components you’ll need, along with their functionality.Create your free Redis Enterprise Cloud account. Once you click on “Get Started”, you will receive an email with a link to activate your account and complete your signup process.Follow this link to enable RedisJSON module under Redis Enterprise Cloud database.Change directory to greenearth directory and install the required dependencies:To connect to Redis Enterprise Cloud Database with RedisJSON enabled, you’ll need to edit the following code in index.js.You’ll also have to run npm start to start the local server at http://localhost:3000.Below there are a number of features that need to be set up for the application to function. Let’s take a look at what needs to be done.UsersCreates user account.SADD – Adds username to global users setHMSET – Creates user object with information including hashed password, description, and profile pictureAllows users to log in to an existing account.SISMEMBER – Checks whether if the entered username is a valid memberHGET – Validates the entered password by retrieving the registered one and comparing themHSET – Adds username with generated session token to active user group br>Logs out of the signed in account.HGET – Retrieves user from the provided session tokenHDEL – Removes the user and session token from the active user groupRetrieves information (username, description, profile picture) of the specified user.HMGET – Retrieves user data from provided attributesSets specific attributes (description, profile picture) of specified user.HMSET – Sets user attributes with provided dataAdds a post from provided user information.HGET – Retrieves user post list IDJSON.SET – Creates a post from the provided information and adds to the user post listJSON.GET – Retrieves user post listEdits the existing post that’s created by the user.HGET – Retrieves user post list IDJSON.SET – Replaces the existing post with a new one based off the provided information in the user post listJSON.GET – Retrieves user post listDeletes the existing post created by the user.HGET – Retrieves user post list IDJSON.SET – Removes post and adds to the user post listJSON.GET – Retrieves the user post listRetrieves information (title, description, image, materials, directions, comments) of specified post.JSON.GET – Retrieves post from the user post listRetrieves all global posts or those created by the specific user.JSON.GET – Retrieves user or global post listRetrieves all drafts created by the user.JSON.GET – Retrieves user draft listAllows users to comment on posts.HGET – Retrieve post comment list IDJSON.SET – Creates comment ands add to post comment listJSON.GET – Retrieves post comment listAnyone who’s on the site can search for materials they have or projects they’re interested in. To begin, simply visit the Green Earth homepage.Click on the ‘posts’ tab at the top of the page. This will take you to a search bar for you to enter the household item you’re looking to recycle. So for example, if you have a lot of tin cans that you want to repurpose, simply type in ‘tin cans’ or variations of that keyword (see image below).Once you’ve typed this in, a list of projects that require your household item will be displayed. Clicking on one of the results will take you to the project page of that post.Here you’ll discover everything you need to know about the project, ranging from user comments to the required materials through to the directions you’ll need to follow (see below).Click on the ‘sign up’ tab at the top of the page. Here you’ll be able to create a username and password for your account.Once you’ve created an account and are logged in, the next step will be to update your profile. You can do this by clicking on your username at the top of the navigation bar and selecting ‘profile.’You’ll then be directed to your profile page which will be empty (see below).From here you’ll be able to update every section of your profile, including your profile photo and bio.Once you’ve set-up a profile, you’ll then be able to leave comments on posts. To do this, first find a post you want to comment on (see ‘how to find a project’ section). The comment section in each post can be found at the bottom of the project page (see example below).To create a post, you first need to go to your profile dashboard. Click on the ‘+’ sign at the top right hand corner of your profile (see below).You’ll then be directed to the ‘create a post page.’ From here you’ll see a number of different fields that need to be filled in, ranging from project name to directions through to the materials required to bring this project to life.Add the details of your project in each section as demonstrated below.Once you’ve completed each section, you’ll have the option to either delete, save or publish this post at the bottom of the page.All of your drafts and posts will be displayed in your profile dashboard.To edit one of these, first, click on the post or draft you want to edit. Next, click on the edit button on the bottom-left-hand side of the page. Alternatively, you can delete the post by clicking on the trash icon next to it.Climate change is on everyone’s radar, galvanizing people to adopt a greener way of living. But for green initiatives to come to fruition, online platforms need to be efficient and powered by a database that’s capable of transmitting data at great speed.This is because any delays will hamper the user’s experience and push people away from these platforms. With Redis however, GreenEarth was able to function at such a speed that it enabled all components within the application to become more interconnected.Thanks to Redis, retrieving user information from the cloud database, along with storing individual user attributes, was an effortless process. From start to finish, everything was carried out with hyper-efficiency.Rajesh is a renowned problem solver in the tech-sphere, providing clients with simple solutions to complex problems.Make sure to check out his GitHub page to see what other projects he’s been involved with.If you want to discover more about how this application was deployed, make sure to check out Rajesh’s YouTube video here.We also have a great selection of different applications for you to check out on the Redis Launchpad where programmers across the globe are harnessing the power of Redis to change everyday lives. Check it out. Be inspired. And join in the fun."
468,https://redis.com/blog/learn-how-redis-simplifies-your-architecture-in-90-seconds/,Learn How Redis Simplifies Your Architecture in 90 seconds,"December 3, 2021",Raja Rao,"We love to write long, deeply technical articles like this, this or this. But sometimes you just don’t have the time. And we hear you. So here’s how Redis simplifies your architecture in one short summary and a video that explains it all in less than 90 seconds.Let’s say you’re building a real-time application on AWS. What would it take to set up?You’d need a combination of different services. You might use Kinesis to collect the data, DynamoDB to persist the data, and ElastiCache to quickly access frequently used data. But that’s not all. You might also throw on TimeStream for backend analytics or OpenSearch for searching. And the list goes on.But these are all different systems, and to make them work together, you  end up using a bunch of lambda functions, Simple Queue service, etc. Suddenly you’re dealing with a very complex system— one that needs multiple experts to manage all those specialized services. Now you have a slow, complex system that’s expensive to manage.With Redis Enterprise, you can minimize all these complexities with a unified real-time data platform. For that same system, you can use the native features of Redis Enterprise, like streams for collecting the data, pub/sub for distributing the data, Lists for queuing, TimeSeries for analyzing, Gears for ETL, and RedisJSON for persisting. And since it’s all in the same system, you don’t need numerous experts to manage it. You end up with one fully managed system, that’s blazing fast and cost-effective.Check out the 90-second video to see what we mean."
469,https://redis.com/blog/4-reasons-to-join-the-redis-hackathon/,4 Reasons to Join the $100K “Build on Redis” Hackathon—And Tips on How to Win,"April 7, 2021",Raja Rao and Ajeet Raina,"The Redis community is constantly innovating, and our annual hackathons bring creative minds together and provide developers with a platform to build new ideas using Redis. Last year we challenged more than 320 hackers to build applications using Redis beyond caching. This year, join us for the “Build on Redis” hackathon for a chance to win one of 52 prizes totaling $100,000 USD! Building begins on Thursday, April 15—five days before RedisConf—and will run for a full month until Saturday, May 15.At its heart, Redis Lab’s hackathons are less group coding events and more like a “communal brainstorming.” We focus on creating an atmosphere where the best coders, designers, and architects come together, learn about a problem statement, and develop working prototypes for real-world problems and solutions for those challenges. This hackathon provides an excellent coding environment that gives you an opportunity to learn about trending technologies and immediately apply them to innovative projects.Redis Enterprise Cloud is not only blazingly fast, but also versatile and packed with functionality and tools—you can build anything and everything using Redis rather than patching together multiple databases. Whether you’re collecting billions of events (Redis Streams), storing JSON documents (RedisJSON), querying and indexing your data (RediSearch), analyzing a stream (RedisTimeSeries), making real-time recommendations (RedisGraph), or detecting fraud (RedisAI), you can do it all and do it fast.We want as many people as possible to experience the speed, simplicity, versatility, and fun of Redis, but don’t just take our word for it—sign up for the hackathon and see how easy it is to build your entire data layer on Redis Enterprise Cloud!Here are four compelling reasons why you shouldn’t miss the “Build on Redis” hackathon:1. Opportunity to meet and challenge fellow high-potential developersWhile the hackathon is a great opportunity for you to meet and collaborate with others, it’s also a competition. You’ll be competing against thousands of other developers—so placing in the top 50 won’t be some small accomplishment. Our hackathon challenges attendees to unveil their ability to innovate and create compelling, real-world solutions by utilizing the latest tools and technology. It is a unique learning opportunity, allowing you to develop new skills, grow your networks, and ship breakthrough solutions.2. Propel your personal brandThe hackathon is not just an event where programmers and hackers unite, but it’s also a personal branding platform. The “Build on Redis” hackathon allows developers to seize the incredible opportunity to showcase their talent and establish a positive personal brand, opening new career growth opportunities and attracting recognition from the top recruiters.The top 10 winners of the Redis hackathon will be promoted on our tech blog and social media—plus, they will also have an opportunity to be a guest on our RedisPods podcast. Top winners may also have an opportunity to write a guest blog.3. Train and practice for a fast-paced, real-world development environmentWhen was the last time you stepped into uncharted territory? It can be very easy to stay in your comfort zone, doing things you know how to do and never really challenging yourself. To win at the “Build on Redis” hackathon, there is no time to idle by—you will be challenged to push yourself. Our hackathon prepares you to find ways to work quickly with more precision, challenging thousands of coders, and prepares you to work in a competitive and collaborative environment. You can ease through the hackathon if you like, but if you want to attain the glory and recognition this event can provide you, it is essential to take risks.4. Win big awards and prizesAnd if you weren’t convinced yet, we’re offering more than 50 prizes totaling $100,000 USD. Here’s what you can win:Plus, all winning teams will receive Redis swag—one Redis t-shirt and sticker per team member—along with the promotion of their projects on the hackathon website, in a Redis blog post, and on Redis social media accounts.Whether you’re a hackathon newcomer or a veteran, following these five tips are a sure path to success:1. Build the right team“Individually you specialize, together we transform.” A hackathon is all about engaging minds. It is not simply an individual achievement. Try to convince and attract competent developers and a good designer (for visually appealing UI/mockups) to your team. Diversified brains from different parts of the world with different types of expertise—and at least one strong communication specialist—can lead your team to success. Join the Redis Discord channel and start hunting for your right team.2. Do your homeworkIt’s important to know your organizers and their purpose in running the hackathon. Focus on the theme of the hackathon event. Learn about the Redis Enterprise Cloud free tier and Redis modules. Read the hackathon rules (and read them again!).3. Choose the right toolChoose your tech stack wisely. You must use at least one Redis Enterprise module. Each team must use a free Redis Enterprise Cloud account (if your app is using just one module) or use the Redismod (Redis Enterprise modules) Docker image if your app needs multiple modules attached to a single Redis database (e.g. RedisJSON + RediSearch). GitHub is a perfect place to showcase your original work.4. Envision your demoIt’s important to come up with a solid business idea that can both be marketable and profitable for you and your hackathon organizer. Start brainstorming from day 1 and try to visualize the end product. Try to come up with a well-validated demo and marketing pitch of your product. Don’t ignore time for presentation preparation.5. Have fun!A hackathon is like a “roller-coaster ride.” It’s full of non-stop ups and downs but at the same time, it’s full of fun. Don’t stress and take a healthy break in between the intensive periods of thinking and coding.Due to legal and fulfillment restrictions, only legal residents from the following countries are allowed to participate at this time:Argentina, Australia, Austria, Belgium, Bolivia, Chile, Colombia, Costa Rica, Denmark, Ecuador, Finland, France, Germany, Greece, Hungary, India, Ireland, Israel, Japan, Malaysia, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Slovakia, South Africa, Spain, Sweden, Switzerland, Turkey, United Kingdom, United States, Uruguay, Canada (excluding Quebec).Take advantage of this free opportunity to challenge yourself to build something memorable, purposeful, and rewarding. The hackathon opens soon, so make sure to register before April 15! Further details will be available after April 15.And if you’d like to brush up on your Redis skills before you start hacking, sign up for our free training courses, which will open a week before RedisConf 2021. These brand new training courses and more than 60 breakout sessions will be available at RedisConf 2021, so don’t wait—sign up now!Ask us anything at redisconf@redis.com or in the #hackathon channel on Discord.Thank you for participating in the 2021 “Build on Redis” hackathon!"
470,https://redis.com/blog/building-real-time-full-text-site-search-with-redisearch/,Building Real-Time Full-Text Site Search with RediSearch,"November 27, 2020",Andrew Brookins,"When we wanted to add real-time full-text search to the Redis documentation site, we turned to RediSearch. The robust search features in the RediSearch module helped us transform a bland form into an awesome search experience. To show off some of what’s possible with RediSearch and to help jumpstart your search projects, I’d like to talk about the architecture of our project and share our code. The Python application we built is called redis-sitesearch. If you want to check out the code or run it for your own site, it’s open source, and you can try RediSearch for free on Redis Cloud Essentials. Read on for the nitty-gritty details!We built a new search experience because our documentation site had outgrown our previous search engine, Lunr.js. Specifically, we wanted a few things from a new search experience:We knew that RediSearch could deliver everything on our list. (If you aren’t familiar with RediSearch, it’s a great querying and indexing system for Redis, and includes powerful full-text search capabilities.) So, we got to work. And within a few weeks, we had a brand-new search API powering search on our documentation site. Let’s look at the pieces of the project from a high level, and then we’ll zoom in closer.Our documentation search has two major pieces: a JavaScript frontend consisting of an HTML input and a search results list, and a REST API backend that queries RediSearch.While you type, the frontend makes search requests to the backend API and renders the results. Thanks to Redis being an in-memory database, and deployments across multiple zones, getting these results is fast—averaging around 40-to-50 milliseconds in testing from Oregon.We know that humans perceive response times below 100 milliseconds as instant, so we were happy to hit the 40ms – 60ms range.What exactly is going on inside the search app to make this happen, though? Here are the pieces involved:Here’s a diagram of how the pieces fit together:We run these pieces inside a container on Google Cloud, and distribute instances of the container worldwide using a multi-zone deployment behind a global load balancer.Now let’s take a detailed look at the four steps involved in the backend application:Before we can handle any search queries, we need to index our documentation site. To do this, we use a RediSearch index definition that contains all of the fields we’ll use to search. A background job in our Python application scrapes the documentation site on a schedule and adds its contents to the RediSearch index. Let’s see how both of these pieces work.The index definitionWith RediSearch, you need to create an index definition before you can make any queries.RediSearch indexes stay in sync with Hashes in your Redis database. In the redis-sitesearch app, we create a Hash for every document that we want to add to the search index.Then we create a RediSearch index for the site we want to index. Each index syncs with the Hashes in keys matching the prefix sitesearch:{environment}:{url}:doc:. Currently, we index only our documentation site, but the design allows us to index multiple sites using the same app instance. Index definitions take a SCHEMA argument that defines which Hash fields should be added to the index. redis-sitesearch indexes the following fields:Using the redisearch-python client, the schema looks like this:You can see that we’ve added a “weight” to the title and body fields. This means that search queries will consider hits in those fields more relevant than other fields.The indexing jobWith the index definition in place, we needed a background job to scrape our documentation site and create the Hashes for RediSearch to index.To do this, we used Redis Queue to build an indexing job that runs every 30 minutes. This job uses the Scrapy library, which we’ve extended with custom logic to parse scraped page content into structured data.Parsing structured page data with ScrapyOne of our goals for the new search experience was to show the section of the page in which we found a search result. For example, if the query “gcp” finds a hit in the “Team management” section of the “Account and Team Settings” page, we want to show both the page and section in the search result. This screenshot of a search result shows an example:In this screenshot, “GCP” is the hit, so that term is rendered in bold, an example of RediSearch’s highlights feature. “Account and Team Settings” is the title of the page on which we found the hit, and “Team management” is the name of the section that contained the hit.We accomplish this by breaking up every page we scrape into SearchDocument objects. SearchDocument is the domain model that our application uses to represent parts of a web page that we want to make searchable.After Scrapy finishes crawling the site, we convert the SearchDocument objects into Redis Hashes. RediSearch then indexes the Hashes in the background.Scoring documentsWhen you create a Hash with a key that RediSearch is configured to index, you can optionally provide a “__score” field. If you do, RediSearch will multiply the final relevance value of that document in a search query by the __score. This means that you can model some relevance-related facts about your data at index time. Let’s review a couple of examples from redis-sitesearch.We created scoring functions to adjust the scores of documents based on the following rules:This kind of fine-grained control is a major reason we switched to RediSearch.Validating documentsWeighting documents differently based on rules helps us boost the relevance of search hits, but there are certain types of documents we want to avoid indexing altogether. Our validators are simple functions that decide whether or not to index a SearchDocument based on arbitrary logic. We use these to skip release notes and any document that looks like a 404 page.With a background job regularly indexing our documentation site in RediSearch, we could finally build our search API! We used the Falcon web framework for the API, because of its fast performance.An effective search API translates the frontend user queries into RediSearch queries. RediSearch supports numeric ranges, tags, geo filters, and many more types of queries. For this app, the best fit was prefix matching. With prefix matching, RediSearch compares all terms in the index against the given prefix. If user types “red” into the search form, the API will issue the prefix query “red*”.With prefix matching, “red*” will find many hits, including:The search form will start displaying results for hits across all these terms as the user types. When the user finishes the phrase they are typing, the results will begin to focus. If the final search is for “redisearch,” the app issues one last query to Redis for “redisearch*” and the results will be specific to RediSearch.Google Cloud made deploying redis-sitesearch easy. We built the backend application as a single container on Google Cloud’s Compute Engine, running the following processes:We deploy this container to instance groups in the US-West, US-East, Zurich, and Mumbai zones on Google Cloud.Note: Compute Engine can run only a single container per node, which is why we run the Python app alongside Redis. However, this has the benefit of reducing latency!A global load balancer routes traffic to these instances based on the geographic origin of the request and the current load within the instance group.We wanted to give our users the best possible documentation search experience. The way to do that, RediSearch, was right in front of us.If you’re building a full-text site search feature, take a look at RediSearch. It keeps your entire search index in memory, so queries run extremely fast. RediSearch can also filter by numerical range, geographic radius, and much more. You can even jumpstart your project by using our open-sourced redis-sitesearch codebase. And if you want to try RediSearch without hosting it yourself, you can do so for free on Redis Cloud Essentials.Want to learn more about RediSearch? Watch our latest YouTube video: Querying, Indexing, and Full-text Search in Redis."
471,https://redis.com/blog/visit-redis-labs-online-at-aws-reinvent-2020/,Visit Redis Labs Online at AWS re:Invent 2020,"November 20, 2020",Paul Bushell,"Most years, when the weather turns cold, tens of thousands of cloud computing professionals around the world get busy booking tickets to Las Vegas for AWS re:Invent for a few days of cloud-centric networking, learning, and partying. Like just about everything else this year, though, AWS re:Invent 2020 is going virtual—but that doesn’t mean it’s not going to be amazing!Because re:Invent is online this year, that means it’s completely free—hundreds of thousands attendees will join for their first re:Invent ever, so there’s no reason not to attend! As a Gold Sponsor for this year’s three-week cloud extravaganza, held from November 30 to December 18, we’re focused on showing how Redis Enterprise is not just any Redis. Only with Redis Enterprise—from the makers of Redis—can you get global distribution and endless possibilities, with the flexibility of working with any real-time data at any scale, anywhere.We’re also excited to share more about Redis Enterprise Cloud, the premiere fully managed version of Redis that allows you to scale your data across hybrid or multi-cloud deployments around the world. Built for modern distributed applications, Redis Enterprise Cloud not only allows you to scale across environments and globally at local latencies, but it also allows you to run customer-facing production apps with sub-millisecond performance at virtually infinite scale without worrying about operational complexity or service availability. Plus, Redis is an AWS Partner Network (APN) Advanced Tier Technology partner—the highest designation an AWS partner can achieve.During re:Invent 2020 we’re hosting two exclusive speaking sessions, where you’ll be able to learn how customer engagement leader Freshworks has scaled its business with Redis Enterprise Cloud on AWS. Plus, you won’t want to miss our roundtable discussion with Yossi Gottlieb and Madelyn Olson, members of the OSS Redis core team, as they discuss the future of the open source Redis project. Read on to learn more about what you can expect from our sessions, how to score some great swag and cool prizes, and more!Your go-to source for everything Redis at AWS re:invent will be our virtual booth, which will go live on November 30. There, you’ll be able to schedule 1:1 meetings with company representatives and dive into what Redis Enterprise is all about with a carefully curated selection of white papers, datasheets, case studies, blogs, and videos. You’ll also be able to access our Freshworks and Redis core team sessions from our booth.Just as important, don’t miss our Redis @ AWS re:Invent page. Head there directly to learn more about our cloud offerings and sign up for our new free Redis Enterprise Cloud Essentials with modules trial. Plus, check out our exclusive giveaway and swag offers. See it all at https://redis.com/aws-reinvent.Date: Thursday, Dec. 3, 3:15–3:45 p.m. PT (Live and on-demand at our virtual booth)Speaker: Vijay Lakshminarayanan, Senior Engineering Manager at FreshworksRedis is the most popular open source cache, and increasingly a primary database for many organizations. As organizations grow business-critical applications, many grapple with choosing the right time to graduate from open source Redis to Redis Enterprise. In this session, representatives from Redis and Freshworks identify the top considerations for moving to the enterprise version of Redis, when it’s the right time, and how to optimize your investment in Redis Enterprise.You’ll learn how Freshworks scaled from 30 million to 500 million requests per day, and its plan to double that to 1 billion requests per day, and why Redis Enterprise Cloud was a key component to enabling Freshworks’ growth. Plus, you’ll get an inside look at running Redis Enterprise seamlessly on AWS!Date: Wednesday, Dec. 2, 12:30–1 p.m. PT (Live and on-demand at our virtual booth)Speaker: Yossi Gottlieb, Chief Architect at Redis, and Madelyn Olson, Senior Software Development Engineer at AWSRedis has been named the world’s most loved database four years running, and to keep that streak going, the open source Redis project is now managed in a light-governance model by a new OSS Redis core team working with the Redis community to help drive its development.In this session, Yossi and Madelyn, two members of that core team, will discuss the origins of the new light-governance model, how we’re working to preserve the best of Redis, and how we want to grow the community. They’ll highlight some successful community members and share how AWS will continue to contribute to Redis. Finally, they’ll look at the immediate future of Redis—in the cloud and beyond.We give you $10, we donate $10!We love to hear from Redis community members, and we’re encouraging AWS re:Invent attendees to share their thoughts: Spend less than five minutes answering our survey we’ll send you a $10 gift card! Plus, Redis will donate $10 to Close the Gap, an organization dedicated to bringing the power of technology to people all over the world. Make sure to share the link and #NotJustAnyRedis on social media!UPDATE: We did it, you did it! Thank you to everyone who responded to the survey. We are excited to announce we have reached our goal of raising $10,000 for Close the Gap, and are no longer accepting survey responses. Thank you for your support in helping bridge the digital divide around the world.As for the promised surprise, because the power of technology can be magical, we asked a famous wizard to help share our message…Book a 1:1 meeting and get a Redis swag packBook a meeting with one of our sales representatives at our virtual booth and you’ll receive a Redis swag pack, available only during AWS re:Invent! You’ll get a print copy of the premiere issue of Rediscover Magazine, a Redis Rubix cube, a Redis pin, and a fun sticker.We’re looking forward to sharing the story of why Redis Enterprise is not just any Redis at AWS re:Invent 2020. It’s sure to be unforgettable… see you on our AWS re:Invent 2020 page any time!"
472,https://redis.com/blog/leveraging-redis-and-kubernetes-to-build-an-air-quality-geospatial-visualization/,Leveraging Redis and Kubernetes to Build an Air-Quality Geospatial Visualization,"September 24, 2020",Alex Milowski,"During the 2020 wildfires in California, I, along with millions of others, have been constantly checking the PurpleAir website to monitor the air quality. In addition to looking for clean air to breathe, I was curious about how PurpleAir aggregated data from its sensors and was excited to discover that it had an API for accessing its data. It seemed a perfect opportunity to demonstrate the power of Redis as a database deployed using Kubernetes.In the past, some of my research focused on exposing geospatial data on the web. For this project, I re-interpreted that work with two new guiding principles: (1) using Redis’ geospatial features to partition data and (2) deploying the whole application on Kubernetes. I wanted to show how DevOps challenges for managing data collection, ingestion, and querying would be easier to address with Kubernetes and the Redis Enterprise operator for Kubernetes.I chose to use a methodology from my dissertation research called PAN (Partition, Annotate, and Name) to produce data partitions organized by inherent facets (e.g., date/time, geospatial coordinates, etc.). Redis provides the perfect building blocks for applying this methodology to the air-quality sensor data collected by PurpleAir. The technique I use maps a geospatial area of interest (the shaded polygon) onto data partitions (the boxes). You can then retrieve and navigate these partitions via their metadata, and you can select partitions over whichever timespan you’re interested in.I was able to quickly produce a working application that collects the data and provides simple interpolation of AQI (Air Quality Index) measurements across a color-coded map. This image was generated using sensor data from August 28, 2020:Taking this further required making all of the pieces operational in a reliable way, and that’s where Kubernetes became essential. Kubernetes made it easy to describe and deploy the data collection, ingestion, Redis database, and the web application as independently scalable components manageable by the cluster:I was invited to speak about this application for the Data on Kubernetes community meetup. I presented some of my past research into scientific data representation on the web and how the key mechanism is the partitioning, annotation, and naming of data representations. I showed how I implemented this for collecting, storing, and using air quality data via Python, Redis, and a Kubernetes deployment. You can watch my full presentation and learn more in the video and podcast embedded below (and see my presentation slides here)."
473,https://redis.com/blog/getting-started-with-redisearch-2-0/,Getting Started with RediSearch 2.0,"September 17, 2020",Tugdual Grall,"RediSearch 2.0 is now out in public preview! Most of the features in this major new release have been driven by your feedback, with a focus on improving the developer experience and enhanced scalability. But this blog post concentrates on helping you get started using RediSearch 2.0’s new data indexing capabilities and better ways to create an index.Having a rich query and aggregation engine inside your Redis database opens the door to many new applications that go well beyond caching. You can use Redis as your primary database even when you need to access the data using complex queries, without adding complexity to the code to update and index data. All this with Redis’ famous speed, reliability, and scalability!For more on what’s new, see Introducing RediSearch 2.0PrerequisitesTo get started with RediSearch 2.0, you’ll need:Get a Redis database with RediSearch enabledYou can install and use RediSearch 2.0 in various ways:For simplicity, this blog post will use the Docker images. (If you have already installed RedisSearch 2.0, you can jump to the next section.) To start your Redis instance with Docker, open a terminal and run the following command:Note: The container will automatically be removed when it exits (–rm parameter).Using your favorite Redis client, connect to the RediSearch database.If you have started your Redis instance with Docker you can use the following command to use the redis-cli embedded in the container:If you want to use Redis Insight, add your RediSearch instance and go to the CLI.Insert dataYou are now ready to insert some data. This example uses movie data stored as Redis Hashes, so let’s insert a couple of movies:The database contains now two Hashes. It is simple to retrieve information using the following command, if you know the key of the movie (movie:11002):But how can you query the database to get a list of movies based using the title, the genre, or the release_year?With “core” Redis data structures, you have to manage your index yourself using Sets to associate the genre to the list of movie IDs, for example, and add a lot of code to your application to manage and query the index.But with RediSearch you can simply define an index associated with your data and let the database manage them. You can then use the query engine to query/search the data using secondary indices.To create an index, you must define a schema to list the fields and their types that are indexed, and that you can use in your queries.For this example you will be indexing four fields:Creating the index is done using the FT.CREATE command:Before running queries, though, let’s take a closer look at the FT.CREATE command:The RediSearch 2.0 engine will scan the database using the PREFIX values, and update the index based on the schema definition. This makes it easy to add an index to an existing application that uses Hashes, there’s no need to change your code.You can see the index information with the following command:Now we’re ready to use the index and query the database.For this section you will use the FT.SEARCH command and its syntax; note that the goal of this blog post is to get you started, so we stick to the basics and don’t go into all the details. To learn more about RediSearch, look at the documentation and the tutorial.Full-text search queriesRediSearch is a full-text search engine, allowing the application to run powerful queries à la Google. For  example, to search all movies that contain “war”-related information, you would run the following command:As you can see, the movie Star Wars: Episode V—The Empire Strikes Back is found, even though you used only the word “war” to match “Wars” in the title. This is because the title has been indexed as text, so the field is tokenized and stemmed.Also, the command does not specify a field, so the word “war” (and related words) is searched in all text fields of the index. If you want to search specific fields, you would use the @field notation, as shown here:You can run additional full-text search queries against this simple dataset, as demonstrated here (Note: to keep the document short, the results of the queries are not shown):Prefix matches:Fuzzy search:Unions:You can find more information about the query syntax in the RediSearch documentation.Tag field searchUse the tag field “genre” to find all “drama” movies:The syntax @field:{value} indicates that you are searching in a tag field. You can find more information about the tag filter in the RediSearch documentation.So far, all the data you are querying was created before the index and indexed during the index creation. Let’s change things up by adding a new movie:You can reuse the earlier queries:As you can see, the new movie has been automatically indexed.Similarly, if you delete or expire a movie, the index will be automatically updated, as shown here:If you wait 15 seconds and run the search query, you will see that the movie has been removed from the index.This is quite powerful when you want to do ephemeral search and let the database manage the expiration of the data and indexes. You can find more information about ephemeral search in our blog post laying out The Case for Ephemeral Search.This post has shared some of the basics of RediSearch, and shown how indexing data is transparent from your application code. This functionality is new in RediSearch 2.0, since in RediSearch 1.x developers had to specifically use the FT.ADD command to index the data.In addition to the search and indexing functionality discussed in this blog post, RediSearch also includes powerful data aggregation capabilities, which are covered in the RediSearch documentation, tutorial, and online course.The tutorial contains the same data, but with a bigger dataset and more sample queries and aggregation. It also contains an application that shows how to use RediSearch with programming languages such as Java, Python, and Node.js. To learn more, check out these additional resources:"
474,https://redis.com/blog/redis-security-course-is-now-live-at-redis-university/,Redis Security Course Is Now Live at Redis University,"August 18, 2020",Kyle Banker,"For the past few months, the Redis education team has been working hard on a new course covering Redis security. Today, we’re pleased to announce the general availability of RU330: Redis Security! If you run Redis in production, then you’ll definitely want to sign up.If you need more persuasion, then please read on.It is a universal truth that any database worth its salt must be secure. But how do you secure Redis? If we’re being completely honest, the early development of Redis generally prioritized utility and stability over security.That’s changed in recent years, though, especially with the release of Redis 6. Now, you can implement the principle of least privilege by taking advantage of access control lists (ACLs), and you can secure your Redis connections by enabling TLS (Transport Layer Security) encryption.Building on this momentum, we wanted to create the definitive guide to Redis security, and, well, RU330: Redis Security is the fruit of that effort.So, who’s teaching this course? Why should you take it? And what are you going to learn?The idea for Redis Security started with Jamie Scott, a Redis Product Manager focused on security. Speaking daily with our customers about security, Jamie has been deeply involved in the security feature development for open source Redis 6, Redis Enterprise, and Redis Enterprise Cloud. Needless to say, Jamie was the perfect person to spearhead this effort, and Jamie is the lead teacher for most of the Security Course.I am Jamie’s co-teacher. As a longtime software engineer, technical author, and experienced Redis University curriculum developer, I’ve helped Jamie to create what we hope is an engaging, informative course that’s well worth your time.Our course takes a holistic approach to security education.Before delving into any Redis-specific topics, we start with some general security principles that we think everyone should know. We cover the basics of information security, including the CIA triad (confidentiality, integrity, and availability), defense in depth, and the principle of least privilege. This sets the stage for a tour of Redis’ basic security controls. Later in the course, we provide a thorough introduction to encryption and public key cryptography before showing you exactly how to implement TLS in your Redis deployment.In explaining the reasoning behind Redis’ security features, our goal is to help you make the best decisions when you go to production. For example, we want you to think hard about what level of Redis access your applications actually need. If you’re running a service whose sole job is to return search queries run against RediSearch, then you’ll want to create a specific ACL user for that service. The ACL user you define might look something like this:user searchservice on &gt;secret +FT.SEARCH ~*This ACL directive creates a user capable of running exactly one Redis command: FT.SEARCH, which queries a RediSearch index. This is a good practice because it reduces the likelihood that your application will do any damage if it’s ever compromised. For instance, your application won’t be able to call FLUSHDB, a command that would drop all Redis data and likely cause your users some discomfort.You can think of this course as a series of techniques for avoiding a Redis horror story. In fact, because Redis is deployed so widely, it can be a target for hackers (the bad kind) and script kiddies. As a kind of motivation, Jamie and I had the idea of featuring a different Redis horror story each week in the course. You’ll learn about some infamous Redis exploits and what might’ve been done to avoid them.This course focuses on open source Redis, so it’s broadly applicable to most Redis users. Occasionally we’ll point out a feature of Redis Enterprise or Redis Enterprise Cloud, which you might need as you scale the use in Redis in your organization. But Redis is fully committed to open source Redis users as well, so we emphasize open source Redis in this course, just as we do in the other six Redis University courses available today.You can learn Redis security, and you can also thwart most attackers by making the right security decisions. RU330: Redis Security teaches you all of this, plus a number of principles that you can apply to any system you need to secure.The course is instructor-paced, lasting three weeks, with one more week for the final exam. Along the way, I will be available in the course Discord channel to answer all your questions or to just say, “Hi.” We hope you’ll join us!"
475,https://redis.com/blog/microservices-and-containers/,Microservices and Containers Explained… Using LEGOs,"June 5, 2023",Angel Camacho,"Just as a LEGO set consists of multiple pieces that you put together to build different structures, microservices are small, independent components that you can combine to build larger applications. This is a stark departure from the traditional monolithic architecture, where an application is developed as a single unit, often leading to less flexible and harder-to-maintain monolithic apps.Developers are familiar with the concept of designing their applications in modules, where each routine is responsible for doing one thing well. That’s a useful starting point for understanding the role of microservices in a modern software architecture, wherein applications are structured as a collection of independent, small, and loosely coupled services.But an even better analogy may be LEGOs, the stackable plastic blocks that have become a ubiquitous children’s toy and a hobbyists’ dream. You can accomplish quite a bit with the simplest elements but also do amazing things when you master the tools. Similarly, mastering container technology opens up new possibilities in software development.Each microservice can be developed, deployed, and scaled independently, so developers can build and maintain complex applications efficiently. Microservices can communicate with each other through APIs, which enables the application to function as a cohesive whole. This allows for a more streamlined process of application development.To visualize the relationships, think of microservices as if they are a LEGO set. Just as a LEGO set consists of multiple pieces that you put together to build different structures, microservices are small, independent components that you combine to build a larger application.And, just as a LEGO set allows builders to swap pieces in and out, developers can replace or update microservices without affecting the entire application or having to deal with a complex monolithic application. This modularity and flexibility are among the reasons that microservices are so popular in software development. In 2022, for example, more than a third of software developers employed microservices.Microservices are:An ongoing computing problem is the need to get software to run reliably when it’s moved from one computing environment to another. Maybe it’s as simple as the desire to migrate from a developer’s laptop to a test environment, or to move from a staging environment into production, where the operating system versions and server configurations are not exactly the same. With a larger corporate scope, though, the issue comes up when a development team needs to shift from a physical machine in a data center to a virtual machine in a private or public cloud.Containers are a way to package and run applications in a portable and isolated environment. They have become the de facto standard way to deploy software applications, allowing them to run reliably and consistently across different environments. A container image includes the application and all its dependencies, packaged into a single deployable unit.Again, consider a LEGO set that has lots of pieces. You want to build a few different things: a car, a plane, and a Star Wars Millennium Falcon. Instead of throwing all the pieces in one big pile and trying to sort through them every time you build something new, you sort pieces into smaller containers based on what they’re used for: big blocks in one container, small blocks in another, and so on.Think of the LEGO pieces as though they represent the code and resources that make up a microservice, and the containers represent the isolated environment where the microservice runs. Using containers to separate the microservices makes it easier to manage and deploy them independently, just as sorting the LEGO pieces into containers makes it easier to build different models without having to search for the right pieces each time. Or step on one.Components of containers:This concept of managing and deploying containers, known as container orchestration, has revolutionized the way developers work, allowing for the easy handling of containerized applications. A container orchestration platform like Kubernetes can manage multiple containers at once, making it easier to handle large applications made up of many microservices. This type of platform is essential in cloud computing, where applications often need to scale rapidly to meet demand.The combination of microservices and containers simplifies the management of individual services, ensures fault isolation, enables seamless scaling, facilitates customization and rapid deployment, and promotes overall efficiency and reliability. This approach also lends itself to a continuous integration and continuous delivery (CI/CD) model, where updates can be frequently released with minimal impact on the application’s functionality.Much like the containerization of LEGOs enhances your creations and the process by which you build them, pairing microservices with containers brings benefits to application architecture. The combination simplifies the management of individual services, ensures fault isolation, enables seamless scaling, facilitates customization and rapid deployment, and promotes overall efficiency and reliability. This approach also lends itself to a continuous delivery model, where updates can be frequently released with minimal impact on the application’s functionality.Redis Enterprise uses containers such as Kubernetes to facilitate the deployment, scaling, and management of containerized applications. This can be viewed as a managed service, where much of the complexity of managing a microservices application is handled by the platform. By running Redis Enterprise on Kubernetes, organizations can benefit from automatic scalability, persistent storage volumes, simplified database endpoint management, zero downtime upgrades, and secure containerized applications. Kubernetes provides a robust foundation to deploy Redis Enterprise clusters, offering dedicated functions like anti-affinity, persistent volumes, and StatefulSets for running share-nothing nodes.To facilitate the management of Redis Enterprise clusters on Kubernetes, Redis developed the Redis Enterprise Operator, which automates the configuration and execution of many Kubernetes functions and uses Redis-specific controls to automate the operation of the data platform, including deployment patterns such as Active-Active databases. The Redis Enterprise Operator ensures that the Redis cluster and Kubernetes orchestration system work together seamlessly. Organizations using Kubernetes can leverage Redis’ expertise in running Redis-as-a-Service and rely on the Redis Enterprise Operator to handle the complex operations of the data platform.Thinking back to our LEGO analogy, the Redis Enterprise Operator acts as a master blueprint that guides you through a project. Redis Enterprise Operator ensures that projects are built consistently and reliably without missing parts.Organizations using Kubernetes in production environments can rely on Redis’ expertise in running Redis-as-a-Service, just as a novice LEGO builder can rely on the master builders’ instructions to create a complex structure.Redis Enterprise on Kubernetes can be deployed on-premises using Red Hat OpenShift, any major cloud vendors, or a combination of them for a true hybrid and multicloud experience. It is also available on the Microsoft Azure Kubernetes Service (AKS) and Google Kubernetes Engine (GKE) marketplaces, enabling organizations to unify their cloud billing and deploy clusters in any region, including their local data center.Learn how Redis enhances messaging, storage, and caching, facilitates interservice communication, and synchronizes data across clusters. Download the e-Book Redis Microservices for Dummies now to unlock the full potential of microservices."
476,https://redis.com/blog/redisearch-redisjson-redisgraph-redistimeseries-and-redisbloom-now-available-on-redis-enterprise-cloud-essentials/,"RediSearch, RedisJSON, RedisGraph, RedisTimeSeries, and RedisBloom Now Available on Redis Enterprise Cloud Essentials","July 13, 2020",Pieter Cailliau,"We’re excited to announce the availability of our most popular modules on Redis Enterprise Cloud Essentials. Redis Enterprise Cloud Essentials databases are ideal for trying out new projects and can grow with you as you scale your application. Even better, Redis Enterprise Cloud Essentials offers a free tier for databases up to 30MB, which can be scaled to higher tiers at minimal cost.We heard your requests for this feature and we agree that it’s an ideal playground to try out our technology for engineers who may have restricted laptops or would like to collaborate in a team. We are gradually rolling out the service, and as such it’s currently available only in the AWS/Mumbai (AWS/ap-south-1) region. We look forward to announcing a full rollout in the second half of 2020.To get started, follow these three steps (you can also find them in the Redis Modules Quickstart Guide):Step 1: Create a new subscriptionTo create a new Redis Enterprise Cloud Essentials subscription:Step 2: Create a database with a module enabledAfter you create a subscription, you can create a database:The database is in “Pending” status. When the database is created, you will be able to see the database settings, including:Step 3: Connect to your databaseNow you can connect to the database with telnet, redis-cli, an application, or RedisInsight.To get started with the modules, go to the quick-start guide for the module that you enabled:RedisInsight is an intuitive and efficient GUI for Redis, letting you interact with your databases and manage your data—with built-in support for most popular Redis modules. RedisInsight makes it easy to interact with your database hosted on Redis Enterprise Cloud Essentials, helping you get started with the rich data structures and providing the appropriate visualizations as well as interactive tools to help you iterate quickly when building your queries. (This is especially helpful when you are not yet familiar with the syntax.)RedisInsight is free and available for Windows, Mac, and Linux:In the following weeks, we plan to publish a series of tutorials to make it easier to experiment with Redis Enterprise Cloud Essentials and each of the modules. The tutorials will include guided flows and sample datasets to play with. You’ll discover new use cases you can address with the world’s most-loved database: Redis!We’d love to hear from users, developers, and the entire Redis community interested in these new capabilities. Please feel free to ask a question on our forums or use the support form from Redis Enterprise Cloud to request help and advice. We look forward to hearing from you!"
477,https://redis.com/blog/start-to-finish-with-databases-on-kubernetes/,Start to Finish with Databases on Kubernetes,"June 26, 2020",Alex Milowski,"With the latest release of Redis Enterprise 6.0.6, our Kubernetes operator includes a new database controller that provides the ability to create databases via custom resources. This mechanism enables database configurations to be packaged in a familiar YAML format alongside other application workloads. This also enables continuous integration/continuous deployment (CI/CD) processes like GitOps for code-driven Infrastructure as Code (IaC) deployments.A database is described via a custom resource containing the minimal necessary requirements and then is created within a namespace by applying that resource to the namespace. For example, a small test database can be easily created on an existing cluster, named “rec”, by a resource described in a YAML format:While there are a variety of options available for controlling how the database is created, the user only needs to specify the minimum necessary features. Once the custom resource is created in a namespace, our database controller will discover the newly created resource and ensure its creation in the referenced cluster. If the description of the desired database changes, the operator will ensure the same changes are reflected in the cluster. In this way, the application developer manages the database in the same way they do other workloads in Kubernetes.For more details about our operator and a live GitOps demo, watch the RedisConf 2020 Takeaway session on Cloud Native Automation with the Redis Enterprise Kubernetes Operator with Amiram Mizne and Roey Prat. This session, embedded below, discusses the architecture of the operator, introduces the database controller, and demonstrates a continuous deployment scenario using the Flux CD system.Let’s walk through the basics of using the database operator. We’ll assume that you have a Kubernetes cluster with the Redis Enterprise operator installed. If not, you can refer to our documentation and GitHub reference material for instructions on installing the operator into your namespace.Creating the database First, we’ll create an example application in a namespace called bdb: kubectl create namespace bdb In this namespace, let’s set up a small test cluster and deploy the sample guestbook application. In this example, we’ll create a cluster with a minimum of 3 nodes and with all the memory, CPU, and other settings with their default values:This will create a small cluster called “rec” on which we can create our database. The cluster will take a few minutes to start and to be ready for database creation. You can monitor the change of the status of your cluster to “Running” by: kubectl get rec/rec -o jsonpath='{.status.state}' Once the cluster is ready, you can create a database by just applying a database custom resource:At this point, the database controller within the Redis Enterprise operator will recognize the new custom resource. It will go through the process of creating the database on the cluster and exposing it as services within your namespace. The database status will change to “active” when it is ready, and can be monitored by: kubectl get redb/smalldb -o jsonpath=""{.status.status}"" That’s all that there is to creating the database. Once we are done with it, we can delete the database by simply deleting the custom resource.Using a databaseWhile there are various ways of controlling access to the database, our example didn’t specify anything other than the size. Consequently, the database controller generated a password for the database and stored it in a Kubernetes secret that is named in a consistent way. This secret also contains the other binding information required by an application, including the database port and service name. With these three items from the secret, we have the necessary information to connect to the database.You can find the database secret name by: kubectl get redb/smalldb -o jsonpath=""{.spec.databaseSecretName}"" Because we let the secret be named by the database controller, the name is consistently generated as “redb-smalldb” where the name of the database is prefixed with “redb-”With this secret, we can deploy an application. For example, we can deploy a simple guestbook application that can access the database using the secret:By applying the above, the application is deployed in a pod where the connection information is stored in environment variables in the container. These values are pulled from the secret in the “env” section where the “secretKeyRef” is used to specify the name/value pair. Our guestbook application is written to use these environment variables to connect to the database.With the above deployment, our application should now be running within our namespace. While there are many ways to expose this application, for testing purposes we can simply forward the web application’s port to our local machine: kubectl port-forward `kubectl get pod -l name=guestbook -o jsonpath='{.items..metadata.name}'` 8080:80 We can then visit http://localhost:8080 in a browser and see the application running. If you add a name to the guest book, you’ll see that it is stored. Try reloading the browser to see that you can retrieve the list again from the Redis database.We have now seen the database controller in action with a simple deployment that included the creation and use of a database. Again, I encourage you to watch the RedisConf 2020 Takeaway session on Cloud Native Automation with the Redis Enterprise Kubernetes Operator to learn more about using the operator in conjunction with GitOps and continuous deployment.Because the database is also a resource description we can write in YAML format, we can manage the database configuration like other code and configuration files. These deployment descriptions can be parameterized with various tools, like kustomize, and used as input to CI/CD systems. The database custom resource and controller gives the application developer a cloud-native mechanism for packaging their database along with their application deployment and the Redis Enterprise operator turns that packaging into action."
478,https://redis.com/blog/graphxr-read-modify-and-write-ontologies-with-redisgraph/,"GraphXR: Read, Modify, and Write Ontologies with RedisGraph","June 24, 2020",Alex Milowski,"Kineviz is a software developer and consultancy focused on visual analytics. Its GraphXR platform provides a unified visual environment for analysts, business users, and investigators to work with big, connected, and high-dimensional data. GraphXR supports a wide range of applications including law enforcement, medical research, and business intelligence, accelerating time to insight and enabling needle-in-haystack discoveries that evade traditional analytic workflow.RedisGraph has been integrated into GraphXR so that it can read and write graphs. Kineviz has prepared a video demonstration of GraphXR, showing how it can read in an Open Biological and Biomedical Ontology (OBO) Foundry ontology, navigate the ontology as a graph, make modifications, and store the resulting ontology back into RedisGraph.Want to learn more about knowledge graphs? Check out our blog on Getting Started with Knowledge Graphs in RedisGraph to see how they work and are used in computing, and explore how to load and process ontologies in the RedisGraph module."
479,https://redis.com/blog/vote-for-the-best-app-in-the-redis-beyond-cache-hackathon/,Vote for the Best App in the Redis ‘Beyond Cache’ Hackathon,"June 11, 2020",Drew Kreiger,"Participants in the Redis ‘Beyond Cache’ Hackathon have submitted their final projects, and now is your last chance to vote for your favorite! This is our first online hackathon and we challenged Redis developers to build applications that use Redis modules like RedisGears, RedisAI, RediSearch, RedisGraph, RedisTimeSeries, RedisJSON and RedisBloom, as well as Redis event-driven capabilities like Redis Streams, Redis for Task Queues, and Pub/Sub. We received more than 48 applications, out of which 26 met our strict criteria. These four projects received the most votes and made it to the final round.Now you, the Redis community, get to vote for the People’s Choice award. 50% of the points awarded will come from the community, with the other 50% coming from our panel of five judges. (There will also be a Judges Award determined solely by the panel.)Please use the following—equally weighted—criteria when considering your vote:This final voting round runs from June 11–18. The winners will be announced on Friday, June 19!The applications are listed in alphabetical order, and for your convenience, I’ve listed the Redis modules and other features each team used in their app. Check out their projects by watching the demo videos and looking at each project page. If you like it, then click here to vote for it on LinkedIn. Checking out all of the projects can be fun and addicting!A real-time video solution that can take student attendance in a classroom using contactless face recognition, so everyone doesn’t have to handle the same sign-in sheet. Uses RedisAI, RedisGears, RedisTimeSeries, and Tensorflow. View submission. Vote!A web application and browser plugin that generates quizzes, notes, and flash cards automatically for use in educational videos. In addition to natural language processing and machine learning, the app uses RedisGears, Redis Pub/Sub, Redis Streams, RedisJSON, Sorted Sets, Hashes, and TypeScript. View submission. Vote!An AI-based tool that checks for PPE (personal protective equipment) on medical staff to meet safety protocols to prevent the spread of virus. Uses RedisAI, Redis Streams, TensorFlow, and Express.js. View submission. Vote!A blockchain-based decentralized e-commerce platform. Uses RediSearch, Redis Pub/Sub, Redis as a cache, Socket.io, and Ethereum. View submission. Vote!Visit the Redis ‘Beyond Cache’ Hackathon page for more details about the hackathon. To learn about other programs and activities offered by Redis, visit the Redis Stars Developer Community."
480,https://redis.com/blog/announcing-the-new-redis-university-youtube-channel-video/,Announcing the New Redis University YouTube Channel [Video],"April 15, 2020",Kyle Banker,"Redis University is pleased to announce a new YouTube channel dedicated to educating developers with short, focused, instructional videos. The first series is called Redis Explained, where we’ll be diving into all of the Redis data structures and the most popular Redis modules.We just released our third video in the series, Sorted Sets Explained (which introduces common Sorted Set commands, including ZADD, ZINCRBY, ZRANGE, and ZRAN, and demonstrates how to use Redis Sorted Sets to maintain an online gaming leaderboard), and the response to the series has been overwhelmingly positive. We’re committed to producing these videos regularly, so if you like what you see, please consider subscribing.We built Redis University two years ago to provide the best online education experience for Redis users. Redis University now hosts five online courses, with two more in development. In addition, Redis University is the home of Redis Certification.Of course, not everyone has time to take a four-week course. For folks new to Redis and for developers needing a quick refresher, a short video can be just the thing you need.After we complete the Redis Explained series, covering Redis data structures and top modules, we’d love to get your suggestions on what to tackle next! Check out the Redis Hashes Explained video below to get an idea of what we’re doing:If this series looks intriguing, subscribe to our YouTube channel so you never miss a video. And if you have any requests, let us know in the comments section or on Twitter!"
481,https://redis.com/blog/redis-labs-now-has-400-employees-and-were-still-hiring/,Redis Labs Now Has 400 Employees—And We’re Still Hiring! [Video],"February 6, 2020",Adi Stern,"Every month new faces appear at Redis—the home of Redis—and this past week we marked a major milestone in our company’s growth as we welcomed our 400th employee to the organization. We’re proud to employ a diverse team of talented and hard-working software engineers, product gurus, technology wizards, super-successful salespeople, marketing experts, and finance and HR professionals, and we’re hiring remotely all around the world and looking to staff our offices in Mountain View, California; Tel Aviv, Israel; London, United Kingdom; and Austin, Texas.We pride ourselves on our fun, supportive, and vibrant workplaces, and we’re thrilled to have been recognized for this in Dun & Bradstreet’ 10 Best Startups to Work for in Israel and Sequoia Consulting Group’s 2019 Healthiest Employers of the Bay Area.For a taste of what life is like at Redis, take a minute to watch The People Of Redis video below, then check out our job board to see if one of our 40 open positions is right for you:We grew by more than 50% in 2019, and we plan to hire at least 150 people this upcoming year. That’s only the beginning as we work to build and grow Redis Enterprise as a high-performance in-memory database based on open source Redis, purposely built for real-time customer-facing applications. We’re proud to be trusted by 7 of the Fortune 10, 3 of the top 5 communications companies, 3 of the top 4 credit-card issuers, and 3 of the top 5 healthcare customers, including household names like Mastercard, Dell, Staples, Home Depot, and British Airways.Besides being a fun place to work (with great snacks!), we also offer a competitive compensation package, an employee stock program, and full benefits.So what are you waiting for? Come join us today!"
482,https://redis.com/blog/should-i-submit-to-speak-at-redisconf2020/,Should I Submit to Speak at RedisConf2020?,"February 4, 2020",Redis,"It’s that time of year again! We’re ramping up for RedisConf2020, and our goal is to provide a platform for the most interesting and useful Redis talks from around the globe. To make sure we host the best conference possible, we’re inviting the global Redis community to engage in our Call for Papers (CFP) process.If you’ve never replied to a CFP before, the process can seem intimidating, but it’s really quite easy. Simply log in to our RedisConf20 CFP form and fill out the required information by February 15, 2020. That’s all there is to it.To help you submit your proposal as quickly and easily as possible, here’s some background on the fields you need to fill out:That’s it for the hard stuff—the rest of the fields are basic questions around sharing and travel. Just don’t sleep on the Notes section—that’s where you can tell the selection committee anything that doesn’t neatly fit into the above questions.Finally, you’ll need to include your biography and a photo of yourself (a good selfie will do but a professional headshot is better). The bio should describe yourself, on a personal level as well as on a technical level. A good conference bio should include the following in a single paragraph:Hit the Submit Session button and you’re one step closer to being a speaker at RedisConf2020!Once we receive the entries, a committee reviews and evaluates which proposals will work best for the conference. Much of this effort is about weeding out duplicates or near duplicates (we don’t need two talks on the same subject, but sometimes we even play matchmaker and help presenters collaborate with each other). Then we look at the technical aspects of the proposals—is this really a Redis talk? (Every year we have to reject a number of interesting but very general proposals that don’t have much to do with Redis.)As the list gets narrowed down we have to make some really tough decisions. This is where we review each proposal technically—for example, if a proposal talks about best practices, we’ll eliminate talks that use deprecated libraries or tools. We also tend to decline talks that apply only to a small technical niche. It might be cool that you are doing something really specific, but we want the talk to attract an audience, too.At the end, we fill three buckets:Waitlisted talks are good enough to be in the conference, but we don’t have room at the time of the decision. Note that at a conference the size of RedisConf, there are always a few changes as we go along, so a waitlisted talk still has a real chance of eventually being selected for the event.Submit an idea or experience you think other Redis users would get value from—it doesn’t have to be a big, groundbreaking idea. Indeed, something that might be mundane to you could be a life-changing insight to someone else, especially Redis beginners. We love to hear about novel uses of Redis, innovations that helped you, and complete narratives of how Redis impacts your day-to-day life.For inspiration, watch these videos of standout talks from previous RedisConf events:Uli Hitzel talks about Bambleweeny, a Redis HTTP interface with OAuthProcessing Real Time Volcano Seismic Measurements Through Redis by David Chaves and Elzbieta MalinowskiShrif Nada, from LiveRamp and his talk “Beyond PFCount: Tiny Big Data Counting Engine”Internet Archive’s Jim Nelson speaking on “Work Stealing For Fun And Profit”Redis Fault Injection by Khalid LafiFail Safe Starvation Free Durable Priority Queues in Redis from Jesse WillettAjay Kemparaj from Adobe talking about Leveraging Redis to Serve and Secure Billions of API RequestsShort answer: Yes!Odds are if you’re reading this blog post and you’re already a Redis user you have something valuable to say. We enthusiastically welcome first-time speakers, experienced speakers, small-business Redis users, web-scale Redis users, and everyone in between as well as, of course, presenters of all genders and all backgrounds. What are you waiting for? Go to the RedisConf2020 CFP form today—before time runs out!Featured image by Kane Reinholdtsen, Unsplash"
483,https://redis.com/blog/redis-day-bangalore-2020/,Don’t Miss Redis Day Bangalore 2020,"December 13, 2019",Paul Bushell,"We love how much passion the Indian tech community has for Redis, so we’re excited to kick off the new year at the first Redis Day Bangalore! We have two jam-packed days planned for January 21–22, including a special keynote from Redis creator Salvatore Sanfilippo, our first-ever hackathon, a training day to help you brush up your Redis skills, and presentations from HolidayMe, Zoomcar, and some of our other Indian customers on the creative ways they’re using Redis.If you’re a developer, engineer, software architect, programmer, or business professional, you won’t want to miss out on this free conference on anything and everything Redis.Register for Redis Day Bangalore 2020 now!And if you’re wondering what’s in store? Check out the highlights from Redis Day London:You’re in for a treat: Flying in from Italy and Israel respectively, Redis creator Salvatore Sanfilippo and Redis Co-Founder & CTO Yiftach Shoolman kick off the main day of programming with a pair of insightful keynotes. You won’t want to miss this special opportunity to hear about the future of Redis.The rest of the day is full of presentations from our customers on creative and innovative ways they’re using Redis. Amit Agrawal, Co-Founder and CTO of sRide, will share how the carpool and bikepool platform uses Redis to reduce response time, authenticate APIs, and scale websocket server. HolidayMe CTO Rajat Panwar will discuss using Redis as a key component for very-low-latency logging framework. And AI researcher and consultant Jayesh Ahire will lead a session on using TensorFlow 2.0 and Redis to deploy a deep-learning model in production.These are just a few of our confirmed speakers—be sure to check our official Redis Day Bangalore programming page to see the full list.If you want to stretch your creativity or show off your Redis expertise, register for our first hackathon in India on Tuesday, January 21. We’re excited to unleash the Redis community to solve real problems. Plus, you’ll have an opportunity to win some fantastic prizes and form friendships with other Redis creators. Stay tuned to our official Redis Day Bangalore programming page for more details.If you’re new to Redis or want to brush up on your skills, join us for our training day on Tuesday, January 21. We have five sessions that will walk you through both common and advanced use cases, like Redis clustering, probabilistic data structures, and Redis streaming architectures.Where: Taj Yeshwantpur, 2275, Tumkur Road, Yeshwantpur, Bengaluru, 560022When: January 21–22, 2020This is just a taste of what’s on tap at Redis Day Bangalore 2020—get more information and check out the full agenda and complete list of speakers on the official Redis Day Bangalore programming page.Register now before we sell out!And if Bangalore is too far away, we’re also hosting Redis Day Seattle on January 13–14."
484,https://redis.com/blog/top-5-takeaways-from-andy-jassys-2019-aws-reinvent-keynote/,Top 5 Takeaways from Andy Jassy’s 2019 AWS re:Invent Keynote,"December 12, 2019",Ryan Powers,"AWS re:Invent really is the Super Bowl for cloud computing and enterprise software. Every company—including AWS—brings their latest and greatest innovations to the show.This year, AWS CEO Andy Jassy made a ton of announcements in his highly anticipated keynote address, which focused on the need to adopt cloud-native architectures for shifting infrastructure to the cloud. He also shared a lot of information on where the cloud giant is heading, as well as benchmarks for where AWS customers are on their cloud journey. Thousands of attendees visited the Redis booth at the show, and not surprisingly many of them wanted to talk about issues raised in Jassy’s keynote. They wanted to know how to future-proof their data layer strategy as they modernize their infrastructure architectures. And many wanted to understand how they could utilize Redis beyond caching and make it their primary database. You can watch Jassy’s entire keynote here, but this post will address our top five takeaways from the almost three-hour talk, and how they affect Redis users.According to Jassy, a whopping 97 percent of the $3.7 trillion IT market is still on premises, in corporate servers and data centers, and not yet in the cloud! Despite the cloud’s undeniable momentum, it remains critical to build and purchase solutions that will work not only in the cloud but also on-premises. In that vein, the most common question from Redis booth visitors was, “How do I reconcile my data in the cloud with data on-premises, and even data in different geographies?”At Redis, we see a lot of companies still maintaining applications on-premises even as they begin shifting workloads to the cloud. That makes it an absolute necessity to have a data layer that enables a hybrid and multi-cloud strategy.And that’s only part of the equation. Not only are modern applications being built on hybrid infrastructure or even multiple clouds, they are also geo-distributed. In efforts to lower latency and comply with data regulations, many cloud-native companies are pushing instances across the US and even around the world. When it comes to supporting multi-geo applications with a scalable, geo-local, low-latency database Redis Enterprises’ Active-Active features have you covered whether you’re using a hybrid or multi-cloud approach.In the prelude leading up to Jassy’s announcement of Amazon’s managed Cassandra service, he talked about AWS’ commitment to creating purpose-built databases that are the right tools for your job. The days of using your Oracle database for everything are over.AWS is suggesting multiple databases because each one works with a particular data model, but here’s the thing: like Oracle, each one is a standalone product. Redis Enterprise, on the other hand, offers the best of both worlds: one cluster, multiple data models. That combination resonated powerfully for the many Redis booth visitors who were looking for a tool that could give them a unified view into their cache and database as well as support their myriad of use cases and data models.Imagine a world where you have the right tools for the job, and they all share one easy-to-learn and easy-to-use user interface. The first step to this promised land is what you get with the newly announced RedisInsight, an open source tool that provides the visibility you need to reduce memory usage, track keys, monitor commands in real-time, and manage your Redis clusters. RedisInsight also gives you visibility into Redis Enterprise’s modules, such as RediSearch, RedisGraph, and RedisTimeSeries, which let our customers use the best data models for their needs—all within the unified Redis Enterprise interface. Teams can build real-time applications quickly, while reducing the operational complexity associated with developing across multiple databases with different UIs.AWS’ announcement of a new managed Cassandra service is effectively building on top of the Cassandra OSS community and providing a managed solution. This is similar to what AWS has done with other open source projects, including ElasticSearch and even Redis. AWS claimed in a press release accompanying the keynote that this move will actually help the Cassandra OSS community. We look forward to AWS making good on its promise to give back to the OSS community that it relies on for the foundation of many of its services.At Redis, open source is core to our DNA. Salvatore Sanfilippo, the founder of open source Redis—the world’s most popular database with more than 5.3 million container launches per day—is part of the Redis team. We have helped guide open source Redis since the beginning, and the vast majority of commits to the open source code have come from Redis.Like other cloud vendors, AWS is putting a lot of eggs in the Kubernetes basket. During his keynote, Jassy revealed that AWS EKS will now be available on AWS Fargate. Containers and Kubernetes are becoming the de facto way of building cloud-native applications along with increasingly popular new serverless compute options. Cloud vendors are racing to create feature-rich experiences for their customers re-architecting their applications to take advantage of the new cloud-native stack.Redis Enterprise is built with this stack in mind. You can seamlessly deploy, monitor, and administer Redis Enterprise containers as a cloud-native database service in your Kubernetes cluster to reduce deployment complexity, enhance operational readiness, and accelerate your application development and delivery. To learn more, download our free Microservices with Redis Enterprise on Kubernetes whitepaper.Jassy announced a number of new and upgraded AWS instances, including the M6g, R6g, CRg EC2 instances, and Inf1 EC2 instances for machine-learning use cases.Fun fact: the largest cost factor in machine learning use cases comes in the work needed to make predictions (up to 90%, according to Jassy). Training the models can also be expensive, but the inferences needed to make predictions generates the bulk of costs. AWS developed the inf1 instances to help drive down the cost-per-inference.The extensive amount of data from modern use cases like machine learning, AI—and the need for low latency data in conjunction with these use cases—was another big topic of conversation at the Redis booth. Inevitably, questions of scalability and performance quickly turn to cost effectiveness. Booth visitors wanted to understand how we compared to other cloud managed Redis services, and we were proud to note that we are objectively one of the most cost-effective and fastest options at scale. We understand the cloud’s promise to drive down costs and that is one of the core values we provide to our customers. Learn more about Redis Enterprise’s advantages here.As you build out your roadmap for 2020, don’t forget about the data layer. Can your primary database handle the scale and low-latency requirements of modern cloud-native applications for all your use cases while minimizing operational complexities and its associated costs?If you missed us in Vegas, no worries, we’ve got you covered. To learn more about how your organization can leverage Redis Enterprise for hybrid and multi-cloud environments and cloud-native architectures with the highest performance at the lowest cost, visit the Redis Growth Happens page now!"
485,https://redis.com/blog/redis-day-seattle-2020/,"Save the Date for Redis Day Seattle—Jan. 13–14, 2020","December 4, 2019",Redis,"2020 promises to be a big year for Redis, and we’re starting off strong with Redis Day Seattle on January 13–14!On Monday, January 13, we’ve planned a full day of training sessions for those new to Redis and those who want to learn more about the latest updates and developments. Then on Tuesday, January 14, join us for a free, all-day, single-track conference featuring a keynote from Redis Lab CTO and Co-Founder Yiftach Shoolman and user stories and use cases from across the Redis community.If you’re a developer, engineer, software architect, programmer, or business professional, you won’t want to miss the opportunity to learn from and network with members of the Redis community.Register for Redis Day Seattle 2020 now!And if you’re wondering what’s in store for you at the event, check out the highlights from Redis Day London:Yiftach will get the day of Redis expert presentations rolling with a keynote on Redis 6.0 and the latest innovations in Redis Enterprise. But that’s only the beginning: Scott Haines, Principal Software Engineer at Twilio, will share info on the “happy marriage” of Redis and Protobuf. You also won’t want to miss Redis’ own Solution Architects Manager, Jane Paek, who’ll explain how to use Redis to protect your API with metering and rate limiting. And Oracle Cloud Senior Software Engineer Dmitry Polyakovsky will cover how to use Redis with Python libraries for advanced data processing.This is just a small sample of our speaker list—be sure to check out our official programming page to see additional speakers as they’re confirmed.If you’re new to Redis or just want to learn what’s new, join us for our training day on Monday, January 13th. Redis’ own Kyle Davis and Loris Cro will lead a full day of sessions on how to get started and get the most out of Redis.Where: Hyatt Regency, 808 Howell St., Seattle, Wash., 98101When: Monday, January 13 and Tuesday, January 14 (both days 8:30 a.m.–5 p.m.)Note: The training day on Monday, January 13 costs $149 per person, while the programming on Tuesday, January 14 is complimentary.This is just a small sample of what’s on tap for Redis Day Seattle 2020—get more information and check out the full agenda and complete list of speakers here.Reserve your seat now before they’re gone!And if you’re on the other side of the world and can’t make it to the Seattle event, we’ll also be hosting Redis Day Bangalore—with a special Redis hackathon!—on January 21–22 in Bengaluru, India.Cover image by Ben Dutton, Unsplash"
486,https://redis.com/blog/redis-labs-a-great-place-to-work-on-three-continents/,Redis Labs: A Great Place to Work on Three Continents,"October 11, 2019",Adi Stern,"At Redis, we’ve always been dedicated to creating a vibrant culture in which our employees feel valued and well taken care of. It’s important to us to foster an environment that encourages employees to invest in their work knowing that their efforts are reflected in the company’s culture, reputation, and growth. We’ve been honored in the last month to win multiple awards highlighting the results of our efforts. We believe that Redis team members play a critical part in making the company a great place to work—anywhere in the world.For the second consecutive year, Redis has been named by Deloitte to its 2018 Technology Fast 500™ Europe, Middle East & Africa (EMEA) list. Compiled by Deloitte Global’s Technology, Media & Telecommunications (TMT) industry group, the program is an objective industry ranking recognizing the fastest-growing technology companies in EMEA from 2014 to 2017. Redis achieved 687% growth over the period, ranking 13th among Israel-based companies, and 144th in the EMEA region overall.Apparently, staffers like contributing to fast growth! Our research and development center, Redis Israel, secured the No. 7 spot on the annual Dun & Bradstreet 10 Best Startups to Work for in Israel list. The rankings are compiled by Dun & Bradstreet through employee questionnaires and HR-manager surveys at leading high-tech companies throughout the country. Factors considered include professional challenges offered, employee impact on company achievements, the ability to offer new ideas, work-life balance, and more.We are gratified to see Redis’ values—including taking ownership, making it happen, fearlessly pursuing your ideas, and embracing continuous learning—reflected in achieving this designation based on our team members’ experiences working at Redis.Watch the video—in Hebrew—below!Staying healthy is a big part of staying happy at work. So we’re equally excited that our Mountain View, California, headquarters has been named an honoree in the 2019 Healthiest Employers of the Bay Area program presented by Sequoia Consulting Group. This award was based on our team’s commitment to employee health, with exceptional corporate wellness programming. An important component of building an inclusive culture for us is the Redis Stay Healthy Program. I’m especially proud this program is getting recognition because we specifically designed it to be an inclusive health and wellness program where employees of all ages can get help staying fit and healthy by getting support for activities they choose to fit their age, health situation, personal interests, and lifestyle.We are thrilled to share these successes with our global team, without whom none of this would be possible. I want to thank every Redis team member for their hard work and dedication to building a strong company and culture. We look forward to building on this success with you!If you aren’t a member of the Redis team, you might find your next great challenge here! Check out our open positions around the world on the Redis Careers page."
487,https://redis.com/blog/redis-enterprise-extends-linear-scalability-200m-ops-sec/,Redis Enterprise Extends Linear Scalability with 200M ops/sec @ <1ms latency on Only 40 AWS Instances,"June 27, 2019",Nail Sirazitdinov and Mikhail Volkov,"All modern consumer applications must scale easily and cost-efficiently, and for this, linear database performance is key. With our shared-nothing architecture, we’ve proven again and again that a single Redis Enterprise cluster can scale infinitely in a linear manner by simply adding shards and nodes. But that doesn’t mean we’re resting on our laurels. At RedisConf18 we demonstrated that a single Redis Enterprise cluster could reach 50 million operations per second (ops/sec) with only 26 AWS instances while keeping latency under 1 millisecond, and today we’re pleased to share that Redis Enterprise has set yet another new industry performance record.In our latest benchmark, Redis Enterprise delivered over 200 million ops/sec, with under 1 millisecond latency, on as little as 40 AWS instances. This represents a 2.6X performance improvement in less than 15 months. Before we get into the configuration, tools, workloads and results of our latest benchmark, let’s quickly review Redis Enterprise clusters.In Redis Enterprise terms, a cluster is a set of nodes composed from either cloud instances, virtual machines or containers/Kubernetes PODs, or bare-metal servers. A cluster allows you to create Redis databases in a memory pool shared across these nodes, while maintaining complete separation between the data path and the control and management path. Each cluster includes Redis shards, a zero-latency proxy, and a cluster manager. Since the open source cluster API allows Redis clients to directly access the shard that holds the key-value object with no additional network hop, a Redis Enterprise cluster can scale very efficiently. The control path (which deals with database provisioning/deprovisioning, auto-failover, shard migration, cluster balancing and much more) imposes very little overhead and doesn’t affect this linear scalability. Last but not least, Redis Enterprise efficiently utilizes all the cores on every cluster node before adding more nodes to the cluster for scaling purposes.Here is a visual depiction of Redis Enterprise’s shared-nothing cluster architecture, configured with the open source cluster API and utilizing all the cores in a given cluster node:As a baseline, we wanted to test how many ops/sec we could achieve when running on a single AWS instance, and keeping in mind the following requirements:To accomplish this, we used the following system setup:1) We installed Redis Enterprise. 
https://docs.redis.com/latest/rs/installing-upgrading/downloading-installing/2) We set up Route53. 
https://docs.redis.com/latest/rs/installing-upgrading/configuring/cluster-name-dns-connection-management/configuring-aws-route53-dns-redis-enterprise/3) We created our database.
Specify cluster_api_user, name, shards_count and portAs shown below, the API will create database name:<name > with shards:<shards_count> listening on port:<port>. Even though we used a single node cluster for this benchmark, we also set the proxy_policy, shards_placement and oss_cluster parameters. These are used for placing shards equally and creating endpoints on all the nodes, in order to stay consistent with the multi-node cluster configuration.curl -v -k -u <cluster_api_user> https://localhost:9443/v1/bdbs -H ""Content-type: application/json"" -d 
'{
""name"": <name>,
""memory_size"": 10000000000,
""type"" : ""redis"",
""sharding"": true ,
""shards_count"": <shards_count>,
""shard_key_regex"": [{""regex"": "".*{(?<tag>.*)}.*""},{""regex"": ""(?<tag>.*)"" }],
""proxy_policy"": ""all-master-shards"",
""shards_placement"": ""sparse"",
""oss_cluster"":true,
""port"":<port>
}'To make sure the test was fair, we first populated the database to avoid GET operations on non-existent keys (specify NUM_OF_KEYS):memtier_benchmark  -s <EndPoint> -p <Port> -t 48 --ratio=1:0 --pipeline=9 -c 1 -d 100 --key-minimum=1  --key-maximum=<NUM_OF_KEYS> -n allkeys --key-pattern=S:S4) We chose the c5.18xlarge instance type on AWS EC2 for our node, with 72 vCPU 144GB and a 3.0 GHz (up to 3.5 GHz using Intel Turbo Boost Technology) Intel Xeon Skylake-SP processor.5) We chose a c5.9xlarge instance type to run the load generation tool for our client machine.6) Finally, for our load generation tool, we used memtier_benchmark with the following parameters:memtier_benchmark -s <redis.endpoint.address> -p <port> -t $Threads —ratio=1:1 --pipeline=9 -c $Clients -d 100 --key-minimum=1  --key-maximum=<NUMBER_OF_KEYS> -n 1000000000 --cluster-modeWe then tried to find the best shards and proxy threads balance, and observed that our best results came with 8 shards per node, which gave us 4.2 million ops/sec. Throughput was unstable, because of proxy threads context switching and Redis process transitions between CPUs on different NUMA nodes. This cross-memory access increased processing latency, even though the system was loaded at only 50%.As shown in the image below, we were able to get much better results with 8- and 10-shard configurations on the c5.18xlarge instance, using NUMA binding for the Redis shards and proxy threads affinity:Note: The scripts below work on two-socket systems only.NUMA_CNT=$(numactl --hardware | grep '^available' | awk '{print $2}')
if [ $NUMA_CNT -eq 2 ]; then
  NODE_ID=$(cat /etc/opt/redis/node.id)
  DMC_HALF_COUNT=$(expr $(/opt/redis/bin/ccs-cli hget dmc:$NODE_ID threads) / 2)
  NUMA0_CPUS=$(numactl  --hardware | grep 'node 0 cpus' | awk -F ': ' '{print $2}' | sed 's/ /,/g')
  NUMA1_CPUS=$(numactl  --hardware | grep 'node 1 cpus' | awk -F ': ' '{print $2}' | sed 's/ /,/g')
  DMC_PID=$(sudo /opt/redis/bin/dmc-cli -ts root list | grep listener | awk '{printf ""%in"",$3}')
  sudo taskset -apc $NUMA0_CPUS $DMC_PID 
    sudo /opt/redis/bin/dmc-cli -ts root list | grep worker | tail -$DMC_HALF_COUNT | 
      awk '{printf ""%in"",$3}' | 
      xargs -i sudo taskset -pc $NUMA1_CPUS {}
fiNUMA_CNT=$(numactl --hardware | grep '^available' | awk '{print $2}')
REDIS_HALF_CNT=$(expr $(pgrep redis-server-5 | wc -l) / 2)
NUMA0_CPUS=$(numactl  --hardware | grep 'node 0 cpus' | awk -F ': ' '{print $2}' | sed 's/ /,/g')
NUMA1_CPUS=$(numactl  --hardware | grep 'node 1 cpus' | awk -F ': ' '{print $2}' | sed 's/ /,/g')
pgrep redis-server-5 | sort | head -$REDIS_HALF_CNT | xargs -i sudo taskset -apc $NUMA0_CPUS {} && 
  pgrep redis-server-5 | sort | head -$REDIS_HALF_CNT | xargs -i sudo migratepages {} 1 0
pgrep redis-server-5 | sort | tail -$REDIS_HALF_CNT | xargs -i sudo taskset -apc $NUMA1_CPUS {} && 
  pgrep redis-server-5 | sort | tail -$REDIS_HALF_CNT | xargs -i sudo migratepages {} 0 1The table below shows how we determined the optimal Redis shards and proxy threads configuration on the AWS c5.18xlarge instance in order to achieve the best possible throughput (in ops/sec) while keeping latency to sub-millisecond:We first wanted to match the 50 million ops/sec benchmark we demonstrated at RedisConf18 with 26 m4.16xlarge instances. This time we were able to reach 51.72 million ops/sec with only 10 AWS 5.18xlarge instances:Our multiple improvements to the Redis core in version 5.0 (especially around big pipeline performance), together with many enhancements to the Redis Enterprise proxy and to the way it communicates with the Redis shards, and combined with the new AWS C5 instance family and a proper NUMA configuration, all helped achieve over 200M ops/sec with only a 40-node cluster while keeping latency under 1 millisecond.Our shared-nothing architecture enables an additional 500K+ ops/sec for every Redis shard and 5M+ ops/sec for every node added to the cluster, in a close to optimal 94% linear scaling. In other words, per node throughput declined by only 6% between a single node and a 40-node cluster.During these tests, we even managed to achieve 800K ops/sec per Redis shard (with consistent sub-millisecond latency), but due to network packet per second limits we were not able to scale our node throughput to achieve higher and stable performance over time.We were amazed by the significant improvement we saw while breaking our own database performance record from 15 months ago:This experiment shows that with proper architecture, as implemented by Redis Enterprise, Redis can break any performance record in the database space while using significantly less hardware resources than other databases."
488,https://redis.com/blog/simplify-building-managing-applications-redis-enterprise-google-cloud-platform/,Simplify Building and Managing Applications with Redis Enterprise on Google Cloud,"April 9, 2019",Sheryl Sage,"We began our collaboration with Google Cloud in 2014 to make Redis Enterprise available on Google Cloud Platform (GCP) as a fully managed Database-as-a-Service, either hosted or in virtual private clouds (VPCs), as well as downloadable software. Since then our partnership has strengthened considerably as we worked together with many joint customers across every industry including retail, financial services, media, social, and gaming.What makes this partnership strong is that we share a common vision that open source accelerates innovation and helps our customers future-proof their technology investments. We are both committed to fostering the growth of the open source community, while delivering enterprise-grade capabilities such as resilience, security, performance and multi-region distributed scale. Combining full resilience utilizing Redis Enterprise’s Active-Active Geo-Distribution capability with GCP’s fast global network, will allow enterprises to deploy distributed applications with sub-millisecond latency.Today, at Google Next ’19, Google Cloud CEO, Thomas Kurian, and our co-founder and CEO, Ofer Bengal, announced an expanded partnership that will deliver Redis Enterprise as a fully managed cloud service, offering a seamless experience integrated with Google Cloud Platform (GCP) console, billing, and support to enterprise customers.  The new tightly-integrated GCP service will make it even easier for organizations to create Redis applications without ever leaving GCP.You can view the announcement belowWith Redis Enterprise as a fully managed service on GCP you will be able to:Watch the demo of Redis Enterprise on GCP belowWe are excited about this new offering, and what comes next as we work together to make Redis Enterprise available as a native GCP service.  You can read more about the announcement on the Google Cloud blog and on our website."
489,https://redis.com/blog/redis-turns-10/,Redis Turns 10 – How it started with a single post on Hacker News,"February 26, 2019",Redis,"10 years ago, a link was posted on Hacker News to a new software project called Redis. In the comments, its creator Salvatore Sanfilippo (a.k.a. antirez) says “one of the major points of Redis is to support more complex types as values”. From Day 1, Salvatore knew what he wanted to build. And to his credit, he stayed focused. Redis now has 10 complex types and a module system that enables developers to build more. And as a result, Redis is the most popular key-value store on the planet.But Salvatore could not have done this alone. Someone had to be the first to share the link, and someone had to be the first to try the software. That first post was shared by David Welton (a.k.a. davidw). I have no idea if he tried the software, or just thought it seemed cool, but he took a risk and shared it on Hacker News.Only five people commented on that first post: three promoted alternative data stores; one suggested Salvatore re-write Redis in Erlang; and one, JUST ONE, offered some help. That one person turned out to be Ezra Zygmuntowicz (a.k.a. ezmobius, RIP), a very popular Ruby on Rails (RoR) developer and one of the founders of Engine Yard.Ezra not only encouraged Salvatore, but implemented the first Ruby client library for Redis. Then he tweeted about it, which spread the word to the Rails community  At the time, RoR was helping developers create beautiful websites, but users of the framework struggled to make it scale. Thanks to Ezra’s efforts, Redis spread among RoR developers, and helped thousands of RoR websites scale. One of those websites was Twitter. Eventually, Redis went on to be a wildly popular add-on for Heroku developers, and then many, many others.There were others after David and Ezra. But, as Derek Sivers says, “It takes guts to be a first follower. You stand out, you brave ridiculing yourself. Being a first follower is an underappreciated form of leadership”. Nine years ago, Salvatore said “Your early adopters are not brave because they are irresponsible, just they can evaluate something without the need to follow the mass”. So thanks to Salvatore, David and Ezra, and other early risk takers, an open source community called Redis was born. And to those first adopters of the Redis community, as we turn 10 years old today, we say thank you.First Follower by Derek SilversWant to meet the Redis community and see what all the fuss is about? Join us as we celebrate 10 years of Redis at RedisConf, the yearly gathering of Redis developers, implementers and other community members. This year it takes place April 2-3 on Pier 27 in San Francisco. Perhaps you’ll find something new that will inspire you to take a risk."
490,https://redis.com/blog/video-walk-throughs-redis-enterprise/,Video Walk-Throughs of Redis Enterprise,"August 14, 2017",Redis,"We’ve been busy working on some video walk-throughs of common situations with Redis Enterprise.In the first video, we talk about how to replicate between different clusters. This feature is useful in several circumstances: recovering from a disaster, speeding up read performance, geo-located databases, and making duplicates for testing or reporting. This can be done entirely from the Redis Enterprise UI and is a short point-and-click operation. Honestly, it’s pretty neat to see replication between clusters occur so seamlessly.In the second video, we go over some setup techniques to achieve high availability. What’s interesting about high availability on Redis Enterprise is how, if properly setup, you can guard against a huge number of situations that would otherwise leave you paralyzed. This video is also a good introduction to some of the core features of Redis Enterprise (sharding and clustering) and, if you pay attention, you can see one of my favourite parts of clustering: hundreds of thousands of operations per second. If you don’t get excited about high performance then go write COBOL on a System/360 or something (actually, if you’re reading this and still writing mainframe COBOL – please reach out – I legitimately want to talk to you).After showing you the setup, we do a manual failover to see how much it affects the cluster (spoiler: not much). Failing over the shard is done in rladmin, the CLI tool for manipulating some of the lesser used, but still important, parts of Redis Enterprise. Finally we segue into regional replication, where we replicate between shards in the same cluster with a few short clicks.What do you think? Do you want to see more of these videos? If so, what kind of topics would you like to see? Let us know by reaching out on twitter @Redis"
491,https://redis.com/blog/redis-as-a-json-store/,RedisJSON: A Redis JSON Store,"March 21, 2017",Itamar Haber,"Redis modules are bundled and packaged as part of Redis Enterprise Software. Download the modules (including RedisJSON) to upgrade Redis Enterprise Software with the latest module version.Ready to jump right into RedisJSON? Check out Getting Started with RedisJSON on the Redis Developer site!——RedisJSON is a Redis module that provides native JSON capabilities—simply run the Docker Image for Redis with RedisJSON, visit the GitHub repository, or read the docs online.Both JSON and Redis need no introduction; the former is the standard data interchange format between modern applications, whereas the latter is ubiquitous wherever performant data management is needed by them. That being the case, I was shocked when a couple of years ago I learned that the two don’t get along.Redis isn’t a one-trick pony–it is, in fact, quite the opposite. Unlike general purpose one-size-fits-all databases, Redis (a.k.a the “Swiss Army Knife of Databases,” “Super Glue of Microservices.” and “Execution context of Functions-as-a-Service”) provides specialized tools for specific tasks. Developers use these tools, which are exposed as abstract data structures and their accompanying operations, to model optimal solutions for problems. And that is exactly the reason why using Redis for managing JSON data is unnatural. So how do we use the RedisJSON module?Fact: despite its multitude of core data structures, Redis had none that fit the requirements of a JSON value. Sure, you can work around that by using other data types: Strings are great for storing raw serialized JSON, and you can represent flat JSON objects with Hashes. But these workaround patterns impose limitations that make them useful only in a handful of use cases, and even then the experience leaves an un-Redis-ish aftertaste. Their awkwardness clashes sharply with the simplicity and elegance of using Redis normally.But all that changed during the last year after Salvatore Sanfilippo’s @antirez visit to Redis’ Tel Aviv office, and with Redis modules becoming a reality. Suddenly the sky wasn’t the limit anymore. Now that modules let anyone do anything, it turned out that I could be that particular anyone. Picking up on C development after more than a two decades hiatus proved to be less of a nightmare than I had anticipated, and with Dvir Volk’s @dvirsky loving guidance we birthed RedisJSON.While you may not be thrilled about its name (I know that I’m not—suggestions are welcome), RedisJSON itself should make any Redis user giddy with JSON joy. The module provides a new data type that is tailored for fast and efficient manipulation of JSON documents. Like any Redis data type, RedisJSON’s values are stored in keys that can be accessed with a specialized subset of commands. These commands, or the API that the module exposes, are designed to be intuitive to users coming to Redis from the JSON world and vice versa. Consider this example that shows how to set and get values:Like any well-behaved module, RedisJSON’s commands come prefixed. Both JSON.SET and JSON.GET expect the key’s name as their first argument. In the first line we set the root (denoted by a period character: “.”) of the key named scalar to a string value. Next, a different key named object is set with a JSON object (which is first read whole) and then a single sub-element by path.What happens under the hood is that whenever you call JSON.SET, the module takes the value through a streaming lexer that parses the input JSON and builds tree data structure from it:RedisJSON stores the data in binary format in the tree’s nodes, and supports a subset of JSONPath for easy referencing of subelements. It boasts an arsenal of atomic commands that are tailored for every JSON value type, including: JSON.STRAPPEND for appending strings; JSON.NUMMULTBY for multiplying numbers; and JSON.ARRTRIM for trimming arrays… and making pirates happy. All with RedisJSON.Because RedisJSON is implemented as a Redis module, you can use it with any Redis client that: a) supports modules (ATM none) or b) allows sending raw commands (ATM most). For example, you can use a RedisJSON-enabled Redis server from your Python code with redis-py like so:But that’s just half of it. RedisJSON isn’t only a pretty API, it’s also a powerhouse in terms of performance. Initial performance benchmarks already demonstrate that, for example:The above graphs compare the rate (operations/sec) and average latency of read and write operations performed on a 3.4KB JSON payload that has three nested levels. RedisJSON is pitted against two variants that store the data in Strings. Both variants are implemented as Redis server-side Lua scripts with the json.lua variant storing the raw serialized JSON, and msgpack.lua using MessagePack encoding.If you have 21 minutes to spare, here’s the RedisJSON presentation from Redis Day TLV:You can start playing with RedisJSON today!There are still many features that we want to add to it, but over all RedisJSON is pretty neat. If you have feature requests or have spotted an issue, feel free to use the repo’s issue tracker. You can always email or tweet at me—I’m highly-available 🙂Ready to jump into RedisJSON? Check out Getting Started with RedisJSON on the Redis Developer site!"
492,https://redis.com/blog/redis-labs-wins-prestigious-ibm-beacon-award/,Redis Labs Wins Prestigious IBM Beacon Award,"February 11, 2015",Leena Joshi,"Redis is proud to announce that we have been selected by IBM as the winner of its prestigious 2015 Beacon Award for Outstanding Service in the Cloud Marketplace! Each year, IBM recognizes IBM Business Partners who have distinguished themselves in delivering business excellence, innovative solutions, ingenuity, and client satisfaction around the world.The general manager of IBM Global Business Partners, Marc Dupaquier, noted: “To keep up with an industry in transition, IBM Business Partners need to deliver innovative solutions that unlock value for customers in a variety of industries. As a Beacon Award winner, we’re pleased to recognize Redis for helping its clients transform for the future and drive growth with IBM-based solutions that are on pace to address today’s fastest growing technology trends.”The entire Redis team is very excited to accept this award. As a leader in cloud offerings, we’re a very natural fit within the IBM Cloud Marketplace ecosystem.Ofer Bengal, our CEO & Co-Founder, said this about the news:”We are proud to have won this year’s IBM Beacon Award. For decades, IBM has fostered innovation by bringing cutting edge technology to its customers’ fingertips. The Redis Cloud and IBM Cloud Marketplace offering together help developers build best-in-class applications leveraging IBM services and the blazing fast database performance we provide.”For more information, visit visit Redis Cloud in the IBM Marketplace."
493,https://redis.com/blog/using-redis-to-create-a-blazing-fast-api-app/,Using Redis to Create a Blazing Fast API App,"November 11, 2014",Leena Joshi,"Every year New York comes alive with tens and even hundreds of database professionals flocking to it to participate in Database Month. This is a video recording of a seesion given by Tung Nguyen from Bleacher Report during Database Month 2014. In it, Tung reviews how Bleacher Report had built a blazingly fast API application using Redis. If you haven’t already, we recommend that you also check out his blog post on how he tested our Redis Cloud service with live traffic."
494,https://redis.com/blog/heads-up-devcontlv-summit-2014/,Heads Up: DevConTLV Summit 2014,"May 13, 2014",Itamar Haber,"This is the third year that DevConTLV will be taking place and by the looks of it, it will be a huge success. All the right ingredients are in place: a schedule packed with interesting talks (i.e. lots of NoSQL), speakers coming from all over the world, a lot of positive buzz in the local high-tech scene and persistent rumors about free food and drinks 🙂Redis is a proud sponsor of the event and I’ll have the pleasure of giving a talk about… drum roll please… NoSQL and Redis! The talk’s working title is “Redis Use Patterns: An Introduction to the SQL Practitioner”. What I intend to deliver are some insights from our users’ that show how Redis is used, alongside an RDBMS, to address scale, performance and throughput challenges.Two talks that I’m really looking forward to listening to are Javier Ramirez’s “API analytics with Redis and Bigquery” (duh!) and Ido Green’s “Google Cloud Platform – Scaling with a Smile” (did I mention that our service also runs there?), but in all likelihood I’ll be there the whole day and attend everything.If you’re already coming to DevConTLV, it’d be great to meet and chat. If you’re still sitting on the fence, trust me and just come – it’s going to be real fun! Oh, and if you want to come but don’t have a ticket, why not try your luck with our raffle? Redis has some FREE tickets that we’ll be giving away to people who’ll tweet us with the #DevConTLV tag and explain why they need a ticket.Questions? Feedback? Email or tweet me – I’m highly available 🙂Update: here are my session’s slides:The session’s video:"
495,https://redis.com/blog/secure-redis-ssl-added-to-redsmin-and-clients/,Secure Redis: SSL Added to Redsmin and Clients,"April 14, 2014",Itamar Haber,"Today we are happy to make two exciting announcements: we’ve made SSL support available for Redsmin and we’re releasing a couple of Redis clients that we’ve patched to support SSL. Since Redis doesn’t include native support for secured communication – an extremely valid design decision – all the heavy lifting (e.g. setting up a secure stunnel proxy, /ht Benajmin Cane, a.k.a @madflojo) is left to Redis admins and developers. As both Redsmin and Redis offer a turn-key solution for Redis needs (each in its own domain), it is only natural that we rise to the challenge and provide a secure and easy Redis environment.Redsmin can now connect, monitor in real-time and manage Redis databases that are protected with SSL. Connecting to such databases couldn’t be simpler – just copy/paste your certificates to Redsmin and you’re good to go! The following screencast shows how simple it is to get started:Another piece missing from the pu-ssl-e was the availability of Redis clients that support SSL. Applications using SSL to connect to their Redis database had to deploy an additional component in the server’s stack by installing stunnel (or any other secure proxy) to successfully establish a connection. This approach had not only introduced complexity to the app’s operations, but was also impractical in cases in which the app is hosted.Luckily, adding SSL support to popular Redis clients proved to be an easy task for our developers and the fruits of their labor are presented in the list below:Currently, these modified clients are yet to be merged into their respective trunks, so feel free download the forks and to cast your vote on the their pull requests. If you have any requests for a Redis client that’s not on the list – just let us know, and if you want to help in developing one you’ll get free Redsmin and Redis Cloud environments from us – so what are you waiting for?"
496,https://redis.com/blog/graphql-and-redis/,GraphQL and Redis: Build Your Own Haunted House Tracker Microservice,"June 7, 2023",Alex Patino,"In a three-part video series, Guy Royse, Senior Developer Advocate at Redis, uses Apollo GraphQL, Redis, and Node.js to build a microservice that exposes data on haunted places.GraphQL is a querying language that delivers a holistic and easy-to-parse description of data stored using an API. Let’s see how Guy Royse uses GraphQL alongside Redis to track down spooky locations with simplicity, speed, and accuracy.Part one of this three-part series begins with showing the data stored in Redis, which includes information on states, cities, and haunted places. GraphQL works best with structured data, so this is an ideal dataset for illustrating how the query language works.Guy installs the necessary packages and positions the code for the Apollo server, defining the type definitions, resolvers, and data sources. He demonstrates querying for states, cities, and coordinates, showcasing the GraphQL server’s functionality, and wrapping things up by successfully retrieving the data on these haunted locales by ID.> Follow along with Part 1 of this series to start building your simple microservice.Guy Royce’s haunted places exploration using Redis and Apollo GraphQL continues with help from RediSearch.Here, Guy explains the process of creating indexes for cities, states, and places using Redis. He demonstrates how to perform full-text searches on indexed fields such as location and description and mentions using tags for categorizing places.In the demonstration, Guy adds two queries for retrieving places by ID and finding places containing specific text. He creates corresponding resolvers for these queries and showcases the results obtained from the search operations.> View the entire demonstration.Guy Royse concludes his ghost-hunting expedition with a demonstration of how to use a data loader, which is a caching mechanism, to improve the speed and efficiency of the process of finding haunted places, as the data loader also reduces duplicate requests to Redis.Guy wraps things up with additional examples, such as querying for cities, states, and places using the data loader, and he explains how it optimizes the retrieval process. He also discusses applying a data loader to search operations, improving performance by minimizing redundant searches.Watch the final chapter of this ghostbusting expedition with Guy Royse.Continue your learning journey with Redis by diving into our Youtube playlist for Operate Redis at Scale (for DevOps)."
497,https://redis.com/blog/build-ecommerce-chatbot-with-redis/,"Build an E-commerce Chatbot With Redis, LangChain, and OpenAI","April 12, 2023",Tyler Hutcherson and Harrison Chase and Fabian Stehle,"Given the recent surge of AI-enabling APIs and web development tools, it seems like everyone is building chatbots into their applications. Want to see what’s involved? Here’s an overview.One new (and wildly popular) framework, LangChain, makes it easy to develop applications that interact with a language model and external sources of data or computation. It does this by focusing on clear and modular abstractions for all the building blocks necessary to build; then it constructs commonly used “chains,” which are combinations of the building blocks. For example, the Conversational Retrieval Chain enables users to have a “conversation” with their data in an external store.How does it do this? OpenAI language models were not trained on your company’s specific data, or certainly not tuned for it. If you want the chatbot to rely on it, you need to provide OpenAI with your data at runtime. The retrieval step fetches relevant data to the user’s query from Redis using Vector Similarity Search (VSS) and then pipes the data into the language model along with the original question. It asks the model to use only the provided sources – what we in AI circles call “context” – to answer the question.Most of the complexity in this chain comes down to the retrieval step. That is why we’re so excited to add an integration between LangChain and Redis Enterprise as a vector database. This combination makes it possible to bridge the gap between complex AI and product development – without breaking a sweat.Don’t believe us? In this short tutorial we build a conversational retail shopping assistant that helps customers find items of interest that are buried in a product catalog. You can follow along with the full code.Before we jump in, we’d like to thank Fabian Stehle from LabLab AI, who put together the initial prototype of this demo. We extended it and layered in additional LangChain components to give it more functionality.First, let’s collect all the pieces we need for the project.This project needs a few Python libraries. These are stored in the requirements.txt file at the github repo.For the retail chatbot, we chose to work with the Amazon Berkeley Objects dataset. This includes a large selection of Amazon products that are perfect for generating a retail assistant. Download the file from the link, or use the gdown command line interface to download the file from a hosted link.We use the pandas Python library to load and preprocess the dataset. While it loads, we truncate the longer text fields. That’s to keep our dataset a bit leaner, which saves on memory and compute time.With our products dataset fully loaded, we perform some final preprocessing steps to clean up the keywords field and to drop missing values.If you’re following along with the code on github, take a peek at the dataframe with all_prods_df.head(). The full dataset contains over 100,000 products, but for this chatbot, we restrict it to a subset of 2,500.Here is an example of one of the product JSON objects we have to work with.LangChain has a simple wrapper around Redis to help you load text data and to create embeddings that capture “meaning.” In this code, we prepare the product text and metadata, prepare the text embeddings provider (OpenAI), assign a name to the search index, and provide a Redis URL for connection.At this point, we’ve successfully processed the Amazon products dataset and loaded it into the Redis database with vector embeddings.Then we bring it all together to create the Redis vectorstore.Now we’re ready to create a chatbot that uses the products’ data (stored in Redis) to inform conversations.Chatbots are hugely popular because they can be immensely useful. In the scenario we build below, we assume that you need fashion advice. You can ask the bot for help in finding a pair of shoes suitable for both casual outings and work-related outings. You want something that pops, but doesn’t cause too much of a distraction. Given the data we fed it already, our chatbot should be able to recommend a few pairs of shoes that fit the requirements.It’s time to bring in more LangChain functionality. To do so, we need to import several LangChain tools.As mentioned in the introduction, this project uses a ConversationalRetrievalChain to simplify chatbot development.Redis holds our product catalog including metadata and OpenAI-generated embeddings that capture the semantic properties of the product content. Under the hood, using Redis Vector Similarity Search (VSS), the chatbot queries the catalog for products that are most similar to or relevant to what the user is shopping for. No fancy keyword search or manual filtering is needed; VSS takes care of it.The ConversationalRetrievalChain that forms the chatbot operates in three phases:Even though LangChain and Redis greatly expedite this workflow, interacting with a large language model (LLM) like GPT requires a “prompt” for communication. We humans create a prompt (set of instructions) to steer the model’s behavior towards a desired outcome. To get the best results from the chatbot, further prompt engineering may help.See the two prompts we define for steps 1 and 3 above. You can always start with these and improve them for your own scenario.Next, we define two OpenAI LLMs and wrap them with chains for question generation and question answering respectively. The streaming_llm allows us to pipe the chatbot responses to stdout, token by token, giving it a charming, chatbot-like user experience.Finally, we tie it all together with the ConversationalRetrievalChain that wraps all three steps.Keep in mind, this is not an all-intelligent being. But with the help of Redis, which stores the example’s entire product inventory knowledge base, we create a pretty neat experience.The bot interacts with you in realtime, and helps you narrow in on interesting product choices based on what’s in the catalog. Here’s a simple example:After the chatbot welcomes you with, “Hi! What are you looking for today?” try a few of these sample prompts, or make your own:One of the best parts about LangChain is that each class abstraction is made so that you can extend or create your own. Below, we customize the BaseRetriever class to perform some document preprocessing before it returns the results.We need to update the retrieval class and chatbot to use the custom implementation above.Done! Now your chatbot can infuse more product information in your conversation as it steers you towards e-commerce glory! Here’s another short conversation example:Building with LangChain and Redis is easy. Try building this chatbot on your own, or customizing it for your use case. Try Redis Enterprise for free or pull our Redis Stack docker container to get started.Interested in getting your hands dirty with AI? Our partners at LabLab AI are hosting a series of hackathons over the next month featuring Redis. Compete for prizes, hype, and fame. In fact, there’s a hackathon with Stable Diffusion starting April 14th!Like what you’re hearing about Generative AI from Tyler and Harrison?  See them share more practical examples during their joint session at RedisDays Virtual, a free virtual event on May 24th.Learn more about Vector Similarity Search in Redis. You may be amazed by what you can accomplish."
498,https://redis.com/blog/ai-impact-on-inventory-optimization-and-demand-forecasting/,Retailers Shop for Real-Time Inventory Solutions,"June 14, 2023",Eric Silva,"Here’s what prominent retailers are doing to enhance inventory management.With inflationary pressures and global economic uncertainties, no one knows what the future will bring for retailers. Because e-commerce customers expect fast delivery times, many retailers have struggled to manage their inventory stock levels and stay competitive. USA-based Kohl’s department stores are revamping their inventory management systems and practices after net sales fell more than 7% in FY2022 and margins shrunk by nearly 5%. Poor inventory visibility and increased stock carrying costs forced store closings and bankruptcy for Bed Bath & Beyond and Revlon. So it’s not a surprise that real-time inventory management was at the top of the list for many retail companies at the Jan 2023 Annual NRF trade show.Those market trends have made real-time inventory management a top priority for retailers.The days of stacking inventory to the rooftops are over. Many retailers see faster inventory turnover and optimization as the key to success. This involves real-time inventory tracking  – where “real time” is measured in millisecond or sub-millisecond database response – so business leaders can make informed decisions about their inventory operations. The elements can include tracking point of sale (POS) data, warehouse stock levels, fulfillment, supplier performance, and other inventory-related data.Both retailers and consumers require accurate inventory data that is available immediately, such as the quantity and location of products ready for sale. Some inventory databases force developers to pre-calculate, pre-aggregate, or manipulate data before using it. This causes data to become stale. By investing in real-time inventory database technology retailers can identify sales trends, reduce inefficiencies in their supply chain, and meet online and mobile customer expectations for accurate information.The pandemic -induced consumer behavior of buy-online-pickup-in-store (BOPIS) and curbside delivery options are here to stay. Sixty-eight percent of automotive merchants, 61% of grocery merchants, 50% of general retailers, and 42% of convenience stores and pharmacies plan to offer that fulfillment innovation. This requires real-time inventory data for order management, inventory tracking, and order fulfillment to keep up with inventory and logistics changes.With consumers back in stores, retailers are looking to elevate in-store and fulfillment experiences. One digital tool employed by retailers (and expected by consumers) is local store inventory checks so that consumers can know if their desired items are available at the store near them. These features also benefit the retailer because it gives them inventory visibility for stock optimization.Major brands are busy updating their systems. American Eagle is investing in real-time store inventory management software including updating its POS systems. And Macy’s spent more than two-thirds of its 2022 $1.3 billion in capital expenditures on inventory management analytics and other supply chain and inventory management software modernization projects.Another retailer, Levi Strauss, created a real-time data repository for machine learning models and algorithms to predict the inventory positions required for every type of product in various channels. Its goal is to reduce excess stock through predictive analytics and to ensure accurate inventory counts for online consumers.To compete for the profitable omnichannel customer, retailers create innovative ways to enhance the shopping experience and maximize sales. Vector similarity search (VSS) is well suited for retail use cases because developers can build shopping applications where consumers can identify products based on characteristics such as color, size, texture, and style.This can be particularly useful in the fashion industry, where customers may look for products by category. For example, a clothing retailer could use VSS to identify products that are similar in style to a particular item in which a customer has expressed interest, with personalized recommendations that match the customer’s preferences.Retailers implementing VSS require a high-performing database to store and query vectors, ensuring instant results for customers.To support inventory visibility and AI-enhanced customer experiences like VSS, an inventory database must be highly performant and flexible. It needs scale to deliver fast and accurate inventory information across multiple locations. With a real time inventory database, retailers ensure consistency across all channels, satisfying demanding customers.Retailers rely on Redis Enterprise to deliver the data required for engaging customer experiences and responsive, resilient applications. With a single investment, retailers can use Redis to solve multiple retail scenarios.For more information about real-time inventory best practices, read our solution brief, Real-Time Inventory With Redis Enterprise. We review the ways that Redis Enterprise customers increase inventory visibility and deliver AI-enhanced, personalized customer experiences."
499,https://redis.com/blog/effective-leader-despite-economic-downturns/,Become an Effective IT Leader Despite Economic Downturns,"June 15, 2023",Redis,"This isn’t the first time the tech industry has encountered hard times. Listen to the advice from IT execs who have been there, done that, and developed practical ways to survive … or even thrive.Articles with advice about how to cut IT costs in down business cycles are common enough to have become their own literary subgenre. The answers are usually the same: examine cloud costs, scrutinize software licenses, automate processes, cut wasteful projects, reduce staff, hire more consultants, hire fewer consultants, and vacuum up spare change in the couches.None of that advice is wrong, and it’s all useful up to a point. However, it doesn’t really get to the nub of the problem. You should never not be looking for places to economize. You shouldn’t waste money just because your CFO is feeling flush this year and is an easy touch.Here’s the key: If IT is a strategic asset (which we know it is), IT leadership needs to be relentlessly strategic too. That means thinking like a leader whether times are good or bad. After all, if you can’t defend your budget and your priorities during times of cutbacks, whatever were you thinking during the good times?It’s harder to do it than to say it. But experienced IT execs know lessons can be learned from every down cycle. Here are some tips that your peers—who have been around this particular block a few times—have found useful.Is this a downturn? Who knows?Gartner is calling our present situation economic turbulence, rather than using more ominous terms. However, even if the analyst firm predicts that worldwide IT spending will grow 5.5% in 2023, other industries are not on steady ground. According to a February 2023 report from the National Association of Business Economics (NABE), 58% of economic forecasters say there’s more than a 50% chance of a downturn in the next 12 months.When forecasters are that unsure whether it’s going to rain, listen to what your mother said: Bring an umbrella.Although IT’s job is to support corporate priorities, the best IT execs don’t necessarily follow those priorities in lockstep. Instead, they find strategic ways to address problems that top executives might not know they have.When the hammer comes down, be prepared to show how you are supporting core priorities or moving the company forward.“I think that’s the job of every CIO: to say, ‘How can I use whatever funds I get to make the organization better?’” says Jonathan Feldman, CIO of Wake County, North Carolina. “Sometimes it’s making the organization better, but then you have to kind of connect the dots for people. Does endpoint protection make the organization better? No. But it sure does keep the hospitals running. So there’s value there.”“Keep the focus on long-term objectives, especially in times of economic hardship,” says Ben Richardson, director of the UK-based software and management training firm Acuity Training. “I’ve seen IT leaders take advantage of downtime to create a more agile IT structure that can respond quickly and effectively to changing market conditions.”“It is important to have a clear vision and strategy,” says Kelvin Wira, founder of the Singapore-based animation agency Superpixel. “Take the initiative to identify potential challenges and opportunities and develop contingency plans and solutions. This may involve re-prioritizing projects, optimizing budgets, or finding new revenue streams. The business landscape can change rapidly during tough times, so IT leaders must be agile and adaptable.”That’s just one piece of advice from experienced tech leaders who have been through previous economic downturns. To learn more, read our e-book, Adopting an IT Leadership Mindset in Tough Times, where the experts share advice that can guide you through this critical time. Download it now."
500,https://redis.com/blog/multi-tenancy-redis-enterprise/,Multi-Tenancy in Redis Enterprise,"June 21, 2018",Roshan Kumar,"Multi-tenancy refers to an architecture where a single software instance serves multiple users or ‘tenants’. Each tenant’s data is isolated and remains invisible to other tenants. In the context of cloud services, multi-tenancy means that a single server or instance can handle the needs of multiple tenants, with each tenant’s data isolated from others.This architecture’s primary importance lies in its efficiency and cost-effectiveness. It allows for maximum utilization of resources, eliminating the need to set up new physical infrastructure for each tenant. This leads to a significant reduction in costs and makes scaling easier, contributing to overall business agility.Multi-tenant software is designed to host multiple tenants sharing the same infrastructure. Amazon Web Services, Microsoft Azure, SalesForce, etc., are popular examples of multi-tenant architecture. Redis Enterprise Cloud is a multi-tenant service with over 500,000 Redis databases sharing the Redis Cloud platform, serving different tenants. However, it’s worth noting that single tenancy is also an option for those needing more customized environments.Redis Enterprise software allows you to create a multi-tenant environment on-premises or in any infrastructure you control, which is particularly beneficial in a cloud computing setting. But why would you need such an environment?A multi-tenant environment can save you time and money if your organization develops its own in-house applications and microservices. A multi-tenant architecture eliminates the setting up of new physical infrastructure for your software and installing new supporting software (a database, for example) in your development, testing, and production environment. It enables you to execute multiple development efforts and testing cycles in parallel with little effort. This can be particularly advantageous for the current tenant user. However, if a specific customer has requirements for data isolation or unique resource requirements, a single-tenant approach may be more suitable. This is beneficial if a single instance of a particular environment is needed for targeted deployment or analytics.A multi-tenant architecture and a multi-instance architecture have notable differences. In the latter, you install a new software instance for each tenant. The picture below illustrates an example of multi-instance architecture for Redis. In this scenario, you spawn a new Redis instance for every tenant, accommodating the need for tenant data segregation. As the number of your tenants grows, the complexity of deploying, monitoring, maintaining, and upgrading multiple software instances grows exponentially.In this case, you deploy Redis as a container or a virtual appliance and allow the underlying management solution to spawn a new Redis instance. Multi-tenancy is achieved at the server/infrastructure layer, maintaining tenant isolation. In many ways, this technique of achieving multi-tenancy is not very different from the multi-instance setup. While the management layer reduces the complexity of provisioning and starting a new Redis service, you still have the same number of Redis services to monitor and manage. Typically, service providers like Redis-as-a-Service vendors such as Elasticache, Azure Redis, Google Redis, and Heroku Redis follow this method. These solutions are priced per Redis instance. The effects of scale and scope economies benefit these service providers more than you.Redis Enterprise offers software multi-tenancy in which a single deployment of Redis Enterprise Software (often deployed as a cluster of nodes) serves hundreds of tenants. Each tenant has its own Redis database endpoint completely isolated from the other Redis databases, providing an efficient multi-tenant database setup.When you deploy Redis Enterprise in your data center, private cloud, or virtual private cloud, you benefit from the scope economies of the multi-tenant architecture. With a single Redis Enterprise cluster of a few nodes, you can support your development and testing efforts and then take it to production, accommodating different requirements of the different tenants.Redis Enterprise architecture implements multiple layers of abstraction to deliver multi-tenancy, high availability, linear scaling, high throughput, etc. Redis Enterprise architecture is comprised of the following components:You may deploy multiple databases in a Redis Enterprise cluster; you are only limited by the total memory size. Every database endpoint has an FQDN. The zero-latency proxies on all of the nodes in a cluster can redirect client requests to the appropriate primary (master) database.Redis Enterprise’s multi-tenancy solves many problems:Redis Enterprise is a market-proven, multi-tenant solution. Redis Cloud is indeed Redis Enterprise’s multi-tenant service. Built-in controls within Redis Cloud ensure that all databases’ throughput and latency requirements are being met, while being shielded from “noisy neighbors” and while staying highly available in scaled out, distributed environments.Redis runs more than 500,000 database endpoints on all the popular public cloud platforms: AWS, Azure, GCP and SoftLayer. Redis Enterprise Software is deployed as a multi-tenant software in over 300 enterprises. Read our documentation to learn how to create and maintain databases in a multi-tenant environment.While multi-tenancy offers numerous advantages, it also comes with potential challenges, like data privacy, noise from other tenants, and resource allocation. Redis Enterprise has developed features to address these issues.Data isolation is meticulously maintained in Redis Enterprise’s multi-tenant environment, ensuring that each tenant’s data remains invisible to others. Redis Enterprise also has mechanisms to handle “noisy neighbors”, ensuring that the activity of one tenant does not negatively impact others. The resource allocation is carefully controlled to maximize infrastructure utilization and to avoid any one tenant monopolizing shared resources.Redis Enterprise has been successfully deployed in numerous organizations. For instance, a large e-commerce company might use multi-tenancy in Redis Enterprise to handle data for different departments, such as sales, marketing, and inventory, each acting as a different tenant.Similarly, a software-as-a-service (SaaS) provider might deploy Redis Enterprise to manage data for different clients. Each client would be a different tenant, with their data isolated from others. In such scenarios, Redis Enterprise’s multi-tenancy helps maintain data privacy while ensuring efficient resource utilization.Security is a paramount concern in any multi-tenant environment. Redis Enterprise is designed with several security measures in place. Data isolation between tenants is strictly enforced, ensuring that each tenant’s data remains invisible and inaccessible to others.Additionally, Redis Enterprise deploys security measures to protect against potential breaches. This includes strong access controls, regular security audits, and the latest encryption standards to secure data both in transit and at rest. This provides an extra layer of assurance for businesses and helps maintain trust in the multi-tenant setup."
501,https://redis.com/blog/install-redis-windows-11/,How to Install and Use Redis on Windows 11,"June 21, 2023",Chris Miller,"Learn how to control a WSL installation of the Redis database from the Windows command line.If you like using Redis for website caching and you write and test code locally from Windows, you’ll want to figure out how to run a local instance of the Redis database.You have a few options. You can run it from another computer that’s running MacOS or something vaguely Linux-like. You can run it from a Docker container under Windows. Or you can run it directly from Windows SubSystem (WSL) for Linux.The process is what I figured out on my own, so accept this tutorial as “one techie’s experience.” There likely are other ways to accomplish this, but this works for me.Windows (Windows 10, Windows 11, Windows Server 2019) has a compatibility layer that lets you run Linux binary executables. The current version is WSL 2, but I refer to it as WSL.If you don’t have WSL installed already, run the following command from an elevated shell (as administrator):This installs the bits that you need and then asks you to reboot your machine. (For more information about installing and configuring WSL, Microsoft has good documentation.)Essentially, once you have WSL in place, getting Redis working is a normal Ubuntu deployment.Ubuntu is the default Linux distribution for WSL. After you reboot, you may see a Ubuntu shell for a while as Windows does its installation. It churns for a little while, and then asks you for a username and password. That password will be your sudo (user root) password.Once that is done, you should see something like this.The next thing to do is to install the Redis database. Before we can install Redis via apt-get we need to update apt-get and remove some of the new install shininess.After that is finished, you can install the Redis database:There is some churn and finally, it is installed.Redis does not run out of the box. For my own application testing, however, I need to test when Redis isn’t running. As a result, my personal preference is to only run Redis when I’m actively using it for development. So I Iike to start and stop it from the command line. Here are the commands that you need to know:I start up redis with the service start command, and it comes back with a message that redis is starting. And you can use the service status command to verify that it’s running. The acid test is to connect to Redis and see if it’s working. You can use the redis-cli tool to set and get a cache value.You should see something like this:So now the Redis database is running. If you close your shell and open up a new one, Redis will still be running. If you restart WSL or Windows, then Redis won’t be running.If it makes sense for your needs, you can use systemctl to enable it to start at boot.You can always pop open an Ubuntu shell and start Redis, but you also can do it from a Windows command line:From the screenshot, you can see that WSL passed along the sudo service command to Ubuntu. Because I used sudo, I was prompted for the root password, and it returned the same message that I would see from the Ubuntu shell.Since I only have Ubuntu installed, that was the default Linux that received the command. If you have multiple distributions installed, you would use wsl -d DistributionName. You can get the names of the installed distributions with the wsl -l command. Unlike from the Linux shell, each time I invoke sudo, I’m prompted for the password. In the Linux shell, you are prompted only the first time you call sudo in a terminal session.The requirement to type in the root password over and over again can be tedious on a development system. There is a way around that annoyance. You can add a file to the /etc/sudoers.d folder in the Linux distribution and remove the root password requirement for the redis-server service. From the Linux shell, do the following:What did we just accomplish? With this set of commands, we make /etc/sudoers.d the current folder. You need root access to work with this folder; use sudo sh to gain root access.  The echo line basically says that you can invoke sudo with no password for redis-server and it writes that setting to a file named allowed-services. The file name allowed-services is arbitrary; I picked it because it made sense to me. A file in the sudoers.d folder named README, explains the file name restrictions.The chmod 0440 command sets the permissions to read-only for the root account and is required for sudoers.d. This allows us to remove the sudo password requirement for redis-server and only for redis-server.Now we can go back to the Windows shell and run the wsl commands without being prompted for a password.You can even run the redis-cli tool from PowerShell:The Windows Subsystem for Linux is one of the hidden gems for developers. And this gem sparkles.Bonus round! Because I’m lazy, I created shortcuts in my PowerShell profile. I added the following functions:And now I can just check the redis-server status via redstat:Another bonus round! If you want Redis to startup when Windows boots up, it is a couple of extra steps. You just need to create a batch file that starts up Redis.In your Windows startup folder, create a batch file. Press Win+R and type shell:startup to l open up an instance of Windows Explorer in the user startup folder. Or use the command line with the following to place you into the same folder.Create a batch file in that folder. I used start redis.cmd, but any file name that the OS recognizes as a batch file works. In that file, add the following line:The next time you boot the computer and then log in, Redis will start. Because the commands that are in the startup folder tend to get run later in the boot process, they may not be available for a minute after you log in to Windows.Now that you have Redis installed and running, it’s time to learn how to use it. The Getting Started guide should be your first step.A previous version of this article appeared on Chris Miller’s blog."
502,https://redis.com/blog/seamless-data-migration-redis-aws/,Frictionless Data Migration With Redis Enterprise Cloud in AWS,"June 27, 2023",Redis,"Moving to the cloud can seem like a huge, complicated undertaking that can disrupt your organization. It doesn’t have to be that difficult, though, if you work with the right tools and partners. If you remove the friction, migration can be downright easy.It’s time. Maybe your hardware is nearing end of life or you’re dealing with a lack of redundancy. Perhaps you need to take your applications to a global audience or make them automatically scale up or down in response to demand. Or maybe you just want to lower your total cost of ownership. Whatever the reason, you’ve realized it’s time to migrate your company’s data to the cloud.Now you need to figure out how to make that transition happen so that your company gets the maximum benefit – and accomplishes that without disrupting users. You want a smooth, painless deployment, resulting in a safe, reliable system that fits your production environment and handles your workload.When it comes to getting the performance, stability, and ease-of-deployment you need, migrating to Redis Enterprise Cloud on Amazon Web Services (AWS) should be at the top of your consideration list. Here is a closer look at how you can have a frictionless experience migrating your data to the cloud and check all the boxes on your wishlist at the same time.We built Redis Enterprise on AWS with developers and operators in mind, so that you can use Redis for more than caching. You can bring together all the systems you use to manage and deliver data and deploy applications using a single platform.That translates to a data stack that delivers the highest level of availability, low latency, and single-digit-seconds failover. You also get all the cost savings, scalability, and other benefits that come with moving to the cloud, including:When you’re ready to make the move to the cloud, you want  to get the data where it needs to be without delay. One benefit of choosing Redis Enterprise Cloud on AWS is ease of migration.Both Redis and AWS have extensive experience helping developers move all kinds of workloads. Using a concrete methodology that simplifies the entire process and ensures maximum uptime, AWS offers automated tools and services that enable you to transition data quickly and securely, whether you have enough data for tens of thousands of servers or you are running a single application.So how should you get ready to make a successful and painless transition to the cloud? We and our partners at AWS recommend these three steps.Focus on developing a three-phase plan that consists of:As you use the cloud, you’ll develop an operating model that works best for your applications, as well as the tools and services that match your organization’s needs. This is where AWS managed services can help with out-of-the-box cloud operating models and publicly available service management framework. Plus, the  partner service program lets you get certifications, third-party audits, and a full lifecycle management service.You don’t have to do it alone. AWS and Redis are here to help make migration as easy as possible. It’s not just about completing a successful migration, or also being there with you as a customer to guide you along the way to enhance and strengthen and optimize your methodology. From the migration acceleration and workload migration programs to automations, tools, training, and support, we ensure you have everything you need to seamlessly migrate your data to the cloud.While the prospect of moving from the systems you know into the cloud can be a daunting one, the end result is well worth going through the process. Migration doesn’t have to be complex. When you have the right tools, the right services, and the right people helping you, you set yourself up for a successful transition.To get a more in-depth look at how easy migrating to the cloud can be, watch our on-demand webinar, Enabling Frictionless Data Migrations to Redis Enterprise Cloud in AWS."
503,https://redis.com/blog/lablab-ai-hackathon-review/,3 Hackathon Winners Show What Generative AI Can Accomplish,"June 28, 2023",Tyler Hutcherson,"Redis has co-sponsored several AI hackathons. These three winners – from Memoiz, RedAGPT, and SmartHealth – may inspire you to add AI features to your own applications.AI has taken significant leaps forward, escaping from university laboratories and corporate research departments. In recent months, the rate of innovation has been staggering. Nvidia CEO, Jensen Huang, called it the ”iPhone moment” of AI.Whatever your opinion of its influence, there’s no denying that this pioneering technology is pushing boundaries and springing surprises. With the attention and engagement of developers worldwide, an array of new applications, solutions, and even businesses have been launched.Over the past three months, Redis and LabLab AI collaborated to fuel an ecosystem of AI-driven applications. This initiative integrated technologies from organizations including OpenAI, Cohere, LangChain, LlamaIndex, Vercel, and, naturally, Redis. During the events, LabLab’s platform fostered discovery, collaboration, and innovation. Redis was featured as a “side quest” technology, providing an extra challenge with additional prize opportunities for developers who integrated Redis into their hackathon applications.Ultimately, 47 high-quality, Redis-powered submissions emerged from the seven AI hackathons. Given that Redis was not a mandatory requirement for any of these events, the sheer amount and caliber of the submissions speaks volumes. We’ve seen and been a part of countless hackathons over the years, but the blend of groundbreaking technology and applicable business ideas from this event was truly remarkable.Below, we examine the top three projects: Memoiz, RedAGPT, and SmartHealth. We distill the core problems the projects tackled and their creative solutions for the broader community’s benefit.In the wake of large language models (LLMs), we’ve seen a surge of applications that encourage users to chat with their notes, capitalizing on LLMs’ language comprehension and language generation capabilities. While the concept isn’t necessarily groundbreaking, its application to specialized use cases, like the one pursued by Memoiz, is potentially transformative.Memoiz is developing a platform for creating, storing, and interacting with personal diary content. It’s akin to having a personal librarian for your memories, thoughts, and moods. You write notes, recall memories, track your daily moods, and explore historical shifts over time. The novelty lies in the ability to “chat” with your personalized librarian as you access past reflections.Our Redis team sees huge potential in an application like Memoiz to shift the way people, young and old, maintain thoughtful access to memories, plans, and personal experiences.When the Redis judges (Brian Sam-Bodden, Paul Ford, Taimur Rashid, Sam Partee, and I) spoke with the Memoiz team, Thanasan Kumdee and Nutchanon Taechasuk, the two Japanese university students’ joy, humility, and honor were palpable. Now backed by the NewNative Slingshot AI Accelerator program, Kumdee’s and Taechasuk’s enthusiasm is obvious. The developers’ plans for the project include integrations with other note-taking applications, such as Notion or Obsidian, for broader access. They also intend to work on improving temporal understanding and querying of the underlying memories.Under the hood, Memoiz uses Redis Cloud for storing and searching through vector embeddings that symbolize the memories and queries. Married with a Cohere LLM, this enables users to delve into their past in a conversational way. The Redis-Cohere combination, an increasingly popular pattern in the generative AI boom, offers superior speed and semantic accuracy, which enhances Memoiz’s end-user experience.The team is ready to build towards the future and has our full support to reach its goals.The RedAGPT team’s AI-powered cybersecurity tool earned second place in the hackathon series. Designed to probe network vulnerabilities in residential and commercial environments, RedAGPT uses AutoGPT, Redis, and Langchain for efficient and effective security testing. Prioritizing rapid engineering and ease of integration, this toolkit perfectly complements security-focused Linux environments, making it an indispensable asset for security professionals.At its core, RedAGPT conducts a series of tests to uncover network and system vulnerabilities, and it leverages Generative AI, specifically the GPT-3 LLM via the Langchain library. The tool detects weaknesses and also quantifies their severity, along with providing actionable recommendations for remediation. The result? A comprehensive security report that transforms raw data into meaningful insights. RedAGPT stands as a testament to the revolutionary application of AI in cybersecurity.SmartHealth is a personalized health assistant that delivers health advice tailored to unique needs. SmartHealth doesn’t just offer a symptom checker; it serves as a comprehensive health companion, sharing personalized tips that help users to take control of their health and well-being.We were impressed with the team behind SmartHealth: a talented group of computer science, medical, arts, and business professionals.The tech stack includes GPT-3, Redis as a vector database, Python, and React. The team assembled a robust database of health conditions, symptoms, causes, and treatments, which, when paired with GPT-3, helps users understand their health status without needing an in-person doctor visit.The team also went to great lengths to address security and privacy concerns — an area we initially flagged while reviewing its submission. However, we were impressed by the team’s extensive measures in this domain, reinforcing SmartHealth’s commitment to deliver a secure, effective, and personalized health platform.We believe that products like SmartHealth are only the beginning of the ways generative AI can be applied in the medical industry, but there’s a long road ahead.The consistent thread running through these projects is their use of generative AI technologies and Redis. While Redis’ involvement may be unsurprising to some, others might find it new, even unexpected.Redis has been stirring up the machine learning (ML) community with its commitment to real-time performance and versatile data structures. The objective is clear: to empower developers to focus on their application creation, minimizing fuss over the data layer. It also didn’t hurt that OpenAI announced to the world that ChatGPT relies on Redis to effectively scale its end-user chatbot user experience.With real-time interaction increasingly demanded in today’s apps, Redis emerges as a Swiss Army knife, solving an array of challenges. It can serve as a low-latency feature store for ML model serving, a vector database for LLM knowledge retrieval, or a caching layer for ML model inference or LLM prompts. In the whirlwind of AI innovation, Redis’ adaptability and extensibility is crucial.Choosing trustworthy tools from burgeoning new technologies and SaaS products can be overwhelming. Redis aims to be a reliable, multi-purpose tool in this intricate ecosystem.Now, with a suite of fresh projects and ideas leveraging these capabilities, it’s your turn. Whether you’re an open-source developer, a startup, or an enterprise, there’s something for you in the Redis universe. Start exploring our curated list of AI resources, join our developer community on Discord, or get started today in Redis Cloud for free."
504,https://redis.com/blog/cloud-migration-strategy-misconceptions/,Cloud Migration Strategy Misconceptions,"July 3, 2023",Redis,"We bust persistent myths about migrating your workloads to a large cloud provider like AWS.You’re committed to a cloud migration. Great. But you need to get the data out of your existing system and into the new one.No problem, you think. You know just the right way to go about it!Or do you? Several experienced tech experts suggest that plenty of people make wrong assumptions that cost time and money while causing unnecessary frustration. For something that no longer seems novel, a remarkable number of misconceptions about migrating to the cloud persist. That’s true even when the issue is the practical matter of the migration process —the data transfer process, as we discuss here—rather than overall misconceptions about operating in the cloud in general.We explore what causes people to get these wrong ideas, explain what the reality is, and offer resources to help you learn more.One common cloud migration misconception is that you should always use a “lift and shift” approach to get your workloads into the cloud as quickly as possible. “Lift and shift” means porting applications and data as is from an on-prem environment to a cloud vendor infrastructure without rearchitecting them for the cloud. Just get it moved over, and think about redesigning later!Many companies employ this strategy with the very best of intentions. They start with a lift and shift to get everything into the cloud as quickly as possible, and they have grand plans to rewrite everything to be cloud-native later on. Spoiler alert: the company often never gets around to doing that.There usually are valid reasons for starting a migration this way. It seems the fastest way to get your workload into the cloud—like throwing all your belongings into boxes and organizing everything once you move into the new house. Sometimes there’s an urgent need to get workloads out of physical on-prem servers and into the cloud, and those reasons are justified, such as an office lease termination, a company acquisition, or fears about aging equipment that is at risk of failing (“We need to get our critical applications off this sinking ship before it’s too late!”).THE REALITY: Sometimes a “lift and shift” approach is the right one. But it should not be your default choice for a cloud migration strategy.One example of the need to think things through is when the legacy system technology architecture is outdated and there are no equal technology components in the new cloud environment. In such a scenario, you don’t really have a choice. You can’t lift and shift because there’s no parallel system to which to “shift” it.Sometimes the decision to migrate is motivated by the fact that your existing licensing costs are too high, and you’ve found an alternative technology in the cloud. But in such a scenario, it’s much better to retire the old system. Take the time to refactor and rearchitect the application from scratch, remodel the data, and run that new cloud native version in the cloud. Because while you can pitch all your belongings into random boxes and sort it out later in the new house, it’s silly to do so if you can go about the process in a more organized manner. If nothing else, it means you have a better chance of finding the coffee maker on the first morning.The misconception is that you should always start with a lift and shift. The problem is with the “always.” Sometimes it actually is the right way to get started. If your strategy when developing your on-prem applications was always to be cloud first—for example, you use containerization technologies in your on-prem environment—then it’s very easy to lift and shift workloads to the cloud because they will run there the same way as they do on prem.So if you have been employing virtualization techniques right from the beginning in your on-prem applications, you are already cloud ready; a lift and shift makes sense. (AWS offers helpful automation tools such as AWS Application Migration Service to help you get started.)Many companies believe that a cloud migration can be handled by a small team of in-house generalists, or that it’s a good idea to hire consultants or contractors to handle the migration and then transfer maintenance to an internal team after it’s complete. A related misconception is that you will save on staff costs after migrating to the cloud.But none of this is necessarily true.THE REALITY: Migrating to the cloud is a complex process that calls for subject matter experts with deep cloud technology expertise to manage the project. A successful migration requires an experienced team. Ideally, those individuals include a migration solutions architect, data architect, cloud solutions architect, enterprise architect, and IT or DevOps engineer. Those are specialized skill sets, and while one person can wear many hats, the roles are not interchangeable.Nor is it true that these roles are superfluous once the migration is done. A cloud migration is rarely a one-time event, especially for large companies with numerous legacy applications to bring into the cloud over time. Plus, data residency requirements often need some upstream or downstream systems to remain on premises, and you must maintain a bi-directional path between the cloud environment and the on-prem one. In such a scenario, you need to retain those experienced cloud experts to maintain these complex systems.Going the consultant/contractor route comes with its own set of problems. Often the handoff to your internal team once the project is finished is rushed and abrupt, and your existing staff doesn’t understand the intricacies of a system that they were not involved in building. Nor are consultants always focused on building a system that is maintainable, so it’s out of date in a few months after they have turned in their deliverable.All of these factors are why it’s important to make sure you have the right team in place to both handle the migration and maintain the system afterward.It’s natural to think in terms of starting small when a company is beginning to migrate its data and applications to the cloud. It’s common to begin by migrating the least important workloads with the fewest dependencies so the team can learn the process and gain confidence in the migration’s technical demands. In fact, this is a recommended approach, and AWS offers advice on how to prioritize applications for initial migration, including choosing low-risk, low-complexity workloads to reduce risk at the outset.But just choosing the “least important” app is not necessarily the right approach.THE REALITY: You should conduct a cloud readiness assessment to prioritize your workloads. Make this a key part of your cloud migration plan.Which workloads should you migrate first? Ideally, choose a workload that has maximum business impact with minimum risks involved. Keep the overall process iterative. Migrate in small increments, with a crawl-walk-run approach as you learn the process and gain confidence.But while you’re learning, you also want to deliver value. No company wants to invest in the cloud without seeing some business value. Part of the workload assessment should include “What’ll impress the boss?” even if you don’t word it that way on the report.A cloud migration is also probably not the best time to “learn on the job.” It’s a complex process that should be managed by a team of people who have cloud expertise. Such professionals should know how to best prioritize workloads for initial migration, ensuring that the cloud migration initiative brings value to the business quickly.It’s easy to think of on-prem legacy systems as antiquated and outdated, while the cloud has a reputation as cutting-edge, advanced technology. So techies can assume that once they migrate their workloads to the cloud, they will have similar or better SLAs right out of the gate.If you drive an old clunker for years and then buy a new sports car, it makes sense to expect a much faster ride the minute you drive your new car off the lot. But when it comes to a cloud migration, that’s not always the case.THE REALITY: When you migrate your existing infrastructure and system architecture to the cloud, it won’t necessarily operate the same way. Nor will it necessarily be faster or better immediately. Even though you may get a better ride quality in your new sports car on day one, it can come at the cost of other things, such as fuel efficiency and familiarity.Improving the SLAs is a laudable goal. But, as with any other element in migrating data to the cloud, it takes planning.First, define your existing system’s SLAs and its configurations. Then run a separate benchmark in the cloud to determine how those SLAs are affected. Is latency increased because of additional hops in the system architecture, or is it much faster? Define those SLAs in the new cloud environment and then choose your cloud resources accordingly, depending upon the system requirements. Then tweak the configurations to achieve those required SLAs.This is work that is normally done by the cloud architect, data architect, and migration solutions architect on your migration team. Only after you have done all this careful configuration should you conduct the actual migration.Another misconception is that, if you bring all of your architecture and technology stack over to the cloud, you no longer need complicated replication and failover plans to protect applications. The thinking is that once your workloads are in the cloud, they are “safe,” like a file that was backed up into cloud storage.This idea might come from the abstraction of the cloud as a non-physical space, as opposed to the very physical presence of on-prem servers, which are clearly vulnerable to damage from natural disasters and other disruptive events. But it’s a dangerous idea.THE REALITY: It’s a misconception that if you simply bring your applications, architecture, and full technology stack to the cloud, then replication, failover, and high availability is all ensured. It’s not. The truth is, it depends on how you configure your cloud instance.When you bring your workloads to the cloud, you must make a choice whether or not to run them across multiple regions. The entire cloud and global infrastructure is divided into regions, and within each region there are data centers, also known as availability zones (AZs). For example, the AWS Cloud currently spans 99 AZs within 31 geographic regions around the world, and Amazon has plans to expand that. You can think of these AZs as “data centers,” each of which is at least 150 miles away from one another. You could design and deploy an application in the cloud and bring it to only one region; if there’s a natural disaster such as an earthquake and the system goes down, then so does your entire application. But if you choose a multi-region deployment, your application will have failover protection. So it really comes down to how you design your deployment.If you are currently running your system on prem, you always want to have multiple copies of your data and multiple server instances running so that if one fails, your data is backed up and you are protected. But now your organization has made the decision to migrate to the cloud. You could just move the bare bones, the one single copy of your application first to get it up and running in the cloud, see how it performs, and go from there. Or you can bring the entire complexity over with it—the replication failover, the entire technology stack. But if you do that, you bring more complexity to the cloud without even knowing how it’s going to run there.Don’t further complicate your migration project. Migrate the application over as is, without worrying about all bells and whistles. Bring the simplest form of your application to the cloud, see how that goes. After the bare bones functionality is working in the cloud—and you can demonstrate its value to upper management—add the high availability aspect, the failover aspect, the replication aspectA well-executed cloud migration plan requires careful planning, expert guidance, and a phased approach that prioritizes workloads effectively. Embrace the opportunities and navigate the challenges of cloud adoption wisely, ensuring a successful and efficient migration process for your applications and data.For detailed, step-by-step instructions on how to get your cloud migration off the ground, read this helpful guide, Migrate Redis Workloads to Redis Enterprise Cloud on AWS.Many thanks to Redis Senior Cloud Solutions Architect Srinivas Pendyala for his insight and help with this post."
505,https://redis.com/blog/redisgraph-eol/,RedisGraph End-of-Life Announcement,"July 5, 2023",Lior Kogan and Pieter Cailliau,"Redis is phasing out RedisGraph. This blog post explains the motivation behind this decision and the implications for existing customers and community members.We introduced RedisGraph, a Redis module that extends Redis into a general-purpose property graph database, about five years ago. Since then, RedisGraph has gained tremendous interest and adoption, and it has been used widely in the commercial, academic, education, and other source-available communities.RedisGraph was part of our commercial offerings for Redis Enterprise Software and Redis Enterprise Cloud customers. It was also part of Redis Stack. In addition, RedisGraph is licensed to the community under the Redis Source Available License 2.0 (RSALv2) or the Server Side Public License v1 (SSPLv1).However, today we are announcing the end-of-life of RedisGraph. This blog post explains why we decided to take this action and the wind-down process.There are multiple reasons why we decided to end-of-life RedisGraph.Many analyst reports predicted that graph databases would grow exponentially. However, based on our experience, companies often need help to develop software based on graph databases. It requires a lot of new technical skills, such as graph data modeling, query composition, and query optimization. As with any technology, graph databases have their limitations and disadvantages.This learning curve is steep. Proof-of-concepts can take much longer than predicted and the success rate can be low relative to other database models. For customers and their development teams, this often means frustration. For database vendors like Redis, this means that the total pre-sales (as well as post-sales) investment is very high relative to other database models.On the other hand, we see impressive growth in the adoption of key Redis Enterprise features like Search and Query, JSON, and Vector. We also see how much faster prospects complete proof-of-concepts, usually with a high success rate. Most customers build scalable solutions with minimal support.This is how Redis is meant to be: simple and joyful. We have strong indications that we are building things that developers love. This is where we always aim to be.We also need to respond to feedback where we miss the mark. We don’t always get it right. We are eager to continue to fulfill the brand promise of Redis by reducing complexity.To be brutally honest, though our graph offering is unique and technically competitive in many aspects, the costs required to grow our technical, sales, and support capacity to address a larger segment of this market is significantly higher than we anticipated. Given the database market dynamics and the other opportunities in front of us, we decided to concentrate our efforts and put our attention on other segments.For any company and any product, end-of-life is a process.During the end-of-life process, there are two important steps:End of sale for new customers is effective immediately. Existing RedisGraph customers can renew their subscriptions with an expiration date no later than January 31, 2025. Annual subscriptions can be renewed up till January 1, 2024.End of support is scheduled for January 31, 2025.After January 31, 2025, RedisGraph commands will be disabled on Redis Enterprise Cloud.If you are an existing Redis Enterprise Cloud or Redis Enterprise Software customer, you can purchase RedisGraph with Redis Enterprise Software (on-premise or self-managed) annual subscriptions until January 31, 2024. After January 31, 2025, you can continue using RedisGraph on Redis Enterprise software indefinitely without needing to extend your RedisGraph subscription (though you still need to acquire a subscription for Redis Enterprise itself). No support will be provided for any Redis Enterprise database with RedisGraph after January 31, 2025.The RedisGraph GitHub repository is now in “maintenance mode.” We will not develop any new features.Until December 31, 2024, we will continue to monitor community-reported issues. We will consider releasing minor versions (patches) based on such reports. Any patches will be released on GitHub. Patch versions will remain aligned between GitHub and our commercial offerings.On February 1, 2025, we will tag the RedisGraph GitHub repository as “deprecated.” No new patches will be released after that date.Beginning with Redis Stack 7.2.x-y version onwards, we will no longer include graph capabilities (RedisGraph).Our Redis Enterprise Software customers should not upgrade to a Redis Enterprise version greater than 6.4 unless otherwise requested. RESP3 support for RedisGraph is not planned.Enterprise customers can contact their account manager directly or reach out to our support team via support@redis.com.Community members can contact us via Discord in the #redis-graph channel or via Redis forums.The RedisGraph team thanks both our customers and the community for its invaluable interest and feedback. It has been a challenging and fascinating journey, and we hope to serve our customers and community with many new and improved Redis capabilities in the upcoming years."
506,https://redis.com/blog/i-have-500-million-keys-but-whats-in-my-redis-db/,I Have 500 Million Keys But What’s In My Redis DB?!?,"June 2, 2014",Yoav Steinberg,"While you could consider them a “Rich People Problems,” big databases do present big challenges. Scaling and performance are perhaps the most popularly-debated of these, but even trivial operations can become insurmountable as the size of your database grows. Recently, while working on a memory fragmentation issue with one of our users’ databases, I was reminded of that fact.For my investigation of the fragmentation issue, I needed information about the database’s keys. Specifically, to understand what had led to the high fragmentation ratio that the database exhibited, I wanted to estimate the size of the keys’ values, their TTL and whether they were being used. The only problem was that that particular database had an excess of 500,000,000 keys. As such, iterating over all the keys to obtain the information I was looking for wasn’t a practical option. So, instead of using the brute force approach, I developed a little Python script that helped me quickly arrive at a good estimate of the data. Here’s a sample of the script’s output:Instead of crunching the entire mountain of data, my script basically uses a small (definable) number of random samples to generate the data I needed (i.e. average data sizes, TTLs and so forth). While the script’s results aren’t as accurate as a fetch-and-process-all maneuver, it gave me the information I was looking for. You can find the script’s source immediately below and I hope you’ll find it useful."
507,https://redis.com/blog/how-to-use-redis-at-least-x1000-more-efficiently/,How to Use Redis at Least x1000 More Efficiently,"May 23, 2014",Itamar Haber,"The recent Redis’ v2.8.9 was “the strangest beast” in its history of releases. Unlike previous versions, this release solely introduced new Redis functionality rather than defect fixes. As we are nearing the completion of 2.8.9’s rollout in our Redis Cloud service, I wanted to use this chance to briefly summarize these updates for our users.First is the addition of the new HyperLogLog data structure to Redis. HyperLogLog allows you to estimate the cardinality of a set or, put differently, get an approximate count of unique items. Even before HLL’s addition, you could use Redis for counting stuff but HLL’s raison d’être is to let you do that using a fixed amount of memory and constant complexity. Since there are no free lunches, the tradeoff is that HLL’s count has a standard error of up to 0.81%. If you want to learn more, check out this post in which antirez has provided an amazing overview of the HyperLogLog algorithm and some of the challenges in implementing it. Besides Phillipe Flajolet’s original paper, the web has several other resources that explain HLL, including this nifty visualization.Let’s see how HLL can be put to good use, for example by adding a sprinkle of real-time analytics to your application. Assume you want to keep track of how many unique users are using your app throughout the day, hour by hour. Before HyperLogLog, one simple way that you could do that in Redis was by storing the IDs of your users (e.g. their emails) in an hourly set, so for each request you’d do something like:SADD users:<date>:<hour> <email>This way, the result of SCARD for any such set would be the number of unique users for that hour. The problem with that approach is that it can consume a lot of memory. If your application is seeing 50K users per hour on average, and assuming each identifier is 25 bytes on average, the average size of any hourly set would be about 12MB. HLL lets you do the same thing using only 12KB, so you could say it is three orders of magnitude more efficient 😛When using HLL we’d count the users with:PFADD users:<date>:<hour> <email>And use PFCOUNT to obtain the hourly count. But what if you wanted (and you usually do) to have aggregates for different time periods, like 3h, 6h, 12h and 24h? With the SADD/SCARD approach, you’ll either need to dedicate a few more counter sets for each resolution or execute on-demand/periodic SUNIONs, costing RAM and CPU. You could, as shown above, gain considerably by using HLL but there’s actually another big saving with it. Since HLLs can be merged very efficiently, you don’t need to save a counter for each of time resolution and just PFMERGE the relevant counters to get the sums.Second, there’s lexicographical ordering of sorted sets. When the scores of the sorted sets’ members are identical, the ZRANGEBYLEX command will return a requested range of members using lexical sorting. Compared to the incumbent SORT command with the ALPHA option, the new lexical commands are not only more performant, but also allow the straightforward implementation of autocomplete-like functionality (although RAM-wise, such sorted sets are more expensive).Despite the ingenuity of Flajolet’s contraption and all the hard work that went into putting it into Redis, I prefer lexicographical ordering (and not only because of the way it rolls off your tongue). I have two reasons for that specific bias and both are entirely subjective. For starters,this and that thread from the Redis mailing list unfold the story behind the feature. Besides all the juicy technical details in these correspondences, they give a brilliant demonstration of how much teamwork goes into the making of an open source project — a shining example of Redis’ great community. On top of that, it’s just more practical and screams to be used.I mean, consider HLL – it lets you count (and I love to count!) but that’s basically it. Sure, you want to count lots of different things in the #IoT but that’s about as exciting as it gets. On the other hand, the ability to lexicographically rank members in a set opens up a world of possibilities, like so many other Redis commands. Here’s one neat use – imagine you have a set of email addresses and you need find all the addresses from a certain domain. By indexing (i.e. ZADDing) the reversed version of each address, this becomes just matter of a ZRANGEBYLEX. To index an email address you can use the following Lua:And the Lua script to search for addresses in a domain:It’s going to be very interesting to see more ways these new features will be put to use. To start using Redis v2.8.9 with Redis Cloud immediately, just create a new database. If you have an existing cloud database that’s using an older version and you want to upgrade it to the latest version, drop our support@redis.com a line. Questions? Feedback? Email or tweet me – I’m highly available 🙂"
508,https://redis.com/blog/turbo-boost-wordpress-with-a-secure-memcached-plugin/,Turbo Boost WordPress with a Secure Memcached Plugin,"July 23, 2014",Matan Kehat,"Recently we released the Memcached Cloud Plugin for WordPress, implementing the WordPress Object Cache. The plugin is based on the PECL Memcached extension (note the ending ‘d’), working against the libmemcached library, ensuring a better and efficient performance as well as advanced features on top of the core memcached functions, such as multi set and get methods, ‘by key’ functions and many more- all are documented on php.net. We basically extended the wordpress-memcached-backend, so credits and many thanks goes to Zack Tollman.As the PECL Memcached extension supports secured connection to Memcached servers using SASL authentication mechanism since version 2.2.0, working against a libmemcached built with SASL enabled, we wanted to provide a simple solution for WordPress users which their site is deployed in untrusted networks, and are requiring secured connection to their Memcached servers, or would like to exercise a bit more control over the clients connecting. Our Memcached Cloud service is of course allowing its users to configure their buckets with SASL effortlessly, providing a perfect and secured backend for the Memcached Cloud Plugin.If you went over the installation section of the plugin, as we encourage you to do, you probably noticed that installing the plugin involves building libmemcached with SASL enabled and then installing the PECL Memcached extension, ver. 2.2.0, against the previously installed libmemcached library.This process can be a bit non-trivial for the average user, and one solution can be taking advantage of the Heroku platform. Heroku’s PHP environment supports third-party extensions, that can be enabled through the composer.json file. Memcached; built against a version of libmemcached with SASL support, is supported as third-party extension. The Memcached Cloud Plugin natively supports Heroku’s Memcached Cloud configuration.Below we’ll demonstrate an easy and straightforward method of deploying a WordPress application to Heroku with Memcached Cloud Plugin for WordPress and Memcached Cloud add-on for Heroku, as a backend:Start by downloading WordPress:Next, download the Memcached Cloud WordPress plugin:Create the file wordpress/Procfile with the following contents:Next, create the file wordpress/composer.json with the following contents:Lastly, edit your wordpress/wp-config.php file and set your database’s parameters.Initialize your git repository:Next, create the Heroku app and add the Memcached Cloud add-on to it:Note: this would also be a good time to add the database add-on to your app.Finally, deploy the app:That’s all there is to it! Your WordPress is now deployed to Heroku and will use a secure connection to Memcached Cloud for that extra performance boost. I’d love to get your feedback on the plugin so please give your rating to it at the WordPress plugin page and discuss it at the plugin’s support forum. Happy blogging!"
509,https://redis.com/blog/top-redis-headaches-for-devops-client-buffers/,Top Redis Headaches for Devops – Client Buffers,"July 28, 2014",Yaron Dolev,"Despite the setbacks that have been the cause of quite a few headaches, solutions do exist, and may be even simpler than anticipated.This series of installments will highlight some of the most irritating issues that come up when using Redis, along with tips on how to solve them. They are based on our real-life experience of running thousands of Redis database instances.Our previous installments in this series had discussed Redis’ replication buffer and timeouts. In this post we’ll fill you in on yet another type of buffer that Redis maintains, the client buffer. In some cases, this bugger may prove to be the cause of many headaches when left untamed.You probably already know that Redis is an in-memory database, which means that all data is managed and served directly from RAM. This allows Redis to deliver unparalleled performance, serving tens and hundreds of thousands of requests at sub-millisecond latencies. RAM is by far the fastest means of storage that technology offers today – to get a sense of latency numbers, have a look at the following:Redis, by name and design, is a remote server and that means that clients (usually) connect to it over a network. That being the case, a client’s request will take significantly more time to return to the client than the actual fetching of data from RAM by Redis’ CPU. The direct implication of this order of magnitude difference is that Redis would have been tied up serving the request for the duration of that time, had it not been for client buffers.Client buffers make up a memory space that is allocated for serving client requests and every connection to Redis is allocated with its own buffer space. After processing a request, Redis copies the response data to the client buffer and proceeds to process subsequent requests, while the requesting client reads the data back over that connection at its own network-dictated pace. Redis’ client buffers are configured in the redis.conf file by the client-output-buffer-limit normal directive (you can obtain this setting in runtime with a config get client-output-buffer-limit). The default redis.conf file defines it as follows:These values represent a buffer’s soft limit, hard limit, and timeout in seconds, respectively (similar to the behavior of replication buffers). They serve as protection, where Redis will terminate the connection – without allowing the client to read the reply – when the buffer’s size reaches a) the soft limit and stays there until the timeout expires or b) the hard limit. Setting these limits to 0 means disabling that protection.However, unlike replication buffers, memory allocation for client buffers is taken from Redis’ data memory space. The total amount of memory that Redis can use is set by the maxmemory directive and, once reached, Redis will employ its configured eviction policy (defined by the maxmemory-policy directive). This effectively means that slow performing clients and/or a large number of concurrent connections may cause your Redis instance to evict keys prematurely or deny updates with an out of memory message (OOM) because its memory usage, primarily being the sum of the dataset’s size and client buffers, had reached the memory’s limit.Due to life’s relativity, a client doesn’t necessarily have to be slow to trigger this behavior. Because of the immense speed difference between accessing RAM and reading from the network, exhausting Redis’ memory with bloated client buffers is actually very easy to accomplish, even with top-performing clients and network links. Consider the (evil) KEYS command, for example, once issued, Redis will copy the entire keys’ namespace to the client buffer. If your database has a significant number of keys, this alone would be sufficient to trigger eviction.Warning: use KEYS with extreme caution and never on a production environment. Besides the possibility of triggering eviction as described above, by using it you risk blocking Redis for a significant period of time.KEYS is not the only command that can cause this scenario, however. Similarly, Redis’ SMEMBERS, HGETALL, LRANGE and ZRANGE (and associated commands) may have the same effect if your values (and ranges) are large enough or, since every connection requires a separate buffer, if you have multiple open connections.It is highly recommended, therefore, to refrain from using these commands irresponsibly. In their place the SCAN family of commands is preferred, that have been available since v2.8. These commands not only allow Redis to continue processing requests between subsequent SCAN calls, but also reduce the chance of exhausting the client buffers.Client buffers are an often overlooked aspect of Redis’ memory requirements and management. Their default setting of being disabled is quite risky, as it can quickly lead to running out of memory. By setting the buffers’ thresholds accordingly – taking the ’maxmemory’ setting into consideration, along with existing and predicted memory usage, and the application’s traffic patterns. Responsibly using the aforementioned commands can prevent unpleasant situations that will leave you with a throbbing headache. Stay tuned for our next installment in the Top Redis Headaches for Devops!"
510,https://redis.com/blog/pop-the-red-boxs-lid-redis-lua-debugger/,Pop the Red Box’s Lid: Redis Lua Debugger,"December 31, 2014",Itamar Haber,"Imagine a red box. You put something in the box and wait for something else to come out (or not). Whatever comes out (or not) of the box depends entirely on what you told the box you wanted it to do. The box originated from Terrah‘s moon so it only understands moon-language, a language that’s both foreign and at the same time similar to what you’re already used to. For it to do your bidding, you need to explain everything you want it to do in meticulous details, or else the box doesn’t work at all. When the box does work, what you get back isn’t always what you thought you’d get. The red box has a wry sense of humor and appears to thrive on your growing frustration as you try peeping inside its workings to understand what went wrong.This is how I sometimes felt when trying to write even the most trivial Redis Lua scripts. It’s not that the box is broken, not at all. The problem’s entirely my programmering skillz and the unique tendency I have of sprinkling bugs all over the place. I recently wrote about the 5 Methods For Tracing and Debugging Redis Lua Scripts, but all of these methods (excluding the last one) require tediously adding and removing debug printouts in your code. So I found a better way to do it and the result is our Redis Lua Debugger (or just “rld”), which boasts the following features:The first thing to know about the Redis Lua Debugger is that it is written in Lua and it runs in Redis. That means you don’t need anything besides the script and Redis to use rld. To use rld, you’ll first need to load it to your Redis by running it like any regular script – this will cause rld to burrow into your Redis’ Lua environment until the next server restart or a call to SCRIPT FLUSH.rld can use both Redis’ log file and Pub/Sub (methods 1 and 4 in the abovementioned post, respectively) for its output. To keep track with it just tail the log file and/or subscribe to the `rld` channel. Note that after you’ve loaded rld, it will stay dormant until explicitly activated by your Lua script and will remain active as long as the script runs or until rld is stopped.So, to actually use rld for debugging your script, you’ll need to switch it on by calling its `start` function – simply add the following line at the beginning of your Lua script:Now you can run your script. It will be executed normally (or rather abnormally since we are talking about debugging after all) , but at the same time rld will be tracking it and printing information about its execution. Read the output carefully until you’ve arrived at that Aha!/#facepalm moment.Here’s an example – consider the following Lua script called prog.lua:Now, load rld (if you haven’t already) and run the script – since I’m passing 42 as the argument, I expect it to be the right answer:Wowza! Wait! What just happened? Easy, just loog in your lok:Can you say Halle-Lua? Isn’t this really va-Lua-ble? rld can be downloaded from https://github.com/Redis/redis-lua-debugger. I hope you’ll find it useful at least as much as I had fun making it. Sure, there’s still a lot that can be done to improve and extend rld but I feel it’s already an MVP so I’m releasing it now. As for prog.lua… well, I’m still trying to remember what I forgot in order to fix it. If you spot the bug, please call tonumber: 555-1234.Questions? Feedback? Feel free to email me (itamar@redis.com) or tweet me (@itamarhaber) about anything and everything – I’m highly avai-Lau-ble 🙂"
511,https://redis.com/blog/the-7th-principle-of-redis-we-optimize-for-joy/,The 7th Principle of Redis: We Optimize for Joy,"February 2, 2015",Itamar Haber,"The 7th principle, as laid out in the Redis Manifesto, really aligns with my beliefs and views, and probably those of all engineers (regardless of experience). There’s little in this world as satisfying as that rush of excitement you get from devising a clever way to do something new and/or better. This joyous rush, in turn, can and often does lead to developing a serious addiction, as recognized by Donald Knuth (“premature optimization is the root of all evil”) and Randall Munroe (e.g. http://xkcd.com/1205/, http://xkcd.com/1445/ and http://xkcd.com/1319/) long ago.I found out, however, that by exercising common sense and cold logic every time I get an urge to embark on an optimization quest, I can keep my addiction satisfied and controlled. This post is about three such urges from the a past week.As an advocate for Open Source Software and a decent netizen at large, I strive to assist others in everything Redis-related. Sure, Redis can be used a dumb key-value store but it is so much more that it practically begs to be used intelligently.So it was during my daily patrolling of the internets that I came across this SO question and started to answer it. In my reply, I deliberately left out the details on how one would go about scanning a big list – the OP appeared to be a beginner and I didn’t think he’d grok it – but I still got that tingle of joy because I had helped someone do something better. Hopefully, I’d also encouraged him to learn and understand the commands he was using before diving into coding (and perhaps I also saved the world some CPU and network resources along the way).While that SO question by itself isn’t something to write home about — or blog about (although I just did) — it got me thinking about that missing LSCAN command. There’s SCAN for scanning the keyspace, HSCAN for Hashes, SSCAN for Sets and ZSCAN for Sorted Sets but nothing for Lists. Since Redis’ Lists (and Quicklists) are regular lists (or linked lists of ziplists), randomly accessing their elements is done in O(N) complexity. Having an efficient LSCAN command means implementing Redis Lists differently, and since there are no free lunches there is a price to pay. Lists are good (i.e. O(1)) for pops and pushes, and given principles #3 (“Fundamental data structures for a fundamental API”) and #5 (“We’re against complexity”) of the Manifesto, if you’re using a List and need to scan it from one end to another, then you probably shouldn’t have used a List in the first place.But, for the sake of argument and just for giggles, can you somehow do what LSCAN would do (had it been there in the first place)? Interesting question… well, sure you can. For starters, if you’re not worried about consistency due to concurrency, you can easily use a loop with RPOPLPUSH and keep a count to traverse the list more efficiently. Of course, if concurrency is a concern, you’ll probably need to find some way to duplicate the list and traverse the copy safely.So how does one go about duplicating a list with 10 million elements? Lets first create such a list (all testing done in a VM on a laptop so YMMV):A fast copy will need to be done on the server, so the naive way to copy the list would be to write a Lua script that will pop elements one by one and push them back to the original and the destination. I wrote such a script and tested it on that longish list…It took forever to run and yielded the expected “Lua slow script detected” message in the log:23 seconds is way too long, and while you could batch some of the calls to LPUSH, maybe LRANGE is a better approach:11 seconds is much better than 23, but how about SORTing it – would that be faster? Probably, lets check it out:An impressive improvement – only 2.248 seconds and about 10 times faster than popping and pushing, but can we do even better? Lemme check if Redis has any COPY, DUPLICATE or CLONE commands… Nope, the closest that there is is MIGRATE, but it only accepts a single key name. But wait! [lightbulb] What about DUMP and its complementary RESTORE… could it be?That’s a really nice trick – copying by dumping and restoring is much faster because you bypass all of the data structure’s management logic (sort of like good ol’ POKE in BASIC, and the furthest you can go until pointers to values are introduced to Redis :P). It is almost 20-times better than the naive approach and in theory it should work for any data type – Lists, Hashes, Sets and Sorted Sets. I’ll have to remember this little hack for when I need to copy values quickly.But hold on! There’s even something nastier I can think of – how about making Redis go faster than Redis? What if someone wrote a piece of code that could spew out Redis-compatible value serializations, like DUMP does? With that functionality, you could load data to Redis really really really fast. Short of trying to write directly to Redis process’ memory space, this approach could potentially outperform any conventional RESP-based client by moving some of Redis’ load to the client itself. Exciting!I know of at least one such code, so now I’m really tempted to have a look at DUMP‘s implementation [insert link] and see if I can rip it out and port it, lets say for a Hashmap. Then I could test loading JSONs by preprocessing and injecting their final serializations directly instead of using HMSET. The possibilities are really endless, but so is the amount of effort needed to make it happen. And even that works and is relatively bug-free (yeah, right), I’ll be relying on Redis’ internal private API, so the potential for breakage is huge. And those concerns aside, it just feels wrong. Perhaps a better direction is to embed Redis in the client app and implement Master<->Master replication to sync between the local and remote instances…So I stop here, leaving my train of thought still on its tracks, and the large part of my wild optimization fantasies unfulfilled. I told you that I can control my addiction to joy, common sense finally pulled the emergency brake and this is the proof. But nonetheless, the other day Salvatore told me (on a different matter altogether) that: “Breaking things is good, the quintessence of hacking :-)”, so perhaps I should…Questions? Feedback? Feel free to tweet or email me – I’m highly available 🙂"
512,https://redis.com/blog/enabling-secure-connections-redis-enterprise-cloud-python/,Enabling Secure Connections to Redis Enterprise Cloud in Python,"June 5, 2018",Tague Griffith,"Enabling encrypted connections on a server is a bit like weaving a magic spell: nothing works until you get the incantations exactly right, but once you do, everything clicks into place and you have established a secure connection to your server. Configuring encryption can get even more arcane when you also use cryptography to authenticate clients on connection.Enabling secure connections to a server should be a part of every developer’s repertoire, so in this post we will walk you through an easy three-step process to turn on, test and configure encrypted connections between Redis Enterprise Cloud and a Python client using SSL.If you’d like to play along at home, you’ll need to get the following tools – we’ll wait while you install them:Your Redis Enterprise Cloud subscription must have the SSL feature enabled to use SSL. If your account isn’t already enabled for SSL, you will need to contact the Redis support team to enable SSL for your subscription. You’ll find a link to contact the support team in the main menu of your account dashboard.The documentation for setting up SSL can be found in the Redis Enterprise Cloud Operations and Administration Guide, but in this post we’re going to walk you through all of the steps necessary to use SSL with Python clients. Steps one and two happen to apply to any language, and we’ll refer back to them in a future post about Java.SSL, or Secure Sockets Layer, is a standard protocol for securing connections between a client and a server. SSL uses encryption to ensure that any data transmitted between the client and server remains private. SSL also uses public-key cryptography to authenticate both the client and the server. Cryptographic authentication is optional, but is almost always required of the server. TLS (Transport Level Security) is the successor to the SSL protocol and serves the same function.The acronym “SSL” is used informally to refer to either SSL or TLS-secured connections. The documentation and the management UI for Redis Enterprise Cloud follow that convention. In this post we are going to follow the documentation convention and use “SSL” to refer to both SSL and TLS.As of June 2018, Redis Enterprise Cloud uses TLS version 1.2 to secure connections between the database and the server.Naturally, the first step in using SSL with Redis Enterprise Cloud is to enable SSL for our database. Start by logging in to your Redis Enterprise Cloud account and creating a new database you can experiment with. As you step through the database creation process, you will get to the Create Database page. Once you reach the Create Database page (you can also turn on SSL from the Edit Database page as well), find the Access Controls & Security group and enable SSL Client Authentication.Client Authentication (sometimes shortened to Client-AUTH) is an optional part of the SSL specification which requires the client, in addition to the server, to authenticate via public-key cryptography. Although it is an optional part of the TLS protocol, Redis Enterprise Cloud requires Client-AUTH to use SSL. Once SSL Client Authentication is enabled, your client software will need to authenticate with both a password and a public-key. In SSL, public-key information is exchanged via digital certificates.As part of enabling SSL Client Authentication, we can have Redis Enterprise Cloud generate a certificate for our client. Press the Generate Client Certificate button and the system will generate a certificate for your client. Pressing the button will also cause your browser to download a zip file (redis_credentials.zip) of credentials and will populate an PEM version of your client’s certificate in the “Client Certificate” field.You can think about the certificate created for your client as a digital ID that it can use to prove its identity. The certificate is digitally signed by the organization issuing the certificate to ensure that it hasn’t been modified.The redis_credentials.zip file contains a copy of all the files necessary to mutually authenticate the client and server. This archive includes the certificate generated for your client (redis_user.crt), the corresponding private key (redis_user_private.key), and a copy of the certificate for the Redis Certificate authority (redis_ca.pem). Special care needs to be taken with both the redis_credentials.zip file and the redis_user_private.key to ensure the security of your keys. All three of these files will need to be distributed out to every client that connects to this particular Redis database instance, so you will want to integrate them into the system you use to manage credentials for deployed software.Setting up an SSL connection for a new piece of software can often be a frustrating experience; nothing works until you get all the components in place, but then everything just works. It doesn’t help that SSL problems are, somewhat by nature, hard to debug. After all, the protocol is designed to prevent you from knowing what’s going across the wire! So prior to using SSL with our client code, we’re going to test that we can establish a secure connection to the database using the OpenSSL s_client command.Using the credentials from redis_credentials.zip that you downloaded in step one, run the following command in the directory where you placed the extracted files:Get the endpoint name for the host parameter and the endport port for the port parameter from the database you used in Step 1. Using this command, OpenSSL will attempt to establish a secure TLS connection to your Redis instance and print the certificate presented by the server. Running the command will produce a long sequence of output about various aspects of the TLS connection with a message at the end that looks like:If you see any other return code or message, the connection wasn’t able to establish securely.If you aren’t able to establish a secure connection, make sure you correctly enabled TLS Client Authentication on your database instance and that you have the appropriate credentials files downloaded for the instance that you are connecting to. Once you can successfully connect to your database using OpenSSL, then enable SSL for your client. Trust me—don’t skip this step!The final step is to modify your client code to establish SSL connections. We’re going to build a sample client in Python to establish a secure connection to our Redis Enterprise Cloud instance. Our sample client will connect to the database and run the Redis INFO command. The code for our client is here:Our client code is nearly identical to the version that doesn’t use SSL, we only need to add five additional arguments to our StrictRedis constructor: ssl, ssl_keyfile, ssl_certfile, ssl_cert_reqs and ssl_ca_certs. The first parameter, ssl, simply enables SSL on the connection. The remainder of the parameters are used to supply the credentials needed to mutually authenticate the client and the server. Be sure to note that you still have to supply the instance with a password, which can be found on the Edit Database screen.The redis-py client uses the Python 3 ssl package to provide an SSL wrapper for the standard socket connections to Redis. Under the hood, the parameters prefixed with “ssl” are keyword arguments that map to arguments of the ssl.wrap_socket method. Detailed documentation of the parameters can be found in the ssl package documentation.The credentials that the client needs to authenticate itself—its certificate and private key (which we downloaded in part one)—are provided to the SSL library using the ssl_keyfile and the ssl_certfile parameters. The Python SSL library works directly with the PEM file formats generated by Redis Enterprise Cloud.The ssl_cert_reqs parameter translates into a value for SSLContext.verify_mode and controls how the server must be authenticated. This parameter can take one of three different values: none, optional, or required. You should always run with ssl_cert_reqs set to required when working with Redis Enterprise Cloud. The SSL library, by default, uses the certificates shipped with the operating system (which includes all the major certificate authorities) to validate servers. Since Redis maintains its own certificate authority, you need to provide the SSL library with the certificate for Redis’ Certificate Authority (redis_ca.pem), so the SSL library can verify the server identity.Once the StrictRedis constructor is changed to set up SSL and load all of the credentials needed to verify the client and server, you can run the code and see how it connects to the database. As an experiment, try removing the ssl_keyfile and ssl_certfile parameters and see how the script fails to authenticate. After that, add back the ssl_keyfile and ssl_certfile parameters and remove the ssl_ca_cert parameter and see how the client responds this time.Configuring redis-py to use SSL is not particularly difficult, but it can be maddening the first time you walk through it. SSL is designed to keep your communications secure, so you may keep getting errors (and seemingly no progress) until everything is set up just right.Hopefully this post provides you with a gentle introduction to the components needed to set up a secure connection to Redis Enterprise Cloud and add SSL to your projects. In a follow up post, we’ll walk through setting up a secure connection between Redis Enterprise Cloud and a Java client.A final heads up: many organizations have specific policies around security and managing credentials like these, so please be sure to work with your operations or security team to ensure that you’re following all applicable policies."
513,https://redis.com/blog/use-redis-content-filtering/,How to Use Redis for Content Filtering,"January 3, 2019",Redis,"Recently, a colleague of mine had a request: how can I create a content filter in Redis? He wanted to filter incoming messages against a list of bad words. It’s a pretty common use case — any app that accepts user input probably wants to do at least a cursory scan for inappropriate words. My first thought was that he should use a Bloom filter, but I wondered if there were better options, so I wanted to test my assumption. I fetched a list of bad words from here and did some initial testing with the text of Ulysses by James Joyce (a big book in the public domain with a lot of interesting language!).For those who aren’t familiar, a Bloom filter is a probabilistic (p11c) data structure that can test for presence. This approach might seem intimidating at first, but it is really quite simple: you add items to a Bloom filter, then you check for existence in the Bloom filter. Bloom filters are probabilistic in that they can’t answer existence definitively — they either tell you an item is definitely not in the filter or probably in the filter. In other words, with Bloom filters false positives are possible but false negatives are not. The really cool thing about Bloom filters are that they use a fraction of the space (versus actually storing the items) and can determine probable existence with modest computational complexity. My wife, who is a literature professor, was very entertained that I picked Ulysses for this test, reminding me that the protagonist of the book is named Bloom. Fitting.Back to the problem at hand. What you want to do is take all the words in a given message and find the intersection with a list of known bad words. Then you identify the words which are problematic and either reject or perhaps hand screen them.To make things as easy to use as possible and avoid any network bottlenecks, I implemented all the solutions in Lua and ran them from within Redis. I used a Lua table with the keys as the inappropriate words and the values as “true” to prevent issuing unneeded Redis commands in all the scripts.A note on message handling: In the scripts, I used a very simplistic method, just splitting by space. Admittedly, this alone would not work in practice because you would need to handle punctuation, numbers, etc. In this case, it worked fine since we were treating the scripts the same way and weren’t trying to measure the speed of Lua string splitting.Aside from a Bloom filter, another way to approach this problem is to use Redis Sets. The first thing I did was add the bad words to a SET key, using the SADD command and over 1600 arguments to that single command. I took the bad word list and surrounded each word with double quotes, then prefixed SADD and the key, and finally saved it all to a text file (badwords-set.txt). I executed it with this redis-cli piping trick:$ cat ./badwords-set.txt | redis-cli -a YourRedisPasswordA simplistic approach would be to break up the message into words, and run SISMEMBER on each word in the message to identify the words. My initial thought was that the sheer number of commands would make this inefficient. Let’s look at how that’s implemented:All we’re doing here is splitting up the first argument (the message, in this case), then iterating over the words and running SISMEMBER for each one. If SISMEMBER returns a 1 then we know it’s a bad word and add it to a table which we return to Redis.Earlier, I talked about an intersection between the bad words and the words of the message. Let’s do that quite literally with SADD and SINTER. Again, this seems like a ton of commands, but here’s how it looks in Lua:This is the same split routine as before, but instead of issuing a SISMEMBER for every word, I added it to a temporary Redis set with SADD. Then I intersected this temporary set with our bad words using the SINTER command. Finally, I got rid of the temporary set with UNLINK and returned back the results of the SINTER. With this method, keep in mind that you’re writing to Redis. This has some implications to persistence and sharding, so the relative simplicity of the Lua script hides some operational complexity.For my second approach, I added all the bad words to a Bloom filter. Like SADD, you can add multiple items at once to a Bloom filter with the BF.MADD command. In fact, I took the text file that contained all the bad words and the SADD command, and just replaced the command (now BF.MADD) and key.With this solution, it’s important to keep in mind that Bloom filters are probabilistic — they can return false positives. Depending on your use case, this might require doing a secondary check on the results, or just using them as a screen for a manual process.First, let’s take a look at a Lua script to check for bad words:Similar to the SISMEMBER method, we can split up the incoming string into unique words and then run BF.EXISTS on each word. BF.EXISTS will return a 1 if it probably exists in the filter, or a 0 if it definitely does not. This solution compares each word in the message with the filter, inserts any bad words into a table and returns it back in the end.We can also use the ReBloom module, which offers the BF.MEXISTS command to check multiple strings versus the filter. It functions the same as BF.EXISTS, but allows you to do more at a time. However, things get a little tricky in this one. Running multiple commands, even from Lua, incurs some overhead for Redis, so ideally, you want to run as few commands as possible, leaving Redis to actually check the data rather than spend resources interpreting thousands of commands. It’s best to minimize command calls by maximizing the number of arguments (that’s the theory at least).My first attempt was to just take all the unique words of Ulysses (45,834) and shove them into one command of BF.MEXISTS using the unpack function. That didn’t work out, as Lua’s fixed stack size was far smaller than my 45k arguments. This forced me to split up my BF.MEXIST into smaller chunks of words. Trial and error helped me discover that I could safely add about 7,000 arguments to the BF.MEXISTS command before Lua complained. This came at the cost of Lua script complexity, but it meant that I could run 66 commands instead of over 45k.Here is how the script ended up looking:As you can see, this is starting to be a substantial amount of Lua code. It goes through all the words to get chunks of 7,000, and then runs the bloomcheck function, which checks the filter for a chunk of values and adds those bad words to the temp table. Finally, it returns all the possible bad words.Running Lua scripts from the CLI is not fun, since you’re dealing with large arguments and things can get sticky. So, I wrote a small node.js script using the benchmark.js framework to run my tests multiple times and get proper stats on each run. Aside from Ulysses, I also ran it on sets of 1,000 and 500 words of lorem ipsum random text to get a feel for shorter messages. All of these tests were run on my development laptop, so they have marginal meaning beyond relative differences.Let’s look at the results:I thought these results were very interesting. My initial assumption was incorrect for this case — Bloom filters weren’t the fastest. Plain old SISMEMBER ran faster in all tests. A few of my important takeaways beyond this include:So why would you use Bloom filters then? Storage efficiency. Let’s look at one final thing:Here, “badwords” is the Bloom filter, while “badwordset” is the Set structure. The Bloom filter is more than 16X smaller than the set. In this case, size doesn’t matter that much since it’s a single list. But, imagine if every user on a site had his or her own badword filter? This would really add up using a Set structure, but using a Bloom filter it would be much easier to accommodate. And this test has shown that the response time is very similar."
514,https://redis.com/blog/write-redis-module-zig/,How to Write a Redis Module in Zig,"January 23, 2019",Redis,"Update (May 9th 2019): Changed the code to account for the new release of Zig version 0.4.0Everybody has a Redis instance somewhere in their stack or at the very least knows Redis. However not everybody knows that Redis supports modules: custom extensions that add new commands, data types, and functionalities to a Redis database.Need a new data structure? Full-text search? Want to store graph data in Redis?All of this is already possible with existing modules that are being actively developed by the Redis community.If you have a specific use case that is not yet covered and you need the performance that Redis offers, you might be interested in learning how to write a module.The default way of building a module is using C and compiling it as a shared dynamic library.If you know C, then all is good, but if you don’t, setting up a brand new C project can feel a bit overwhelming. It’s easy to forget one small detail about the compilation process and end up with a binary that segfaults for no apparent reason.It can get even more frustrating when you have to deal with cross-platform compilation, for example when you’re developing on macOS or Windows, and you intend to deploy on Linux.Zig is a brand new language developed by Andrew Kelley that puts a lot of emphasis in bringing modern comfort to the C way of writing programs. The result is a language that allows you to build fully C ABI compatible binaries while having access to language features such as advanced error checking by the compiler, better metaprogramming facilities, generics, optionals, error types, and more.Redis modules are object files that can be dynamically loaded by Redis at runtime. A module expects access to a few functions exposed by Redis that allow it to operate in the Redis ecosystem. The only interface that the module is required to implement, is a function called RedisModule_OnLoad(), which is generally used to register in Redis all the new commands that the module offers.This means that you can use any language that can be compiled to a C-compatible dynamic library, and Zig makes it particularly easy while allowing you to use more modern abstractions in your private code.Let’s write a very simple module called testmodule that implements a test.hello command, which only sends “Hello World!” to the client when called.In C, the most straightforward way of writing a Redis module is to download a copy of the redismodule.h header file from Redis’ official repository (unstable branch), include it at the beginning of your code, and write the module.A header file is the C way of describing the interface of another piece of code that won’t actually be part of the compilation phase, letting the compiler know if any unimplemented symbols are being used incorrectly.This is how our module code looks like in C:Other than the header file import, the script only contains two function implementations: HelloWorld_Command() and RedisModule_OnLoad().As mentioned before, RedisModule_OnLoad() is invoked by Redis when loading the module, while HelloWorld_Command() is our sample commandNow that we have written a simple HelloWorld module in C, let’s look at how can we do this in Zig.Before we start writing in Zig, you may be wondering, how do you import all the definitions of a C header file if you’re not writing in C?That already seems a big blocker, but Zig makes integrating with C projects a priority and so it offers two ways of quickly importing a header file.The fastest method is to just call @cInclude(), which allows to import a header file directly, while the second is to use a compiler command to translate the C code to Zig (which is what @cInclude() does under the hood).While immediate, in our case the first option is not the best for a few reasons, the most important being that Zig is a safe language, and automatic translation of the header file results in Zig assuming that every pointer could be null, and thus requiring the user to explicitly check before any operation can be done.Unfortunately, null is a valid value for pointers in C, and because of that there is no way of specifying if a pointer is allowed to be null or not. This means that when importing function signatures from a header file, every formal argument that is a pointer, is going to be translated in a Zig optional pointer, which is the correct and safe choice, but makes using the imported symbols unnecessarily verbose, if you know for certain that a given pointer is never going to be null.To show you explicitly, this is what HelloWorld_Command() would look like when importing the header file directly:Making sure we don’t try to dereference a null pointer is very good, but it is a bit verbose if we know that a pointer will never be null, as it is the case with formal arguments of Redis commands, for example.The solution is to use:$ zig translate-c redismodule.hThis will obtain a Zig file equivalent to the header file and change a few type signatures to spare ourselves some unnecessary optional unwrapping.To make it easier, you can download an already cleaned up copy of redismodule.zig (compatible with Zig 0.4.0 and Redis 5.0). Just know that is not officially supported and that you might have to get your hands dirty in case something doesn’t work.This is then what the module looks like when written in Zig:As you can see, the translation is very straightforward, the main difference being that the symbols imported are stored inside the redis constant.Another thing of note is that C often has unwieldy interop with other languages because of inconsistencies in how strings are represented in memory. In C, strings are pointers to null-terminated byte arrays, while other languages have a variety of different representations. Zig itself does not use C-style strings but it makes easy to produce them by prefixing `c` to a string literal, as you can see in our Zig code.This simple example shows that C code has a mostly straightforward translation to Zig, but don’t think that you then end up with a “C with extra steps”. There are many features that you will surely appreciate when you move into writing a real module that moves data around.Consult the official Zig documentation for a complete list of features.Zig has a dedicated build command for shared libraries:$ zig build-lib -dynamic module.zigIf you need to cross-compile for 64-bit Linux:$ zig build-lib -dynamic module.zig –target-os linux –target-arch x86_64To try out your module you can use the following commands in redis-cli or through the Redis Enterprise GUI:> MODULE LOAD /path/to/module.so.0 (or libmodule.0.0.0.dylib on macOS)OK> test.helloHello World!We saw how Zig makes integrating with the C ecosystem a reasonably enjoyable experience. You have great shortcuts such as @cInclude() and c”Hello World” as well as very practical escape hatches for when you need more control, like with the translate-c compiler command.The result is a language that gives you more safety guarantees and a dead simple building process that even supports cross-compilation in a single command.The only thing that now remains to know, is what you can actually do inside a Redis module.The official website contains a list of published modules and the full module API reference.If you want to share a Redis module written by you that you think the community would appreciate, send a pull request to antirez/redis-doc to have it added to the list.Happy hacking!"
515,https://redis.com/blog/create-simple-application-redis-cloud-node-js/,How to create a simple application with Redis Cloud and Node.js,"January 24, 2019",Redis,"Sometimes, I “think” in Redis. It’s a bit hard to describe, but I tend to think of real-life problems in terms of how I would solve them in Redis (I… think it might be a sickness). I’ve been renovating my house for a couple of years now and one of the most unexpected challenges is dealing with how long it takes to plaster, paint, apply adhesive and the like to dry and cure. Some things take weeks to cure before you can move on to the next step. I had a minor annoyance/disaster this weekend and I told myself that I’m not going to let it happen again. So… off to Redis to do an application speedrun.In this post I’m going to build a small application that uses Node.js and Redis Enterprise Cloud to keep track of when things are “dry” or cured. You can record when you applied “paint” (we just use this as a generalized “material” but it could be anything) and it won’t let you paint again if it’s not dry. Finally, you can also check on the status of a coat of paint by finding out how long it’s got left. I know, it’s elegant, yet way over engineered – I’ll have VC funding in no time!First things first, get a small Redis instance. Redis Enterprise Cloud is great for this as they give you a free 30 mb instance for as long as you use it. I suggest reading the Redis Enterprise Cloud quick setup – it will walk you through the process in detail. I estimate that each coat of paint and all the keys associated with it in Redis will take up about 300 bytes. So, the free instance can easily handle about 100,000 rooms – big enough for my house.After signing up you should have a port, a host and a password. You can use these to create a small JSON file. We’ll use this to instruct the Node.js script of your credentials and hostname. Keep in mind that all of this is private, so don’t put in your project folder (too easy to accidentally upload it to github or something). Here is an example of how the JSON file should be structured:Now, let’s create a new directory, initialize npm, and load some dependencies.Let’s go over the three dependencies:Before we dive in, let’s look at the theory of what we’re going to do. To achieve the functionality, described above we need just a handful of commands. We have two application operations –  paint to apply a layer of paint and readytopaint to check if you’re ok to paint. Both of these operations require two commands together in a way that is dependent on the sequence, we’ll use a MULTI/EXEC block.Let’s look at the paint operation first. We can achieve this operation with the Redis commands of SADD (set add) and SET. SADD will track if a room exists over the long term (more on that later) and enable multiple users. I did warn you that this was over-engineered. The key for the set data type key is structured like rooms:userid, in this case, the userid can be anything. The member is the name of the room you’re painting. The second operation is a SET command, but with some unusual arguments. The key is structured to include both the userid and the room and looks like paint:userid:room. The first argument is the value – here we actually don’t care about the value (we just set it to “p” as a placeholder) – all the data is held in the key itself. The next two arguments are EX and time it takes the paint to dry in seconds. Finally, we have the argument NX. These last three values are what make it work – starting from the end the NX argument means “Only do this command if it’s a completely new key.” The EX and time to dry are actually TTL values, meaning that the key will last until the paint is dry and then the key will evaporate, just like the paint solvent.The SADD operation will return a 1 if the item was added to the set or 0 if the item already exists in the set. The SET … EX … NX will return either “OK” if it set the key or null/(nil) if the operation couldn’t happen because the key already exists. What’s nice about this is that you have two operations that can boil down into two boolean values. Let’s map out the logic here:The other operation is to check if the room is ready to be painted. We can achieve this with SISMEMBER and TTL again in the MULTI/EXEC block. We use the key patterns as mentioned above and the arguments are straight forward:SISMEMBER will return either a 0 for not a member of the set and 1 will be returned if it is a member of the set. TTL, on the other hand is a little more complicated as far as return values. If there is a time to live, it will be 0 or above. If the key doesn’t have a TTL but the key exists, it will be -1 and if the key doesn’t exist -2. In this case, we really shouldn’t have a -1, so we can just see if it’s 0 or higher. So, we can still have two boolean values:To facilitate all this, we’ll use yargs and define each operation as a command off the yargs module. Each command takes a object where we will define the command syntax and the desc (which defines help text) and finally, the actual execution of the command occurs in the handler function. After the Redis command executes and closes the redis connection so the process can terminate.Finally, you have demandCommand() that triggers the requirement the command and errors out if it’s not there and the help() that takes care of rendering help text if needed. Finally, you end with .argv which will sit this all in motion.To run the application you can paint a room with:If you want to check if a room is paintable, then run the command:There you have it! In less than 100 lines we have a useful little utility that uses Node.js and Redis Enterprise Cloud. Now, you might think the usefulness as a command line utility is limited, but the same base here could be easily ported with little effort over to a web framework like Express and you too can start your service that watches paint dry."
516,https://redis.com/blog/use-redis-event-store-communication-microservices/,How to Use Redis as an Event Store for Communication Between Microservices,"February 11, 2019",Martin Forstner,"Click here to get started with Redis Enterprise. Redis Enterprise lets you work with any real-time data, at any scale, anywhere.In my experience, certain applications are easier to build and maintain when they are broken down into smaller, loosely coupled, self-contained pieces of logical business services that work together. Each of these services (a.k.a. microservices) manages its own technology stack that is easy to develop and deploy independently of other services. There are countless well-documented benefits of using this architecture design that have already been covered by others at length. That said, there is one aspect of this design that I always pay careful attention to because when I haven’t it’s led to some interesting challenges.While building loosely coupled microservices is an extremely lightweight and rapid development process, inter-services communication models to share state, events and data between these services is not as trivial. The easiest communication model I have used is direct inter-service communication. However, as explained eloquently by Fernando Dogio, it fails at scale—causing crashed services, retry logics and significant headaches when load increases—and should be avoided at all costs. Other communication models range from generic Pub/Sub to complex Kafka event streams, but most recently I have been using Redis for communication between microservices.Microservices distribute state over network boundaries. To keep track of this state, events should be stored in, let’s say, an event store. Since these events are usually an immutable stream of records of asynchronous write operations (a.k.a. transaction logs), the following properties apply:With Redis, I have always easily implemented pub-sub patterns. But now that the new Streams data type is available with Redis 5.0, we can model a log data structure in a more abstract way—making this an ideal use case for time-series data (like a transaction log with at-most-once or at-least-once delivery semantics). Along with Active-Active capabilities, easy and simple deployment, and in-memory super fast processing, Redis Streams is a must-have for managing microservices communication at scale.The basic pattern is called Command Query Responsibility Segregation (CQRS). It separates the way commands and queries are executed. In this case commands are done via HTTP, and queries via RESP (Redis Serialization Protocol).Let’s use an example to demonstrate how to create an event store with Redis.I created an application for a simple, but common, e-commerce use case. When a customer, an inventory item or an order is created/deleted, an event should be communicated asynchronously to the CRM service using RESP to manage OrderShop’s interactions with current and potential customers. Like many common application requirements, the CRM service can to be started and stopped during runtime without any impact to other microservices. This necessitates that all messages sent to it during its downtime be captured for processing.The following diagram shows the inter-connectivity of nine decoupled microservices that use an event store built with Redis Streams for inter-services communication. They do this by listening to any newly created events on the specific event stream in an event store, i.e. a Redis instance.Figure 1: OrderShop ArchitectureThe domain model for our OrderShop application consists of the following five entities:By listening to the domain events and keeping the entity cache up to date, the aggregate functions of the event store has to be called only once or on reply.Figure 2: OrderShop Domain ModelTo try this out for yourself:Below are some sample test cases from client.py, along with corresponding Redis data types and keys.I chose the Streams data type to save these events because the abstract data type behind them is a transaction log, which perfectly fits our use case of a continuous event stream. I chose different keys to distribute the partitions and decided to generate my own entry ID for each stream, consisting of the timestamp in seconds “-” microseconds (to be unique and preserve the order of the events across keys/partitions).I choose Sets to store the IDs (UUIDs) and Lists and Hashes to model the data, since it reflects their structure and the entity cache is just a simple projection of the domain model.The wide variety of data structures offered in Redis—including Sets, Sorted Sets, Hashes, Lists, Strings, Bit Arrays, HyperLogLogs, Geospatial Indexes and now Streams—easily adapt to any data model. Streams has elements that are not just a single string, but are objects composed of fields and values. Range queries are fast, and each entry in a stream has an ID, which is a logical offset. Streams provides solutions for use cases such as time series, as well as streaming messages for other use cases like replacing generic Pub/Sub applications that need more reliability than fire-and-forget, and for completely new use cases.Because you can scale Redis instances through sharding (by clustering several instances) and offer persistence options for disaster recovery, Redis is an enterprise-ready choice.Please, feel free to reach out to me with any questions or to share your feedback.ciao"
517,https://redis.com/blog/getting-started-redis-apache-spark-python/,"Getting Started with Redis, Apache Spark and Python","February 12, 2019",Redis,"Apache Spark is one of the most popular frameworks for creating distributed data processing pipelines and, in this blog, we’ll describe how to use Spark with Redis as the data repository for compute. Spark’s main feature is that a pipeline (a Java, Scala, Python or R script) can be run both locally (for development) and on a cluster, without having to change any of the source code.Spark allows this flexibility by cleverly using delayed computation or, as it is called in some contexts, laziness. Everything starts with the classes RDD, DataFrame and the more recent Dataset, which are each distributed lazy representations of your data. They use distributed file systems, databases or other similar services as actual storage backend. Their operations — such as map/select, filter/where and reduce/groupBy — do not really make the computation happen. Rather, every operation adds a step to an execution plan that is eventually run when an actual result is needed (e.g., when trying to print it to screen).When launching the script locally, all computations happen on your machine. Alternately, when launching on a distributed cluster, your data is partitioned to different nodes; the same operation happens (mostly) in parallel within the Spark cluster.Over time, Spark developed three different APIs to deal with distributed data sets. While each new addition added more capabilities over the previous ones, no single API is a complete replacement of what came before. In order of creation (oldest to newest), here’s an overview:For more details, check out “A Tale of Three Apache Spark APIs” by Jules Damji.spark-redis is an open source connector that allows you to use Redis to store your data.Three main reasons to use Redis as a backend are:In this article we will focus on getting started with Python and how to use the DataFrame API. At time of writing, Scala which can be considered the “native” language of Spark, has access to some of the more advanced features of the integration, like Redis RDDs and Streams. Since Scala is a JVM language, by extension Java can also use these features. With Python, we’ll need to stick to DataFrames.Our first step is to install pyspark using pip. You will also need Java 8 installed on your machine.$ pip install pysparkNext, we’ll need Maven to build spark-redis. You can get it from the official website or by using a package manager (e.g. homebrew on macOS).Download spark-redis from GitHub (either git clone or download as a zip), and build it using Maven.$ cd spark-redis$ mvn clean package -DskipTestsIn the target/ subdirectory, you’ll find the compiled jar file.If you don’t have it already, you will need a running Redis server to connect to. You can download it in a number of ways: from the official website, package manager (apt-get or  brew install redis) or Docker Hub (psst, this might be a good moment to try out Redis Enterprise).Once you have it up and running, you can launch pyspark. Note that you’ll need to change VERSION to reflect the version you downloaded from GitHub.$ pyspark –jars target/spark-redis-VERSION-jar-with-dependencies.jarIf your Redis server is in a container or has authentication enabled, add these switches to the previous invocation (and change the values to fit your situation).–conf “spark.redis.host=localhost” –conf “spark.redis.port=6379” –conf “spark.redis.auth=passwd”Now that we have a functioning pyspark shell that can store data on Redis, let’s play around with this famous people data set.After downloading the TSV file, let’s load it as a Spark DataFrame.Now, invoking .dtypes shows a list of all the columns (and relative types) in the data set. There are many things that could be interesting to investigate in this data set, but for the purpose of this example let’s focus on finding, for each country, the most frequent occupation of its famous people.Let’s start by keeping only the columns relevant to our goal.This will create a copy of the original DataFrame that only contains three columns: the unique ID of each person, their country and their occupation.We started by downloading a small data set for the purpose of this blog post, but in real life, if you were using Spark, the data set would likely be much bigger and hosted remotely. For this reason, let’s try to make the situation more realistic in the next step by loading the data into Redis:This command will load our data set into Redis. The two options we specified help define the data layout in Redis, as we’ll now see.Let’s jump for a moment in redis-cli to see how our DataFrame is stored on Redis:SCAN shows us some of the keys we loaded to Redis. You can immediately see how the options we gave before were used to define the key names:Let’s take a look at the content of a random key:As you can see, each row of our DataFrame became a Redis Hash containing countryCode and occupation. As stated earlier, en_curid was used as primary key, so it became part of the key name.Now that we’ve seen how the data is stored on Redis, let’s jump back into pyspark and see how we would actually write a pipeline to get the most common occupation for the famous people of each country.Even though we should have the data still loaded into memory, let’s load it from Redis in order to write code that’s more similar to what you would do in real life.This is how your Spark pipeline would start, so let’s finally do the computation!Now each row represents the count of all the present (country, occupation) combinations. For the next step, we need to select only the occupation with the highest count for each country.Let’s start by importing a few new modules we need, and then defining, using windows, the code to select the most frequent occupations:This code grouped the original rows by countryCode, ordered the content of each group by count(en_curid)in descending order, and took only the first element. As you can see, within this small sample, politician seems a very common occupation.Let’s see for how many countries this is true:Wow, that’s a lot, considering there are 195 countries in the world as of today. Now, let’s just save the remaining countries in Redis:If you now jump into redis-cli, you will be able to see the new data:If you want to practice more, inspect the original data set and see if you find other details that pique your interest.A very important point worth reiterating is that every operation on a RDD or DataFrame/set object will be distributed on multiple nodes. If our example was not just about famous people, we’d have had tens of millions of rows at the beginning. In that case, Spark would scale out the computation. But if you only had one Redis instance in place, you’d have N nodes hammering on it, most probably bottlenecking your network bandwidth.To get the most out of Redis, you’ll need to scale it appropriately using the Redis Cluster API. This will ensure that all your computing nodes are not starved when reading, or choked when writing.In this post, we explored how to download, compile and deploy spark-redis in order to use Redis as a backend for your Spark DataFrames. Redis offers full support for the DataFrame API, so it should be very easy to port existing scripts and start enjoying the added speed Redis offers. If you want to learn more, take a look at the documentation for spark-redis on GitHub."
518,https://redis.com/blog/how-to-embed-redis-into-your-continuous-integration-and-continuous-deployment-ci-cd-process/,How to Embed Redis into Your Continuous Integration and Continuous Deployment (CI/CD) Process,"April 26, 2019",Shabih Syed,"Earlier this week, I wrote about how Redis can benefit distributed development teams by helping them release new features safely and roll them back with minimal impact when required. Today, I’ll dive into specific details about how feature toggles, feature context and error logs can enhance your continuous integration and continuous deployment (CI/CD) process.“Feature toggling” is a set of patterns that help you deliver new functionality to application users rapidly but safely. Feature toggles are also referred to as feature flags, feature bits or feature flippers.In Redis Enterprise, it’s very easy to structure a toggle strategy using the native Redis HASH data structure.With Redis-cli or any Redis client for the language of your choice, you can create HASH keys (such as “useNewAlgorithm”) with values that set a toggle flag, release number, developer names, an override flag, etc.Using this type of data structure, you can set up as many feature toggles in Redis as you need. Your application can then look these up at runtime to check the real-time flag status and metadata.Using this type of data structure, you can set up as many feature toggles in Redis as you need. Your application can then look these up at runtime to check the real-time flag status and metadata.Response:Again, using the native Redis HASH data structure and Redis-cli (or any Redis client), you can store session data in keys and look it up at runtime to read real-time values.For example, in Figure 6, the ‘sessionAuthorized’ method checks if a user has logged in with Facebook using the ‘authType” value stored in the session key.Response:Feel free to read up more on session stores here.An “error database” is a centralized data store for errors reported during runtime. Errors can be stored in RediSearch, a powerful text search and secondary indexing engine, which provides a fast search for looking them up. The version of RediSearch that comes with Redis Enterprise supports scaling across many servers, allowing it to easily grow to billions of documents on hundreds of servers.You can push any errors reported during runtime into RediSearch, along with other useful information from the feature toggle and feature context, which will help with the triage process.Create search indexes with Redis-cli or any Redis client.For example, let’s create a new index called “toggle_errors_db” to store all errors reported whenever a feature toggle is used.Let’s add some data to this index with a new key using the format:useNewAlgorithm:03-12-19-10-32-05Try a search on this index for any object with keywords, e.g. search for developer name: JohnResponse:Search for feature toggle name: useNewAlgorithmResponse:A “logs database” is a centralized data store that tracks log messages. You can use Redis to store a recent list of log messages, which will give you a snapshot view of your logs at any time.To keep a recent list of logs, you can LPUSH log messages to a LIST and then trim that LIST to a fixed size.Later, if you want to read those log messages, just perform a simple LRANGE to fetch them.Read up more about this approach here.SEVERITY = {
logging.DEBUG: 'debug',
logging.INFO: 'info',
logging.WARNING: 'warning',
logging.ERROR: 'error',
logging.CRITICAL: 'critical',
}
SEVERITY.update((name, name) for name in SEVERITY.values())//Set up a mapping that should help turn most logging severity levels into something consistent
def log_recent(conn, name, message, severity=logging.INFO, pipe=None):
severity = str(SEVERITY.get(severity, severity)).lower()
//Actually try to turn a logging level into a simple string.
destination = 'recent:%s:%s'%(name, severity)
//Create the key that messages will be written to.
message = time.asctime() + ' ' + message
//Add the current time so that we know when the message was sent.
pipe = pipe or conn.pipeline()
pipe.lpush(destination, message)
//Add the message to the beginning of the log list.
pipe.ltrim(destination, 0, 99)
//Trim the log list to only include the most recent 100 messages.
pipe.execute()Each of these techniques brings an opportunity to take your continuous updates to the next level and minimize the time and headaches inherent in managing development for large-scale apps with frequent releases. Of course, Redis Enterprise is a great way to bring more power and flexibility to the whole CI/CD process. Thankfully, creating a Redis cluster with Redis Enterprise Pro (which includes the RediSearch module and features data persistence) is easy and free.Get Started with Redis Enterprise today."
519,https://redis.com/blog/caches-promises-locks/,"Caches, Promises and Locks","May 29, 2019",Redis,"Instagram recently published a post on their engineering blog about the concept of promisifying cached values. The idea is that, on a cache miss, it takes a while to fetch the missing value, which could cause stampedes on the underlying DBMS that the cache is supposed to protect. A stampede, among other things, consists of multiple parallel requests that, on a cache miss, trigger multiple instances of the same work to populate the cache (see below)In their post, Instagram’s Nick Cooper showcases the idea of storing a dummy value (i.e. the promise) in the cache to signal to competing requesters that someone else is preparing the data, so they know to wait instead of hitting the DBMS to death. This is the article I’m referring to, it also received a few comments on Hacker News.This idea is not new (it’s one of the features you get out of the box with read/write-through caches, since they handle the database transparently), but I think it’s very good and worth discussing. In this post, I’ll share a simple implementation of this approach and how you can get out of it even more benefits than typically offered with (r/w)-through caches.Before going into the details of how my implementation works, here’s an overview of what I was able to achieve after mapping this use case to Redis operations. My implementation:My implementation primarily relies on three Redis features: key expiration (TTL), atomic operations (SET NX) and Pub/Sub. More generally, I made good use of the principle I explained in a previous post:Shared state begets coordination and vice-versa.Redis understands this very well, which is why it’s so easy to build this kind of abstraction using it. In this case, the Pub/Sub messaging functionality helps us tie together a locking mechanism to easily create a real cross-network promise system.I’m calling my implementation of this pattern Redis MemoLock. At a high level, it works as follows:In practice, the algorithm is slightly more complex so we can get the concurrency right and handle timeouts (locks/promises that can’t expire are borderline useless in a distributed environment). Our naming scheme is also slightly more complicated because I opted to give a namespace to each resource, in order to allow multiple independent services to use the same cluster without risking key name collisions. Other than that, there is really not much complexity required to solve this problem.You can find my code here on GitHub. I’ve published a Go implementation, and soon I’ll add a C# one in conjunction with my upcoming talk at NDC Oslo (and probably more languages in the future). Here’s a code example from the Go version:If you run two instances of this program within 5 seconds, “Cache miss!” will only show once, regardless of whether your first execution already concluded (i.e. the value was cached) or is still computing (sleeping instead of doing useful work, in the case of this sample code).Two big reasons:This brings us to the reason I called my solution MemoLock. If you generalize the promisified caching idea to cache not only queries, but any (serializable) output of a (pure-ish) function call, you are talking about memoization (not memorization). If you’ve never heard about memoization, read more about it — it’s an interesting concept.The pattern described by Instagram isn’t surprisingly new, but it’s worth discussing. Better caching can help a ton of use cases, and we, as an industry, are not always fully familiar with the concept for promises, especially when it’s outside the scope of a single process. Try out the code on GitHub and let me know if you like it."
520,https://redis.com/blog/async-await-programming-basics-python-examples/,Async/Await Programming Basics with Python Examples,"September 9, 2019",Redis,"In recent years, many programming languages have made an effort to improve their concurrency primitives. Go has goroutines, Ruby has fibers and, of course, Node.js helped popularize async/await, which is today’s most widespread type of concurrency operator. In this post, I will talk about the basics of async/await, using Python as an example. I chose Python, since this capability is relatively recent in Python 3, and many users might not yet be familiar with it (especially considering how long it took for Python 2.7 to get to end of life).The main reason to use async/await is to improve a program’s throughput by reducing the amount of idle time when performing I/O. Programs with this operator are implicitly using an abstraction called an event loop to juggle multiple execution paths at the same time. In some ways, these event loops resemble multi-threaded programming, but an event loop normally lives in a single thread—as such, it can’t perform more than one computation at a time. Because of this, an event loop alone can’t improve the performance of computation-intensive applications. However, it can drastically improve performance for programs that do a lot of network communication, like applications connected to a Redis database.Every time a program sends a command to Redis, it has to wait for Redis to formulate a reply, and, if Redis is hosted on another machine, there’s also network latency. A simple, single-threaded application that doesn’t use an event loop sits idle while it waits for the reply to come in, which wastes a lot of CPU cycles. Keep in mind that network latency is measured in milliseconds, while CPU instructions take nanoseconds to execute. That’s a difference of six orders of magnitude.As an example, here’s a code sample that tracks wins for a hypothetical game. Each stream entry contains the winner’s name, and our program updates a Redis Sorted Set that acts as the leaderboard. The code is not very robust, but we don’t care about that for now, because we’re focusing on the performance of blocking versus non-blocking code.To write an equivalent async version of the code above, we’ll use aio-libs/aioredis.The aio-libs community is rewriting many Python networking libraries to include support for asyncio, Python’s standard library implementation of an event loop. Here’s a non-blocking version of the code above:This code is mostly the same, other than a few await keywords sprinkled around. The biggest difference is what happens in the last couple of lines. In Node.js, the environment loads an event loop by default, while, in Python, you have to start it explicitly — that’s what those final lines do.After the rewrite, we might think we improved performance just by doing this much. Unfortunately, the non-blocking version of our code does not improve performance yet. The problem here lies with the specifics of how we wrote the code, not with the general idea of using async / await.The main issue with our rewrite is that we overused await. When we prefix an asynchronous call with await, we do two things:Sometimes, that’s the right thing to do. For example, we won’t be able to iterate on each event until we are done reading from the stream on line 15. In that case, the await keyword makes sense, but look at add_new_win:In this function, the second operation doesn’t really depend on the first. We would be fine with the second command being sent along with the first, but await blocks the execution flow as soon as we send the first. We would like a way to schedule both operations immediately. For that, we need a different synchronization primitive.First of all, calling an async function directly won’t execute any of its code. Instead, it will just instantiate a “task.” Depending on your language of choice, this might be called coroutine, promise, future or something else. Without getting into the weeds, for us, a task is an object representing a value that will be available only after using await or another synchronization primitive, like asyncio.gather.In Python’s official documentation, you can find more information about asyncio.gather. In short, it allows us to schedule multiple tasks at the same time. We need to await its result because it creates a new task that completes once all the input tasks are completed. Python’s asyncio.gather is equivalent to JavaScript’s Promise.all, C#’s Task.WhenAll, Kotlin’s awaitAll, etc.The same thing we did for add_new_win can be also done for the main stream event processing loop. Here’s the code I’m referring to:Given what we have learned so far, you will notice that we are processing each event sequentially. We know that for sure because on line 6 the use of await both schedules and waits for the completion of add_new_win. Sometimes that’s exactly what you want to happen, because the program logic would break if you were to apply changes out of order. In our case, we don’t really care about ordering because we’re just updating counters.We are now also concurrently processing each batch of events, and our change of code was minimal. One last thing to keep in mind is that sometimes programs can be performant even without the use of asyncio.gather. In particular, when you’re writing code for a web server and using an asynchronous framework like Sanic, the framework will call your request handlers in a concurrent way, ensuring great throughput even if you await every asynchronous function call.Here’s the complete code example after the two changes we presented above:In order to exploit non-blocking I/O, you need to rethink how you approach networked operations. The good news is that it’s not particularly difficult, you just need to know when sequentiality is important and when it’s not. Try experimenting with aioredis or an equivalent asynchronous Redis client, and see how much you can improve the throughput of your applications."
521,https://redis.com/blog/introducing-redis-launchpad/,Introducing Redis Launchpad,"September 22, 2021",Mike Anand and Raja Rao,"The Redis community has always been at the core of what makes Redis great. Thanks to this group, Redis, for the 5th time in a row, was picked as the most-loved database in the Stack Overflow’s developer survey. As Redis’s popularity grows, so do the use cases across developer communities, industries verticals, and geographies.Empowering, growing, and harnessing the power of Redis through a single vision with the love of our community is why we dreamed up Redis Launchpad. Today, we are excited to introduce Redis Launchpad, a hub of 75+ sample applications built by us and you on Redis. Redis Launchpad provides developers and architects an easy, tangible way to find and visualize numerous sample apps that use Redis as a real-time data platform and primary database in one central location. Here you can dive into high-quality sample apps that show different architectures, data modeling, data storage, and commands, allowing you to start building fast apps faster.These apps come in various languages (JavaScript, Java, Python, etc.), cater to different industry verticals (Financial services, Gaming, Retail, etc), use different Redis modules (RedisJSON, RediSearch, etc), and showcase varying capabilities. You can even drill down and search for individual commands to see how they are used in different apps and different languages!Well, simply hop over to https://launchpad.redis.com and search for any app based on various criteria. Click on the app and you should see a video and detailed description of how that app was built and works. This includes things like data modeling, commands to add the data, and commands to retrieve the data.What’s more, in some of the apps, you get ready to use deploy buttons. So you can quickly deploy it into Heroku, Vercel, Google Cloud, and more.As they say, we love to drink our own champagne. We’re thrilled to be able to use our own product and showcase the power of Redis beyond cache.Let’s get into the technical aspects of how we built it and how it works.We currently use Redis Hashes to store the metadata of apps, and index it using RediSearch. Then everything else—the left pane filters, the fuzzy search, the apps gallery, and pagination—are all supported just by RediSearch!In order to make a Github repo part of the Launchpad, you need to first make it part of the redis-developer Github account. And secondly, also have a “marketplace.json” metadata file. This file describes everything about the app, including the app’s name, description, the programming language used, videos, commands, and so on. The content of this file is the only thing that’s added to the Redis database for searching and filtering purposes.And here is how the rest of it works:And once we have the data in the database, the next step is to simply query it and show the result in the browser. This is how it works:We’d love to see you add your app to the Launchpad. And we’d also be happy to promote it on our social media on a case-by-case basis. The first requirement is that your app should be built on Redis and use Redis as the primary database. Secondly, you should have clear and detailed instructions for us to QA and for the community to easily understand how it works.Once you think you have those requirements met, follow these instructions to add the metadata file and let us know:This file contains your app’s metadata. For the latest details, please click on the “Add your app” button in Launchpad. But here are the details as of this writing:Example metadata.json:We’re super excited to launch this. Redis is very versatile and Redis Launchpad will now show exactly how you can harness the power of Redis to use it as a real-time data platform and a primary database.Check it out and let us know what you think by tagging @redisinc on social media. 🚀"
522,https://redis.com/blog/redismart-real-time-json-product-catalog-service/,"Building a Fast, Flexible, and Searchable Product Catalog with RedisJSON","November 16, 2021",Adi Wabisabi,"RedisJSON powered by RediSearch is now out in public preview. In this blog post, we’ll dive into getting you started using RedisJSON’s new JSON indexing, querying, and full-text search capabilities by looking at how it was used to build RedisMart’s product catalog service. In case you missed it, RedisMart is the fully-functional real-time retail store we demoed during the RedisConf 2021 keynote presentation. We also published a blog with a deep-dive into the main requirements and architecture of the RedisMart retail application.RedisJSON is a high-performance document store built on top of Redis open source and available as a source-available license. Its primary goal is to allow you to take the JSON objects you’re likely already using in your application and make them accessible and searchable within Redis. It also offers a more sophisticated data API than the simple hash API, allowing you to expand your data model without having to use multiple keys.There are a few ways that you can get started using RedisJSON:For this post, we’re going to be using the Docker container option:Once you have that running, you can connect to it using redis-cli:RedisMart is built in Python, so we’ll connect using the recently simplified developer tools.At Redis, we tend to prefer using Poetry for dependency management, but the process for adding redis-py to your application is roughly the same for both:Or, of course:Once we have redis-py added to our Python environment, we can connect to our Docker container and do a simple “hello world” application:Now that we have the basics in place, let’s create our data model and index.To paraphrase the great Carl Sagan, if you wish to make a RedisJSON product catalog service from scratch, you must first create a search index. For RedisMart, we used a realistic, but straightforward, data model. (See the full Gist here.)Taking it and creating an index is pretty straightforward:Notice how, for each field, we’re using the as_name argument to set an alias for the full path. This is helpful for two reasons: one, you don’t have to specify the full path when you’re using that attribute in your query. And two, it allows you to change the underlying path of that attribute without having to change the code that calls it.Once the index is created, you’ll use the same object to search it. Best practice is to first check to see if the index is created by using the .info() method, and if it isn’t, to do so. Let’s expand our search index creation code to add those cases:A nice thing about using RedisJSON is that you can easily add new fields to your index with the FT.ALTER command.Next, we’ll make this search functionality accessible to the rest of the application.One requirement for this project was that we’d be able to query the product catalog using different attributes. Querying by name of the product is an obvious choice, but we also implemented filtering by price, rating, and category as well. In the faceted navigation menu, you can use it to quickly find what you are looking for.The category also shows up in the autocomplete drop-down powered by the fuzzy search feature of RedisJSON:Fuzzy search is easy to do using the Suggestions feature, which we can add to any data that we’re adding to the catalog:Now that we have our index and suggestions setup, let’s build a search query function for our product microservice:Last, but not least, we’ll need to add the ability to add and modify items from the rest of the application.For RedisMart, we put the product catalog inside of a microservice.In order to complete the REST API, we’ll need to add our creation, update and delete flows.By now, you can see how to easily take your JSON-based product data and make it searchable and accessible to any modern application by using a microservice. In addition to indexing, searching, and full-text search features on JSON documents, RedisJSON powered by RediSearch also includes powerful data aggregation capabilities (see tutorial and online course). Here’s some links to get you started"
523,https://redis.com/blog/5-6-7-methods-for-tracing-and-debugging-redis-lua-scripts/,5 6 7 Methods For Tracing and Debugging Redis Lua Scripts,"December 2, 2014",Itamar Haber,"If you’ve ever written even a single line of code, you know that Benjamin Franklin’s famous quote should be amended to:Software defects are a fact of life because software is made by fleshware, and humans err. Even if you are a good programmer who writes good code (or a great programmer who steals it), use proven methodologies and design patterns, employ only best-of-breed tools, and submit to peers’ code reviews… despite your best efforts, you’re likely to find yourself time and time again banging your head against the wall because of an elusive gremlin.Tracking down these issues isn’t an easy task. It requires patience, effort and in many cases a touch of inspiration to correctly identify the root cause of a failure. When developing Lua scripts for Redis (a feature that’s available from version 2.6 onwards) this can become even trickier. That’s because your code runs within the server itself, making it much harder to gain visibility into the code’s innards. To make this somewhat easier (and perhaps save your wall from a few bangs), here are five ways you can gain Superman-like X-Ray vision into your Lua script.Redis’ embedded Lua engine provides a function that prints to its log file (configured with the loglevel and logfile directives in your redis.conf). This function, conveniently named redis.log, is dead simple to use – just call it from your script like so:The redis.log function accepts two arguments: the first one is the message’s log level (choose being between LOG_DEBUG, LOG_VERBOSE, LOG_NOTICE and LOG_WARNING), and the second argument is the to-be-logged value. For more information, see EVAL‘s documentation.Pros: Using the log file is often the easiest way to trace your workflow. In addition, you can view the logged message in near real-time (i.e. by tailing the log file).Cons: There are cases in which this approach isn’t a viable option (e.g. when you don’t have access to the Redis host or when the log is too noisy to work with comfortably). Don’t despair, however, because there’s more than one way to skin a cat (meeeeeow!?!).Lua’s tables are associative arrays and are the only “container” data structures of the language. You can easily use them to store messages in a log-like manner and return the resulting array once your code finishes running. Here’s an example:Running the example above yields the following:Pros: Works everywhere, and requires very little setup.Cons: Returning the log table as your code’s reply prevents it from returning anything meaningful. This approach consumes memory to store the intermediate log table, and you need to wait for the script to finish before you can get the log messages.If you require your Lua code to return a meaningful reply once it finishes execution and still retain a logging mechanism, you can use Redis’ List data structure for storing and retrieving your messages. Here’s a snippet that shows this approach:Note that the script’s 2nd line deletes the log’s key for housekeeping. Running this script followed by an LRANGE to obtain the “log” results in the following:Pros: Like Lua tables, this is straightforward and just works.Cons: Similar to the tables approach plus there’s an arbitrary size limit (2^32) on the List’s length. You also need to pass the List’s key name to ensure cluster-compatibility and most importantly, since you’ll be performing a write operation to Redis this can’t be used with non-deterministic commands.Pub/Sub has many wonderful uses, but did you know it can also be used for debugging? By having your script publish log messages to a channel and subscribing to that channel, you can keep track of what’s going on. Here’s how to do it:Before running this script, open another terminal window and subscribe to your log channel. While the script is running, you should get the following output in your subscriber terminal:Pros: Little overhead, real-time display of messages.Cons: Pub/Sub’s delivery isn’t guaranteed, so there’s a chance you could miss a revealing log message (but that’s really a long shot).Adding tracing to your code can only get you so far when you’re trying to grok a piece of code, because sometimes you really need/want a full-fledged debugger at your disposal. This clever trick provided by Marijan Šuflaj from @Trikoder gives you exactly that – a debugger for your Lua scripts with Redis.The gist of Marijan’s idea is using a freely-available Lua debugger to debug your Lua code and adding Redis-specific commands with thin wrapping code. His blog post takes you through the steps to accomplish that feat, and even includes sample code for the Redis-specific commands wrapper. While this doesn’t exactly let you debug the code in vivo, it’s the closest you can get and for all practical purposes is as good.Pros: Use a full-blown debugger for your Lua code.Cons: Requires a degree of non-trivial setup.MONITOR and ECHOI’ve stumbled on another useful way for tracing that is very similar to using the log, but has an additional perk. The idea is to open a dedicated connection to your server and run the MONITOR command (note that this will return a stream of all commands executed by Redis so you don’t want to do this on a busy server). Once you have the monitor set up, you can call ECHO from the script to trace stuff (e.g. redis.call('ECHO', 'the value of foo is' .. foo)). The added perk is that your monitor stream also shows on every other redis.call that your script executes inline with your traces – neat-o!Lua’s print command works just great in Redis’ script and you can call it at will and trace to your heart’s content. The only problem with it, however, is that it outputs everything directly to stdout, so unless you’re watching the server’s output you’re apt to miss it.There are many ways your code can (and will) fail, and tracking down the cause can be a daunting task. When it comes to developing Lua scripts for Redis, I hope you’ll find these methods useful in squashing the pesky critters that keep your code from running as you expect it to. Questions? Feedback? Any other tricks, tips or subjects that you’d like to see covered? Email or tweet me – I’m highly available 🙂"
524,https://redis.com/blog/security-notice-heap-overflow-vulnerabilties/,Redis Security Notice: Heap Overflow Vulnerabilities,"July 10, 2023",Redis,"Here’s what you need to do about the CVE-2022-24834 and CVE-2023-36824 vulnerabilities, as well as the updates available for affected customers.We were made aware that Redis was affected by two security vulnerabilities, CVE-2022-24834 and CVE-2023-36824. CVE-2022-24834 uses a specially crafted Lua script in Redis that can trigger a heap overflow in the cJSON and cmsgpack libraries, resulting in heap corruption and potentially remote code execution. CVE-2023-36824 extracts key names from a command and a list of arguments that can also trigger a heap overflow and result in reading heap memory, heap corruption, and potentially remote code execution.Redis has, of course, taken action to prevent everyone from harm.Here’s the current situation, so we can bring the Redis community up to date about CVE-2022-24834 and CVE-2023-36824. The fix for these vulnerabilities are available in the following releases:CVE-2023-36824 only affects 7.X versions.No action is needed by Redis Enterprise Cloud customers. However, we encourage all Redis Enterprise, Redis Open Source, and Redis Stack customers to upgrade to a supported and patched version immediately.We thank the security research community for helping us to keep Redis secure!"
525,https://redis.com/blog/zerobrane-studio-plugin-for-redis-lua-scripts/,ZeroBrane Studio Plugin for Redis Lua Scripts,"January 4, 2016",Itamar Haber,"Just before Christmas, Salvatore Sanfilippo published the first release candidate for Redis v3.2. The upcoming v3.2 delivers new features (such as geo spatial indexes and cluster rebalancing) and many great improvements (including Matt Stancliff’s Quick Lists and Oran Agra’s SDS and Jemalloc optimizations). For this post, I’ll focus on just one of these – the Redis Lua Debugger (LDB) – and also share a New Year’s present from Redis (spoiler).LDB is a mode for executing Redis Lua scripts that allows you to step through your Lua script, set conditional and line breakpoints, inspect variables, trace the stack, print debug output, evaluate Lua expressions, and run Redis commands. It comes in both asynchronous and synchronous sub-modes, the former of which is non-blocking and discards updates to the dataset, while the latter blocks and retains data changes. LDB is implemented in the Redis server, and the command line interface (redis-cli) provides the respective console when running scripts in debug mode. You can learn more on the documentation page or in this short video introduction (33:40m) by Salvatore Sanfilippo.The ability to execute embedded Lua scripts in Redis is the best feature in the history of humankind, and one of Redis’ top-k. The language’s simple syntax, convenient data constructs and base libraries easily allow you to perform complex operations on your data right where it is managed. Besides extending Redis in any imaginable direction, you usually also save on latency and bandwidth. For me, the new Lua debugger is a dream come true – I’ve always been enamored with Redis’ scripts and equally frustrated by the inability to debug them.Being the uber-developer that I am, I rarely code bugs, perhaps as little as three or four in each LoC… To hunt these down I’ve tried different methods for tracing, and even resorted to writing a Redis Lua debugger as Redis Lua script (that debugger is incompatible with v3+, so regrettably, it’s no longer maintained). But these were never quite enough.At the Redis Developers Day in October, I gave a short session that was essentially a Lua smorgasbord (and also secondary indexes – see redimension and lua-redimension for that). It was definitely not the first time a Redis user had asked for a Lua debugger, but I do believe I was the last one to make the request… A couple of weeks later, the collective wish for a debugger was fulfilled as LDB made its debut to unstable (and I got to test it, so if you find any issues, you can blame my sloppiness ;)).Once LDB was ready, I knew there was one other step that must be taken, so I reached out to Paul Kulchenko and asked him to make us a Redis plugin for ZeroBrane Studio (which I also tested, so again, any issues – mea culpa). ZeroBrane Studio is THE Lua IDE. It is open source, lightweight, cross-platform, portable, rock stable, passionately maintained and covers any possible need I have for Lua development. It also boasts a very handy integrated debugger that now, thanks to Paul’s work and Redis’ sponsorship, plays very nicely with Redis’ LDB. Check out the screencast (10:26) for an introduction, or follow these steps to get to Lua debugging heaven:That’s all there is to it – fire up ZBS and you’re all ready to go! It is as intuitive as any IDE, but ZBS also has tutorials and ample documentation if you’re looking for extra resources.To get you up to speed with using ZBS and Redis, some quick pointers… Once the IDE loads (deliciously fast BTW), you’ll want to switch from the default Lua interpreter to Redis – you can do that from the Project->Lua Interpreter menu or by clicking the window’s lower right corner and selecting it from the pop up menu.With the Redis interpreter selected, Lua scripts that you execute or debug will be sent to a Redis server. Every time you start a new IDE session, the plugin will prompt you for the server’s URI, and store it for the next session (for security purposes the password isn’t saved). Even if you don’t provide a password and connect to a password-protected Redis server, the plugin will prompt you for one.Specify your script’s KEYS and ARGS via the Project->Command Line Parameters dialog. In it, use the syntax that redis-cli --eval expects: a) a space-delimited list of key names that’s followed by b) a space, a comma (‘,’) and another space, which is in turn followed by c) a space-delimited list of arguments. This example shows how to pass the script two key names (foo and bar) and a single argument (42)…The editor is simple but packs a lot of punch on top of syntax highlighting, helpful tooltips and autocompletion for everything including the Redis Lua API. You can do regular expression searches, rename variables, jump to definitions, comment/fold/ident code and, my personal favorite, use Shift-Alt-Arrows to edit multiple lines at once.The real magic happens when you hit that little green triangle and start debuggin’. You can step through your script, toggle breakpoints, define watches and view the stack trace – everything just works as you’d expect (which is what makes this so damn cool IMO).But before you run off to try all this yourself (and you really should!), the last feature you need to know about is the Remote console. When in a debug session, the remote console allows you to type in Lua statements that are evaluated by the Redis server. You can also use your script’s variables, but because the statements are executed in a different frame, you won’t be able to modify the variables themselves.Even more handy is the ability to call any Redis API command (that’s allowed from the scripts) in the remote console by either prefixing the command with a ‘@’ or just by CAPITALIZING it. This allows you to change the database’s contents during the script’s execution (though remember that changes will be discarded or retained depending on LDB’s mode):The full Redis Lua API and libraries (e.g. cjson and cmsgpack) are supported, including the new debugger commands, so you can call redis.debug to print messages to the output console and trigger conditional breakpoints with redis.breakpoint. Quick jumping to a problematic line of code by double-clicking on the error in the console and infinite loop detection are included as well. The plugin’s configuration is documented at the end of its file.A couple of additional points to note:This being only the initial release for the plugin, you can expect improvements down the road. For that, we’re depending on your feedback – issues and pull requests can be opened directly at the repository, and for everything else there’s always Twitter and email."
526,https://redis.com/blog/redismart-retail-application-with-redis/,RedisMart: A Fully-Featured Retail Application With Redis,"July 29, 2021",David Maier and Doug Tidwell,"Do you remember RedisConf’s keynote demo? If yes, you might enjoy seeing the behind-the-curtain development of the retail application (RedisMart) that was presented. If not, then it’s time to watch Yiftach and Ash present it. Here’s the link to the video again:This article is the first of a series. It gives you some insights into the main requirements and the architecture of the RedisMart retail application by looking at how you can implement a product catalog, a distributed real-time inventory, and an AI-powered product search. You’ll also see how Redis Enterprise powers all those functionalities.As often in software development, let’s start by discussing some basic requirements. Here are some informally noted user stories:Now that we know the requirements, let me draw an idea of how Redis is able to help us:From there, it wasn’t too hard to imagine the following design:The blue boxes represent services. The red boxes show the databases that are used by those services.We followed some microservices approaches like:RedisMart offers a user interface that’s served by the web-shop web application. RedisMart has a frontend UI (the customer-facing retail website) and a backend UI (for managing the inventory). We implemented a bunch of services that are leveraged by the application behind the scenes.As mentioned before, this is the first article of a series. Please stay tuned to learn more about how we implemented the individual services.Now that we’ve looked behind the curtain, let’s see how the application looks on stage.The home page shows you the main product categories. A click on a category triggers a search query via the product catalog service by returning the first 16 products that belong to such a category.The “Search products” field allows you to perform a full-text search for products. It leads to the following search results page:The search results page has two sections: Faceted search and the actual result list. The faceted search can be used to further limit the search results. You can do so by filtering through main category, sub-category, price, and rating. We’ll talk about the implementation details of how such a faceted search is realized with RediSearch in part two of the blog series. Let me give you a hint by letting you take a look at RedisMart’s debug view:As you can see, tags and aggregation play a role.Clicking on the little camera icon in the upper right corner allows you to take an image of something that you want to find within the product catalog. The following photo of Doug didn’t find any other Dougs in our database……but it nicely found some headphones.Let’s assume that you decide on a pair of headphones and want to purchase them. After making your choice, RedisMart allows you to add them to your shopping cart. During the checkout process, you can decide to get them delivered or collect them at a close-by location.RediSearch’s geo-search powers this local pickup feature. The debug view gives you again a hint of how this is realized behind the scenes.As soon as a customer completes the purchase, the inventory service gets involved by reducing the number of items in stock. This brings us directly to the backend of the application, which allows us to manage the inventory. RedisMart visualizes how immediate inventory updates are observed on each of the replicated sites. A purchase in the US (GCP us-central1) is replicated with a blink of an eye to Europe (Azure north-europe).Once again, we’ll cover more details later. The main point here is that you can access the data from a close-by location at a very low network latency, while counter losses are prevented when data is modified concurrently across multiple sites.You can see the application in action by watching the following video:We hope you enjoyed reading this first part of the blog series about building a fully-featured retail application with Redis. As you can see, the Redis real-time data platform allows us to address requirements like immediate access (less than 100ms end-to-end latency) to product information by leveraging the document database capabilities of RediSearch + RedisJSON. A combination of RediSearch + RedisGears + RedisAI enables AI-powered image search for finding similar products within the product catalog. In addition, features like faceted and geo-search were covered. Last, but not least, we showed you that you can easily build a geo-replicated, real-time inventory based on Redis Enterprise’s Active-Active feature. With all that, the Redis real-time platform helps retail companies positively impact the overall buying experience, provide the best possible fulfillment experience for customers, and optimize inventories in the most cost-effective way.Do you want to learn more about the individual services that we implemented for RedisMart? Then stay tuned for the next blog article of the series!Want to try it yourself? Here are some links to get your hands dirty with Redis as a real-time data platform:A big thank you to everyone who contributed to this demo application:We want to dedicate this blog series to the primary developer of the RedisMart application, Martin Forstner. It comes with great sadness to share that he recently passed away. Martin’s knowledge, talent, and sense of humor were second to none. He was much more than a Software Engineer for Redis—he was a colleague, teammate, mentor, and a friend. Rest in peace, Martin, we miss you!"
527,https://redis.com/blog/holiday-shopping-season/,The Holiday Shopping Season Is Here. Are You Ready?,"November 3, 2021",Henry Tam,"Another year has flown by, and Black Friday/CyberMonday (and the holidays!) are right around the corner. If forecasts are accurate, shoppers and retailers are going to be busier than ever. Historically, purchases in November and December represent 20% of annual retail sales. This year,  NRF predicts 57% of shopping will be done online—and consumer expectations are higher than ever. Unfortunately, 75% of the retailers experienced increased latency and downtime during last year’s holiday season (Retail Holiday Reality Report 2020 by GCP Harris Poll). According to Akamai, one-second delay in response time can cut conversions by 7% and customer satisfaction by 16%.This year retailers will face even more challenges from supply chain disruptions, frictionless omnichannel experiences, and continued cost and margin optimizations. It’s vital for retailers to accelerate their digital initiatives to meet the holiday deluge and differentiate themselves at the same time. However, per Deloitte’s 2021 Retail Industry Outlook report, only three in ten executives rated their organizations as having mature digital capabilities.But it’s not only about a fast responsive e-commerce website. Retailers need to provide personalized omnichannel experiences that support “buy online, pickup in-store”(BOPIS), accurate views of what products are in stock, and durable shopping carts, all of which require real-time applications. Underlying a great shopping experience is the ability to use and present the latest product and customer data effectively and consistently across all sales channels. Below are three specific use cases for leveraging Redis Enterprise and retail data in real-time to enhance the customer shopping experiences and become a real-time retailer:Time is precious, and waiting for a website to respond hurts your brand and sales. What do we mean by slow? At Redis, we define a real-time user request and response as happening in under 100ms, which means your database has to provide the relevant data in 1ms or less while under heavy load (hello holiday shopping). And it has to happen at all stages of the customer journey—browsing of product catalog, online search, accurate and up-to-date information on products in stock, or shopping cart process—to offer a seamless, user-friendly, and fast shopping experience.Redis Enterprise improves response time by caching frequently used data from a slow disk-based database attached to the network with minimal resources and overhead. Or it can be the primary database with built-in search engine that supports fast continuous indexing of the product catalog and full-text search support—especially critical when you have hundreds or thousands of SKUs. Redis is not only ideal for caching because of its speed, but because it natively supports a variety of modern highly optimized data structures such as Hash, List, Sorted Sets, streams, and GeoSpatial indexes, as well as data models like JSON, graph, time series, and artificial intelligence (AI). This multi-model integration enables Redis Enterprise to work as a search engine, graph database, time-series database, document store, and more—all in a single database platform. Redis Enterprise’s in-memory architecture offers performance that scales linearly and high availability (99.999%) under heavy load  with diskless replication, instant failure detection, and single-digit-second failover.According to a Square report, 74% of retailers plan on using real-time inventory technology this year.  Without real-time inventory, retailers can’t optimize inventory, improve yield management, fine-tune supply-chain logistics, reduce shipping costs, and more. For example, if a customer orders an item online or at their favorite store, the retailer can speed up the process and reduce costs by having the item delivered from a closer store or warehouse, or even let the customer know when there’s an item at a dropbox or micro-store near them available for immediate pickup. It’s all about making sure items are in the right place, at the right time, at the right price.Delayed or inaccurate inventory information can frustrate customers, leading to shopping cart abandonment and order cancellations, lost revenues, higher costs, and brand damage. Redis Enterprise can provide data consistency and bilateral updates between stores, distribution centers, and other channels in real-time, while elastically scaling on-demand with zero downtime to support increased traffic during seasonal events like Black Friday. A real-time inventory management system can even support the use of predictive data analytics to send the appropriate stock to stores before it’s needed.Retailers need to sell and fulfill in more channels than ever: in-store, online, mobile, email, 3rd party marketplaces, and even via social media. Retail customers want the convenience of online shopping with the instant gratification of having the item in hand, like with the ability to buy online and pick up in-store, or have items shipped to their home, office, or other site, then return via mail. To tie all this together, retailers need to build an integrated end-to-end digital touchpoint that enables a personalized omnichannel experience.Retailers looking for new revenue streams are looking at models like subscriptions and memberships.  They’re also using personalization for tailored promotions and recommendations, say, with a monthly box of new clothing curated for a specific customer based on past browsing, purchases, or shopping cart history. Redis Enterprise can cache, manage, and search user sessions to provide a seamless and personalized journey across different e-commerce platforms. For deeper insight into customers’ habits and preferences, RedisAI can utilize AI and machine-learning models directly to data stored in Redis for real-time inferencing and analysis.If you’re not ready for the holiday season, put a powerful real-time in-memory NoSQL database like Redis Enterprise on your wish list—one that can scale up to handle Black Friday and peak holiday shopping. But it’s about more than speed. Redis can help maximize revenue and profits, retain customer loyalty, and delight your customers. Personally, I wish for all my favorite retailers to get ready and avoid those midnight calls from angry customers—especially me.——"
528,https://redis.com/blog/what-gap-and-alliance-data-say-about-the-power-of-redis-enterprise/,What Gap and Alliance Data Say About the Power of Redis Enterprise,"December 12, 2019",Redis,"You probably already know Redis as the most launched, most used, and most loved database in the world. What you may not know is that Redis has evolved from a caching and session-storage solution into the primary database for high-performance stateful applications.At AWS re:Invent 2019, our friends Junaid Fakhruddin and Bhilhanan Jeyaram from Gap and Brandon Mahoney of Alliance Data joined Redis Chief Product Officer Alvin Richards on the stage in a breakout session—Using Redis Beyond Caching—sharing their Redis journeys and how their organizations went from using Redis for simple use cases to powering business-critical applications with it. You can watch the entire session below, but let’s dive deeper into a few of the lessons they learned along the way.Gap and Alliance Data were looking for a new level of database sophistication in order to successfully implement real-time applications at enterprise scale. Gap needed an online inventory management platform that could quickly display store inventory and instantly calculate shipping dates as shoppers purchased items. But its existing inventory platform—which was a single monolithic application backed by a relational database—could run its order fulfillment algorithms only asynchronously. Similarly, Alliance Data worried that it wouldn’t be able to update its slow and unreliable legacy content management system quickly enough to keep up with newly released features from nimbler competitors.Both organizations focused on the value of performance and high availability when building an application meant to serve millions of customers, and the importance of choosing the fastest possible database, not only compared to relational databases, but also to other non-relational (NoSQL) databases.Redis Enterprise provides a fast database that helps everyone more efficiently build and operate applications. Redis’ easy-to-learn data structures and modules are flexible enough to cover a variety of use cases—and Redis Enterprise features such as persistent-memory storage and shared-nothing cluster architecture help reduce operational burden.But database performance is far from the only place where it’s critical to go fast. Organizations are increasingly turning to microservices and event-driven architectures to increase the speed at which they can respond to customer requests, and the need for real-time response is only boosting the pressure to cut end-to-end latency.Redis was a natural fit for caching in Alliance Data’s new microservices architecture, explained Brandon Mahoney, because it provided incredible database speed while remaining stable and predictable. For Gap’s real-time inventory management platform, meanwhile, Redis Enterprise’s RedisSearch module was the key to providing incredibly fast search queries and secondary indexing.Of course, speed that’s hard to harness isn’t always useful, which is why Redis Enterprise’s ease of use and operational simplicity across multiple data structures and modules is so important. Instead of spending time on repetitive maintenance or other forms of undifferentiated heavy lifting, Redis Enterprise helped the Gap and Alliance Data teams to quickly onboard developers and focus on building application logic and new features that deliver a competitive advantage.Conversations around digital transformation tend to revolve largely around development agility—the move to cloud infrastructure, DevOps practices, continuous integration and continuous deployment (CI/CD), containerization, and open source software components. But it’s easy to forget that all of this work is intended to deliver better software to people.Both Gap and Alliance Data want to deliver faster, more personalized, and more innovative experiences to their customers. Gap, for example, knew that in order to provide its more than 3 million customers with the best possible online shopping experience, it had to optimize fulfillment and shipping of online orders: Using Redis to power inventory searches resulted in a 100x improvement in query response!Alliance Data, meanwhile, quadrupled throughput and boosted application uptime from 70% to 99%. It was even able to onboard customers its systems could not previously support.Our theme for AWS re:Invent this year was “growth happens. Put simply, that means Redis Enterprise delivers the super-fast database you love, at the scale you need to cope with ballooning web traffic, increasing customer orders, and heightened demand for “instant” experiences. For Gap, that means building an online inventory management platform to display store inventory and calculate shipping dates in real time. For Alliance Data, it’s all about modernizing its legacy content management system fast enough to stay ahead of its competitors.To learn more about how your organization can leverage Redis Enterprise to successfully implement real-time applications at enterprise scale, visit the Redis Growth Happens page now!"
529,https://redis.com/blog/why-real-time-inventory-is-so-important-for-retailers/,Why Real-Time Inventory Is So Important for Retailers,"June 10, 2020",Fredric Paul,"This blog post was excerpted from our new e-book, Real-Time Inventory: Building Competitive Advantage with Redis Enterprise. Download it for free now!Meet Dave.Dave is a suburban dad looking to buy a new instapot to make preparing his family’s meals faster and more convenient. Dave doesn’t have a lot of time, so he’s looking online to find out whether nearby stores have the device available for immediate purchase or whether their online counterparts have ones available with guaranteed two-day delivery.For smart retailers with real-time inventory, Dave is an opportunity to make a sale and gain an appreciative customer. But for retailers that don’t know—in real time—what their stores have in stock, Dave represents an opportunity cost.If Dave doesn’t get his instapot delivered on time, or makes the long drive out to the mall across town and the item isn’t actually available, he’s likely to be extremely annoyed, go elsewhere to make his purchase, and perhaps even share his frustrations on social media.Yikes! Nobody wants that!That’s just one reason why managing real-time inventories across multiple physical and digital channels and locations is so important.But that’s only the beginning of the story. What if the retailer had lots of instapots in one location, but only one left in another? To avoid losing future sales, retailers should strive to democratize inventory between well-stocked stores and those low on available-to-promise products to avoid selling out the last item from a particular location.Similarly, real-time inventory is essential for optimizing order fulfillment and shipping costs. For example, Dave might order his instapot online or at his favorite store, but omni-channel retailers might be able to speed the process and reduce costs by having the device delivered from a closer store or warehouse, or even one already delivering to other locations near Dave. It’s all about making sure items are in the right place, at the right time, at the right price.But wait, there’s more!Without real-time inventory, retailers can’t optimize inventory, yield management, and supply-chain management. Relying on historical data makes inventory forecasting less accurate, increasing costs from carrying excess inventory and requiring unnecessary shipping.Retailers can also face reduced yields due to poor execution of enterprise-wide pricing and promotional strategies—for example, the inability to allocate available inventory to the highest-margin locations. Real-time inventory is also an essential component of a unified national order-fulfillment strategy, letting retailers pool geographically clustered store locations and warehouses to contribute to a single inventory.Finally, retailers without real-time inventory management risk product unavailability in the face of natural events and disasters. Before the event, real-time inventory management lets companies redirect fulfillment to healthy regions or proactively stock potentially impacted areas. For example, if a hurricane is predicted, retailers can boost inventory in the affected area of everything from food and water to sandbags and plywood. More importantly, the store database must remain available even if it becomes cut off from the enterprise. This lets the store continue operating with assurance that all of its inventory information will automatically sync with the enterprise database—without any conflicts—once connections are re-established.In sum, real-time inventory enables an omni-channel retail strategy, delivering a unified, seamless, and consistent customer experience across all channels, including in-store, websites, mobile apps, email, and social media. A typical customer journey, for example, might begin with discovery on social media, browsing on a mobile app, purchasing in-store, shipping to a home address, authorizing return via email, and physically returning the item via snail mail.This approach democratizes regional inventory based on geographic availability instead of limiting the sales opportunity to a single store location. And it enables retailers to implement and monitor key capabilities like shipping to and from a store, finding items in a particular store, reserving an item in store for pickup, enabling customers to buy online and pick up in store, and many more. Without it, retailers risk leaving money on the table, inflating costs, frustrating customers, and reducing the accuracy of their forecasts and planning.This excerpt shows why real-time inventory is essential for large omni-channel retail enterprises, but building and maintaining these complex systems in the real world can be a daunting task. Want to learn why traditional inventory systems based on RDBMS technology don’t measure up in the modern omni-channel retail environment? Download the full e-book, Real-Time Inventory: Building Competitive Advantage with Redis Enterprise, and see how Redis Enterprise supports real-time inventory management by providing optimum database performance at peak scale and ensuring deep consistency among multiple channels (stores/websites/mobile/social/more) while minimizing infrastructure and technology sprawl."
530,https://redis.com/blog/redis-labs-at-aws-reinvent-2019/,Redis Labs at AWS re:Invent 2019,"December 10, 2019",Mike Kwon,"Like everyone else, we rocked a week of sleepless nights filled with networking, partying, and gambling, all in the name of AWS re:Invent 2019… but we wouldn’t have it any other way. More than 65,000 cloud professionals descend on Las Vegas the week after Thanksgiving for the cloud industry’s biggest event of the year, and we’re happy to say that this was our favorite and most successful re:Invent yet.This year, our theme was Growth Happens: as your company and database needs grow, Redis Enterprise offers the best Redis for dealing with that growth smoothly and effectively. Read on for how we tackled re:Invent, including our greenery-filled booth, #RedisLive news crew, sumo wrestling extravaganza, and much more.And to really feel the energy, check out this quick video of highlights from the week:The #RedisLive news crew was all over the Las Vegas strip this year. We took our crew to the streets to ask re:Invent attendees three simple questions:Check out their answers on Twitter, but spoiler alert… most people interpreted “cache” as something more Vegas than cloud.Even if you didn’t spot the #RedisLive crew, you might have seen us in the sky—our digital billboards told the Redis story in McCarran International Airport, along the freeway, and on the Strip.Plus, we polled Twitter on attendees’ re:Invent experiences. Turns out that while most people were walking one to three miles a day, Uber and Lyft were voted the best way to get around town. But by day four, many people were feeling a bit lost.We spent our Wednesday evening at the Omnia Nightclub watching the Sumo Logic Slam Jam, a sumo wrestling event we hosted with along Sumo Logic, PagerDuty, Cloudflare, McAfee, and Hewlett Packard Enterprise.Each company sponsored a wrestler who brawled with the others over the course of roughly 15 rounds. We could not have been more excited when, in an epic tussle in the center of the club floor, the neon lights danced and the crowd roared as Takeshi, Redis’ sponsored sumo wrestler, tossed his opponent out of the ring to win the tournament!It was a huge upset, as Takeshi was smaller than the other wrestlers, but just as Redis database technology is lighter and faster than our competitors, his speed and agility combined for a winning strategy that allowed him to knock his last opponent off his feet and out of the ring.Hundreds of attendees packed the room for our Thursday breakout session, Using Redis Beyond Caching. Our own Chief Product Officer Alvin Richards kicked off the session with a short recap of Redis’ 11-year history and its popularity as the most-launched, most-used, and most-loved database.Then our customers Alliance Data and Gap shared how they are using Redis to grow their businesses. Alliance Data recently transitioned to a microservices architecture, helping to boost its uptime from 70% to 97 – 99%. And Gap shared how it uses Redis for inventory management for more than 3,100 stores and 9 distribution centers globally.We also announced the winner of our raffle for a LEGO Millennium Falcon. Craig Bryan, who had traveled from Canada for re:Invent, became the lucky owner of the iconic starship.If you missed it, you can watch the full session here:Vines, plants, and leaves covered our entire Redis booth in the Venetian. Thousands of people attended our educational sessions in our booth theater, often with standing-room-only crowds. Attendees were able to learn from our Redis experts about microservices architectures, visualizing data with RedisInsight, 10 ways to scale their websites, and more. Plus, attendees left holding our special “Growth Happens” t-shirts, while more than 180 people opted to plant a tree instead of grabbing a t-shirt, and Redis more than matched that to plant a total of 500 trees.Our Redis Geeks were also available to answer any questions on Redis. Plus, we raffled off a VanMoof electric bike and electric scooters, and gave away plenty of swag, including limited-edition pins and socks.It wasn’t all Redis experts, though. The booth also welcomed Redis customers, like MGM Grand and HolidayMe, who were available to answer questions about how they used Redis to grow their businesses. Rajat Panwar, Chief Technology Officer at HolidayMe, explained that if Redis was a superhero, its strength would be in simplicity: “I’ve been using Redis for the past seven years. It was pretty simple even using it in that instance, and right now it’s still pretty simple even though the performance and features have increased.”Redis was also in the news at AWS re:Invent:CTO Dose’s Keith Townsend spoke with Redis Chief Product Officer Alvin Richards about open source and enterprise IT.Plus, stay tuned for a podcast HolidayMe recorded with Alvin at our booth, launching this week.Above all, we had a blast engaging with developers and database professionals passionate about Redis and introducing people who were new to it. See you all again next year in Vegas, or in one of our upcoming RedisDays events:Redis Day SeattleWhen: January 13–14, 2020Where: Hyatt Regency, 808 Howell St, Seattle, WA 98101Redis Day BangaloreWhen: January 21–22, 2020Where: Taj Yeshwantpur, 2275, Tumkur Road, Yeshwantpur, Bengaluru, 560022RedisConf 2020When: May 12–14, 2020Where: SVN West, 10 S Van Ness Ave, San Francisco, CA 94103"
531,https://redis.com/blog/how-to-build-an-app-that-allows-you-to-build-real-time-multiplayer-games-using-redis/,How to Build an App That Allows You to Build Real Time Multiplayer Games Using Redis,"February 10, 2022",Redis Growth Team,"Online games are rapidly becoming one of the most popular forms of entertainment. The technological boom, along with our increased usage of the internet, has made games more accessible, allowing people from all over the world to compete against each other.Because of this geographical spread of players, games must operate with a low latency database to allow users to interact with each other in real time. Any instances where there’s a delay between the command and gaming response will create friction and hamper engagement.Advanced programmer Tinco Andringa took this challenge on and created his own gaming platform, Topscorio. By using Redis, data is sent, processed, and retrieved in real time with exceptional consistency, creating a gaming platform that’s hyper-responsive to player commands.Let’s take a look at how Tinco put this application together. But before we go any further, we’d like to point out that we have an exciting range of applications for you to check out on the Redis Launchpad. So make sure to give them a peek after this post!You’ll build an app that will allow developers to create their own online multiplayer game. Topscorio optimizes the gaming experience by keeping track of high scores without having to worry about players cheating and managing big network infrastructures.Below we’ll show you how to build this application from the bottom up, highlighting what components you’ll need along with its functionality.git clone https://github.com/redis-developer/topscorioCreate your free Redis Enterprise Cloud account. Once you click on ‘Get Started,’ you’ll receive an email with a link to activate your account and complete the signup process.Next, you’ll have to create a Redis Enterprise Cloud subscription. In the Redis Enterprise Cloud menu, click ‘Create your Subscription.’Follow https://developer.redis.com/create/rediscloud/ to create Redis Enterprise Cloud with RedisJSON module enabled.Click ‘Create Database’. Enter the database name and choose RedisJSON module.Create an empty .env file and add the below parameters:Fill in database addresses, passwords, and the sendgrid API key as needed.Then in one terminal run:Then, in another terminal, run the following command:And navigate to http://localhost:5010 in your browser.Click ‘Start now’ and enter the email address:The app has an example chess game implemented, opening up multiple browser sessions to test this.Users and sessionsUser and session storage is managed in backend/authentication_store.ts. This generates a unique session token for each session. Here’s an example:Sessions are resumed by fetching the setting from localStorage and then retrieving the session from Redis. Here’s an example:New authentication tokens are stored in the same way but with a shorter expiry.Users are stored using the following command:Note: The email address is used as the key.Users are retrieved using the following command:Note: This is used as the path to retrieve the whole object.Games and game logs storage are managed in backend/games_store.ts. A cache of all games is stored in the Redis database under the following key: all-games. When the server starts up it’s first retrieved using the following command:If it’s null, then it will populate with the below command:When the server starts up, it will subscribe to the newest channel on the games database as well as the open channel on the game logs database. It does this with the SUBSCRIBE command like so:Whenever a client shows interest in a channel, the server will initiate a subscription. For example, when a user joins a game with the id abc123, the server will subscribe to that game using the following command:At the moment, the server does not UNSUBSCRIBE. When a user creates a game the server publishes a message on the newest channel, like so:Moreover, the server carries out the following command to add the user’s game to all of the game’s newest array:To make sure that it doesn’t overflow, the server will follow up with the following command to shorten its backup:Once created, the games are retrieved using the following command:Signing in/signing upBoth signing in and signing up share the same process. First, you need to click on the ‘Start Now’ button on the homepage. Next, you’ll be required to provide your email address for a confirmation email. Open this email in your inbox, copy the sign-up link and then paste it onto your browser.You’ll then be taken to a new page. Click on the ‘Start Now’ button to get started.Creating a new gameWhen you log in you’ll immediately land on the ‘Create a new game’ page. At the top, enter the name of your game in the given field. You’ll then need to paste your game logic in the big empty box below.Once you’ve done this, press ‘create game’ to proceed to the next step.Players today expect games to be hyper-responsive in all aspects, where delays between commands are almost non-existent. Even just one lag is enough to damage the user experience and frustrate players, which will only push them towards games that can run smoothly without any hiccups.With Redis, Tinco was able to maximize the gaming experience through its ability to provide users with real-time responses. Components within the architecture became more interconnected where data transmission became both seamless and consistent, allowing players to compete with each other online in real-time, irrespective of their location.To get the full visual demonstration of how this application was made, watch Timo’s YouTube video here. If you enjoyed this article, you should know that this is only the tip of the iceberg when it comes to Redis’ capabilities.All around the world, programmers are using Redis to build innovative applications that are having a profound impact on everyday life. Make sure to check out the Redis Launchpad to have access to an exciting array of applications to inspire you.Tinco is a leading software engineer at AeroScan. Make sure to visit his GitHub page to keep up to date with all of the projects he’s involved with."
532,https://redis.com/blog/tech-behind-dynamic-email-marketing/,The Tech Behind Dynamic Email Marketing,"September 19, 2017",Redis,"E-mail marketing is an effective way to reach customers, but it comes with a variety of challenges that can be difficult to overcome. The nature of email is static after delivery: once the message has left your e-mail server, you have very little control — or do you? Let’s take a look at a technique that can provide some level of dynamic control after you send your e-mail.Imagine you’re sending out an email with three deals. You’re not sure which deals will perform the best. After the email is sent, you’ll have a very clear picture — which had the best click-through-rate and frequency. Herd mentality is a very real thing in e-commerce. Telling a user that an item is “hot” or “popular” can give a sense of confidence in the item and we can communicate this via a “badge.” But to be effective, it should be real-time and not from a previous cycle.To achieve this, we’ll be using Redis and a Node.js server as well as whatever tool you would use to normally send out your emails. The heart of this technique is pointing both your badge image URL and item URL not at the actual image or web page, but at intermediate URLs.The intermediate badge URL calculates the “hotness” and the popularity of the item with data from Redis then sends an HTTP 307 forward request back to the email client. The forward is pointed at the correct badge image — one for hot, one for popular and a special empty/transparent image for items that are not hot nor popular.The item intermediate URL is a little simpler — this URL records that a visit occurred at a specific time and increments a counter for number of click-throughs. Once complete, it always forwards onto the same destination URL.Here is a diagram of the whole process:For the purpose of this script, an item is considered “hot” if it has been accessed several times in the past few minutes. To determine the “hotness” were counting minutes and counting bits.Redis is able to flip individual bits in a string and we can exploit this feature to be extremely granular in a minimal amount of storage space.Each “item” is represented by a string in Redis with a key derived from both the campaign and some form of item identifier. We’ll make the key look something like this:We need to start counting from a fix point in time — much like the UNIX epoch time system, we’ll just use some point. For this simple example, we’ll just pick July 1, 2017 at midnight (GMT) — the countEpoch. We’ll compare this with the current timestamp using the date-utils Node.js module.As an example, at 1am on July 1, 2017, the minutesSinceEpoch would be 59 (not 60, because of zero-based counting). We’ll flip a single bit each time a person interacts with an item. Note that if two users interact with the same item during the same minute period, it is only counted once — we’re getting the activity rather than the count in this case. This is very space efficient and provides some very rich data with a minimal storage footprint. Each day from the countEpoch would consume 180-bytes, ~5.4kb a month or ~65kb per year. Not bad.To flip the bits, we can use the Redis function SETBIT with the offset being the minutesSinceEpoch and the value being a 1, representing a visit. Give the example above (1am on July 1), our Redis command would look like this:SETBIT is a computationally inexpensive command, being O(1) and otherwise on the app-level we just need to do a little subtraction to get the minutesSinceEpoch.Querying the “hotness” bits requires a little more work. We can use the Redis command BITCOUNT. With this command we can find the number of 1’s set a given string. While this would be useful on it’s own, we want to add in a recency- for our application it’s irrelevant to determine that the item was being interacted with 2 months ago — we need to find just the recent activity. With BITCOUNT we can supply the optional start and end arguments to slice out only a little bit of the data. The start and end arguments also “wrap” with negative numbers, so you can only the count the bits at the end of the data. In our example, let’s work with the last three bytes:While this command sets bits, it’s important to understand that Redis only deals with bytes in the range arguments of BITCOUNT. This introduces some sloppiness into the calculation: Think about if you set the 17th byte in the example below:Running the above command would count five 1 bits of the last 3 bytes, which is not perfectly 24 minutes as 3 bytes might lead you to think. So, we’re actually saying that 5 of the last 16–24 minutes were active. In our use-case, we’re really just trying to get some relative sense of hotness not an absolute so it’s acceptable given the space/time efficiencies.In our use case, we’re going to find the item with the most number of click throughs regardless of time. This is a straightforward process in Redis. The sorted set data structure excels at creating leaderboards — our popularity is calculated by finding the top item among all those in the group.To record the popularity, we just need to increment the score of an item in a ZSET every time it is clicked on. We can make this happen by using the ZINCRBY command. Their is a single key for the entire campaign and the member is the item ID. We want to increment it by one. In Redis, we’d do something like this:To be clear, the score is just acting as a counter. So, if the camera item in the august17 campaign was clicked 47 times, 46 on day one and once more on day 400, the score would be 47.For each badge, we’ll check to see if the related itemId is the most popular one. We can do this by running:If the response of this matches the itemId then the item is the most popular.As you might have noticed from the process diagram, this produces a very “chatty” system with several moving parts. HTTP forwards add an extra network round trip and additional transit time. It’s critical to minimize the amount of time spent both recording and calculating the popularity. Redis is well known for being low latency and quick to calculate values.The dynamics of this type of email being delivered to a large list of clients at one time also presents challenges. Let’s say your email is delivered to 500,000 recipients in a short amount of time and they start interacting with and view the email simultaneously. With a single instance of Redis, you may be able to weather the storm, but the single-threaded nature of the server could create higher than desired latencies under load. In addition, we’re creating a single point of failure that is risky. A good match for this use-case would be Redis Enterprise. Redis Enterprise can provide high-availability with automatic failover to ensure that your email assets have no single point of failure and it also can provide clustering that can spread the load out over multiple threads and/or machines reducing latency.Beyond these other use cases, the data can be leveraged in other ways. The “hotness” data is a bitmap that can be calculated to see how many hours of activity a given item has over a given range of time using BITCOUNT with offsets based on the calculated minutes. You can also aggregate the activity of multiple items using BITOP:Then you can slice out a given time period of camera-and-watch to determine the hotness for a timeframe.The technique of shimming Node and Redis between your interaction points and assets is not limited to serving out email images but can be integrated into any platform to dynamically change the images as data changes. In it’s current state, it could be applied to a e-commerce system with very little modification to deliver the same “badging” information. Integrating with other data could vary the resulting badge based on more than just “hotness” and popularity but also variables like item stock (“Almost Gone!”) or discounts (“Save 20%”). In addition, paired with a user system it could be good for personalized deals (“Reorder” or “Our Pick for You”). Imagine even integrating customer location conditions (“Beat the Heat” on an air conditioner if the customer’s location is warm).The source code can be found on Github.(This post originally appeared in the Node / Redis series on Medium)"
533,https://redis.com/blog/set-docker-based-development-environment-active-active-redis-enterprise/,How to Set Up a Docker-based Development Environment for Active-Active Redis Enterprise,"October 1, 2018",Roshan Kumar,"Redis Enterprise as an active-active database is ideal for geo-distributed apps. Its architecture is based on breakthrough academic research surrounding conflict-free replicated data types (CRDT). This approach offers many advantages over other active-active databases, including:Recently, we published a tutorial on how to develop apps using active-active Redis Enterprise.  In order to simulate the production setup, developers or testers need a miniaturized development environment — something that’s easy to create with Docker.Redis Enterprise is available on the Docker hub as redis/redis, and we’ve shared detailed step-by-step instructions on how to set up Redis Enterprise on Docker on the Redis Enterprise documentation page and the docker hub itself.In this blog, we’ll walk you through the steps to create your Docker-based Redis Enterprise cluster — all through the command line. Here’s a high-level overview of the process (more details below):Before you start, make sure you have a bash shell in place, and enough memory allocated to your docker processes. You can check your memory by going to Docker -> Preferences -> Advanced.The following script creates a CRDT-based Redis Enterprise database on a 3-node cluster. Save this in a file and give it a name, like “create_3_node_cluster.sh. Then change the mode to executable (chmod +x create_3_node_cluster.sh), and run the script ([path]/create_3_node_cluster.sh).#!/bin/bash# Delete the bridge networks if they already existdocker network rm network1 2>/dev/nulldocker network rm network2 2>/dev/nulldocker network rm network3 2>/dev/null# Create new bridge networksecho “Creating new subnets…”docker network create network1 –subnet=172.18.0.0/16 –gateway=172.18.0.1docker network create network2 –subnet=172.19.0.0/16 –gateway=172.19.0.1docker network create network3 –subnet=172.20.0.0/16 –gateway=172.20.0.1# Start 3 docker containers. Each container is a node in a separate network# These commands pull redis/redis from the docker hub. Because of the# port mapping rules, Redis Enterprise instances are available on ports# 12000, 12002, 12004echo “”echo “Starting Redis Enterprise as Docker containers…”docker run -d –cap-add sys_resource -h rp1 –name rp1 -p 8443:8443 -p 9443:9443 -p 12000:12000 –network=network1 –ip=172.18.0.2 redis/redisdocker run -d –cap-add sys_resource -h rp2 –name rp2 -p 8445:8443 -p 9445:9443 -p 12002:12000 –network=network2 –ip=172.19.0.2 redis/redisdocker run -d –cap-add sys_resource -h rp3 –name rp3 -p 8447:8443 -p 9447:9443 -p 12004:12000 –network=network3 –ip=172.20.0.2 redis/redis# Connect the networksdocker network connect network2 rp1docker network connect network3 rp1docker network connect network1 rp2docker network connect network3 rp2docker network connect network1 rp3docker network connect network2 rp3# Sleep while the nodes start. Increase the sleep time if your nodes take# longer than 60 seconds to startecho “”echo “Waiting for the servers to start…”sleep 60# Create 3 Redis Enterprise clusters – one for each network. You can login to# a cluster as https://localhost:8443/ (or 8445, 8447). The user name is# r@r.com, password is password. Change the user echo “”echo “Creating clusters”docker exec -it rp1 /opt/redis/bin/rladmin cluster create name cluster1.local username r@r.com password testdocker exec -it rp2 /opt/redis/bin/rladmin cluster create name cluster2.local username r@r.com password testdocker exec -it rp3 /opt/redis/bin/rladmin cluster create name cluster3.local username r@r.com password test# Create the CRDB echo “”echo “Creating a CRDB”docker exec -it rp1 /opt/redis/bin/crdb-cli crdb create –name mycrdb –memory-size 512mb –port 12000 –replication false –shards-count 1 –instance fqdn=cluster1.local,username=r@r.com,password=test –instance fqdn=cluster2.local,username=r@r.com,password=test –instance fqdn=cluster3.local,username=r@r.com,password=testRun redis-cli on ports 12000, 12002 and 12004 to verify that you can connect to all three Redis Enterprise nodes. If you are connecting your app to Redis Enterprise, you will need three instances of your app connecting to the three different ports. For example:$ redis-cli -p 12000127.0.0.1:12000> incr counter(integer) 1127.0.0.1:12000> get counter“1”Splitting the networks helps you to introduce “network partition” between Redis Enterprise replicas. When you design your app, you must design it to work seamlessly when the replicas are disconnected. This script helps you to isolate the three replicas. Save this script in the file “split_networks.sh”, and change the mode to make it executable (chmod +x split_networks.sh) before running it.#!/bin/bashdocker network disconnect network2 rp1docker network disconnect network3 rp1docker network disconnect network1 rp2docker network disconnect network3 rp2docker network disconnect network1 rp3docker network disconnect network2 rp3When you run the script “split_netorks.sh”, the local replicas will stop sharing their database updates with their peers. Restoring the connection will enable them to exchange all updates and arrive at the same final state, thanks to the strong eventual consistency offered by Redis Enterprise. The script below restores network connections between the replicas. Save this in a file “restore_networks.sh”, and change the mode to make it executable (chmod +x restore_networks.sh).#!/bin/bashdocker network connect network2 rp1docker network connect network3 rp1docker network connect network1 rp2docker network connect network3 rp2docker network connect network1 rp3docker network connect network2 rp3When you are done with your development and testing, you could stop all three nodes of Redis Enterprise by running the following script. Save the script in a file, name it “stop.sh”, and change the mode to make it executable (chmod +x stop.sh).#!/bin/bashdocker stop rp1 rp2 rp3docker rm rp1 rp2 rp3docker network rm network1docker network rm network2docker network rm network3That’s it. With this as a starting point, you now have your very own Docker-based Redis Enterprise active-active setup. If you have any questions, feel free to email us at product@redis.com.If you are looking to get started with Redis Enterprise in the cloud, you can sign up for free here."
534,https://redis.com/blog/5-reasons-you-should-attend-redisconf-2021/,5 Reasons You Should Attend RedisConf 2021,"March 8, 2021",Mike Anand,"Rediscover the power of real-time data at RedisConf 2021! Registration is now open for our annual real-time data conference, to be held virtually on April 20-21. Thousands of developers, cloud architects, devops professionals, and IT leaders from around the globe will join Redis product leaders, architects, partners, and experts to dive into the latest Redis capabilities and best practices.Explore demos, pro tips, and new features designed to help you build competitive advantage by delivering real-time digital experiences at any scale, for critical use cases like gaming leaderboards through fraud detection for financial services to real-time inventory for retailers. Brush up on your Redis skills with our training courses or show off what you know by building with Redis in our hackathon, where we’re giving away $100,000 total in prizes! And throughout it all, you’ll meet and network with other Redis enthusiasts eager to share and learn from each other.Here are five compelling reasons to attend RedisConf 2021:This year we’re offering three main tracks:With something for everyone, you’ll be sure to learn a new feature, pro tip, or best practice for your Redis applications. All breakout sessions will be available for on-demand viewing, but the keynote sessions that will be followed by fireside chat with industry luminaries will be broadcasted live. Make sure you mark your calendars, as those are keynotes you don’t want to miss!  Stay tuned for our full list of speakers!We’re offering four brand new training courses, each combining a video-based, on-demand component coupled with optional, but recommended, virtual live sessions with the instructor. In addition to the live sessions, you’ll have exclusive access to a dedicated Discord channel to support your learning. The four courses are:The courses and live sessions will be conducted the week prior to RedisConf, beginning on Monday, April 12, but the courses will be available on-demand throughout the rest of the conference.Redis is not only blazing fast, it’s also incredibly versatile—you can build anything and everything with Redis instead of having to rely on multiple databases. The theme of the Hackathon is “Build on Redis.” Whether collecting billions of events (Redis Streams), storing JSON (RedisJSON), searching (RediSearch), analyzing (RedisTimeSeries), recommending (RedisGraph), processing events (RedisGears), or detecting fraud (RedisAI), with Redis you can do it all—and do it in efficiently and in style.Join the hackathon to show us what you can build on Redis for a chance to win big prizes totaling $100,000! Registration for the hackathon is also open now and hacking starts on Thursday, April 15.Last year we invited attendees to explore the virtual Redis world, where nearly 4,000 Redis lovers created custom avatars, hung out in themed group-discussion rooms, talked to other attendees in the Lounge, and played more than 1,500 games of Redis Pong.We’re planning an even better conference for 2021, with plenty of opportunities for you to connect with peers and win exciting prizes. Stay tuned because you won’t want to miss all the fun!Seriously—we’re providing full access to our training courses, keynotes, breakout sessions, networking opportunities, and more, completely free of charge. And even if you can’t attend RedisConf live on Tuesday and Wednesday, April 20-21, you’ll still get access to all the sessions on-demand to view whenever it’s best for you."
535,https://redis.com/blog/iso-certification/,Redis Gains ISO 27017 and 27018 Certifications,"July 12, 2023",Redis,"Redis announces the company’s certification for compliance with ISO 27017:2015 and ISO 27018:2019, added to our existing ISO 27001:2013 certifications.Every organization is concerned about securing its cloud environments, and cognizant of the challenges of doing so. Cloud security frameworks are gaining traction in the security community as one way to address the issues, by providing specific guidance about controls (including intent and rigor), control management, validation and other information related to securing a cloud use case. One advantage of such frameworks is that they have certifications to assure users that an organization meets expectations. And now Redis has added two more certifications.Prominent among them, the International Organization for Standardization (ISO) is an international group that establishes technology and business standards, typically focused on data protection and security, with third-party audit practices to confirm adherence to best practices. Performed by independent, third-party auditors for Redis, these certifications demonstrate the maturity of our security program, and provide additional confidence in our security and privacy practices.These certifications further demonstrates our commitment to keeping your data secure and private. The company’s full package of customer security and compliance documentation is available in the Redis Customer Trust Center.Want more information about Redis security features? We have an entire Redis University course about security topics, covering access control, data protection and encryption, secure Redis architectures, and secure deployment techniques. And it’s free!Redis Cloud Introduces Short-Lived TLS Certificates"
536,https://redis.com/blog/bloom-filter/,Bloom Filter Datatype for Redis,"July 14, 2022",Redis,"Click here to download the Probabilistic module which supports scalable bloom and cuckoo filters.A Bloom filter is a probabilistic data structure conceived in 1970 by Burton Howard which provides an efficient way to verify that an entry is certainly not in a set. This makes it especially ideal when trying to search for items on expensive-to-access resources (such as over a network or disk): If I have a large on-disk database and I want to know if the key foo exists in it, I can query the Bloom filter first, which will tell me with a certainty whether it potentially exists (and then the disk lookup can continue) or whether it does not exist, and in this case I can forego the expensive disk lookup and simply send a negative reply up the stack.While it’s possible to use other data structures (such as a hash table) to perform this, Bloom filters are also especially useful in that they occupy very little space per element, typically counted in the number of bits (not bytes!). There will exist a percentage of false positives (which is controllable), but for an initial test of whether a key exists in a set, they provide excellent speed and most importantly excellent space efficiency.Bloom filters are used in a wide variety of applications such as ad serving – making sure a user doesn’t see an ad too often; likewise in content recommendation systems – ensuring recommendations don’t appear too often, in databases – quickly checking if an entry exists in a table before accessing it on disk, and so on.Bloom filters work by running an item through a quick hashing function and sampling bits from that hash and setting them from a 0 to 1 at particular interval in a bitfield. To check for existence in a Bloom filter, the same bits are sampled. Many item may have bits that overlap, but since a hashing function produce unique identifiers, if a single bit from the hash is still a 0, then we know it has not been previously added.Most of the literature on Bloom filter uses highly symbolic and/or mathematical descriptions to describe it. If you’re mathematically challenged like yours truly, you might find my explanation more useful.A Bloom filter is an array of many bits. When an element is ‘added’ to a bloom filter, the element is hashed. Then bit[hashval % nbits] is set to 1. This looks fairly similar to how buckets in a hash table are mapped. To check if an item is present or not, the hash is computed and the filter sees if the corresponding bit is set or not.Of course, this is subject to collisions. If a collision occurs, the filter will return a false positive – indicating that the entry is indeed found (note that a bloom filter will never return a false negative, that is, claim that something does not exist when it fact it is present).In order to reduce the risk of collisions, an entry may use more than one bit: the entry is hashed bits_per_element (bpe) times with a different seed for each iteration resulting in a different hash value, and for each hash value, the corresponding hash % nbits bit is set. To check if an entry exists, the candidate key is also hashed bpe times, and if any corresponding bit is unset, the it can be determined with certainty that the item does not exist.The actual value of bpe is determined at the time the filter is created. Generally the more bits per element, the lower the likelihood of false positives.In the example above, all three bits would need to be set in order for the filter to return a positive result.Another value affecting the accuracy of a Bloom filter is its fill ratio, or how many bits in the filter are actually set. If a filter has a vast majority of bits set, the likelihood of any specific lookup returning false is decreased, and thus the possibility of the filter returning false positives is increased.Typically Bloom filters must be created with a foreknowledge of how many entries they will contain. The bpe number needs to be fixed, and likewise the width of the bit array is also fixed.Unlike hash tables, Bloom filters cannot be “rebalanced” because there is no way to know which entries are part of the filter (the filter can only determine whether a given entry is not present, but does not actually store the entries which are present).In order to allow Bloom filters to ‘scale’ and be able to accommodate more elements than they’ve been designed to, they may be stacked. Once a single Bloom filter reaches capacity, a new one is created atop it. Typically the new filter will have greater capacity than the previous one in order to reduce the likelihood of needing to stack yet another filter.In a stackable (scalable) Bloom filter, checking for membership now involves inspecting each layer for presence. Adding new items now involves checking that it does not exist beforehand, and adding it to the current filter. Hashes still only need to be computed once however.When creating a Bloom filter – even a scalable one, it’s important to have a good idea of how many items it is expected to contain. A filter whose initial layer can only contain a small number of elements will degrade performance significantly because it will take more layers to reach a larger capacity.Bloom filters work by running an item through a quick hashing function and sampling bits from that hash and setting them from a 0 to 1 at particular interval in a bitfield. To check for existence in a Bloom filter, the same bits are sampled. Many item may have bits that overlap, but since a hashing function produce unique identifiers, if a single bit from the hash is still a 0, then we know it has not been previously added.Bloom filters have been used with Redis for many years via client side libraries that leveraged GETBIT and SETBIT to work with a bitfield at a key. Thankfully, since Redis 4.0, the ReBloom module has been available which takes away any Bloom filter implementation overhead.A good use case for a Bloom filter is to check for an already used username. On a small scale, this is no problem, but as a service grows, this can be very taxing on a database. It is very simple to implement this with a ReBloom.First, let’s add a handful of usernames as a test:Now, let’s run some test versus the Bloom filter.As expected, fred_is_funny yields a 0. A response of zero means we can be sure that this username has not been used. A response of 1 means it might have been used. We can’t say for certain as it might a case of overlapping bits between multiple items.Generally, the chances of false positives are low, but non-zero. As the Bloom filter “fills up” the chances increase. You can tweak the error rate and size with the BF.RESERVE command.In this video, Guy Royse, developer advocate will explain what a Bloom filter is, how they work, and how to use one in Redis.Redis features are written in high performance C, and are able to expose their own commands and data types, as well as define how those data types get persisted. This allowed us to simply create a very powerful Bloom filter module, and have it easily added to Redis.Getting the module and using it is very straightforward:You should first download and compile the module:You should now have a rebloom.so in the rebloom directory.Once you’ve built the filter module you, point your redis.conf or the redis command line to the module using loadmodule or –loadmodule respectively:redis.conf:Command-lineYou can play with it a bit using redis-cli:You can also create a custom Bloom filter. The BF.ADD command creates a new Bloom filter suitable for a small-ish number of items. This consumes less memory but may be less ideal for large filters:Rebloom uses a modified version of libloom, with some additional enhancements:Once all was done, I benchmarked it and compared it to some other implementations. I wrote this filter initially because a previous filter implementation was too slow. Compared to a Lua implementation which yielded 30k/sec, redablooms was slower with 20k/sec.Running the equivalent command with rebloom:With pipelining and perhaps modifying the thread count, it would have been possible to get even nicer numbers, but a fair benchmark is an apples-to-apples comparison.I compared bloomd’s own benchmark to an equivalent rebloom commandset. Bloomd by default uses an initial capacity of 100k and an error ratio of 1/10k, or 0.0001. Its benchmark program simply sets 1,000,000 items and then reads them. It uses fire-and-forget semantics when setting items so network latency doesn’t become a factor when sending the commands:So we get 1M sets in 4 seconds, or 250K op/s, and 1M reads in 6 seconds, or 166K ops/sec.The equivalent with rebloom:I let this loop for a while because unlike the bloomd benchmark which uses sequential keys, redis-bench uses random IDs. this means that there would be a greater chance for collision.Rebloom performs at 370k/s, or about 50% faster than bloomd.For reading:Which is about 150% faster than bloomd!Finally, I added a BF.DEBUG command, to see exactly how the filter is being utilized:This outputs the total number of elements as the first result, and then a list of details for each filter in the chain. As you can see, whenever a new filter is added, its capacity grows exponentially and the strictness for errors increases.Note that this filter chain also uses a total of 5MB. This is still much more space efficient than alternative solutions, since we’re still at about 5 bytes per element, and the uppermost filter is only at about 12% utilization. Had the initial capacity been greater, more space would have been saved and lookups would have been quicker.You can download rebloom yourself at https://github.com/RedisModules/rebloom. If you’re using Redis Enterprise, it will be bundled with version 5.0.Let me know if you think you can make it even faster, or file a github issue if you are having problems."
537,https://redis.com/blog/diving-into-redis-6/,Diving Into Redis 6.0,"April 30, 2020",Redis,"Download the latest version by clicking here.You know the warning on the shallow end of the pool where it says “NO DIVING”? Well, the new Redis 6 is no shallow update to the world’s most-loved database—it’s so deep you can dive right in. Now that Salvatore Sanfilippo has made Redis 6 generally available, let’s take a dip in the new changes and features.The new stuff can be divided into a few different categories: security, performance, ease-of-use, and even some entirely new functionality. Each category holds a number of improvements, so read carefully to learn how they can fundamentally change how you use Redis.Perhaps the biggest, most game-changing feature of Redis 6 are access control lists (ACLs). ACLs bring the concept of “users” to Redis. Each user can have a defined set of capabilities that dictate which commands they can run and on what keys. If you’ve been using Redis for a while, you’ve probably put this feature on your wish list—it reduces the need for oopsy moves like running FLUSHDB on production servers, and lets you do more sophisticated tasks like creating specific users for specific actions so that every action operates with the least required privilege.Redis clients in Java, Node.js, Python and .NET already support ACLs, and we expect support to rapidly expand to more languages and libraries now that Redis 6 is generally available.In addition to ACLs, Redis 6 brings the ability to encrypt traffic over SSL. Up until this version, encryption in Redis was deferred outside the process, meaning it required other applications to provide encryption and that many instances were left unencrypted. This is an important step forward for Redis, allowing for use in more environments where encryption is a critical requirement.Despite Redis’ well-deserved reputation for high performance, its single-threaded architecture has been controversial among engineers who wondered if Redis could be even faster. Redis 6 rings in a new era: while it retains a core single-threaded data-access interface, I/O is now threaded.By delegating the time spent reading and writing to I/O sockets over to other threads, the Redis process can devote more cycles to manipulating, storing, and retrieving data—boosting overall performance. This improvement retains the transactional characteristics of previous versions, so you don’t have to rethink your applications to take advantage of the increased performance. Similarly, Redis’ single-threaded DEL command can now be configured to behave like the multi-thread UNLINK command that has been available since Redis version 4.The performance of a local variable is almost always unbeatable, Finally, even a database as high performance as Redis will be much slower than accessing something from the stack or heap. Redis 6 adds a new technique for sophisticated client libraries to implement a client-side caching layer to store a subset of data in your own process. This implementation is smart enough to manage multiple updates to the same data and keep your data as in-sync as possible—while retaining the advantages of Redis with the speed of local variables.For years, the second version of the Redis protocol (RESP2) has proven to be remarkably flexible. It supports not only Redis’ built-in data structures but also Redis features and the new commands and data that those bring along. Redis 6 starts support for a new version of the protocol, RESP3. This new protocol is an evolution of the previous version that adds richness to results, which let interfacing libraries better map Redis responses with variable types in the host language. Additionally, this version of the protocol paves the way for thinner client libraries with less code and will eventually allow for more-rapid adoption of new commands and features. RESP2 will be with us for quite a while as it will take the community some time to migrate software, tools, and client libraries to the new protocol. But if you want to dive in headfirst, you can try out RESP3 on Redis 6 now—just understand that RESP3 is still in an early phase of development.Redis Cluster greatly expands the variety of Redis uses, but it does require a more complex client library. In smaller language communities, the client libraries to support cluster never fully emerged. Thankfully, Redis 6.0 comes complete with a cluster proxy to assist language platforms that do not support the Redis Cluster API to connect to Redis clusters. This masks the complexity, so only a simple single-instance library implementation is required.Since Redis 1.0, developers have been able to set keys to expire after a given time, a feature indispensable for caching. This expiration has always relied on sampling techniques to avoid unpredictable delays when many keys expire at the same time. The expiration cycle has been rewritten in Redis 6.0 to allow for much faster expirations that more closely match the time-to-live (TTL) property. Additionally, you can now tune expirations to zero in on the accuracy required for your particular situation.And now for something completely different. In a recent surprise for the community, Salvatore, the author of Redis, released a new command family for version 6. The longest common subsequence (LCS) commands can be used to find non-contiguous sequences among strings. If you’ve ever used a diff, then you’ve indirectly used this algorithm. In Salvatore’s own eponymic example:STRALGO LCS STRINGS salvatore sanfilippo
""salo""How did this command come up with that result?salvatore sanfilippoAs you can see, the STRALGO LCS STRINGS command is skipping over a number of bytes—and that can be a different number for each argument trying to find the longest sequence of common bytes.That’s only the beginning, there is a lot more to this new capability of Redis. Since the LCS family of commands works on binary data—like almost everything else in Redis—just think about the possibilities outside of text processing. Salvatore has mentioned the possibilities of using it for RNA and DNA analysis. Pretty neat, but keep in mind this is a very new command that you may want to treat like a sandbox—the command name changed as of last week, so don’t be surprised if it isn’t supported everywhere.Redis 6 opens up vast new possibilities for the Redis community. It brings everything from better security to incremental performance improvements, not to mention being easier to use and introducing new ways to use Redis. If you’re ready to get started, you can download the brand new version right now from redis.io! To learn more about general availability of Redis Enterprise 6.0, which utilizes Redis 6’s improvements and takes Redis security to an even higher level, see our post on Rediscover Redis Security with Redis Enterprise 6.0."
538,https://redis.com/blog/if-you-think-goto-is-a-bad-idea/,"If You Think Goto Is a Bad Idea, What Would You Say About Longjmp?","September 16, 2019",Roi Lipman,"I honestly disagree with the conventional wisdom of never using a goto in your code. There are several situations where I find it to be not just convenient but good practice. The most common case is goto cleanup. Consider the following:Without goto:With goto:Instead of keeping tabs after which pointers need to be freed whenever a condition is met, we simply jump, free whatever was allocated, and return. In my eyes, this design is cleaner and less prone to error, but I can understand why others are against it.Recently, we wanted to introduce error reporting to handle failures while evaluating expressions. For example, evaluating the static expression toUpper(5) would fail as the toUpper function expects its argument to be a string. If this assumption isn’t met, toUpper should raise an exception:Unfortunately, C doesn’t come with a built-in exception mechanism like many other high-level languages do.What we were after is a try catch logic:A nice thing about this design is that regardless of where an exception was thrown within the execution path taken by our call to work, the stack is automatically restored, and we resume execution within the catch block.The function work, in our case, is replaced by a call to ExecutionPlan_Execute, which actually evaluates a query execution plan. From this point onwards, we must be prepared to encounter exceptions, but the road which ExecutionPlan_Execute takes in unwinding and deep, consider the following call stack:Execution call stack.The exception was raised way up the stack, in this case, we want to:We could have introduced a check for error within each function on our execution path, but by doing so, we would hurt performance (branch prediction) and overcomplicate our code with if(error) return error; logical constructs all over the place.And so, the jump is the first option that comes to mind, but note jump can only jump into a location within the function it is called in.Another idea we had was calling ExecutionPlan_Execute within a new thread, such that when an exception was thrown, we would simply terminate the thread and resume execution within the “parent” thread. This approach would have saved us the need to introduce extra logic or code branching:But this design would introduce an overhead of additional thread execution (even if we were to use a thread-pool), and we didn’t want to give up too much control to the OS scheduler.Ultimately, we found out about longjmp, which is similar to jump but not restricted in scope to the caller function. We can simply jump from anywhere to a preset point somewhere else in our call stack, and the best part is our stack would unwind to that point as if we’ve returned from each nested function. kinda going back in time, if you will.This is the design we’ve introduced recently. In case you ever run a query which violates the assumptions of a called function, this mechanism will be used to report an error back.Error reporting via redis-cli.Out of curiosity, I searched cpython github repository (Python implementation) to see if there’s a reference for longjmp. I was wondering if they’ve applied the same approach to exception handling as we did, but my search came out with no results – I’ll have to investigate further."
539,https://redis.com/blog/json-web-tokens-jwt-are-dangerous-for-user-sessions/,JSON Web Tokens (JWT) are Dangerous for User Sessions—Here’s a Solution,"June 24, 2021",Raja Rao,"Download the JSON Web Tokens (JWTs) are not safe e-book hereSometimes, people take technologies that are intended to solve a narrow problem and start applying them broadly. The problem may appear similar, but utilizing unique technologies to solve general issues could create unanticipated consequences. To use a metaphor, if you are a hammer, everything looks like a nail. JWT is one such technology.There are many in-depth articles and videos from SMEs of companies like Okta talking about the potential dangers and inefficiencies of using JWT tokens[1]. Yet, these warnings are overshadowed by marketers, YouTubers, bloggers, course creators, and others who knowingly or unknowingly promote it.If you look at many of these videos and articles, they all just talk about the perceived benefits of JWT and ignore the deficiency. More specifically, they just show how to use it but don’t talk about revocations and additional complexities that JWT adds in a real production environment. They also never compare it with the existing battle-tested approaches deep enough to really weigh the pros and cons.Or maybe it’s the perfect, buzzworthy, and friendly name that’s leading to its popularity. “JSON” (generally well-liked), “Web”(for web), and “Token”(implies stateless) make people think that it’s perfect for their web authentication job.So I think this is a case where the marketing has beaten engineers and security experts. But, it’s not all bad because there are regular long and passionate debates about JWT on Hacker News (see here, here and, here), so there is hope.If you think about it, these constant debates themselves should be a red flag because you should never see such debates, especially in the security realm. Security should be binary. Either technology is secure or it’s not.In any case, in this blog post, I’d like to focus on the potential dangers of using JWT and also talk about a battle-tested solution that’s been around for a decade.For further understanding, when I talk about JWT, I mean “stateless JWT,” which is the primary reason for the popularity of JWTs and the biggest reason to use a JWT in the first place. Also, I’ve listed all the other articles in the resources section down below that go into the nitty-gritty of JWT.Before we understand why it’s dangerous, let’s first understand how it works by taking an example use case.Imagine you are using Twitter. You log in, write a tweet, like a tweet, then retweet someone else’s tweet. So you did four actions. For each action, you need to be authenticated and authorized before you can perform that specific action.Below is what happens in the traditional approach.The problem is that step four is slow and needs to be repeated for every single action the user does. So every single API call leads to at least two slow DB calls which can slow down the overall response time.There are different ways of achieving this.Now let’s look at the JWT way.JWT, especially when used as a session, attempts to solve the problem by completely eliminating the database lookup.The main idea is to store the user’s info in the session token itself! So instead of some long random string, store the actual user info in the session token itself. And to secure it, have part of the token signed using a secret that’s only known to the server.So even though the client and the server can see the user info part of the token, the second part, the signed part, can only be verified by the server.In the picture below, the pink section of the token has the payload (user’s info) and can be seen by both the client or the server.But the blue part is signed using a secret string, the header, and the payload itself. And so if the client tampers with the payload (say impersonates a different user), the signature will be different and won’t be authenticated.Here is how our use case would look like with JWT:Going forward for every user action, the server simply verifies the signed part, gets the user info, and lets the user do that action. Thus completely skipping the DB call.But there is one additional and important thing to know about the JWT tokens. And that is that it uses an expiration time to expire itself. It’s typically set to 5 minutes to 30 minutes. And because it’s self-contained, you can’t easily revoke/invalidate/update it. This is really where the crux of the problem lies.The biggest problem with JWT is the token revoke problem. Since it continues to work until it expires, the server has no easy way to revoke it.Below are some use cases that’d make this dangerous.Imagine you logged out from Twitter after tweeting. You’d think that you are logged out of the server, but that’s not the case. Because JWT is self-contained and will continue to work until it expires. This could be 5 minutes or 30 minutes or whatever the duration that’s set as part of the token. So if someone gets access to that token during that time, they can continue to access it until it expires.Imagine you are a moderator of Twitter or some online real-time game where real users are using the system. And as a moderator, you want to quickly block someone from abusing the system. You can’t, again for the same reason. Even after you block, the user will continue to have access to the server until the token expires.Imagine the user is an admin and got demoted to a regular user with fewer permissions. Again this won’t take effect immediately and the user will continue to be an admin until the token expires.It’s been found that many libraries that implement JWT have had many security issues over the years and even the spec itself had security issues. Even Auth0 itself, who promotes JWT got hit with an issue.In many complex real-world apps, you may need to store a ton of different information. And storing it in the JWT tokens could exceed the allowed URL length or cookie lengths causing problems. Also, you are now potentially sending a large volume of data on every request.In many real-world apps, servers have to maintain the user’s IP and track APIs for rate-limiting and IP-whitelisting. So you’ll need to use a blazing fast database anyway. To think somehow your app becomes stateless with JWT is just not realistic.One popular solution is to store a list of “revoked tokens” in a database and check it for every call. And if the token is part of that revoked list, then block the user from taking the next action. But then now you are making that extra call to the DB to check if the token is revoked and so deceives the purpose of JWT altogether.Although JWT does eliminate the database lookup, it introduces security issues and other complexities while doing so. Security is binary—either it’s secure or it’s not. Thus making it dangerous to use JWT for user sessions.There are scenarios where you are doing server-to-server (or microservice-to-microservice) communication in the backend and one service could generate a JWT token to send it to a different service for authorization purposes. And other narrow places, such as reset password, where you can send a JWT token as a one-time short-lived token to verify the user’s email.The solution is to not use JWT at all for session purposes. But instead, do the traditional, but battle-tested way more efficiently.I.e. make the database lookup so blazing fast (sub-millisecond) that the additional call won’t matter.Is there any database out there so blazing fast that it can serve millions of requests in sub-milliseconds?Of course, there is. It’s called Redis! Thousands of companies that serve billions of users daily use Redis just for this exact purpose!And Redis Enterprise is an enhanced version of the Redis OSS that provides 99.999% availability and can serve trillions of requests. It can be used as free software on private clouds or in the cloud on any of the top-3 clouds.What’s more? Redis Enterprise has now evolved from just being a cache or session store, to a fully-fledged multi-model database with its module ecosystem that runs natively with core Redis.  For example, you can use JSON (10x faster vs. the market leader) and essentially have a real-time MongoDB-like database, or use the Search and Query feature (4–100x faster) and implement real-time full-text search like Algolia.If you simply use Redis as a session store and some other Database as a primary database, this is how your architecture would look.  One thing to note is that Redis Enterprise provides four types of caching: Cache-aside (Lazy-loading), Write-Behind (Write-Back), Write-Through, and Read-replica, unlike Redis OSS which only provides one (Cache-aside).Note that the lightning emoji indicates a blazing fast speed. And the snail emoji indicates slow speed.As mentioned earlier, you can also use Redis as a primary database for your entire data layer. In this scenario, your architecture becomes much simpler and basically, everything becomes blazing fast.Of course, companies use Redis not just as a standalone database but as a cluster of geographically distributed databases."
540,https://redis.com/blog/meet-top-k-awesome-probabilistic-addition-redis/,Meet Top-K: an Awesome Probabilistic Addition to Redis Features,"July 2, 2019",Ariel Shtul,"You may find yourself wondering whether you should use more probabilistic data structures in your code. The answer is, as always, it depends. If your data set is relatively small and you have the required memory available, keeping another copy of the data in an index might make sense. You’ll maintain 100% accuracy and get a fast execution time.However, when the data set becomes sizeable, your current solution will soon turn into a slow memory hog as it stores an additional copy of the items or items’ identifiers. Therefore, when you deal with a large number of items, you might want to relax the requirement of 100% accuracy in order to gain speed and save memory space. In these instances, using the appropriate probabilistic data structure could work magic – supporting high accuracy, a predefined memory allowance and a time complexity that’s independent of your data set size.Finding the largest K elements (a.k.a. keyword frequency) in a data set or a stream is a common functionality requirement for many modern applications. This is often a critical task used to track network traffic for either marketing or cyber-security purposes, or serve as a game leaderboard or a simple word counter. The latest implementation of Top-K in our Probabilistic feature uses an algorithm, called HeavyKeeper1, which was proposed by a group of researchers. They abandoned the usual count-all or admit-all-count-some strategies used by prior algorithms, such as Space-Saving or different Count Sketches. Instead, they opted for a count-with-exponential-decay strategy. The design of exponential decay is biased against mouse (small) flows, and has a limited impact on elephant (large) flows. This ensures high accuracy with shorter execution times than previous probabilistic algorithms allowed, while keeping memory utilization to a fraction of what is typically required by a Sorted Set.An additional benefit of using this Top-K probabilistic data structure is that you’ll be notified in real time whenever elements enter into or expelled from your Top-K list. If an element add-command enters the list, the dropped element will be returned. You can then use this information to help prevent DoS attacks, interact with top players or discover changes in writing style in a book.Initializing Top-K key in Probabilistic requires four parameters:As a rule of thumb, width of k*log(k), depth of log(k) or minimum of 5, and decay of 0.9, yield good results. You could run a few tests to fine tune these parameters to the nature of your data.Redis’ Sorted Set data structure provides an easy and popular way to maintain a simple, accurate leaderboard2. With this approach, you can update members’ scores by calling ZINCRBY and retrieve lists by calling ZREVRANGE with the desirable range. Top-K, on the other hand, is initialized with the TOPK.RESERVE command. Calling TOPK.ADD will update counters, and if your Top-K list has changed, the dropped item will be returned. Finally, TOPK.LIST, as you would expect, returns the current list of K items. To compare these two options, we ran a simple benchmark with the leaderboard use case.In this benchmark, we extracted a list of the most common words in the book War and Peace, which contains over 500,000 words. To accomplish this task, Redis Sorted Set took just under 6 seconds and required almost 4MB of RAM with guaranteed 100% accuracy. By comparison, Top-K took, on average, a quarter of that time and a fraction of the memory, especially for lower K values. Its accuracy was 100% in most cases, except for very high Ks where it ‘only’ achieved 99.9% accuracy3.Some interesting takeaways from the results above include:It is important to note that different types of data might be sensitive to different variables. In my experiments, some data sets’ execution time was more sensitive to ‘K.’ In other instances, lower decay values yielded higher accuracy for the same width and depth values.The new Top-K probabilistic data structure in Probabilistic is fast, lean and super accurate. Consider it for any projects with streams or growing data sets that have a low memory usage requirement. Personally, this was my first project at Redis and I feel lucky to be given the opportunity to work on such a unique algorithm. I could not be more happy with the results!If you have a use case you would like to discuss, or just want to share some ideas, please feel welcome to email me.[1] HeavyKeeper: An Accurate Algorithm for Finding Top-K Elephant Flows by co-primary authors: Junzhi Gong and Tong Yang.[2] The Top 3 Game Changing Redis Use Cases.[3] Lower results in the graph didn’t follow guidelines for width and depth."
541,https://redis.com/blog/microservices-and-the-data-layer-new-idc-infobrief/,Microservices and the Data Layer—a New IDC InfoBrief,"January 7, 2021",Mike Anand,"If you still harbored any doubt that microservice architectures are dominating today’s application development, it’s time to get over it. According to IDC’s new InfoBrief on The Impact of Application Modernization on the Data Layer, sponsored by Redis, 89% of some 300 North American enterprise survey respondents are already using microservices. And that comes on top of IDC’s 2019 prediction that “By 2022, 90% of all new apps will feature microservices architectures.”Microservices’ momentum is now undeniable, driven by enterprises’ need to more rapidly develop, deploy, and update high-quality apps and services to meet ever-rising customer and business requirements. According to the InfoBrief, “microservices apps are already used in business-critical roles, with 24% of microservices apps identified as business-critical.” For almost half of microservices apps (42%), for example, downtime leads to lost revenue.But that doesn’t mean the microservices revolution is complete. In fact, it’s just getting started. According to the IDC survey, all those enterprises embracing microservices are doing so with just 17% of their app portfolios already migrated away from the monolith. The InfoBrief authors, Carl W. Olofson and Gary Chen, explain the situation succinctly: “Although development of microservices-based applications is still in an early phase, its significance in future development is clear.”Learn more: Read the Redis Microservices for Dummies e-bookSo what will it take to fulfill the promise of microservice architectures, and what challenges are enterprises facing as they embrace microservices? Microservices adoption can add significant complexity, especially when it comes to the data layer. According to the survey, nearly half of enterprise microservices apps (47%) rely on a database, but nearly a third of respondents (31.5%) cite database management as a top-three challenge. “Decomposing applications exponentially increases the number of logical components that have to be managed,” the authors note.That can leave data siloed, more complex, and expensive, leading many enterprises to conclude that orchestration is essential to deploying microservices. So it’s not surprising that around a third of respondents named reducing management costs and increased operational efficiency (35.6%) and supporting new, modern cloud-native or microservices applications (33.2%) as top-three drivers of orchestration deployment.An e-commerce solution, for example, might employ a number of services—application server, content cache, session store, product catalog, search and discovery, order processing, order fulfillment, analytics, and many more—and each service may have its own database, as shown in the diagram below:So, how do you choose and build applications with the right architecture? What characteristics do you need to look for? Research suggests focusing on four key must-haves. Let’s take a closer look:As IDC noted, “over 95% of respondents favor database type or performance as criteria” and almost half of survey respondents (45%) cited performance as a top-three factor (trailing only database type) when selecting a database. It’s no surprise that performance is so important, since in a microservice environment you need both real-time performance and the ability to scale in order to fully deliver on the promise of distributed architecture.Named the most loved-database four years running, Redis is well known for delivering sub-millisecond performance. With Redis Enterprise’s low-latency database, you can create instant user experiences or perform real-time analytics while keeping a small footprint with the ability to scale on-demand. As the InfoBrief notes, “This indicates that how the database works and performs is critical to microservices application development success.”Learn more: See our new Latency Is the New Outage white paperThe IDC survey revealed that almost a quarter (24%) of enterprise microservice applications are already being used in business-critical roles, where high availability is critical. More generally, “for 42% of microservices apps, experiencing downtime results in direct loss of revenue for the organization,” the InfoBrief notes, while downtime in the remaining 58% of apps leads to loss of productivity.That’s a big potential issue, because even though a microservice architecture incorporates many connected services, it faces the same performance and reliability demands as other development approaches. Ensuring your data is always available around the world is another challenge, for example. To ensure your applications remain available at all times, you need a database that is fault tolerant at all levels.Learn more: See how Redis Enterprise’s high-availability technologies guarantee four-nines (99.99%) uptime—and five-nines (99.999%) in Active-Active geo-distributed deployments.Developers leverage microservice architectures to build better apps. Selecting the right data model is essential for speeding time to market, and more importantly it optimizes data-access patterns and performance requirements. This way, each service can use a database purpose-built for its own data model.A microservice may employ a data model based on key-value, JSON, time series, and search engines, among other things. But that means “many microservices will use a database per service,” the InfoBrief notes, which “increases the number of databases, the number of software components accessing databases, and the need for databases to be included in modern workflows.” That’s why, as noted above, database management with microservice apps is a top-three challenge for nearly a third (32%) of respondents.To minimize this complexity, your database should support multiple data models. That makes it easy for enterprise architects to choose the right data model for each service without sacrificing performance or having to learn and maintain multiple different databases, which simplifies operations and helps limit technology sprawl.Learn more: See our white paper on Strategic Data FlexibilityEven as DBaaS gains momentum, a great deal of enterprise data remains on-premises, leading many enterprises to use multiple clouds and hybrid infrastructures, as shown in the IDC survey. In a microservices environment, you need the ability to optimize the data layer to give you the flexibility to run your database without silos or data loss.But as the IDC authors point out, “Many databases were not designed to be cloud-native, compatible with containers, or orchestrated by Kubernetes.” Merely packaging a database within a container, for example, doesn’t make it right for a microservice architecture; the database must be lightweight and tunable to meet your data needs.Enterprises need a database platform that offers flexible deployment models to run wherever it’s needed, whether that’s on-premises or in any cloud, in a multi-cloud or a hybrid-cloud architecture, in containers, or as a Kubernetes Pod.Learn more: Check out Redis Enterprise Software Deployment OptionsAccording to the report, the rise of microservice architectures significantly impacts the data layer architecture used to support these services, so enterprises looking to develop applications using microservice architectures should pay special attention to technical excellence and fit for purpose. That is precisely where Redis Enterprise shines.Redis Enterprise offers performance at microservices scale, delivers sub-millisecond latency for all Redis data types and modules, and the ability to scale instantly and linearly to almost any throughput needed. Designed for fault tolerance and resilience, Redis Enterprise uses a shared-nothing cluster architecture and offers automated failover at the process level, for individual nodes, and even across infrastructure availability zones, as well as tunable persistence and disaster recovery. Redis Enterprise also makes it easy for developers to choose the data model best suited to the performance and data access requirements for their microservice architecture deployment, while retaining a unified operational interface to limit technology sprawl and simplify operations. Critically, Redis Enterprise can be deployed anywhere—on any cloud platform, on-premises, or in a multi-cloud or hybrid cloud architecture.To learn more about the importance of the data layer in microservice deployments, download the full IDC InfoBrief—The Impact of Application Modernization on the Data Layer. And check out the microservice architecture resources below to more insight into the many ways Redis Enterprise is uniquely suited for microservice deployments:"
542,https://redis.com/blog/new-pricing-for-redis-enterprise-cloud/,New Pricing for Redis Enterprise Cloud,"February 3, 2021",Aviad Abutbul,"For the last 12 months, we have been working closely with our cloud customers to better understand how our cloud service can meet their needs. Based on those discussions, today we are announcing a new pricing model for our Redis Enterprise Cloud service.This new pricing model is designed to be as simple as possible, let our users scale up and down instantly, and pay only for the resources they need in a completely serverless fashion. Customers who choose our annual plan can enjoy significant discounts, and use their existing public cloud commits to pay for what we believe is the most advanced Redis cloud service in the market, created by the people behind open source Redis.Redis Enterprise Cloud is a fully managed cloud service that allows you to use Redis for more than just simple caching use cases. It offers advanced capabilities like data persistence by default, five-nines (99.999%) availability, Active-Active multi-region deployment, and hosting large datasets on SSDs to dramatically reduce operating costs. With Redis Enterprise Cloud, you can design your real time applications using Redis features, including Search and Query, JSON, Time Series, Probabilistic, and soon more.Our new pricing mechanism also lets you use Redis Enterprise Cloud as a simple caching engine at a very attractive price point, with no operational overhead. You can start using the service with any of the four different plans, and you have the freedom and flexibility to move between plans according to your application needs:The Free plan lets you run a single non-highly available Redis database with up to 30MB and 30 connections, forever (or as long as you use it)! It lets you run Redis Enterprise Cloud in a selected region across all three major public clouds (Amazon Web Services, Microsoft Azure, and Google Cloud). You can use  all available Redis features: Search and Query, JSON, Time Series, and Probabilistic. Best of all, it’s totally free, no credit card required.Try our Free plan now.Redis Enterprise Cloud’s Fixed plans let you select anything from 100MB to 10GB of Redis memory; you can create multiple dedicated databases within the plan limit. In addition to supporting all Redis modules, the Fixed plans allow you to deploy your Redis database in a highly available manner over one or more availability zones, with instant failover times.As the name suggests, the Flexible plan lets you consume Redis Enterprise Cloud resources in a fully flexible manner. You can create any number of databases, scale them to multiple-terabyte datasets with hundreds of millions of ops/sec, and support an unlimited number of connections, while deploying over a dedicated virtual private cloud (VPC) environment on AWS and Google Cloud.Besides supporting single- or multi-availability zone deployments with instant failover and enriching your Redis functionality with Redis modules, the Flexible plan allows creating Redis on Flash databases. Redis on Flash uses a combination of DRAM and SSD to store your datasets at up to 70% cost savings compared to a pure DRAM deployment.Furthermore, with the Flexible plan, you can deploy your Redis databases across multiple regions and clouds in an Active-Passive or an Active-Active manner, allowing you to run Redis with five-nines availability and instant failover across regions and clouds.Active-Active Redis lets you deploy your application in a full globally distributed manner, as close as possible to where your users are located. It uses our conflict-free replicated data types (CRDTs)-based technology that enables read and write Redis operations from any location without worrying about conflict-resolution problems and maintains Redis’ sub-millisecond local latencies, even across global deployments. (Contact support@redis.com for more information on Active-Active deployments.)Finally, the Flexible plan lets you consume Redis in a serverless manner, without worrying about clusters and nodes, and to be charged only for the memory and throughput you provision.Learn more about our Flexible pricing plan.By committing to a predefined annual consumption in the Annual plan, you can get a significant discount compared to Flexible plan prices. The yearly commitment applies to all your workloads across multiple clouds and regions. The Annual plan lets you use your AWS or Google Cloud commits to consume Redis Enterprise Cloud resources.Learn more about our Annual pricing plan.Let’s take a closer look at each of the new pricing models.Sizing your Flexible plan workloads is done using an online calculator.  Simply specify your database memory limit (and if your database is highly available, make sure your memory limit takes into account the size of your replica), the expected throughput (in ops/sec) and the number of databases of this type. The screenshot below shows an example of a three-database deployment:We use a shard-based pricing model to size your deployment, in which a shard is composed of a Redis server plus the infrastructure needed to host Redis:In order to provide the best prices across the majority of the Redis use cases, we defined five types of shards, each with a different memory and throughput characteristics and a different price per hour (Note: shard rates vary between cloud providers and regions). The diagram below shows how it works:Here’s how this new shard-based pricing approach maps to the three-database deployment outlined above:As you can see:Note that we enforce memory and throughput limits per shard to better utilize the memory of underlying cloud instances with minimal overhead. This also lets us parallelize replication and data persistence processes. Enforcing the ops/sec limit helps customers better predict their Redis behavior when combining low and high computational commands. That said, we allow those users who know that a single instance of Redis can process more than 25K ops/sec for their use case to size their databases according to the number of shards rather than by ops/sec.The Flexible plan is built with the mindset of serverless computing, in which you pay only for the memory and the throughput limits you provision, without having to deal with the cloud infrastructure that runs behind the scenes. In other words, you can “forget clusters, forget nodes.”Redis Enterprise Cloud will always choose the most attractive price/performance cloud infrastructure (instances, network, and storage) to meet your workload needs, and will place and balance your Redis databases across them. The beauty of this approach is that with a single API call you can scale your database up and down and your usage bill will be (instantly) adjusted accordingly. This lets you modify your Redis deployments to account for peak and slow periods in a matter of seconds, keeping your Redis operation as efficient as possible.This example shows the power of this approach:As noted, the Annual plan provides significant discounts compared to the Flexible plan prices, by committing to a predefined annual consumption. Signing up for an Annual plan requires a short, three-step interaction between you and a Redis sales representative:Step 1: Our sales representative will walk you through a sizing exercise to estimate your usage and annual spending.Step 2: Establish your annual commit and discount level based on the sizing exercise. For instance, if your spending estimate is $100K/yr and you get a 10% discount, you will pay just $90K to consume $100K Redis Enterprise Cloud resources.Step 3: Once you sign your annual agreement and start consuming your Redis Enterprise Cloud resources, we will monitor your usage on an hourly basis to help you to better understand how your actual consumption compares with your initial commitment.An annual deal allows you to consume resources across any cloud (currently AWS and Google Cloud) or region. It  also lets you consume Redis Enterprise Cloud resources using your existing cloud commits.We believe that our new cloud pricing approach is a key step towards improving the Redis Enterprise Cloud experience for both existing and new customers. It allows you to scale up and down instantly and pay only for the amount of data and throughput you were provisioned at any given time, all at an attractive price for memory and throughput.If you have questions or want more information about our new Redis Enterprise Cloud pricing model, please contact us at support@redis.com."
543,https://redis.com/blog/real-time-data-for-improving-player-engagement/,4 Ways to Improve Player Engagement with  Real-Time Data,"January 14, 2022",Henry Tam,"Player engagement is the number one priority for game companies. The gaming industry is fiercely competitive and it’s sink or swim. To keep your head above the water and overtake your competition, it’s crucial that you optimize all areas of performance to keep players tethered to your game.The smallest deficiency in your game is enough to snap users out of focus and create tension. Players are brutal in their expectations and won’t stomach any shortcomings, and if they do they’ll simply switch to a game that can meet their demands.A seamless playing experience is dependent on your ability to power your game’s features with real-time data. Leaderboards, game inventories, personalization, and smart matchmaking are all crucial elements that require real-time data processing to eliminate lags and provide instantaneous responses, both of which ultimately sky-rocket engagement.Without real-time data, games are exposed to the crippling effects of lags that pulverize the playing experience. This can kickstart a toxic spree of events that lead to a drop in DAUs, bad publicity, and an eroding brand reputation—all of which will plummet profits.Player engagement is the lifeblood of any game. To maximize it, you’ll need a database that can elevate all areas of performance with real-time data.A strong online community is the beating heart of any successful game. It’s also the cash cow that places you in a golden situation where players will naturally gravitate towards your brand and spark a chain of events that accelerate growth.But a strong community starts with maximizing engagement. This often hinges on a game’s ability to carry out the matchmaking process with lightning speed. Matchmaking is all about setting players up with others of a similar skill level to optimize the playing experience.Novices pitted against elite performers are likely to suffer heavy consecutive defeats, which will kill motivation to pick up the controller again. After all, nobody likes losing all the time. But neither do they enjoy victories that come too easy since competition is what gets the adrenaline pumping.For matchmaking to meet industry standards, it needs to be fast, consistent, and accurate. Clunky databases with weak processing power make matchmaking slow and laborious, forcing players to wait impatiently in lobbies until they’re matched with the right opponents. Today’s standards are so high that players expect seamless responsiveness and will jump if they experience long loading times.This is where real-time matchmaking comes in. It identifies the right connections between data with hyper-efficiency, backfills players to the right server spots based on their preference, and fires through the matchmaking queue so fast that players don’t even notice.Inventories are a crucial component of gaming. Games today are flooded with millions of inventories for weapons, equipment, swag, currencies, and more.Databases are drowning in the data demands required to provide players seamless access to inventories with no lags, no delays, and no freezes whatsoever. At any given moment, a database may have to process millions of queries at lightning speed with impeccable consistency. Any failures will harm the playing experience and hamper player engagement levels.First-person shooter games are a perfect example. These are wildly intense, and fast reaction times give any player the upper hand. Players need to juggle between inventories and select weapons that are most optimal for unique situations. Lags occurring from database deficiencies hinder players’ ability to compete, potentially robbing them of a victory. This is incredibly frustrating and will only create tension between the player and the game, eventually encouraging players to turn to one that can meet their demands.Real-time data processing removes these deficiencies and creates a more immersive experience that maximizes player engagement and pulls them deeper into the game. The secret to achieving this is through a real-time database so its low latency and lightning-quick data processing abilities guarantee instant game-inventory responsiveness.Personalization is the magic that allows players to blend their tastes, preferences, and styles in with the playing experience. It’s a powerful component of gaming that can make gameplay truly immersive.You don’t have to dig deep to find out why personalization is a crucial engagement factor. The more personal an experience is, the more attached we feel. Personalization entwines players more deeply into the playing experience and is part of a winning formula that creates a loyal, actively-engaged player base with high retention.This level of attachment propels engagement levels to a place where players will resist any reason to put the controller down. But personalization comes with skyscraper expectations. From start to finish, the entire process must be seamless with no lags. And players just won’t tolerate a jarring playing experience.Databases must meet the demands of each personalization type to maximize engagement. These include:Any hiccups will allow lags to bombard the personalization process and frustrate players, which will only hamper engagement levels. But whether it’s game-led or player-led personalization, databases have to process a colossal amount of data—and the amount of data involved is increasing significantly.The reality is, players expect real-time responsiveness and nothing else. They want the entire personalization process to be both effortless and seamless, something that can only be achieved through a real-time database.Leaderboards are more sophisticated than ever. With potentially millions of players logging in around the globe, databases have a phenomenal amount of data to process to meet player expectations. Modern multiplayer games are now expected to provide leaderboards for different leagues, player rankings, locations and more.It’s all about breaking everything down into granular detail and tailoring everything to the player. A racing game, for example, may have a dedicated leaderboard to measure the performance of different player levels.  This can be taken one step further by segmenting leaderboards based on the racing track that players are competing on and by the cars they’re racing with. As a result, players are provided with leaderboards that allow them to measure their performance only with those on a similar level to them.Breaking down leaderboards into such granular detail preserves the competitive spirit by preventing players from comparing their rankings against a field of millions, which they’ll have no chance of coming out on top. Creating a more level playing field where victory seems attainable is fundamental to boosting player engagement since it incentivizes players to keep competing.And now we have to pivot to another complexity that comes with leaderboards – identifying cheats. Not having a firm grip on this area allows bad actors to gain an unfair advantage and dominate rankings, spoiling all the fun for others. Players who feel cheated will lose all trust and won’t hesitate to walk away from the game – this is a sure way to lose members fast.Leaderboards often have special cheating analytics engines or checks built into their rankings to detect patterns that indicate cheating. These are crucial to keeping the game fair but they also increase the complexity of the leaderboard service.Being able to personalize leaderboards and optimize cheat analytics requires the integration of complex architectures of services and technologies. Leaderboards have to be capable of processing millions of queries for millions of users simultaneously to meet industry standards.The bar has been set and it’s sky-high: players demand real-time responsiveness in all aspects of gaming. Whether it’s game inventory, smart matchmaking, personalization or leaderboards, players won’t stomach any lags in performance and will turn to other games that can meet their demands.Real-time data processing is the aim of the game for any developer looking to maximize player engagement. But real-time data requires a real-time database, which can be challenging. A real-time database has to meet a long list of technical requirements for it to be able to supply all game features with real-time data.If you want to discover what these are and come one step closer to enjoying the benefits of real time data, then make sure to download our free e-book below."
544,https://redis.com/blog/redis-clustering-best-practices-with-keys/,Redis Clustering Best Practices With Multiple Keys,"June 1, 2022",Redis,"What is a key anyway in Redis? The original intention of Redis (or any key-value store) was to have a particular key, or identifier, for each individual piece of data. Redis quickly stretched this concept with data types, where a single key could refer to multiple (even millions of) pieces of data. As modules came to the ecosystem, the idea of a key was stretched even further because a single piece of data could now span multiple keys (for a Search and Query index, for example). So, when asked if Redis is a key-value store, I usually respond with “it descends from the key-value line of databases” but note that, at this point, it’s hard to justify Redis as a key-value store alone. One place, however, where keys in Redis are still vitally important is clustering.Get Started With Redis Cloud: Try FreeRedis Cluster is a distributed implementation of the Redis data store that allows data to be sharded across multiple Redis nodes. In a Redis Cluster, data is partitioned across multiple Redis nodes, so that each node only holds a portion of the total data set. This allows the cluster to scale horizontally and handle increased load by adding additional nodes. In Redis, data resides in one place in a cluster, and each node or shard has a portion of the keyspace. A cluster is divided up among 16,384 slots — the maximum number of nodes or shards in a Redis cluster. Of course, when running with high availability, your data may reside in a replica, but at no point is a single key split among multiple nodes.Since most clusters consist of a much smaller number of nodes, these hash slots are logical divisions of the keys. In an oversimplified example of a 4-node cluster, we’d have the following layout:Note: In Redis Enterprise, there is a further division into shards on each node. It is done the same way, but instead of nodes, it’s divided into shards.As an example, if you have a key that you know is in slot 2,000, then you know the data resides on Node #0. If the key is in slot 9,000, then it’s on Node #2. In reality, it’s much more complex than this (with slots moving and rebalancing all the time), but for the purposes of understanding transactions and keys, this simplified conceptual understanding of clustering will do.So how are keys related to slots? These slots are actually hash slots, in which each key is put through a hashing function to mathematically derive a single number from a string of characters of any length. Hashing enters public conversations most often when it comes to password hashing, which is a related but much more complex calculation. In the same way your password is not directly stored, but rather a mathematical representation of that password, the key you request actually boils down to its mathematical representation (in this case with the CRC16 hashing function). CRC16 will return a 14-bit number we can then modulo by 16384. Interesting coincidence that this is the number of hash slots available, no?Transactions in Redis only occur within the same hash slot, which ensures the highest throughput. Since there is no inter-node/shard communication needed, many failures scenarios are eliminated. Given this, you have to be sure that when you go out to do a transaction, all the keys involved are in the same slot. So, how do you know if your key is on the same slot (and the same node/shard) as another key in a transaction?While it is possible for many keys to be in the same hash slot, this is unpredictable from a key naming standpoint and it’s not sane to constantly check the slot (with CLUSTER KEYSLOT in open source or Enterprise in Cluster API mode) when naming keys. The best way to approach this is with some advanced planning and a feature called hashtags. In open source Redis, curly braces ({ and }) are signifiers of a hashtag and the string between these two characters is put through the CRC16 hashing function. Let’s take a look at a few examples:Given these examples, you can see that Redis would not allow a transaction over keys  user-session:1234 and user-profile:1234, but would allow one with user-profile:{1234} and user-session:{1234}.Note: You might think, “Great, put everything under one hash slot and I won’t have to worry about clustered transactions!” You wouldn’t be alone, as I’ve heard this ill-guided plot more than once. Redis won’t stop you from doing this or similar things, but you’ll end up with an unbalanced cluster, or worse, one full node and many empty nodes. Use hashtags only when needed and, even then, sparingly.Redis Enterprise can use this strategy but it also adds another feature to make sharding more transparent. Instead of using curly braces, you can use regular expressions to define a particular part of the key to be put through the hashing function. With the regular expression  /user-.*:(?<tag>.+)/ let’s revisit some of our examples from above:This regular expression would be flexible enough to also handle other keys that start with “user-”, so we could have keys like “user-image” or “user-pagecount”. In this scheme, each user’s information would be kept on a single hash slot, enabling all sorts of transactions to occur within the scope of a single user.Let’s extend this example a bit further. Say a user changes some information on their profile and we want to update both the profile and session information, and also extend their session so it doesn’t expire. Here is a typical (if simplified) version of a transaction:> MULTI
OK
> HSET user-profile:1234 username ""king foo""
QUEUED
> HSET user-session:1234 username ""king foo""
QUEUED
> EXPIRE user-session:1234 7200
QUEUED
> EXEC
1) (integer) 1
2) (integer) 1
3) (integer) 1This would work just fine in Redis Enterprise with the regular expression setup because the hashed portion of both the keys is the same. If you were to run this on open source Redis, you’d need to make sure your keys have curly braces, otherwise, you’d encounter a CROSSSLOT error. The nice thing about this type of error is that Redis will immediately notify you of the invalid transaction/slot crossing violation:> MULTI
OK
> HSET user-profile:1234 username ""king foo""
QUEUED
> HSET user-session:1234 username ""king foo""
(error) ERR CROSSSLOT Keys in request don't hash to the same slot (command='HSET', key='user-session:1234') within 'MULTI'
> EXPIRE user-session:1234 7200
(error) ERR CROSSSLOT Keys in request don't hash to the same slot (command='EXPIRE', key='user-session:1234') within 'MULTI'
> EXEC
(error) EXECABORT Transaction discarded because of previous errors.Keep in mind that some hash slots and key issues are not quite transactions, but are somewhat similar in behavior – single commands that operate over multiple keys. Take this example:> LPUSH my-list 1 2 3
(integer) 3
> RPOPLPUSH my-list my-new-list
(error) ERR CROSSSLOT Keys in request don't hash to the same slot (command='RPOPLPUSH', key='my-new-list')RPOPLPUSH is an atomic operation that takes an element off one list and pushes it onto another. The operative word is atomic. If these two lists reside on two different hash slots (much like in a transaction), you’ll get the CROSSSLOT error. Open source Redis is quite strict about this and any command that manipulates multiple hash slots is forbidden. Redis Enterprise has a few workarounds for simple commands, notably MGET and MSET.If you’ve been a power user of a single instance of Redis, moving to a cluster can feel a bit odd. Some of the commands and/or transactions you’ve relied on will no longer work on specific keys and, if you’re really unlucky, the way you designed your keyspace could be problematic. Here are a few tips for designing your application to work best on a cluster:Watch our recent Tech Talk on Buy vs Build: Clustering & Provisioning in Redis Open Source vs Redis Enterprise!"
545,https://redis.com/blog/redis-redisbloom-bloom-filters/,"Probably and No: Redis, Probabilistic, and Bloom Filters","March 26, 2020",Guy Royse,"I’ve got a problem. I have tens of millions of players in my online game World of EverCraft (Not a real game. But if it was, it would be from Blizzards of the Coast!). This is a great problem to have. So many gamers!But when a new user signs up, it takes forever to determine if their desired username is taken or not. It’s a simple enough query but the vast amount of data makes it slow.Switching to Redis helped. Instead of a simplistic but slow SQL query:> SELECT COUNT(*) FROM Users WHERE UserName = 'EricTheCleric'We ended up with a simplistic but fast Redis command:> SISMEMBER Users 'EricTheCleric'Win!But all these users are still taking up a mountain of memory. Tens of millions of users will do that. Is there some way to make our data smaller, more compact?Yes. I present to you the Bloom filter.A Bloom filter is a type of probabilistic data structure. We’ve talked about them before. In short, a probabilistic data structure uses hashes to give you faster and smaller data structures in exchange for a bit of uncertainty. Think of a Bloom filter as a set with limited capabilities. We can only add things to it and check if specific things are in it. But the responses to that checking is a little weird. A Bloom filter has two possible responses:Might. That’s the weird part, the uncertainty. But fear not. The degree of uncertainty is small and completely under your control using the Bloom filter in the Probabilistic feature . I’m going to show you how to use it and control the uncertainty.Let’s go back to World of EverCraft. I have tens of millions of unique usernames for my game that I need to check before I create a new user.Before, I was doing this:> SISMEMBER Users 'EricTheCleric'And then this if the username was available:> SADD Users 'EricTheCleric'Let’s look at this process with a Bloom filter. There are two possible scenarios here:Alice the Allomancer wants to create a World of EverCraft account. We check to see if her username is in the Bloom filter:> BF.EXISTS UserFilter 'AliceTheAllomancer'BF.EXISTS returns 0, indicating that AliceTheAllomancer is not in the set and is available. We add her:> BF.ADD UserFilter 'AliceTheAllomancer'> SADD Users 'AliceTheAllomancer'Easy peasy.Bob the Barbarian also wants to create an account. We look to see if his desired username is in the Bloom filter:> BF.EXISTS UserFilter 'BobTheBarbarian'BF.EXISTS returns 1. The username is probably taken. Pick a new one.Now, I know what you’re thinking. What if it actually isn’t taken. Shouldn’t we check? What if we have callously denied Bob the Barbarian his desired username when it was actually available?While I can appreciate not wanting to make a barbarian wielding a large battleaxe angry, this is the essential tradeoff being made. We are gaining performance and compactness at the cost of a limited amount of uncertainty. In this case (especially since the battleaxe is virtual) this is a good deal.If you just call BF.ADD and allow it to create a Bloom filter for you, you’ll end up with default values. These are fine for small Bloom filters of dozens of items, but for the above example, they are entirely inappropriate.When setting up a Bloom filter, use BF.RESERVE to control the settings:> BF.RESERVE UserFilter 0.001 100000000This sets up a Bloom filter under the key UserFilter with an error rate of 0.001 or 0.1% and a capacity of 100,000,000 usernames.The implementation of a Bloom filter is surprisingly easy for the power they contain. In preparation for writing this blog post, I (and others at Redis) wrote a Bloom filter in JavaScript using Node.js and another in C# using .NET Core. Mind you, I wrote these Bloom filters for education and not for performance, so don’t use them in production.At heart, a Bloom filter is an array of bits of a specified width. When an entry is added to the filter, it is run through a specified number of hashing functions (typically the same hashing algorithm with differing seeds). The results of these hashes are used to generate indices for the bit array. For each index, put a 1 in the array.An example may make this more understandable. Let’s create a tiny filter with a bit width of 8 bits:We use a hashing function with two different seeds. When we add an entry, we will call the hashing function twice to get two numbers. We will modulo those numbers by the bit width to generate our indices. Here it is in pseudocode:index = some_hash_function('AliceTheAllomancer', seed) % 8Let’s say that for Alice the Allomancer, this yields us the numbers 2 and 6. We then update our bit array by placing a 1 in positions 2 and 6.Let’s add Bob the Barbarian. We put him through our hash functions and get a 3 and a 6 back. Our bit array now looks like this:Great. Note that Alice and Bob both ended up setting bit 6 to a 1. A hash collision. Not a problem. This is why we use more than one hashing function.Now let’s see if something is in the Bloom filter. We do this by running the entry we want to check through the same hashing functions. If we run Alice the Allomancer we get 2 and 6, just like before and if we run Bob the Barbarian we get 3 and 6, just like before. This is expected because hashing functions are deterministic.For Alice we check bits 2 and 6. They are both 1 so Alice is probably in the Bloom filter. For Bob we check bits 3 and 6. Same result.Let’s check for someone we know is not in the filter, Eric the Cleric. Eric’s hash yields a 0 and a 6. Since all of the bits are not set to 1 for Eric, he is definitely not in the Bloom filter.But what about Fritz the Fighter? We haven’t seen him yet. His hash yields a 2 and a 3. Bits 2 and 3 are both set to 1, 2 by Alice and 3 by Bob. Fritz wasn’t added but he is probably in the filter. Hmm.This is the source of the uncertainty of a Bloom filter. It can be ameliorated by increasing the bit width and/or the number of hashes. Doing either of these consumes more memory, so there is always a trade-off to be made. Such is life.You can calculate the error rate using some algebra and the following formula, where p is the error rate, k is the number of hashes, m is the bit width, and n is the maximum number elements you expect to insert:p=(1 – e-kn/m)kOr you can just use an online calculator like this one.There’s a lot more to Bloom filters that we haven’t gone into.We explored a simple use case, but there are many many more. If you have a lot of data and don’t need perfection, Bloom filters are there for you. You could use a Bloom filter to track URLs that a web crawler has crawled or to remember suggested friends on social media so the user doesn’t keep seeing the same “Perhaps You Know” suggestions when they already said they didn’t. Or maybe just feed it all the words in a dictionary and use it as a spellchecker.We also didn’t perform any benchmarks with Redis to compare sets and Bloom filters. But we’ve blogged on that before so no need!If you want to more deeply explore Bloom filters, check out the Redis documentation on Bloom filters to see everything you can do with them. And if you want to really internalize how they work, write your own Bloom filter in your language of choice. And please share it with me on Twitter!"
546,https://redis.com/blog/t-digest-in-redis-stack/,t-digest: A New Probabilistic Data Structure in Redis Stack,"March 14, 2023",Lior Kogan and Pieter Cailliau,"With the introduction of the latest Redis Stack, we celebrate a new probabilistic data structure: t-digest.You can use t-digest to efficiently estimate percentiles (e.g., the 50th, 90th, and 99th percentile) when your input is a sequence of measurements (such as temperature or server latency). And that’s just one example; we explain more of them below.The support for t-digest extends the existing functionality of Redis Stack and its support for data models and processing engines, which includes RedisInsight, search and query, JSON, time series, and probabilistic data structures.Here we explain what t-digest is when it’s a good option, and how to use it.Redis already has plenty of probabilistic data structures: HyperLogLog, Bloom filter, Cuckoo filter, Count-min sketch, and Top-k. You can use them to answer common questions concerning data streams and large datasets:Getting accurate answers can require a huge amount of memory. However, you can reduce the memory requirements drastically if you are willing to sacrifice accuracy. Each of these data structures allows you to set a controllable tradeoff between accuracy and memory consumption. In addition to requiring a smaller memory footprint, probabilistic data structures are generally much faster than accurate algorithms.As you can see, probabilistic data structures are really fun! (Well, at least for software developers.)t-digest is a probabilistic data structure that can help you answer questions like:Ted Dunning first introduced t-digest in 2013 and described it in several publications:Practically speaking, t-digest may help in several ways. Here are a few scenariosYou are measuring your online server response latency and want to know:Millions of people are playing a game on your online platform. You want to provide each player with the following information:You are measuring the number of IP packets per second transferred over your network. You want to quickly detect potential denial-of-service attacks. You might want to query:You are measuring a machine’s readings, such as noise level or current consumption. To detect suspicious behavior, you can query:Of course, you can. But t-digest makes it easier. Let’s compare a before-and-after scenario.Suppose you want to measure HTTP request latency on a website you run. The HTTP latency is the amount of time it takes from when a request is made by the user to the time it takes for the response to get back to that user).Latency varies greatly depending on many factors, so it is common to measure the 50th, 90th, and 99th percentile of the latency. Trivially, half of the requests are served in less than the 50th percentile, 90% of the requests are served in less than the 90th percentile, and so on.How would you determine these statistics without t-digest? The trivial way would be to store all measured latencies (which can be millions or billions per day) in a sorted array. To retrieve the 90th percentile, for example, you would read from the sorted array the value in an index equal to 90% of its size. More complex data structures and algorithms could be used, but usually under a given set of assumptions, such as the latency range and resolution, its distribution, and the set of constant percentiles that would be queried.With t-digest, no such assumptions are needed. In addition, the memory footprint is small, and adding and querying data is very fast.So what is the catch? Be ready to tolerate a very small (usually negligible) relative error in the estimations. In the vast majority of cases where statistics are concerned, a small error in estimators is acceptable.Let’s get practical and see how it works.Let’s continue the HTTP request latency example. One option is to create a t-digest with TDIGEST.CREATE and add observations – or measurements, if you prefer – with TDIGEST.ADD.TDIGEST.CREATE key [COMPRESSION compression]This initializes a new t-digest data structure (and emits an error if such a key already exists). The COMPRESSION argument specifies the tradeoff between accuracy and memory consumption. The default is 100. Higher values mean more accuracy.To add a new floating-point value (observation) to the t-digest, use:TDIGEST.ADD key value [value ...]For example, to create a digest named t with a compression setting of 1000 (very accurate) and add 15 observations, we’d type in:TDIGEST.CREATE t COMPRESSION 1000TDIGEST.ADD t 1 2 2 3 3 3 4 4 4 4 5 5 5 5 5You can repeat calling TDIGEST.ADD whenever new observations are available. To estimate values by fractions or ranks, use TDIGEST.QUANTILE:TDIGEST.QUANTILE key quantile [quantile …]It returns, for each input fraction, an estimation of the value (floating point) that is smaller than the given fraction of observations. In other words, fraction 0.9 is equivalent to the 90th percentile.The following query retrieves the latency that is smaller than 0%, 10%, 20%, …, 100% of the observed latencies:TDIGEST.QUANTILE t 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 11) ""1""2) ""2""3) ""3""4) ""3""5) ""4""6) ""4""7) ""4""8) ""5""9) ""5""10) ""5""11) ""5""The query contains 11 fractions; hence the reply is an array of 11 latency values.When the fraction is 0 (0%), the result is the smallest observation – which is 1, in this example. Similarly, when the fraction is 1 (100%), the result is the largest observation (or 5 in the example). For these two fractions, the results are always accurate. The results for any other fraction are estimates.You can see that 10% of the latencies are smaller or equal to 2; 20% and 30% of the latencies are smaller or equal to 3; 40%, 50%, and 60% of the latencies are smaller or equal to 4; and lastly, 70%, 80%, 90%, and 100% of the latencies are smaller or equal to 5.You can also query for the n minimal measured latencies. Use TDIGEST.BYRANK for this.TDIGEST.BYRANK key rank [rank …]It returns, for each input rank, an estimation of the value (floating point) with that rank.The following query retrieves the first, second, …, and ninth smallest observed latencies:TDIGEST.BYRANK t 0 1 2 3 4 5 6 7 8 91) ""1""2) ""2""3) ""2""4) ""3""5) ""3""6) ""3""7) ""4""8) ""4""9) ""4""10) ""4""The query contains 10 ranks; hence the reply is an array of 10 values.When the rank is 0, the result is the smallest observation (1 in this example). Similarly, when the rank is equal to the number of observations minus one,  the result is the largest observation. For these two ranks, the results are always accurate; the results for any other rank are estimates. When the rank is equal to or larger than the number of observations, the result is inf.You can see that the second and third smallest latencies (the observations with ranks 1 and 2, respectively) are 2. Similarly, the third, fourth, and fifth smallest latencies (the observations with rank 3, 4, and 5, respectively) are 3, and the sixth, seventh, eighth, and ninth smallest latencies (the observations with rank 6, 7, 8, and 9 respectively) are 4.And, of course, you can query for the n largest measured latencies. Do so using TDIGEST.BYREVRANK.TDIGEST.BYREVRANK key reverse_rank [reverse_rank …]It returns, for each input reverse rank, an estimation of the value (floating point) with that reverse rank.The following query retrieves the first, second, …, and ninth-largest observed latencies:TDIGEST.BYREVRANK t 0 1 2 3 4 5 6 7 8 91) ""5""2) ""5""3) ""5""4) ""5""5) ""5""6) ""4""7) ""4""8) ""4""9) ""4""10) ""3""The query contains 10 reverse ranks; hence the reply is an array of 10 values.When the reverse rank is 0, the result is the largest observation (5 in the example). Similarly, when the reverse rank is equal to the number of observations minus one, the result is the smallest observation. For these two reverse ranks, the results are always accurate. The results for any other reverse rank are estimates. When the reverse rank is equal to or larger than the number of observations – the result is -inf.You can see that the first, second, third, fourth, and fifth largest latencies (the observations with reverse ranks 0, 1, 2, 3, and 4, respectively) are 5. Similarly, the sixth, seventh, eighth, and ninth-largest latencies (the observations with reverse ranks  5, 6, 7, and 8, respectively) are 4.To estimate fractions by values, use TDIGEST.CDF:TDIGEST.CDF key value [value …]It retrieves, for each input value, an estimation of the fraction of observations smaller than the given value and half the observations that are equal to the given value.The following query retrieves the fraction of latencies that are smaller than 0, 1, 2, 3, 4, 5, and 6 milliseconds respectively:TDIGEST.CDF t 0 1 2 3 4 5 61) ""0""2) ""0.033333333333333333""3) ""0.13333333333333333""4) ""0.29999999999999999""5) ""0.53333333333333333""6) ""0.83333333333333337""7) ""1""The query contains seven latency values; hence the reply is an array of seven fractions.As you can see, all the estimations in this simple example are accurate: the fraction of latencies smaller than 0 plus half the latencies that are equal to 0 is 0. Similarly, the fraction of latencies smaller than 1 plus half the latencies that are equal to 1 is 3.33%, etcSometimes, you want to estimate the number of observations instead of the fraction of observations. For this, you can use TDIGEST.RANK and TDIGEST.REVRANK.The following query retrieves the number of latencies that are smaller than 0, 1, 2, 3, 4, 5, and 6 milliseconds respectively:TDIGEST.RANK key value [value …]This is similar to TDIGEST.CDF, but it returns, for each input value, an estimation of the number of observations smaller than a given value added to half the observations equal to the given value.TDIGEST.RANK t 0 1 2 3 4 5 61) ""-1""2) ""1""3) ""2""4) ""5""5) ""8""6) ""13""7) ""15""The query contains seven latency values; hence the reply is an array of seven ranks.Again, all estimations in this example are accurate: there are no latencies smaller than 0; hence, the resulting rank is -1. The number of latencies smaller than 1 plus half the latencies that are equal to 1 is 1. Similarly, the number of latencies smaller than 2 plus half the latencies that are equal to 1 is 2, etc.TDIGEST.REVRANK key value [value …]This is similar to TDIGEST.RANK, but returns, for each input value, an estimation of the number of (observations larger than a given value and half the observations equal to the given value).The following query retrieves the number of latencies that are larger than 0, 1, 2, 3, 4, 5, and 6 milliseconds respectively:TDIGEST.REVRANK t 0 1 2 3 4 5 61) ""15""2) ""14""3) ""13""4) ""10""5) ""7""6) ""2""7) ""-1""The query contains seven latency values; hence the reply is an array of seven reverse ranks.Once again, you can see that all estimations in this example are accurate: the number of latencies larger than 0 plus half the latencies that are equal to 0 is 15. Similarly, the number of latencies larger than 1 plus half the latencies that are equal to 1 is 14. There are no latencies equal to or larger than 6; hence the resulting reverse rank is -1.We can see that TDIGEST.RANK(v) + TDIGEST.REVRANK(v) for any v between the minimum and the maximum observation – is equal to the number of observations.Calculating the average measurement value is a common operation. However, sometimes measurements are noisy or contain invalid values. For example, consider a noisy, invalid latency of 999999999 milliseconds. When this is possible, a common practice is to calculate the average value of all observations ignoring outliers. For example, you might want to calculate the average value between the 20th percentile and the 80th percentile.To estimate the mean value between the specified fractions, use TDIGEST.TRIMMED_MEAN:TDIGEST.TRIMMED_MEAN key lowFraction highFractionTDIGEST.TRIMMED_MEAN t 0.2 0.8""3.8888888888888888""TDIGEST.TRIMMED_MEAN t 0.1 0.9""3.7692307692307692""TDIGEST.TRIMMED_MEAN t 0  1""3.6666666666666665""Sometimes, it is useful to merge t-digest data structures. For example, suppose we measure the latencies for three servers, each with its own t-digest, but then we want to calculate the 90%, 95%, and 99% latencies for all the servers combined.Use this command to merge multiple t-digest data structures into a single one:TDIGEST.MERGE destKey numKeys sourceKey… [COMPRESSION compression] [OVERRIDE]TDIGEST.CREATE s1TDIGEST.ADD s1 1 2 3 4 5TDIGEST.CREATE s2TDIGEST.ADD s2 6 7 8 9 10TDIGEST.MERGE sM 2 s1 s2TDIGEST.BYRANK sM 0 1 2 3 4 5 6 7 8 91) ""1""2) ""2""3) ""3""4) ""4""5) ""5""6) ""6""7) ""7""8) ""8""9) ""9""10) ""10""Use TDIGEST.MIN and TDIGEST.MAX to retrieve the minimal and maximal values in the t-digest data structure, respectively.Both return nan when the data structure is empty.TDIGEST.MIN t""1""TDIGEST.MAX t""5""Both commands return accurate results and are equivalent to TDIGEST.BYRANK key 0 and TDIGEST.BYREVRANK key 0 respectively.Use TDIGEST.INFO to retrieve additional information about the t-digest, including the number of observations added to the data structure and the number of bytes allocated for the data structure.To empty a t-digest data structure and re-initialize it:TDIGEST.RESET keyt-digest extends Redis’s growing set of probabilistic data structures, which helps you address more use cases related to streaming data and huge datasets. It does so with sub-millisecond latency, sub-linear memory requirements, and in an extremely accurate fashion.This blog post is a general overview. All t-digest commands are explained on redis.io.Download the latest version from our download center or install it via FlatHub or Snapcraf."
547,https://redis.com/blog/the-challenges-in-building-an-ai-inference-engine-for-real-time-applications/,The Challenges in Building an AI Inference Engine for Real-Time Applications,"April 17, 2020",Yiftach Shoolman,"The artificial intelligence (AI) boom took off when people realized that they can utilize GPU technology to train deep-learning models much faster than waiting days for a general-purpose CPU to complete one cycle of model training. (Check out this informative Quora thread for more details.)Since 2016, when GPU manufacturers like NVIDIA, Intel, and others created their first AI-optimized GPUs, most AI development has been related to how to train models to be as accurate and predictive as possible. In late 2017 many enterprises and startups began to think about how to take machine learning/deep learning (ML/DL) to production. Successful open source projects like MLFlow and Kubeflow aim to move AI from its research and scientific phase to solving real-world problems, by managing the entire AI lifecycle. They introduce similar approaches for managing the machine learning pipeline lifecycle, as nicely presented in the graphic below taken from Semi Koen’s Not yet another article on Machine Learning! blog):At a very high level, one of the most critical steps in any ML pipeline is called AI serving, a task usually performed by an AI inference engine. The AI inference engine is responsible for the model deployment and performance monitoring steps in the figure above, and represents a whole new world that will eventually determine whether applications can use AI technologies to improve operational efficiencies and solve real business problems.We have been working with Redis Enterprise customers to better understand their challenges taking AI to production and, more specifically, their architecture requirements from an AI inference engine. After multiple interactions with many customers, we came up with this list:Our first mission at Redis is to help our customers and users solve complex problems at the speed of a millisecond. So let’s take a look at the first challenge in our list—fast end-to-end inferencing/serving—and see how to achieve that goal when adding AI to the production deployment stack.1. The new AI inference chipsets can help, but address only part of the problemThere has been plenty of coverage (for example here and here) on how chipset vendors are making significant efforts to provide highly optimized inferencing chipsets by 2021. These chipsets aim to accelerate inference processing such as video, audio, and augmented reality/virtual reality by increasing the processes’ parallelism and memory bandwidth. But accelerating only the AI processing portion of the transaction chain may provide only limited benefits, as in many cases the AI platform should be enriched with reference data scattered across multiple data sources. The process of retrieving and enriching AI with reference data can be orders of magnitude slower than the AI processing itself, as shown in this transaction scoring example:Let’s walk through what’s happening here:As shown above, even if we improve the AI processing by an order of magnitude (from 30ms to 3ms) the end-to-end transaction time inside the data center remains about 500ms, because the AI processing represents less than 10% of the overall transaction time.So for use cases like transaction scoring, recommendation engines, ad bidding, online pricing, fraud detection, and many others where the AI inferencing time is mainly associated with bringing and preparing the reference data to the AI processing engine, the new inferencing chipset can only marginally improve the end-to-end transaction time.2. Run your AI inference platform where your data livesAs most of the reference data of a latency sensitive application is stored in a database, it makes sense to run the AI inference engine where the data lives, in the database. That being said, there are a few challenges with this approach:1. In cases where the application data is scattered across multiple databases, which database should the AI inference engine run on? Even if we ignore the deployment complexity and decide to run a copy of the AI inference engine on every database, how do we deal with a situation where a single application transaction requires bringing the reference data from multiple databases? A recent survey from Percona nicely demonstrated how multiple databases represent the deployment architecture of most applications:2. To achieve the low-latency AI inferencing requirements, reference data should be stored in-memory. Many people believe that by adding a caching layer on the top of existing databases this problem can easily be solved. But caching has its own limitations. For instance, what happens in cache misses events where the application doesn’t find the data in the cache, forced to query the data from a disk-based database and then update the cache with the latest data? In this scenario the probability of infringing your end-to-end response time SLA is very high. And how do you make sure your database updates are in-sync with your cache and immune to consistency problems? Finally, how do you ensure that your caching system has the same level of resiliency as your databases? Otherwise your application uptime and your SLA will be driven by the weakest link in the chain, your caching system.To overcome the need for maintaining a separate caching layer, we believe that the right architecture choice to deploy the AI inference engine is an in-memory database. This avoids problems during cache misses events and overcomes data-synchronization issues. The in-memory database should be able to support multiple data models, allowing the AI inference engine to be as close as possible to each type of reference data and avoid having to build high-resiliency across multiple databases and a caching system.3. Use a purpose-built, in-database, serverless platformIt is easy to imagine how latency-sensitive applications can benefit from running the AI inference engine in an in-memory database with multiple data models for solving these performance challenges. But one thing is still missing in this puzzle: Even if everything sits together in the same cluster with fast access to shared memory, who will be responsible for collecting the reference data from multiple data sources, processing it, and serving it to the AI inference engine, while minimizing end-to-end latency?Serverless platforms, like AWS Lambda, are often used for manipulating data from multiple data sources. The problem with a generic serverless platform for AI inferencing is that users have no control where the code is actually executing. This leads to a key design flaw: Your AI inference engine is deployed as close as possible to where your data lives, in your database, but the serverless platform that prepares the data for AI inferencing runs outside your database. This breaks the concept of serving AI closer to your data, and leads to the same latency problems discussed earlier when the AI inference engine was deployed outside your database.There’s only one way to solve this problem: a purpose-built serverless platorm that is part of your database architecture and runs on the same shared cluster memory where your data and your AI inference engine live.Going back to the transactions scoring example, this is how fast (and simple) the solution can look if we apply these principles:Taking AI to production creates new challenges that did not exist during the training phase. Solving these problems requires many architectural decisions, especially when a latency-sensitive application needs to integrate AI capabilities in every transaction flow. In conversations with Redis customers who are already running AI in production, we found that in many cases a significant part of the transaction time is spent on bringing and preparing the reference data to the AI inference engine rather than on the AI processing itself.We therefore propose a new AI inference engine architecture that aims to solve this problem by running the system in an in-memory database with built-in support for multiple data models, and uses a purpose-built, low-latency, in-database serverless platform to query, prepare, and then bring the data to AI inference engine. Once these ingredients are in place, a latency-sensitive application can benefit from running the AI on a dedicated inference chipset, as the AI processing takes up a more significant portion of the entire transaction time.Finally, adding AI to your production deployment stack should be done with extreme care. We believe that businesses that rely on latency-sensitive applications should follow these suggestions to prevent the user experience being degraded by slowness in the AI inference engine. In the early days of AI, the slow performance of general-purpose CPUs created headwinds for developers and researchers during the training phase. As we look toward deploying more AI applications into production, architecting a robust AI inference engine will ultimately separate the winners from the losers in the pending AI boom."
548,https://redis.com/blog/the-case-for-ephemeral-search/,The Case for Ephemeral Search,"March 10, 2020",Redis,"Let’s say you’re working on an e-commerce website for home improvement products, like nails, screws, wood, tile, putty knives… that kind of thing. These types of stores (brick-and-mortar or online) typically sell a huge variety of products. Notably, when a person buys something from a store like this, it’s pretty common to need more of the same items at a later time—because who knows exactly how many nails you’ll need for a project?Providing a fast and easy to use purchase history is pretty vital to delivering good user experience, but the devil lies in the details. A simplistic approach would be a reverse time-ordered list of items, but this could get frustrating after a few items. What customers really need is a way to efficiently search the purchase history of a single user.One approach might be to keep a list of all products you’ve ever stocked, and relate a table row for each item and do a simple full-text search over the relational database system. Unfortunately the full-text search capabilities of relational databases are often lacking compared to a true full-text search engine.Another option would be to use a true search engine working with a search index that holds the user’s name as well as product name, description, and purchase time—each as a document. This would give you true, full-text search capabilities if you constrained results to a particular user. Unfortunately, this approach can be tough to pull off, because the indexed size of the search grows rapidly as each purchase adds more and more to a single index, and every search requires querying every purchase ever by any user. Let’s evaluate how large the index would be:Even with a relatively modest number of users, this type of index quickly grows out of control. What happens when you have to five million users? Fifteen million users?The pattern of usage on this feature is pretty interesting—across all users, there is probably a fairly normal distribution of usage at any given time. Without factoring in waking hours, you can assume that it won’t get surge-y. Thinking laterally, an individual user is probably touching this feature very infrequently. Indeed, in most cases, each user is not spending more than a few minutes a week on your e-commerce platform. At any given time, only a small fraction of the search index is ever being used. While a general site search should probably be available to all users at all times, the purchase-history search is inherently a logged-in-only feature.So, what if we could have a dynamic search index for each user—when the user logs on to the site, the purchase-search history is populated from another data store and then marked to expire in a specific amount of time, just like their session? If a user manually logs out, you’re safe to delete the the search index.This pattern requires a search engine that supports:If you have all of those things, what does it get you? Let’s revisit our simple math, but let’s add an additional assumption: 2% of the user base is logged in at any given time.This number of documents is far more manageable than the original strategy. Additionally, it scales based on the actual usage of your website, not cumulative purchase history. So, if your site becomes way busier (usually a good thing), then you can scale up that purchase history search with the increase in business.Since purchase history changes infrequently, the other data store in the picture doesn’t need to be very sophisticated nor high performance—you’re accessing it only when a user logs in or when items are purchased. It could be as simple as a flat file.Since you’re reading this on the Redis website, you might imagine that you can use this pattern with Search and Query. Indeed, Redis has several properties and features that align nicely to this use case. First, since Search and Query is a Redis feature, it inherits much of the performance of Redis itself. Like Redis, Search and Query is in-memory first, which means that writing and reading are on more equal footing, unlike disk-based systems that take much longer to alter or delete data. Quickly populating a purchase history into Search and Query is not a performance issue. Additionally, Search and Query is optimized to quickly create indexes as well as delete or expire indexes.Digging a bit deeper, let’s see how to create your index on a per-user basis:> FT.CREATE history:user:1234 TEMPORARY 3600 SCHEMA title TEXT description TEXT purchased NUMERICThe only unusual thing about creating this index is the TEMPORARY argument. This tells Search and Query to make the search index ephemeral and delete it after the specified 3600 seconds (or whatever aligns with your session timeout). Any time the search index is used, adding/deleting documents or querying, resets the idle timer. Once the time expires the index will be deleted. Also, note the name of the index includes a user identifier.At login, the index would be populated with  FT.ADD from the other data source. Nothing special needed here—Search and Query will take care of the document and keys as temporary with no other syntax. Adding the documents will be quick—in the low-single-digit-milliseconds range for most documents. This doesn’t have to be done synchronously, so as a user initially browses the site, the purchase history can be loaded in the background.One general note about Search and Query that should be restated, especially in this multi-index context: all document names should be unique across all indexes to prevent key contention at the hash level. Finally, in some circumstances you can save space by using the NOSAVE option on FT.ADD. This will not store the document, but rather just index it, providing you with the document ID only on FT.SEARCH, though this does complicate the result retrieval process.Implementing the search functionality itself is straightforward. Take the user input as the query argument to FT.SEARCH. The only difference from any implementation of Search and Query is that the index name will be derived in some way from the user identifier.When a user explicitly logs out of the service, the FT.DROP command removes the index and documents. Strictly speaking, this is not a required operation since the TEMPORARY index will expire automatically, but using the explicit FT.DROP will free up resources a little sooner.This particular pattern isn’t restricted to e-commerce applications. This is a viable pattern any time you have a personalized set of documents to search for a particular user. Imagine an invoice or bill search for a financial portal. Each user will have only a handful of documents specific to them, but the search experience is vital to finding specific information. In a messaging app, meanwhile, you may want to search your chat history, which again, is needed only while you’re interacting with the application and this search is specific to a single user’s chat history.This pattern provides a way to optimize the experience for users without creating a massive, cumbersome global index that can be challenging to maintain and scale. This capability hinges on the ability to create many lightweight indexes that have expiry, and to rapidly index documents on the fly.To get started with this pattern, download Redis at redisearch.io or take Redis Search and Query on Redis University."
549,https://redis.com/blog/top-5-reasons-why-redisinsight-is-a-perfect-tool-for-redis-developers/,Top 5 Reasons Why RedisInsight Is a Perfect Tool for Redis Developers,"October 5, 2020",Ajeet Raina,"For developers who are building applications with Redis, RedisInsight is a lightweight multi-platform management visualization tool that helps you design, develop, and optimize your application capabilities in a single easy-to-use environment. RedisInsight provides an intuitive and efficient GUI for Redis databases, making it easier to interact with your databases and manage your data—with built-in support for most popular Redis modules. It provides tools to analyze the memory and profile the performance of your database usage, and helps guide you toward better Redis usage. It manages Redis data via GUI by scanning existing keys, adding new ones, performing CRUD or bulk operations, displaying objects in a pretty-JSON object format, and supporting friendly keyboard navigation.Put it all together, and RedisInsight is an essential tool for Redis developers. We’ll lay out five key reasons why in a moment, but first let’s take a quick look at exactly what RedisInsight is, what it does, and how to get it.RedisInsight is available today as a non-commercial, free-of-charge tool. It is fully compatible with Redis Enterprise. It works with any cloud provider as long as you run it on a host that has network access to your cloud-based Redis server. It supports Redis Enterprise Cloud, Redis Cloud Pro, Amazon Elasticache, and Microsoft’s Azure Cache for Redis. With RedisInsight, it is easy to discover cloud databases, making it possible to configure connection details with a single click. It allows you to automatically add Redis Enterprise Software and Redis Enterprise Cloud databases. (Note: Auto-discovery requires a Redis Enterprise Cloud Pro subscription).RedisInsight 1.7 is the latest release, and comes with new capabilities and enhancements designed to make your developer experience even more enjoyable, with support for Redis 6 and its new access control lists (ACLs) compatibility. Recently introduced features like TLS (transport layer security) support and RedisGears beta support along with enhancements like multi-line query editing, full-screen mode, and more will make your experience more efficient. RedisInsight is a full-featured desktop GUI client and is available for Windows, macOS, and Linux, and is also available as a Docker container.Local installation:Download RedisInsight for Windows, Mac, and Linux from the Redis website.Here are five key capabilities that make RedisInsight a perfect tool for Redis developers:Redis modules allow developers to build new application services on top of Redis while continuing to enjoy Redis’ sub-millisecond speed. Redis modules enrich the Redis core data structures with search capability and modern data models like JSON, graph, time series, and others. With RedisInsight, developers can explore, visualize, and interact with Redis data, including complex Redis data structures and modules.Full screen support for Time Series, JSON, Redis Streams, and Search and Query is available in RedisInsight. As a developer, you can query and interactively manipulate graph, streams, and time-series data flawlessly. You can even build queries, explore the results, optimize, and quickly iterate with a multi-line query editor. These data structures can be viewed visually and all traditional operations can be performed using an updated command-line interface (CLI) and graphical commands, making it easier for you to execute commands for all data structures and modules.Don’t miss: RedisInsight 1.6 Brings RedisGears Support and Redis 6 ACL CompatibilityRedisInsight lets you browse and explore your Redis databases and intuitively interact with your data. It allows you to view real-time metrics from Redis. It allows you to create tabular views from your Redis keys and export data in different formats. You can also visualize and update data from Redis Streams, Search and Query, and Time Series. Beginning with the RedisInsight 1.6.3 release, filtering of keys in the browser is possible, so you can more easily navigate through your data and find the keys that are the most relevant to you.RedisInsight comes with a built-in CLI that lets you run commands against a Redis server. You don’t need to install anything, as soon as you are connected to the database, the integrated web is CLI available, just there for you! RedisInsight also makes your life simpler with all the command’s syntax—the integrated help shows you all the arguments and validates your commands as you type.RedisInsight provides syntax highlighting and auto-complete and employs integrated help to deliver intuitive, in-the-moment assistance. Hence, you can view all the traditional operations that can be performed using an updated CLI and graphical command builders, making it easier to write commands for all data structures and modules.Don’t miss: Modernizing Legacy Applications with Redis and Microservices (video)RedisInsight is aimed at helping developers get the most out of Redis. It is basically a suite of tools that can help developers throughout the development lifecycle. There are built-in tools for the design phase during prototyping, while other sets of tools help in the implementation phase.For example, RedisInsight allows developers to perform bulk operations such as renaming, expiring, and deleting a large number of keys in one go. It gives developers visibility into their slow logs so that they can identify, troubleshoot, and fix bottlenecks and find optimization opportunities. It helps developers identify top keys, key patterns, and commands. With RedisInsight, developers can filter by client IP address, key, or command across all nodes of a cluster. They can effectively debug Lua scripts with less complexity.Software firms need advanced tools to make development straightforward and fast. With the right tools, developers can save time, deliver high-quality applications, and run a sustainable enterprise. As a Redis developer, it’s important to identify efficient and easy-to-use tools that help you to understand how an application behaves and interacts with the database.Developers use the profiler feature of RedisInsight to help identify performance problems without having to touch the code. Some commands may take a long time to process on the Redis server, causing the request to time out. A few examples of long-running commands are met with a large number of keys, keys *, or poorly written Lua scripts. The RedisInsight profiler runs the Redis MONITOR command, which analyzes every command sent to the Redis instance. It parses the output of the MONITOR command and generates a summarized view. All the commands sent to the Redis instance are monitored for the duration of the profiling.Profiler gives information about the number of commands processed, commands/second, and number of connected clients. It also provides information about top prefixes, top keys, and top commands. It’s useful for understanding the nature of the traffic seen by your Redis database, which in turn can help debug performance problems in production environments.Redis is an in-memory data store. This means that the entire dataset is stored in memory (DRAM). This is great for performance, but as the size of your data set grows, you need more DRAM to hold all that data. Few developers want to spend their time learning about Redis memory issues, so RedisInsight provides recommendations for developers on how to save memory. The recommendations are specially curated according to the Redis instance, based on industry standards and Redis’ experience.RedisInsight helps developers reduce memory usage and improve application performance. It offers several tools to manage and optimize Redis. RedisInsight analyzes memory-usage offline—without affecting Redis performance—by key patterns, key expiry, and advanced search to identify memory leaks. It can even show you total memory consumption by key pattern, and also the biggest keys within that key pattern.RedisInsight’s memory analysis helps you analyze your Redis instance to minimize memory usage and improve application performance. Analysis can be done online and offline:Want to try RedisInsight and see if it fits into your development and operational toolkit? Click below to start your journey today:"
550,https://redis.com/blog/what-are-json-databases/,A Cat Lover’s Guide to Understanding JSON Databases,"February 8, 2023",Redis,"How do JSON databases function, and what value do they bring to application developers? Maybe examples from a pet cat’s point of view can put things into perspective.Let’s start out by defining the terms. And the cat.Whether developers are creating business logic, user interfaces, databases, or backend systems, they need a way to describe and exchange data. One system says to another, “I have the data you need!” and they need to structure how data is exchanged. The data exchange may be using binary data, or it may be text-based.JavaScript Object Notation (JSON) is a text-based data interchange format native to JavaScript. Because it is text, it is readable by both people and machines. JSON is commonly used to store and transmit data to applications.Like XML, JSON is a data interchange format rather than a programming language, in the sense that it is not Turing-complete. But, also like XML, in some ways it can be used as though it’s a programming language because of its readability and power. As JSON.org explains, JSON “uses conventions that are familiar to programmers of the C-family of languages,” such as Python and Java. That makes JSON a comfortable tool to use for sharing data among platforms.JSON has a dual structure:A JSON value can be an object, keyed list, record, dictionary, associative array, string, number, true, false, or null.For example, suppose the key in question is a kitten; its value is floofy. Together, these two make up an object, as long as the object is surrounded by curly braces. (The cat can be surrounded by whatever it likes.) A full JSON string describing this kitty might look like:In this example, the attributes on the left, such as name, age, and floofy are the keys in question, while Jason, 1, and true represent the values.Ready to see how Redis Enterprise native JSON allows developers to build modern applications?  Read more about Redis JSON.A JSON database (alternately spelled JSON db) is a document database, sometimes called a document store. The data is expressed in text-based documents rather than in the column or tabular form you may be familiar with from SQL databases.Structurally, a JSON database is a NoSQL database that reads and stores semi-structured data using JSON documents, such as a PDF, a document, or an XML or JSON file. Column, graph, key-value, in-memory, and document are all different types of NoSQL databases.The advantages extend beyond the user-friendliness of its data structure. From storage, schema, and indexing flexibility, to horizontal and vertical scaling, here are some ways JSON databases simplify developers’ lives.One important element in JSON databases is that they are dynamic. SQL databases have fixed field definitions and field sizes, so restructuring a database requires reindexing and other complexities. JSON databases are far more flexible because the key/value structure can adapt to changes in the data model or application requirements.Imagine a web application that depends on user profiles and login authentication. A JSON database’s key value can store the user IDs, user preferences, multiple ID mappings, and additional information so that the app can quickly look up a user and authenticate access. If the application requirements change – perhaps it’s important to include geographic data for compliance reasons – the database schema can change without a complete overhaul.A JSON document database is a compact arrangement of stored data with a flexible structure that the developer defines. The leanness of a JSON file greatly accelerates the speed by which the data interacts with the application.A JSON database schema permits objects to be embedded or linked, including circular references. The earlier example, introducing Jason the cat, intentionally is “hello world” simple. However, JSON can also model complex data structures such as object graphs and cyclic graphs. In particular, JSON databases support nesting, object references, and arrays. That gives developers clarity, because it decouples objects into different layers, making the database easier to maintain.With nested JSON database schemas, some values are other JSON objects.High-performance computing requires databases to scale to meet demand, in both the long-term (supporting huge data sets) and the short-term (holiday shopping periods during which everyone buys more cat toys). Horizontal scalability, a way to perform load balancing to prevent one node from bearing the brunt of everything required, works particularly well with JSON databases. That’s because it’s possible to partition JSON data and indexes over several shards and nodes for improved speed and memory management.JSON databases have become extremely popular in data science and analytics applications. These demanding big data applications benefit from JSON databases’ horizontal architecture and support for multiple concurrent queries.That’s because of JSON databases’ flexible schema and horizontal and vertical scaling. These enable document databases to store large datasets and to add more nodes when needed. Partitioning allows the data to be balanced across the nodes to increase the speed of reads and writes and to ensure availability.Indexing is a strategy for retrieving data. Document databases support all kinds of indexes, such as sorted sets, lexicographically encoded, geospatial, IP range, full-text search, and partitioned indexes.To see a real-time JSON demonstration, Redis’ Justin Castilla’s fun live stream, “Do Birds Dream of JSON,” illustrates how to search and extract information from huge banks of information. He uses data from bird sightings to highlight the capabilities of JSON – without, alas, a cat to supervise the birds.If you need more information on how JSON functions, our YouTube channel has a wide array of live streams, how-to videos, and webinars on this popular data notation format. Visit our channel anytime you run into a dead-end. We usually have a visual answer to guide you."
551,https://redis.com/blog/what-is-fuzzy-matching/,What is Fuzzy Matching?,"July 15, 2022",Eric Silva,"Fuzzy matching (FM), also known as fuzzy logic, approximate string matching, fuzzy name matching, or fuzzy string matching is an artificial intelligence and machine learning technology that identifies similar, but not identical elements in data table sets. FM uses an algorithm to navigate between absolute rules to find duplicate strings, words/entries, that do not immediately share the same characteristics. Where typical search logic operates on a binary pattern, (i.e.: 0:1, yes/no, true/false, etc) – fuzzy string matching instead finds strings, entries, and/or text in datasets that fall in the in-between of these definitive parameters and navigates intermediate degrees of truth.Approximate string matching assists in finding approximate matches, even when certain words are misspelled, abbreviated, or omitted, a function heavily used in search engines. In the end, approximate string matching provides a match score and since it is used to identify words, phrases, and strings that are not a perfect fuzzy match, the match score will not be 100%.Learn how to enrich search experiences with Search and QueryIt’s important to land on the right fuzzy matching algorithm to help determine the similarity between one string and another. In one case, you could have a single character distance from “trial” to “trail,” or search for “passport” when the existing string reads “passaport” – a typo. Of course, not every fuzzy logic case will be a single character distance matter. “Martin Luther Junior” is quite similar to “Martin Luther King, Jr.” Distances vary and there are various fuzzy name matching algorithms to help bridge those gaps.There are some drawbacks to running a fuzzy logic search with loosely-defined rules for matching strings. Using a weak system increases the chance of false positives. In order to keep these false positives at a bare minimum, or, ideally, non-existent, your approximate string matching system should be rather holistic. It needs to account for misspellings, abbreviations, name variations, geographical spellings of certain names, shortened nicknames, acronyms, and many other variables.Though there are a good many string matching algorithms to choose from when reconciling datasets, there isn’t a one-size-fits-all solution for all use cases. Here are a few of the most reliable and often used string matching techniques used in data science to find approximate matches.The Levenshtein Distance (LD) is one of the fuzzy matching techniques that measure between two strings, with the given number representing how far the two strings are from being an exact match. The higher the number of the Levenshtein edit distance, the further the two terms are from being identical.For example, if you are measuring the distance between “Cristian” and “Christian,” you’d have a distance of 1 since you’d be one “h” away from an exact match. This term is oftentimes interchangeable with the term “edit distance.”Named for American mathematician Richard Hamming, the Hamming distance (HD) is quite similar to Levenshtein, except that it is primarily utilized in signal processing, whereas the former is often used to calculate the distance in textual strings. This algorithm uses the ASCII (American Standard Code for Information Interchange) table to determine the binary code assigned to each letter in each string to calculate the distance score.Take the textual strings “Corn” and “Cork.” If attempting to find the HD between these two, your answer would be a distance of 2, not 1, like you’d get with the Levenshtein algorithm. To get that score, you have to look at the binary assignation of each letter, one by one. Since the ASCII Binary Character Table assigns the code (01101110) for N and (01101011) for K, you’ll note that the difference between each letter’s code occurs in two locations, thus an HD of 2.This LD variant also finds the minimum number of operations needed to make two strings a direct match, using single-character distance operations like insertion, deletion, and substitution, but, Damerau-Levenshtein takes it one step further by integrating a fourth possible operation – the transposition of two characters to find an approximate match.String 1: MichealString 2: MichaelaOperation 1: transposition: swap characters “a” and “e” Operation 2: insert “a” (end of string 2)Distance = 2Each operation has a count of “1”, so each insertion, deletion, transposition, etc is weighted equally.FM’s use cases are vast, with lots of real-world applications, deduplication being one of the most popular among them. Imagine continuously serving the same digital ad to a user who has already reacted negatively to that ad and favorably to another. How would the user experience be impacted should a financial institution impose fraud detection on a transaction the user repeats on a weekly basis? It’s the use of approximate string matching that has allowed deduplication to streamline records within so many of our modern data systems.When we launched Search and Query back in 2016, one of its key features was an auto-suggest engine with FM. Anyone who has ever surfed the web has seen auto-suggest in action on a search engine. Speaking of search engines, have you ever misspelled a word while conducting a Google search, but still got the results you were looking for? Google will actually serve up what it believes you meant to type out as the main query while providing an option for searching the word(s) as you typed them just below. In this way, fuzzy matching has helped shape how AI/ML has helped improve our most trusted search engines.Research has found that human error accounts for a considerable amount of the duplication that occurs in record keeping and data management. An Online Research Journal study on Perspectives in Health Information Management found that duplicated medical records are not only common but dangerous and costly. The study, led by Beth Haenke Just, MBA, RHIA, FAHIMA, used a multisite dataset of 398,939 patient records and found that the majority of name field mismatches were the result of misspellings (54.14% in first names fields, 33.62% in last names fields, and 58.3% for middle names). Human error is often the biggest obstacle in data management and record linkage. FM has become an indispensable tool for joining imprecise datasets in the medical field, financial services, identifying social security fraud, and much more. In the end, FM has helped save modern enterprises countless man-hours on the often onerous and painstaking work of manual deduplication.Other benefits of FM includeFuzzy Matching algorithms can be implemented in various programming languages like:The implementations are similar, with all languages comparing sets, matching patterns, and determining the statistical distance from the perfect match.With FM, reliability is not a surefire guarantee. Sometimes false positives surface, which calls for manually checking for errors. It’s important to ask: Will a few false positives outweigh the benefit of correctly matching exponentially more data? If it’s negligible, perhaps spending time manually checking for errors would not be time well spent. Matching the right algorithm and programming language with the correct use case is the best way to prevent errors when applying fuzzy logic to data matching.Fuzzy matching with Redis has been available since Redis Stack v1.2.0. It uses the LD algorithm. Learn more about query syntax, used for complex queries, and how Redis uses FM as one of its core rules."
552,https://redis.com/blog/why-the-financial-industry-needs-redis-enterprise/,Why the Financial Industry Needs Redis Enterprise,"October 7, 2020",Prasanna Rajagopal,"The financial industry faces massive challenges. Consumer expectations have increased while regulators have ramped up their scrutiny of financial institutions.Consumers have many choices when it comes to meeting their financial needs and they expect real-time performance and fast decisions from their financial institutions. That puts immense downward pressure on fees while increased volatility and competition is pressuring returns. Investment opportunities vanish as quickly as they appear.At the same time, amidst the global pandemic and associated economic turmoil, central banks are holding interest rates near zero and even discussing negative interest rates, which has affected net interest income and inflated asset prices. Financial institutions are responding by looking to improve profitability by making business decisions faster, for example by approving loans and credit applications more quickly.At the center of these myriad challenges is the need for real-time data. Algorithmic and retail trading volumes have exploded, for example, and consumers now expect real-time data on their banking and brokerage applications, while an explosion in exchange traded funds (ETFs) has created arbitrage opportunities for authorized participants. To take advantage of these opportunities, financial institutions require a high-performance database that delivers sub-millisecond response times for reads and writes, stores data from dozens of data sources in multiple data models, and provides high availability and multi-layered security. Redis Enterprise—built on the popular open-source Redis database—is the perfect tool to address these issues and help make banks competitive.Redis Enterprise can add value to every facet of finance. In this blog, I highlight use cases in three areas of the financial industry—risk modeling, apps for banking and brokerage, and solutions for buy-side institutions—that illustrate the power and potential that Redis Enterprise can bring to your financial institution.Risk and financial modeling have always been an integral part of the industry. But now stricter regulations, increased complexity, and the large volume of transactions undertaken by many institutions make modeling a central tool.Systems built on traditional relational databases can fall short when it comes to the streaming of market data and providing query answers in real time. Modern models must account for thousands (if not millions) of data points and transactions each day, deal with data in multiple formats such as time series for asset pricing and executed trades, JSON for details on each asset or trade, and so on. The complexity of storing and executing multiple risk models at the same time is difficult to architect in a relational database.But that’s only part of the problem. Since the subprime mortgage crisis of 2007-2010, lawmakers have introduced regulations requiring banks to model their financial losses on loans and credit cards for various economic outcomes. The stress tests of the U.S. banking system conducted by the Federal Reserve are an example of this type of risk assessment and modeling. This type of modeling has real implications for the bank’s capital requirements, to shareholders, and even to the economy. The results of these models can force banks to increase their loan-loss reserves, capital available to make new loans and thus earn a profit, or constrain the bank’s ability to pay dividends or do share buybacks.In response to the financial crisis, the Financial Accounting Standards Board (FASB) introduced the Current Expected Credit Loss (CECL) as the new standard to recognize expected losses. Companies must now be able to model and forecast losses for a multitude of economic outcomes.Earlier this year, PwC noted that the COVID-19 pandemic is putting this standard to the test in real time. The FASB intends to have companies recognize losses on a timely basis. As Deloitte has pointed out, new accounting standards usually impact just the accounting department and the software it uses. But CECL requires a robust credit-risk modeling, financial reporting, and governance model. The implementation of these standards and regulations must be underpinned by a robust, fast, and flexible data infrastructure. And these are just two examples of the importance of risk modeling in a financial institution.With Redis Enterprise, you can be proactive in addressing your regulatory and compliance needs. Redis Enterprise can easily scale to store years of data on risk models in memory and score it in real time while offering high-availability deployment models that ensure you are protected against data loss. Changes can be quickly applied and the results can be assessed in seconds or minutes whereas a traditional relational database could take hours to return results.Significantly, Redis Enterprise can be described as a multi-model database. You do not need a separate database management system for each model. For instance, detailed loan information for each loan can be stored in JSON. Time-series data on past or expected losses for various economic scenarios can be stored in Time Series.  You can use Probabilistic to help detect unusual account activity.In a recent interview with Barron’s magazine, Tim Stuart, CFO of Microsoft’s Xbox division, said: “I like to talk about how engagement equals currency…”. This summarizes the attitudes of today’s customers. They prize engagement over everything else.With that in mind, consumer expectations from their financial institutions have never been higher. Their default mode of interaction is now via a mobile device—younger consumers rarely visit a physical banking location. Consumers demand their apps be engaging to use and responsive to the touch. Mobile banking and brokerage apps now rank among the most-used apps by consumers. These consumers demand real-time data about their financial status—at any time and in any location.A sleek user interface is the minimum bar for a client-facing financial application. Getting the financial data to be responsive is a tougher data challenge. Many banks have built their banking applications on top of relational databases built for an era of few transactions and minimal customer queries. They weren’t designed for millions of customers constantly accessing their accounts and transacting millions of times.In some cases, banks have improved the scalability of their relational databases by adding a cache. In other cases, they have upgraded to more expensive, specialized database hardware appliances to buy time. These architecture and hardware changes improve scalability, but often at the expense of increased cost, complexity, and management.You can rethink your mobile application to be responsive, scalable, and highly available using Redis Enterprise. You can use Redis as your primary database and thus reduce complexity while meeting your customers’ ever-increasing demands. When a customer  makes a banking transaction or searches for an analyst’s opinion on a stock, Search and Query—a powerful text search and secondary indexing engine built on top of Redis—can provide real-time data.Asset managers have seen their management fees fall. Clients in search of superior returns eagerly switch from one firm to another. This has increased costs for asset managers as they must offer more incentives to attract investors.Portfolio managers, meanwhile, must analyze thousands of investment opportunities each day to find the best investment ideas. The number of data sources that portfolio management software must deal with has exploded. Real-time risk analysis of a portfolio can be a challenging exercise. Managers want to see a real-time Investment Book of Record (IBOR) on their positions to make timely decisions during trading hours. Asset managers also have a need for net asset value (NAV) calculations during the trading hours. NAV calculations for ETFs are a good example of this. Generating an accurate IBOR or NAV during the trading day can be technically challenging without a data infrastructure that offers millisecond-level latency.In today’s financial industry, milliseconds can make or break a trade. Redis Enterprise can ingest and process millions of data points per second with sub-millisecond latency. Calculating accurate position data in IBOR becomes a breeze at the speed of Redis. Redis Enterprise offers multiple modules such as Search and Query, Time Series, JSON, and others that make life easy for technology teams. Asset management firms can reduce the complexity of their technology stack, reduce cost, and make critical information available in real time to asset managers.These three use cases represent just the tip of the iceberg when it comes to the value that Redis Enterprise can bring to your financial institution. Financial solutions can leverage Redis Enterprise to help reduce costs and friction when dealing with complex financial data from multiple sources and improve overall customer responsiveness while reducing the risks facing your enterprise.To go deeper, read our whitepapers on Building the Highway to Real-Time Financial Services and The Power of Personalization: Driving Digital Banking Success and check out our case study on Deutsche Börse.Cover image via Chronis Yan on Unsplash."
553,https://redis.com/blog/why-your-oracle-db-needs-redis/,3 Reasons Why Your Oracle Database Needs Redis Enterprise,"September 15, 2022",Talon Miller,"How can Redis Enterprise support your Oracle SQL database as a cache or primary database? Migrating to a NoSQL database can be tricky, but luckily, Redis Enterprise can seamlessly fit into your current data stack, allowing it to work in conjunction with your existing architecture. Below, we’ve broken down three reasons why Redis is the answer to your most pressing Oracle obstacles. For a thorough breakdown, explore our solution brief below.Download Modernize Your Oracle Database With Redis EnterpriseOracle is a widely used relational database management system (RDBMS) and well known… sometimes not for the best reasons. When it comes to Oracle performance, we naturally think of Oracle performance tuning for complex, expensive SQL queries and more. When Oracle cost is concerned, we naturally think of astronomical license costs and read/write access costs, to name just a few.What about using Oracle for modern use cases? You might think of Oracle cache, also known as Oracle Times Ten, which is Oracle’s in-memory solution for Oracle caching but doesn’t provide flexibility with data types like hashes, sorted sets, streams, and so many more. This impacts performance and locks development options out of modern use cases.Redis Enterprise, the leading real-time data platform, can be used alongside Oracle to store data in-memory to greatly relieve these common Oracle speed, cost, and data type limitation challenges. Time to do more with your Oracle data and enable modern solutions!Oracle lacks the speed needed to power modern use cases like real-time analytics, fraud detection, and financial transaction processing, just to name a few. Oracle needs frequent tuning, and it’s not a once-and-done kind of a thing; it’s a constant state of “room for improvement.”This is not necessarily unique to Oracle – it’s a common RDBMS challenge! RDBMS require continuous tuning and streamlining to execute SQL statements to improve query response times and application operations. This Oracle performance tuning process starts by pinpointing the source of bottlenecks – is it a complex SQL statement that needs simplifying,  an optimizer problem, or even an issue with the hardware itself?And those are only the high-level issues to troubleshoot; we can dig deeper into input and output (I/O) measures, optimizer metrics, and instance settings before tuning SQL statements one by one.Once you’ve done all you can with I/O measures, the optimizer, and instance settings, performance rests on identifying and optimizing cost-intensive SQL queries, especially if these queries are frequently read!But it doesn’t have to be this way if you use Redis. Consider using Redis Enterprise with your Oracle database to store frequently read data in-memory and gain sub-millisecond performance.Redis Enterprise can also be used for secondary indexing to perform sub-millisecond queries on Oracle data held in secondary keys. No need to avoid mixing data types that eventually cause bottlenecks in your Oracle database; there is no need to reverse engineer complex queries. Once you’ve identified the cost-intensive and complex queries that need fast response times, you can use Redis Enterprise to offload those bottlenecks to improve Oracle performance.Redis Enterprise not only adds speed but greatly reduces the number of read requests sent to your Oracle database, significantly reducing costs. Speaking of Oracle costs, let’s dive into what Redis Enterprise can do for those ever-growing Oracle database license costs.With Oracle, costs can be another major challenge. For starters, Oracle database license costs can add up quickly if you use user- or core-based licenses. Because of the high volume of reads and writes and complex queries that require vast database resources, the costs can add up quickly.This model often leads to an “only use it when you need it” mentality when so many other solutions don’t punish you with user thresholds or inflexible resource billing. So what can we do with our Oracle costs? We return to those two main pain points – frequently accessed read/writes and resource intensive/complex queries.Redis Enterprise can be used to store frequently accessed data where it won’t incur costs based on how many users read/write. Redis Enterprise is also much faster at processing queries,  which ultimately reduces the costs and resources needed. Using Redis Enterprise alongside your Oracle database allows you to incur expensive Oracle database executions only when needed, reducing Oracle transactions (while adding real-time speed) and optimizing those frequently accessed data and pesky queries.And the cherry on top? Redis Enterprise has flexible scalability – use it as you need it. Scale up or down without being locked to a certain amount of cores, nodes, or clusters.Oracle wasn’t built to support the diverse data needs of modern applications. In today’s world, applications need flexible data types and data models that are easily and quickly deployed anywhere in the world. Need more Oracle use cases? Look no further.Redis Enterprise can unlock data in your Oracle database by supporting the diverse data types and data models that are the backbone of modern-day applications.For example, Redis Enterprise has a built-in real-time search engine that can be used on top of your Oracle database to speed up complex queries dramatically. This reduces the time it takes to return that data to your customers and services and could also be used to offload those expensive and complex queries. That’s just one example; there are many other data engines and data models that Redis Enterprise supports out of the box to help consolidate all of your real-time needs on one scalable platform, not just for Oracle:This should be a good start on transforming your Oracle database’s speed and costs while adding modern use cases. The next question you might have is, “How do I start this Oracle integration with Redis Enterprise?” We have Redis Connect, our CDC (change data capture) tool that can help move part of your frequently accessed data or complex queries in this Oracle migration. Check out how to get more out of your Oracle database with Redis Enterprise in our solution brief below."
554,https://redis.com/blog/introduction-to-redisgears/,Introduction to RedisGears,"July 16, 2019",Redis,"At RedisConf19, we announced the release of a new module called RedisGears. You may have already seen some other modules by either Redis or the community at large, but Gears will defy any expectations you have. It really pushes the limits of what is possible with modules. The only caveat is that it’s still in Preview so, while you can already try it out, you will have to wait a bit more for it to get to General Availability and become officially supported.At first glance, RedisGears looks like a general-purpose scripting language that can be used to query your data in Redis. Imagine having a few hashmaps in your Redis database with user-related information such as age and first/last name.Here is the execution breakdown for the RedisGears script:This simple example showcases how you can use a Gears script similar to how you would use the query language for any other database. But, in fact, RedisGears scripts can do much more because they are Python functions running in a full-fledged Python interpreter inside Redis, with virtually no limitations. Let me show you why that matters:In this example, I’ve installed numpy in my server using pip so I can use it inside my scripts. This means that all the Python libraries you love, and even your own code, can now be used to process data inside Redis. How neat is that?In this gist, you can read how to install Python packages in our RedisGears Docker container.You might have noticed by now that one-liners inside redis-cli are not a super clear way to write RedisGears scripts. Thankfully, the RG.PYEXECUTE command is not limited to those. You can also feed it full-fledged Python source files. This also means that the script can contain normal Python functions, so you’re not forced to use lambdas if you don’t want to. Let me show a couple of ways to load a Python script. Here’s a more readable version of the previous example:RedisGears can also understand your cluster’s topology and propagate commands accordingly. We already made implicit use of that feature in our previous examples, since the scripts would behave as intended when run in a cluster (i.e., each shard would do its part of the job and finally aggregate all the partial results if necessary).You’ll occasionally need more fine-grained control over how your computation is executed, especially for multi-stage pipelines where you have an intermediate aggregation/re-shuffle step. For this purpose, you have at your disposal collect and repartition. These will, respectively, go from a distributed sequence of values to a materialized list inside a single node and, inversely, back to a distributed stream sharded according to a strategy of your choice.You can also launch a job that doesn’t require the client to stay connected, and wait for a result. When you add the optional UNBLOCKING argument to RG.PYEXECUTE, you’ll immediately get a token that can be used to check the state of the computation and eventually retrieve the final result. That said, know that RedisGears scripts are not limited to one-off executions when invoked from a client.Have you ever had the need to launch operations inside Redis in response to a keyspace event, or to quickly process new entries in a stream for a situation where spinning up client consumers seems wasteful?RedisGears enables reactive programming at the database level. It’s like using lambda functions, but with a dramatically lower latency, and with much less encoding/decoding overhead.Here’s a script that records all commands run on keys that have an audited- prefix:This second script then reads the audit-logs stream and updates access counts in a sorted set called audit-counts:If you register both queries, you will see that both the stream and counts update in real time. This is a very simple example to show what can be done (clearly not a great audit logging system). If you want a more concrete example, take a look at some recipes.Don’t be afraid to launch demanding jobs. RedisGears scripts run in a separate thread, so they don’t block your Redis instance. This also means that Gears queries can’t be embedded inside a Redis transaction. If you have a cluster constantly under memory pressure or running transactional workloads, Lua scripts will be your best choice to add custom transactional logic to your operations. For everything else, there’s Gears.The quickest way to try out RedisGears is by launching a container. Keep in mind that our modules also work with open source Redis.We have a Docker container on DockerHub that contains all the Redis modules:docker run -p 6379:6379 redis/redismod:latestWe also have a version that contains RedisGears only:docker run -p 6379:6379 redis/redisgears:latestRead more about Redis programmability"
555,https://redis.com/blog/the-results-are-in-redis-usage-survey-2016/,The Results Are In – Redis Usage Survey 2016!,"December 14, 2016",Leena Joshi,"Over Oct and Nov, Redis conducted two surveys on Redis usage. One survey was of Redis customers, conducted by TechValidate, who provides third-party validation of the results. A second survey was done through SurveyMonkey and was mostly promoted to open source Redis users. The two surveys yielded many similarities and differences, highlighted in the details. But to keep the line clear between validated input from Redis’ customers and general input from a larger audience, we divide this into two blog posts.Some notes on this survey – this survey was promoted to the community via various channels, and 80% of the folks who took this survey are open source redis users. A link to the full results is here, but in the interest of protecting the privacy of the survey takers, it is password protected. I will do my best to make the results clear in this blog post. ( but interested parties may contact me for additional insights). So 96 folks took this survey, and 80% of them were OSS Redis users.We asked the Redis use cases question, with more options than last year – and we see the results below:Compared to last year’s results, we see a distinct uptick in use cases such as job & queue management, and real-time analytics but we also see substantial usage as a user session store, for high-speed transactions, notifications, distributed lock, and even machine learning! (we hadn’t asked precisely this last year – I was still new ☺). We were more successful in unearthing the broad range of uses that Redis supports this year as compared to last!The industry-wide applicability of Redis remains broad as ever, but this time around there were distinct mentions from customers of IoT, bots, meteorology, environmental data and much more.The next question was asked slightly differently from the Techvalidate survey ( I neglected to include a tier for mobile applications!). Other than that, the responses line up pretty closely. I had also mistakenly included caching in here, I have excluded those responses since we already caught the content caching in the above question.Similar to the Redis users, when asked, “Is Redis used for data not stored in any other database”, about 67% of survey respondents use Redis as their primary datastore for some of their data.When asked “Do you want to increase your Redis usage”, about 90% of users intend to increase their Redis usage, and they want to for similar reasons as the TechValidate survey.Reasons for increasing Redis usage are below:There are few conclusions to draw about Redis usage from both surveys:If you have additional questions about either of the surveys, you can always reach me on twitter (@socialeena)."
556,https://redis.com/blog/how-redis-enterprise-manages-growing-data/,How Redis Enterprise Manages Growing Data,"August 28, 2019",Roshan Kumar,"We recently surveyed our customers and received some great and interesting feedback. For instance, we found out that 71% of users increased their usage of Redis Enterprise because their business grew.Figure 1. Redis customers increased their Redis usage.Across our customer base, we’ve seen how rapidly database size and throughput requirements can grow. Let’s say you launched a new app some time ago, and it stores data in Redis. Suppose your dataset size was about 1 GB when you launched, but your app became very popular. Your Redis dataset may have grown to over 20 GB, and be on track to reach about 50 GB in the next few days. In this type of scenario, a 100,000 operations/second launch throughput could now be inching towards a requirement of 1,000,000 operations/second – a challenge for any database.Redis Enterprise Cloud users in this situation can easily increase their database size and throughput capacity by simply switching over to a higher limit in the Redis Enterprise Cloud console. This fully managed serverless DBaaS (database-as-a-service) is powered by the Redis Enterprise architecture, which lets you increase your dataset size and throughput in a completely transparent way.Figure 2. Redis Enterprise Cloud includes an administrator dashboard for increasing memory size and throughput.Behind the scenes, this all works seamlessly thanks to the shared-nothing, symmetric architecture of Redis Enterprise. This technology consists of a cluster and a set of physical or virtual nodes. Every node includes zero to a few hundred databases that could be configured as a simple database, a high availability (HA) database, a clustered database or an HA clustered database.Figure 3. Redis Enterprise databases can be configured in simple, HA, clustered and HA clustered modes.As depicted in figure four below, a Redis Enterprise node in the cluster contains Redis shards, a zero latency proxy, a cluster manager and a secure REST API. The proxy maintains the database endpoint and masks cluster complexity by hiding the details. For example, for a clustered database with a dataset spanning multiple shards, the proxy is the single endpoint and forwards each request to the appropriate shard.Figure 4. Redis Enterprise node componentsWhen you increase the memory or throughput size of your database, Redis Enterprise computes the extra number of shards and cluster nodes you need based on your requirements, adds new shards and nodes to the cluster, and rebalances the data for best performance. Redis Enterprise also optimizes your shard size to keep it small enough for fast replication, shard migration, backup and recovery. Figure five shows what happens when you add a new pair of master-replica shards to an existing database.Figure 5. Using Redis Enterprise to scale out with resharding.The proxy in Redis Enterprise plays a key role in this operation. Since it enables a single endpoint to your Redis database that spans multiple shards, even after a resharding, your application still connects with the same endpoint. This ensures you don’t have to make any changes to your application code as your database size increases.Figure 6. A Redis Enterprise database with the proxy front-ending master and replica shardsFor more on exactly how this works, let’s explore the four main steps Redis Enterprise performs during resharding.Step 1: The platform builds two new replicas (R1 and R2), moves half of your dataset from the master to R1 and the other half of the (mutually exclusive) dataset from the replica to R2.Step 2: Next, Redis Enterprise drains all outstanding requests before completing the resharding process. This guarantees consistency and avoids losing any ‘write’ operations.Step 3: The third step includes converting the master to master 1, and the replica to master 2 and then stopping the draining process. Now, new requests are all processed by both master 1 and master 2, and the new shards that were introduced in step 1 (R1 and R2) will become replicas of the two new master shards.Step 4: Finally, the platform trims master 1 and master 2, so that each shard holds a dataset associated with half of the Redis hash-slots.As you can see, Redis Enterprise Cloud manages shards and proxies in a true serverless fashion. As a user, you don’t have to worry about nodes or clusters, and the scaling operation described above is completely transparent to you. If you are using Redis Enterprise software, the admin console provides all the tools you need to scale out linearly in a seamless manner.If your goal is to scale out even higher, you could configure multiple proxy servers in Redis Enterprise. With this configuration, you can leverage our cluster API to connect with all the proxy servers your application requires. Recently, we demonstrated how this unique shared-nothing architecture can scale to 200 million operations/second in a fully linear manner.Redis on Flash (RoF) is another way to scale your database, while saving over 70% in infrastructure costs. This option can be extremely cost effective when your dataset size gets over 100GB, and the average size of values in your dataset is higher than the average size of your keys. Furthermore, RoF is optimal when most database queries are targeted to a subset of your dataset, which allows RoF to keep a working dataset in memory (RAM) only.Figure 7. Redis on Flash’s multi-layered memory architectureHere at Redis, we strive to understand and meet our customers’ needs. When we heard about the pain points around managing growing data, we invested in building the right capabilities to overcome them. Today, over 7,400 enterprise customers benefit from the Redis Enterprise scaling features we described above.Find out about the best scaling option for your situation. Call our experts today at (415) 930-9666. If you already know what size database you need, contact us for pricing."
557,https://redis.com/blog/use-redis-enterprise-kubernetes-release-pivotal-container-service/,Why Use Redis Enterprise Kubernetes Release on Pivotal Container Service?,"August 29, 2018",Vick Kelkar,"Over the last few months our team has been busy working on deploying Redis Enterprise on Kubernetes. Our journey started with writing a simple controller for the Kubernetes release of Redis Enterprise. A few months later, we introduced Helm Chart support, and over the last couple of months, we have been writing an operator for our Kubernetes release.Through this blog post, I would like to present the principles we used for deploying the Kubernetes release of Redis Enterprise on a Pivotal Container Service®  cluster.It is imperative to manage containerized microservices in a cloud-native way, and Kubernetes has become the de facto unit of deployment for microservices architectures. PKS, a certified Kubernetes distribution, can be used in combination with the Pivotal Application System® (PAS)  to manage your entire application lifecycle. Both of these platforms are governed by Cloud Foundry® and BOSH, a cloud orchestration tool. Together they provide an efficient approach for managing stateful services (e.g. your databases), as well as stateless services (e.g. your applications).The three main components of PKS are:For the purpose of this blog post, let’s assume that these components are successfully installed on an underlying infrastructure like vSphere.Redis Enterprise combines the advantages of world-class database technology with the innovation of a vibrant open source Redis community to gain:We use four important principles for Redis Enterprise on PKS in order to maximize a robust deployment:The docker image of the Redis Enterprise for PKS deployment is located here. You can read more about the architecture for Redis Enterprise Kubernetes release here.To get started:In order to measure performance, you can create a Redis database using the Redis Enterprise UI (or API) with the following parameters (Note: this setup assumes there are enough cores in the Kubernetes node to support Redis Enterprise cluster. In the example below we used a 14-shard database):Next, deploy memtier_benchmark on another POD on the same Kubernetes cluster and run memtier_benchmark with the following parameters:-d 100 –pipeline=35 -c 10 -t 8 -n 2000000 –ratio=1:5 –key-pattern=G:G –key-stddev=3 –distinct-client-seed –randomizeUse the metrics screen in the Redis Enterprise UI to monitor the performance of your database under load. As you can see in the figure below, Redis Enterprise can easily reach over 0.5M ops/sec using just one of the cluster nodes over Kubernetes infrastructure, while keeping latency under sub-millisecond.What’s Next?In this blog post, we demonstrated a simplified deployment of PKS by deploying all the resources on a flat network. To write this blog, we used the tech-preview version of the Redis Enterprise release for PKS. As we work towards general availability, we will continue to explore our PKS integration efforts around areas of network segmentation as well as the Kubernetes ingress primitive. Furthermore, we will soon add support for Active-Active geo-distributed Redis CRDTs over PKS to serve globally distributed applications.If you would like to start experimenting with our Redis Enterprise release for PKS, please contact us so that we can help you with your Redis needs."
558,https://redis.com/blog/redis-labs-headed-kubecon-cloudnativecon/,Redis Labs is headed to KubeCon and CloudNativeCon,"December 10, 2018",Vick Kelkar,"This December, Redis — the company behind Redis — is headed to KubeCon and CloudNativeCon in Seattle. Redis is a NoSQL in-memory database and is known for its simplicity and efficient performance. Furthermore, Redis is also the most popular database on Stackoverflow and has surpassed one billion container downloads on Docker Hub. While in Seattle, our team will be showcasing the work that makes Redis Enterprise containers a great choice for developing containerized, stateful microservices on a cloud native platform like Kubernetes.Our Enterprise product addresses the needs of cloud native production environments by providing multi-tenancy, high-availability and automatic failovers on cloud native platforms. We have enhanced the security of our managed Redis offering with Two-Factor authentication. We are working with our partners like RedHat and Pivotal to develop solutions for Kubernetes-based distributions like OpenShift and PKS. For our Redis Enterprise containerized release, we are leveraging Operators to offer a persistent stateful service for your containerized workloads and microservices.Redis Enterprise is an excellent choice of database for your microservices. It supports multi-tenancy: you can configure multiple, isolated databases on a single Redis Enterprise cluster. You can run multiple database endpoints on a simple three-node cluster, allowing each microservice to access its respective database with minimal server infrastructure and/or operational overhead. Redis Enterprise’s Kubernetes integration includes:Come to the Redis booth at KubeCon and CloudNativeCon to hear about new features of Redis, like the Redis Streams data structure, or how you can extend the functionality of Redis using modules like RediSearch and RedisGraph. If you would like to learn how your business can run Redis Enterprise on Kubernetes and/or OpenShift cluster, please don’t hesitate to stop by our booth! If you would like to start experimenting with our Redis Enterprise release for cloud native platforms, please contact us."
559,https://redis.com/blog/results-redis-enterprise-primary-database-session-store-high-speed-transactions/,"The Results Are In… Redis Enterprise for Primary Database, Session Store, High-Speed Transactions and More!","March 6, 2018",Priya Balakrishnan,"Over the course of the last month, we surveyed 130 customers using a third-party validation service called TechValidate. Our goal was to better understand how organizations are using Redis Enterprise and we were honored and humbled by what our customers had to say. In fact, nearly 80% of respondents said they intend to increase their usage of Redis Enterprise in order to serve growing business needs.We’d like to thank all our customers who responded to the survey. We can’t post all the results here but I’ve cherry-picked some of our favorite findings that describe how Redis Enterprise is used in practice. If you’re interested in more details, Techvalidate has published a range of TechFacts, charts and over 51 customer case studies that you can read at your leisure.Without further ado, let’s jump into a brief summary of the results…1We have always known that as our customers begin to use Redis Enterprise in their organization, there is often a viral effect and usage expands to beyond just caching. What is interesting in this latest poll is that the caching use case actually ranked lower than user session stores and high-speed transactions, which are now the top two most common uses for Redis Enterprise. This is validation that Redis Enterprise’s versatile data structures and sophisticated enterprise-class features make it a valuable database across a growing variety of use cases.Compared to last year’s results, we noticed a significant uptick in how organizations are using Redis Enterprise for high-speed transactions and messaging. And we’re seeing consistent usage across real-time analytics, notifications, time-series data and geospatial indexing as well. We believe that machine learning (ML) took a small dip in Redis adoption because the reality is there’s still ways to go when it comes to thoughtfully applying ML in many organizations — despite all the market buzz.2Redis Enterprise is well known for its record-setting performance. That said, we were pleased to see the value our customers place on high availability as well — nearly 80% said they most value the data persistence, auto-failover, cross-zone/multi-region/multi-data center in-memory replication that Redis Enterprise delivers.For example, Serge Aube from Staples shared his love for Redis Enterprise’s impressive performance and availability:Following fast performance and high availability (which were not surprising results), nearly 50% of our customers said they love our built-in seamless scale and clustering capabilities. For instance, at Microsoft, Redis Enterprise was instrumental in handling election night traffic, seamlessly scaling to handle over two billion requests without any service interruption or performance degradation.3As customers learn the shortfalls of their current database deployments, they are looking to move additional data from other databases into Redis Enterprise:This migration is a result of common frustrations organizations face due to the limitations of their current databases. The chart below describes the top reasons why our customers choose Redis Enterprise over other databases.Of course, Redis Enterprise solves all of these challenges around scale, speed, availability, durability, cross-geo-distribution and future-proofs applications with a modern solution.4Lastly, but most noteworthy, we were delighted to learn that a staggering 67% of respondents use Redis Enterprise as their primary database, not storing data in any other system. What this means to us (and to you) is that companies are graduating from using Redis Enterprise as a caching service to using it as a full-fledged database.Since 2014, Redis Enterprise has trailblazed the database world, allowing organizations to solve complex problems and deliver modern applications at record-setting speeds, with reduced infrastructure footprint and greater simplicity. As its reputation and popularity continues to grow and new enterprise-class capabilities continue to be added, we’ve seen a significant acceleration in Redis Enterprise’s adoption amongst some of the largest companies in the world. We are delighted by the depth and breadth of Redis Enterprise usage across a variety of solutions including e-commerce, personalization, customer engagement and social, IoT, fraud mitigation and much more.This April, many of these customers will share their Redis Enterprise experiences and best practices at our annual user conference, RedisConf. We’d love for you to take advantage of this great opportunity to meet with your peers and industry experts and have your questions directly answered by the Redis team.These new Techvalidate results have further energized us, and we’re raring to execute on our exciting roadmap to serve your data needs. You’ll hear more from us in the coming months on the incredibly game-changing capabilities we’re working on. In the meanwhile, if you are interested in learning more, please contact us.Hope to see you at Redisconf!"
560,https://redis.com/blog/how-fast-is-flash/,How fast is flash?,"October 20, 2017",Redis,"With Redis Enterprise, we recently enabled the ability to extend your RAM based storage into Flash memory. Don’t confuse this with some form of persistence– this is a way to let Redis break out of the bounds of the server RAM and into Flash storage as needed. With the advances in Flash memory (NVMe-based SSD storage), the performance becomes very viable, although not quite as quick as RAM alone. This allows you to have hybrid storage -in which data moves between fast RAM and Flash as needed, all managed by Redis Enterprise and without changes to your code.Let’s take a look at the performance characteristics of Redis Enterprise on Flash and how you can test the performance yourself. We suggest installing Redis Enterprise directly as described in our documentation. While we offer instructions on how to use Docker to install Redis Enterprise to test out the product, in this case that method will not yield the highest performance.To best utilize the Flash memory capabilities, we suggest using i3.2xlarge AWS instances. These instances have Non-Volatile Memory Express (NVMe) SSD drives that are key to high-performance hybrid memory extension. The test setup is as follows:Load generation is provided by the memtier_benchmark. This should be in the same region/zone/subnet of your cluster, but should be in a dedicated instance.For this benchmark, we’ll focus on a cluster with replication. Here are some more specifications of our test setup:Everything else should use the default settings.We’ll use memtier_benchmark to fill the database using these arguments:You will need to alter the values of the two items in red. You can find the endpoint address and port on the Redis Enterprise dashboard by selecting your database on the Database page and then clicking on the Configuration tab; the table should have a line that looks similar to this:This benchmark will fill the database with 75 million keys each with a 500 byte payload.The next step is centralization of your keys – effectively, we’ll be loading keys around the median into RAM with the rest being in Flash. This process allows for simulating a more realistic access pattern and controls for the otherwise random nature of the benchmark (which wouldn’t reflect realistic access patterns). This will also mean that subsequent tests will be more consistent. You can read more about this feature on a previous blog post about memtier_benchmark.To centralize, we’ll generate about 20.5 million items in RAM by running:Now that we have prepped the database, let’s generate some load now that we’ve prepped the database.Once you’ve got your test running you’ll be able to monitor the results in the Redis Enterprise dashboard. You should see about 115,000 ops/sec with sub-millisecond latency in this access pattern. It’s also important to note that you may see lower values if you’re looking at the output of memtier_benchmark itself, but this takes the network latency into account, so it’s not really measuring Redis Enterprise directly.This experiment shows that you can achieve an average throughput of more than 100,000 ops/sec at a sub-millisecond latency with only four shards running on a three-node cluster with two serving nodes, which meets the performance requirements of many real world database use cases. In case more throughput or memory is needed, you can scale your cluster by adding more shards and nodes.If you wanted to build a RAM-only version of this database, you’d need three times the number of nodes. Which, in turn, would translate to three times the infrastructure cost for running the database. Using F lash as a RAM extension provides quite the savings.To get started with Redis Enterprise Flash please visit the quick setup instructions here. To find out more about Redis Flash architecture, refer to the Redis Enterprise Flash Architecture blog post. You can start a Redis Enterprise Flash free trial here. Finally to find out more about the performance characteristics of Redis Enterprise with Flash memory extension, contact us."
561,https://redis.com/blog/nosql-data-modeling/,NoSQL Data Modeling with Redis,"August 17, 2022",Will Johnston,"“8 Data Modeling Patterns in Redis,” a comprehensive e-book on data modeling in NoSQL, thoroughly examines eight data models that developers can utilize in Redis to build modern applications without the obstacles presented by traditional relational databases. Here’s a bit of what you can expect within its pages.Download “8 Data Modeling Patterns in Redis“Unlike relational databases, you don’t have to have your schema design figured out at the onset. That’s what makes NoSQL such a sound choice for many developers. In SQL, your data has to fit the data model, which can lead to lost data. In a schemaless design, your data remains untouched and remains accessible whenever needed.When modeling data within a relational database (SQL database), developers often need to anticipate the future of the application from the beginning. What are the possible features? How can you design a data model such that it is flexible enough to accommodate any change that the business might require?Relational databases are inflexible. They require lots of upfront knowledge and make storing large sets of data in varying models more than just a little difficult. From transferring data into tables or collections, and the complex queries required to disperse it, the traditional SQL route of data modeling is losing popularity in the face of all the nuances required to build modern enterprise applications.Here are eight prime data modeling techniques developers use in Redis and NoSQL.The Embedded Pattern allows for two separate tables or collections to be bundled together, with one table embedded into the other. The Embedded Pattern is a great model for keeping different tables with information that relates to one another, like a table of product names and one of the product details, in the same place. This makes it easier to find all relevant data and understand what their relationship to each other is in the data structure. It also helps your application retrieve the data in both tables in one query, enhancing its overall read performance.As you’d find in a relational database, the above description describes how a one-to-one data model works.Say you’re building a product catalog for a clothing retailer. The first table could contain the product ID number, name, image, and price, while the other table stores dimensions, specifications, etc.So, why two tables for data that seem inextricably linked? Why not just one table for data storage and call it a day? Namely, because each table functions as its own view in an application. When showing a list of products, you might only show name, image, and price. The detailed view of a single product would show the rest of the fields. In SQL, the easiest way would be to use two tables, so it’s easier to query for only the specific fields you need. In NoSQL, you would only need a single collection since you can nest the details inside a “details” field and not retrieve it if you don’t need it.Take a look at a quick one-to-one relationship model in Redis:First, it’s worth noting that one-to-many and many-to-many use both the Embedded and the Partial Embed Pattern.When you want to model one-to-many relationships, you embed for bounded lists (i.e. lists of a known size) and keep separate collections for unbounded lists.When building an app that contains products, you typically need real product feedback on your app to establish credibility with your customers. In this case, the product is the “one,” and the many reviews, which include an author name, date of publication, rating, and comment, are the ‘many’ variables in question.See it in action when you import the Redis OM library and start modeling:SQL would require you to create a separate table to store the relationship between the datasets of two tables, whereas, in NoSQL, you can create what’s called two-way embedding.The video below presents an instructor/course example to illustrate how this model works, using one table for instructors and another for courses. NoSQL simplifies the relational database solution by embedding a list of instructor keys in the course JSON document and a list of course keys in the instructor JSON document.Bounded sides mean there’s a limitation in how many data points there are in a dataset. We know there is not an infinite number of professors or courses. Bounded just means it’s not infinite.In SQL, the unbounded sides of a many-to-many relationship could take shape as a table of instructors, and another table with students, where the number enrolled could be infinite in this example. In non-relational databases, what you’d want to do is embed your list on the bounded side.So, if both sides are bounded, it’s possible to embed on both sides, but if only one side is bounded, that would be the collection in which to embed your list.Take a look at what many-to-many relationships look like in RedisInsight:When building read-heavy applications, consider using the Aggregate Pattern to reduce the overhead at read time created by calculating aggregate information on-the-fly. Also known as the Computed Pattern, this model precalculates certain fields during writes rather than reads, which saves read time and overhead on the server and database.Watch how we model with the aggregate pattern, importing Redis OM for Node.js in Redis Stack:In a relational database, you need separate tables to store specifics about the different types of products you are working with. To get all your products, all tables with all their separate specifics must be joined together, which creates serious overhead.When building data models, the polymorphic pattern allows you to store many different kinds of products and their unique fields all within the same collection.With Redis Stack, all schemas are flexible and let you distinguish type fields that group collections together. Start reducing the number of collections used to store data and simplify app logic in queries using the polymorphic pattern with Redis Stack:When building read-heavy applications, consider using the Bucket Pattern to reduce the overhead at read time created by storing and aggregating time-series data as you go. Storing each bit of data as it trickles puts extra strain on your system. Instead, the Bucket Pattern helps to – you guessed it – “bucket” your time series data according to certain spans of time, lowering your processing time.Watch as we demonstrate the Bucket Pattern using Node.js and Redis Stack’s built-in time-series capabilities:How are you tracking your ongoing document changes? That’s where the Revision Pattern applies.Chances are, you’ve probably used Google Docs in your professional life. Imagine you want to post about the latest product you and your team just developed. The living document displays the post as it stands with the latest revisions, but all the other revisions from all past contributors are still stored and available if needed. This is a perfect example of how the Revision Pattern works. With it, you can store all the revisions you’ve made to a post, and the post itself, into one single document, simplifying your queries and finessing your schemas.From the legal industry to financial services, healthcare, publishing, and insurance industries, the Revision Pattern can be leveraged in many use cases reliant on real-time data.See how we model using the revision pattern in Redis OM for Python:Typically, you want to avoid JOIN operations in non-relational databases. The Tree and Graph Pattern is particularly useful when those are unavoidable in your schema, and you’re working with heavy JOIN-based operations like HR systems, CMSs, product catalogs, and social networks.The tree pattern isn’t a stranger to the relational model – you’ll often see the tree pattern like you would in an organization chart or lineage map. In NoSQL, you can leverage these patterns to cut through the complications of JOIN operations without the complexity of SQL queries.Watch how we model an organizational chart using the Tree and Graph Pattern in Redis Stack:One useful protocol to change your data model and upgrade your data is the Schema Version Pattern. When building applications with a SQL database, it’s very difficult to make any changes to your model and application logic once you’ve started building your schema. In NoSQL, you can leverage this model (try Node.js in Redis Stack) to pivot more easily.It is recommended you always assign a version to your documents so that you can change them in the future without having to worry about immediately migrating all of your data and code. The Schema Version Pattern is a way of assigning a version to your data model, usually done at the document level. You can also choose to version all of your data as part of an API.You can apply the Schema Version Pattern to your existing code. See how Redis Stack makes it possible:"
562,https://redis.com/blog/data-economy-podcast-novartis-data-science-company/,The Data Economy: How Novartis is Becoming a Data Science Company with AI Innovations,"April 26, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.“The decisions that we make based on the models will affect patients’ lives.”Bülent Kiziltan, Ph.D., Head of Causal and Predictive Analytics at Novartis’ AI and innovation lab, shared this important point during his recent interview on The Data Economy CXO podcast, presented by Redis and hosted by Michael Krigsman of CXOTalk. A big part of the AI lab’s mission is to improve efficiencies in bringing new drugs to market, and Kiziltan stresses how data, analytics, industry partnerships, and talent from diverse backgrounds are core to their strategy. Kiziltan discloses that the goal is to augment the drug development and discovery process, and they are partnering, not replacing the lab because the stakes are so high.Here’s how Kiziltan articulates the goal: “The cost goes up linearly with the increased time spent on a particular drug development and discovery process. We can shorten the period by creating additional evidence using the data science methodologies that we are developing, and also target specific patients and cohorts that will be more suitable for specific studies.”Kiziltan says that Novartis started its transformation into a data science company several years ago. The AI innovation lab is over a year old, and in addition to the causal and predictive analytics discipline that he leads, there are natural language processing and image analytics pillars. The three groups have access to 20 petabytes of data, including patient data from 2 million people, images, scans, and a library of close to 2 million compounds.And how is all this data managed? Novartis technology group adopted a hybrid-cloud strategy to meet their needs and partnered with Microsoft Research on the technology and implementation. The AI lab is one consumer of this technology. In the domains of healthcare and biotech, “the more data, the better,” says Kiziltan. “But the amount of data is a constant problem because it takes a long time to gather and clean. Providing access across different business units can be challenging because of the stringent regulation.”Novartis is a Swiss multinational pharmaceutical company with over $48 billion in revenue and 100,000 employees. Now that it has transformed into a data science company, Novartis is reimagining its enterprise for both long-term opportunities and short-term gains.In the episode, Kiziltan shares a key lesson for CIOs, Chief Data Officers, and AI/analytics leaders: “To sustain the value creation that comes from data science and AI, a smart, strategic roadmap is to invest both in R&D-driven, exploratory core capabilities as well as building strategies for real-world impacts that can be executed immediately.”Enterprise CIOs leading digital transformations should note how Kiziltan guides investments toward long-term competitive advantages while reducing delivery costs. Novartis’ investment in its AI lab is a business opportunity to improve resiliency and agility.In the podcast, Kiziltan shares some of the operational and data challenges his team faces:CIOs and their teams likely face some of these challenges, and Kiziltan suggests following a single strategic approach to collaborating and finding solutions.While a variety of technical skills are required to solve these kinds of problems, Kiziltan recognizes that hiring a diverse team with strong problem-solving skills is also important because data science is changing so rapidly.“We have been attracting talent that can bring in their diverse backgrounds into our operations,” he says. “We hire from very different domains, including physics, mathematics, bioinformatics, and chemistry. Even people with a social sciences background have built some analytical skills that are contributing to our ongoing efforts. I focus on curiosity and the potential to learn.”In fact, Kiziltan himself comes from an astrophysics background and has applied mathematics from this discipline to his work.  He says, “Innovation can happen in two ways: One way to innovate is to build and develop new methodologies, but the other means of innovating is to apply methodologies to new problems.”Please tune in to the podcast to hear more of Kiziltan’s insights on AI and the transformation of Novartis into a data science company.Watch more episodes of The Data Economy podcast."
563,https://redis.com/blog/redisdays-london-2022-recap/,RedisDays London 2022 Overview,"March 28, 2022",Henry Tam,"RedisDays, our three-part worldwide virtual event kicked off with a stop in London this year, where our Redis experts and esteemed guests dug deep into the power of sub-millisecond speed.RedisDays is an opportunity to share with our community all the learnings and technical strides we’ve made in the past year, with new product announcements, best practices, and new customer use cases for real-time data. Below are some of the highlights from RedisDays London 2022.Ofer Bengal, Redis Co-Founder and CEO, kicked things off with a keynote speech that boils down to one core idea: Many people use real-time data without noticing it. Real-time data is the technical backbone of modern living, with use cases that extend to manufacturing, airline, hospitality, telecommunications, healthcare, online advertising, cybersecurity, gaming, digital mobility, shipping services, social media, among many others.Besides highlighting how real-time data at scale is used daily to improve customer experience in many applications and touch points, here are the key takeaways and product announcements from London’s keynote:Click here to register and view the keynote on-demand.In this session, Ash Sahu, Sr. Director of Product Marketing, and Pieter Cailliau, Director of Product Management at Redis, focused on the real-time JSON document store. This presentation addressed how many types of data, including JSON, are needed for modern digital applications, but are restricted by rigid schemas of relational database management systems (RDBMS) and the slow performance of disk-based document databases.Here are some noteworthy highlights from this presentation:Want to learn more? Watch the session in its entirety by registering here.This session, led by David Loshin, President of Knowledge Integrity Inc., and Henry Tam, Sr. Solutions Marketing Manager at Redis, showcased best practices for modernizing data architectures still hampered by legacy technologies.So what were the key takeaways from this session? Here they are at a glance:Missed the session? Watch it here.RedisDays London wrapped up with a fireside chat between Allen Terleto, Field CTO at Redis, and Alexander Mikhalev, AI/ML Architect at Nationwide Building Society, who discussed building real-time natural language processing (NLP) data pipelines using Redis to transform financial services processes.Here are some of the main points addressed during this one-on-one conversation:Interested in learning more about NLP data pipelines? Take notes by watching on-demand."
564,https://redis.com/blog/building-a-health-care-provider-finder/,Building a Health-Care-Provider Finder,"May 26, 2020",Redis,"For patients, navigating the healthcare landscape can be bewildering. The terminology and options are often unfamiliar, and it can be difficult to even figure out how to get started. A “store finder” function, commonly found on retail sites, can help by letting you find a doctor or other healthcare provider located near you, based on a postal or zip code.Unfortunately, though, this functionality isn’t always that easy to create. We live on a sphere, and if we want to find locations within a given distance in any direction, we have to project a circle onto a sphere and use a coordinate system that isn’t perfectly straightforward. All this equates to not-so-simple math. Redis, thankfully, has a wide array of geospatial capabilities to help you build out this type of functionality.The most basic question we’re answering in this problem is pretty straightforward: “What health-care providers are near me?” To do this, you need the list of healthcare providers, the providers’ locations in latitude and longitude, and the location of the patient asking the question. The list of providers could look something like this:This is a small list for demonstration purposes, but in Redis this list could be quite vast, limited only by the amount of memory available.Finding the patient’s location can be done a number of ways: reverse IP, postal-code matching (called a geocoder), or even device GPS. Each method has pros, cons, and specific implementation details that are beyond the scope of this post. Regardless of the method, however, they all resolve down to a latitude and longitude. We’ll just assume the location of 53.5469, -113.4977 for now.Getting the data into Redis can be a one-time affair or can be updated regularly as the list of providers changes. The GEOADD commands add a location to what’s known as a geoset. Geosets are a variant of Sorted Sets that encode the longitude and latitude into a GeoHash, which is stored in the Sorted Set member’s score. Let’s see what that looks like:> GEOADD providers -113.4967 53.5574 royal-alex
(integer) 1
> GEOADD providers -113.5313 53.5177 cross-cancer-institute
(integer) 1
> GEOADD providers -113.4283 53.4608 grey-nuns-community-hospital
(integer) 1
> GEOADD providers -113.5247 53.5205 university-of-alberta-hospital
(integer) 1
> GEOADD providers -113.6119 53.5207 misericordia-community-hospital
(integer) 1Of note in this example are the arguments. The first argument is the key, as typical in Redis. The second argument is the longitude, the third is the latitude, and finally the last argument is the member. (It’s super important to note that order of longitude and latitude are reversed from what you’re probably used to seeing.)Now that we have our data in the system, whenever a user wants to find a provider, we just need to run a single Redis command to find the locations close to them:> GEORADIUS providers -113.4977 53.5469 5 kmThe first argument is our same key from before and the second and third arguments are the longitude and latitude (respectively) of the user. The fourth and fifth arguments are the distance and unit (km for kilometers, mi for miles, f for feet and, m for meters). The output will look like this:1) ""royal-alex""
2) ""university-of-alberta-hospital""
3) ""cross-cancer-institute""The example above would enable patients to find the healthcare nearest facility, but what if they want to go to a provider that offers something specific? What if they need a facility that offers more than one speciality? Redis can help with this, too.Instead of using the GEO family of commands, we’ll use the Search and Query module. Search and Query has a much richer capability to query data, yet retains the geospatial capabilities of the GEO family of commands. These richer capabilities do, however, require us to create a schema first.> FT.CREATE ft_providers SCHEMA name TEXT services TAG location GEOThis creates an index called ft_providers with three fields: name, services, and location. name is a text field, so it can hold human language; services is a tag field that holds the tags representing the services offered; and finally location holds the latitude and longitude for the location.Now, let’s add our locations to the index. We’ll use the FT.ADD command, which requires the index followed by a document ID to uniquely identify the document and then a document score. After this, the SCHEMA reserved word demarcates the options from the fields of the document, which follow in field name, value order. Let’s take a look:Once we have the data in Search and Query, we can start searching for facilities. In Search and Query we use a command called FT.SEARCH. The first argument is the index to search and the second argument is the query. In Search and Query, you specify a query that will determine what results are returned back to you. Queries can be quite simple or very complex, but, unlike in some other databases, the queries never do anything administrative or destructive.Let’s say you want to find providers near the sample location used above, but only providers that offer ultrasound services. In Search and Query, the query would look like this:The @ symbol in Search and Query means “search in a specific field.” The first part of the query is a search clause for a geospatial field named location. You’ll notice between the square brackets are the same arguments you’d see in a GEORADIUS command: longitude followed by latitude, then the search radius and unit. The second @ symbol is followed by the field name services, which means we’re searching in the services field of each document. Enclosed in curly braces is the name of tags that are required to be in the field. In this case, we’re looking for documents that contain the tag ultrasound. There is a space between the location and services search clauses—in Search and Query this is an implicit AND.When building a UI, you can compose various user input into the same query to refine the results. Here’s a rough sketch of how this might work:From the perspective of your application, the UI elements are a representation of the string, and as they change you template them into the string that is passed to FT.SEARCH. String interpolation inside things like SQL queries is risky (see little Bobby Tables) due to administrative operations in the SQL language. In contrast, Search and Query queries can only find documents and not perform administrative operations, so only simple input validation and sanity checks are required.Stepping up the complexity of the search, if a user checks both ultrasound and geriatrics, you would change the services clause to include both by inserting a pipe (|) between the two tags, as shown here:Search and Query is flexible enough to handle practically any other feature you’d need in this type of application. Operations like name and name prefix search, sound-alikes names, and street names are all just incremental steps on this type of query. For example, you could add a cool wait-time feature by including a numeric field in the schema—because Search and Query is a real-time search engine, it is possible to update this value rapidly, so patients trying to find the provider with the shortest wait time would always get-up-to-date results.Redis can help you build a simple health-care-provider finder with just a few lines of code, and Search and Query can fill out practically any feature you might need so patients can find the provider that meets their needs. Because Redis and Search and Query have built-in geospatial capabilities, you don’t need to do any math. Search and Query gives you a rich query capability that is safe to accept input from patients and is able to be updated in real-time, meaning you can make changes without delivering out-of-date information to your patients."
565,https://redis.com/blog/redis-enterprise-google-cloud-platform-marketplace/,Get Redis Enterprise Cloud Through Google Cloud Marketplace,"October 24, 2019",Aviad Abutbul and Hayley Johnson,"Today, we’re excited to announce the next step in Redis’ partnership with Google Cloud: general availability of Redis Enterprise Cloud, the fully managed Database-as-a-Service (DBaaS) on Google Cloud, which was announced earlier this year at Google Cloud Next ‘19At Google Cloud Next, Google Cloud CEO Thomas Kurian and Redis co-founder and CEO Ofer Bengal stood together on stage to announce an expanded partnership with the goal of giving our joint customers a simplified and streamlined experience for building and running modern high-performance applications. With a commitment rooted in open source, they promised to offer Redis Enterprise as a tightly integrated service on Google Cloud, giving end users access to the unmatched speed and performance of Redis without the barriers associated with procurement and management.Redis Enterprise Cloud is now available for purchase through the Google Cloud Marketplace as an on-demand service metered by per-minute usage. When deploying Redis Enterprise Cloud through the Google Marketplace, users will benefit from unified billing—which means you’ll get a single bill from Google covering your Redis Enterprise Cloud along with your existing Google Cloud usage. Just as important in many cases, customers can use their financial commitments with Google Cloud towards the purchase of Redis Enterprise Cloud.Beyond the ease of purchasing, joint customers will also be able to leverage the capabilities of Redis Enterprise without having to deal with the operational overhead of installing and managing it.Today, leading companies in industries ranging from retail and financial services to gaming and healthcare rely on Redis Enterprise on the Google marketplace. The combination helps improve operational efficiency and increase developer productivity, while enabling the always-on availability and performance required from the applications that power their businesses.Redis & Google Cloud Marketplace lets you into Google’s world-class and global cloud infrastructure, including fully managed, serverless offerings, to build, operate, and grow your business with industry-leading security, easy-to-use and implement artificial intelligence and machine learning capabilities, and the flexibility to work with hybrid and multi-cloud architectures.This tight-knit integration between Redis and Google Cloud makes it simple to leverage “the most loved database” by developers, for building modern cloud-native applications. Using Redis Enterprise Cloud services on Google Cloud delivers a long list of turn-key operational benefits:Together, Redis Enterprise on Google Cloud marketplace allows your developers to do what they do best: build innovative software that powers your business, backed by the tools they love.Deploy Redis Enterprise Cloud on the Google Cloud Marketplace. We look forward to the continued growth of our partnership with Google Cloud, and the value this service will provide to our customers."
566,https://redis.com/blog/azure-cache-for-redis-enterprise-tiers-general-availability/,"Azure Cache for Redis, Enterprise Tiers Are Now Generally Available","March 2, 2021",Amiram Mizne,"This morning, Microsoft and Redis jointly announced the general availability of Azure Cache for Redis, Enterprise tiers. The service has been in public preview since last October, and is already serving customers with production Redis workloads. The GA release is now enhanced with previews of active geo-replication (with up to 99.999% availability), and disk persistence with recovery while being rolled out to an increasing number of Azure regions.The following Enterprise and Flash tier capabilities are now generally available:In addition to going GA, Azure Cache for Redis, Enterprise tiers now includes previews of powerful new features.We’re excited to announce the public preview availability of active geo-replication. Enterprise tiers’ CRDTs (conflict-free replicated data-types) based technology allows developers to create geo-distributed applications that enjoy local sub-millisecond Redis read/write latencies with much better resilience to failure.Active geo-replication empowers operators to deploy Redis datasets across multiple Azure regions, with managed multi-primary replication across the Azure backbone network. Whether deploying a nationwide multi-region application or a globally distributed one, active geo-replication addresses key use-cases such as global session management, world-wide fraud detection, geo-distributed search, and real-time inventory management.When it becomes generally available later this year, active geo-replication will provide up to 99.999% availability of service, enabling operators to bring the power of Redis to their organizations’ most mission-critical applications.A demonstration of the active geo-replication capabilities will be available in the Azure Cache for Redis session at Microsoft Ignite.Another preview feature now available is persistence to disk and managed recovery from persistence.Redis persistence to disk provides durability in the rare cases where data stored in RAM is lost due to underlying compute failure of both the primary and replica Redis servers, which are deployed on separate compute nodes by default.Enterprise tiers provide two modes of persistence to disk storage attached to the Enterprise cluster nodes:Head over to Azure Cache for Redis documentation to learn more about persistence.Azure Cache for Redis Enterprise tiers are powered by Redis Enterprise Software and operated as a fully managed service by Azure. This unique integration lets developers and operators create, manage, and consume Enterprise tier featured Redis workloads natively in the Azure environment.The purchasing process is made seamless through integral billing, allowing customers to procure Enterprise-tier services as they do other Azure Cache for Redis tiers during the resource-creation process. Most importantly, for customers with a Microsoft Azure Commitment to Consume (MACC) agreement, their Redis Enterprise spend will automatically be applied to consume their Azure commitment.Familiar Azure tools natively support database resources CRUD (Create, Read, Update, and Delete) operations. The Enterprise tiers’ entire lifecycle is manually managed through the Azure Portal or Azure CLI, and PowerShell. Automation of operations is achieved by employing the Azure Terraform provider, ARM templates, and REST API while monitoring through Azure Monitor or via the Redis datasource for Grafana.Many Azure services are already pre-integrated with Azure Cache for Redis and with the Enterprise tiers.A recent benchmark study conducted by Microsoft and GigaOm demonstrated a more than 800% throughput performance improvement and a more than 1,000% latency improvement to Azure SQL and PostgreSQL by deploying Azure Cache for Redis with your application. Read more about how Azure Cache for Redis can improve Azure SQL and Azure Database for PostgreSQL performance in the Azure Cache for Redis Benchmarking Study.Enterprise tiers also operate seamlessly with the large ecosystem of clients and development frameworks, including Azure Spring Cloud.The Azure Cache for Redis Enterprise tiers extend beyond the native data structures of Redis, allowing developers to do more with Redis by leveraging Redis modules. This represents a huge advantage for developers by allowing them to address more advanced use cases. The modules supported in public preview are:The Enterprise tiers create a natural progression of capabilities, extending the existing Azure Cache for Redis tiers with incremental features, novel use-cases, enhanced service availability, and higher performance.This table compares the essential dimensions of each tier:In a recent benchmark conducted the Enterprise tier (Redis on RAM) performed up to 70% more operations per second and provided up to 40% improved latency versus the Premium tier.The benchmark compared E20 and E100 Enterprise tiers with their memory-size equivalent P3 and P5 tiers, using the memtier-benchmark tool, and the following key parameters:The benchmark measured average latency as seen from the client including RTT (roundtrip time), and the overall maximum achievable throughput.Note that this benchmark represents Azure Cache for Redis performance, across tiers, at scale 1x deployment. Users can expect up to Nx ops/sec improvement at each of the scale-out levels and up to 10x improvement at the current maximum scale of 10x.Head over to the Azure Marketplace to deploy the Azure Cache for Redis, Enterprise tiers and experience these exciting new capabilities first hand. You can also visit the Azure page on Redis and share your information—a member of our team will contact you.Learn more about the Azure Cache for Redis offering here:"
567,https://redis.com/blog/build-your-financial-application-on-redistimeseries/,Build Your Financial Application on RedisTimeSeries,"December 9, 2020",Prasanna Rajagopal,"Broadly speaking, there are two types of investors in the world. Fundamental investors look at the fundamental metrics, such as a company’s business model, revenues, earnings, cashflows, current valuation, risks, and growth prospects, when deciding whether to invest in a company. Warren Buffett became famous and one of the world’s richest men by finding undervalued companies through his fundamental research.On the other hand, technical investors pay very little or no attention to the company’s fundamentals, instead focusing their attention on deriving buy, sell, and hold signals from hundreds of technical indicators. These technical investors trade on a daily, weekly, or monthly basis.I am not here to argue for or against either of these investing styles. But both fundamental and technical investors study, analyze, and make decisions based on hundreds, if not thousands, of data sources.Fundamental investors study balance sheets, income statements, cash flow statements, demographic trends, thematic trends, and social trends across thousands of companies and hundreds of industries around the globe. They build financial models or analytics on top of that data to better understand the trajectory of companies and the growth prospects of companies.Technical investors look at hundreds of technical indicators on an hourly or a daily basis to derive investing signals. Investors who rely on technical indicators have to deal with a vast array of data across multiple time frames to build their trading strategies. They may need to query a vast number of indicators and require virtually instant answers. They may need to quickly adapt their algorithms and trading strategies to fast-changing markets. Redis Enterprise can address all these challenges.Recently, I wrote about why Redis Enterprise is an essential tool for the financial industry. In this blog post, I will show how RedisTimeSeries can be used to store, aggregate, and query stock prices and technical indicators. The principles I will outline can also be used to store and query financial information from companies or any other time-series data used by fundamental investors.The RedisTimeSeries module can ingest and query millions of events, samples, and pricing data. RedisTimeSeries is best suited for storing related pairs of times and values to help spot trends in use cases ranging from IoT to healthcare to finance. RedisTimeSeries offers aggregations such as average, sum, minimum, maximum, standard deviation, range, etc. to help you easily analyze data and make decisions.In this post I am going to illustrate a model for using RedisTimeSeries to store stock prices and technical indicators. I will cover the creation of various time series for prices and indicators, show how to create aggregations on top of the raw time series, and demonstrate how easily bulk time series can be ingested and queried using various RedisTimeSeries commands. I have also provided sample code in Python that you can use as a starting point for your own use case.Rightly or wrongly, many people consider the Dow Jones Industrial Average (DJIA) the bellwether for the U.S. economy. I wish to track a technical indicator of all the 30 stocks in the DJIA along with the price and trading volume of the stocks. I kick things off by tracking the stock prices of Dow-component Goldman Sachs and one of the technical indicators for that stock. One of my favorite technical indicators is the Relative Strength Index (RSI). RSI is a momentum indicator used to measure a stock that may be in overbought or oversold territory. When the RSI moves near or below 30, a stock may be considered oversold and may present a buying opportunity. Likewise, when the RSI moves above 70, it may be entering overbought territory and signaling a good time to sell.Relative Strength Index for Goldman Sachs GroupDuring trading hours, the RSI, just like any other technical indicator, varies as trades are executed based on demand and supply of Goldman Sachs’ shares. We can use the RedisTimeSeries module to help answer a number of key questions:We can use RedisTimeSeries queries to programmatically identify the minimum and maximum RSI values and stock prices during a designated time period. For example, what if I wanted to find the minimum and maximum RSI numbers during each 15-minute period of the trading day? When the RSI is at or near 30, I may want to generate an alert, place a trade, or kickoff another complex trading workflow to analyze other technical indicators before buying or selling. Anything you wish to do can be easily modeled in a RedisTimeSeries database.The easiest way to get hands-on experience with RedisTimeSeries is to run the Docker image for RedisTimeSeries.Execute the following command to pull and run the Docker image:The Redis team made a deliberate design decision for the RedisTimeSeries module to hold a single metric in each time series. This simplicity in the data model makes the insertion and retrieval of data super fast. You can add new time series as needed without worrying about breaking your existing application. That means you can add new data sources with ease without worrying about breaking the database schema or your application.Once you have the RedisTimeSeries container up and running you can connect to the server (make sure you have the right IP address or hostname) using Python as follows:With the design principle of holding a single metric in each time series, we can store the intraday stock price and RSI for each of DJIA stock in its own time series. Below, I have created a time series for the Goldman Sachs Group, Inc. (NYSE: GS). I have named the intraday RSI for Goldman Sachs ‘DAILYRSI:GS’ and I have applied various labels to each time series—labeling a time series lets you query across all keys using a label.Here, I have created a time series for Goldman Sachs’ intraday stock prices called ‘INTRADAYPRICES:GS’:Next, I created various aggregations on the RSI data within certain 15-minute timeframes. (We could have created aggregations at longer or shorter time frames to suit our trading needs.) These aggregations will let us look at first, last, min, max, and range values for RSI. To create an aggregation, we first create a time series to store the aggregation and then create a rule to populate the time series with the aggregated value. In this case, we created a time series called ‘DAILYRSI15MINRNG:GS’ to store the range for RSI within a 15-minute time period. The ‘createrule’ applies a range function on the raw data (‘DAILYRSI:GS’) during each 15-minute time frame and aggregates that into ‘DAILYRSI15MINRNG:GS’.Here, I created a couple of rules to calculate the range and standard deviation for Goldman Sachs’ stock price during each 15-minute interval during the trading day:You can create time series for all 30 stocks in the Dow Jones Industrial Average (DJIA)  in a similar fashion.There are two ways to ingest data into the RedisTimeSeries. The TS.ADD command allows you to add each stock price or technical indicator to a time series. But because data from the financial market is produced almost continuously and you need to add multiple samples to RedisTimeSeries, it’s better to use the TS.MADD method. The MADD function takes a list of tuples as an argument. Each tuple takes the name of the time-series key, the timestamp, and the value:You can use the following Python command to insert data into a time-series key:As with most things in RedisTimeSeries, it’s easy to query the database. Range and standard deviation for a stock price or a technical indicator is an indication of volatility. The following query would let you see the price range and standard deviation for the Goldman Sachs stock price within every 15-minute interval:This query of the range aggregation gives you the following result set:As this query result shows, at the start of the trading day at 9:30 a.m. on October 26, 2020 ET (integer timestamp = 1603704600) the Goldman Sachs stock was volatile, trading with a range of $1.75. Now compare this to the next 15 minutes, starting at 9:45 a.m. on October 26, (integer timestamp = 1603705500), where the volatility dropped with the range of $0.77. The range of prices for Goldman Sachs stock continued to drop in the subsequent 15-minute intervals and the volatility never returned to the levels reached during the opening 15-minute interval.This data from RedisTimeSeries can be easily visualized in dashboards and charts. The charts below show Goldman Sachs’ price range in 15-minute intervals:The chart below shows the same data in a line chart format, indicating that the price range in which Goldman Sachs trades becomes much tighter as time progresses through the day (this chart shows integer timestamps on the x-axis):You can do a similar query with another measure of volatility—standard deviation—as the aggregation function:This opens up many intriguing possibilities. Say you wanted to know the price of Goldman Sachs’ stock when the intraday RSI value is between 30 and 40. You can query the time series for RSI and the time series of the stock prices to identify profitable entry points for a trade.Here, I am querying the RSI value for Goldman Sachs at a time frame between 1605260100 (9:35 a.m. ET on November 13, 2020) and 1605260940 (9:49 a.m. ET on that day).The query finds that the RSI value was 34.2996427544861 at 1605260820 (9:47 a.m. ET).Now I can query the Goldman Sachs’ intraday prices for the same interval as the one used for the RSI query, or change the query programmatically to reflect when the RSI value was at 34.29. Here’s an example using  the same time frame as the RSI query.The query returns Goldman Sachs’ stock prices for the specified range: At 1605260820 (9:47 a.m. ET on November 13, 2020) the price was $217.18.RedisTimeSeries offers a powerful way to query across multiple time series at once. The TS.MGET command lets you query across multiple time series using filters. We have already created various time series and attached labels to them. Now those labels can act as filters to query across the time series.The following Python code applies two filters based on a couple of labels: “DESC” and “TIMEFRAME”. The parameter “with_labels=False” allows the result set to be returned without the labels for each value:This query would present a result similar to the one shown here, which returns the last RSI values across a number of stocks:If I had set “with_labels=True”, then the result would have included all the labels on each of the time series, as shown here:This blog post has illustrated some of RedisTimeSeries’ many commands to flexibly build a financial application that is dependent on time-series data. Stock traders, for example, need to be able to make high-stakes decisions in real time based on dozens of variables. RedisTimeSeries is schemaless, which means that you can load data without defining schema, add new fields on the fly, or change your data model should your business circumstances change. Its real-time performance and simple developer experience makes it fun to work with time-series data!You can find the sample code for this blog post on GitHub here.  And you can learn more about RedisTimeSeries here."
568,https://redis.com/blog/6-books-new-redis-developers-should-read/,The 6 Books New Redis Developers Should Read,"September 27, 2022",Redis,"Just getting started with Redis? These books help you grasp the technical essentials and smooth the learning curve.Adopting any new-to-you technology can be daunting. There’s a lot to learn, from philosophical underpinnings to pragmatic how-to.That’s true for Redis newbies too. So we asked Redis experts – inside the company and in the larger community – for the technology references they recommend for today’s developers. This curated book list can help you conquer the Redis learning curve and come up to speed swiftly.You need to integrate NoSQL databases into your architecture to store, process, and retrieve data efficiently. Seven Databases in Seven Weeks offers a thorough dissection of the NoSQL database ecosystem. Perkins presents conceptual introductions to seven databases – one of them being Redis – including how to deploy each one, when to use it, when not to use it, its benefits, its downsides, and how it relates to a real-life project.It’s an extensive investigation that highlights the most important characteristics of each database without diving too deep into the technicalities. The book demystifies NoSQL and gives readers more confidence to navigate through the NoSQL space.Doing so reveals how we’ve progressed from batch-processing systems to technologies like MapReduce through to stream-based processing.Perhaps the overarching benefit of Designing Data-Intensive Applications is that it teaches readers how to compare technologies and to become more analytical and efficient problem solvers.As Fowler wrote on his own website, the authors aimed to provide a background on how NoSQL databases work so that you can make those judgments yourself without having to trawl the whole web. “We’ve deliberately made this a small book (just 152 pages), so you can get this overview pretty quickly,” he wrote.Carlson introduces the foundational topics, shedding light on important areas such as how to minimize the chances of data loss. Then it dives into common challenges, including how to model non-trivial data, deal with index data, and search, sort, and filter data. Scaling is covered in some depth, including techniques to help you scale read queries, write queries, total memory available, and suggestions for scaling complicated queries.It’s a starting point – which is just what newbies need. You get a holistic understanding of Redis to help you navigate concepts and features, and you can build on that knowledge.Atchison provides readers with a comprehensive insight into what caching is, why and when it’s needed, and how to maximize application performance through specific caching techniques.Talon Miller, Redis Technical Product Manager, says, “Redis was somewhat of a complicated database for me to grasp, specifically because of all the variety of data structures. Redis for Dummies simply explained all of the core basics that I needed to know about Redis to get started using it.”The book unravels microservice functionality, introduces the key Redis microservices terms and concepts, and shows how everything can be synchronized to optimize application performance.These books can help anyone new to Redis get to grips with important concepts in the Redis sphere and propel you forward to a new level of understanding."
569,https://redis.com/blog/three-ways-to-maintain-cache-consistency/,Three Ways to Maintain Cache Consistency,"January 20, 2022",John Noonan,"Download Caching at Scale e-book.  A primer you need to understand what application caching is, why and when it’s needed, and how to get the best performance from your applications.If you believe Ralph Waldo Emerson, a foolish consistency may be the hobgoblin of little minds, but when it comes to implementing a scalable, successful enterprise-level caching strategy, there’s nothing foolish about consistency. In fact, one of the greatest challenges in managing the operation of an enterprise database is maintaining cache consistency.Why do we bother with caches in the first place? The principal advantage of an enterprise cache is the speed and efficiency at which data can be accessed. While each call to the primary database can be expensive in terms of both time and processing power, a call to a cache can be lightning-quick and have no impact on the primary database.Of course, these advantages rest on the fundamental assumption that the data in the cache—or caches—always maintain the same values as the source data. Although this may seem like a straightforward goal, it’s easier in theory than in practice. In fact, there are three pitfalls that can derail it:1. When changes to the primary database aren’t reflected in the cacheBecause accessing data via a cache is, by definition, faster than accessing the data via the primary database, if a specific item is requested, the cache will be consulted first. Assuming the item exists in the cache, it will be returned far more quickly than it would from the primary database. This strategy is known as the cache-aside pattern. The cache is checked first by default. If the data isn’t in the cache, the application queries the primary database and deposits the result in the cache on the way back to the user.The problem arises during the gap between when the data in the primary database is changed and when the cache is adjusted to reflect said change. This is influenced by how frequently the application checks the cache. However, each check comes at the cost of processor resources. The same processor may simultaneously be handling numerous other functions or transactions, some of them as important, if not more so, than updating the cache.The challenge comes in finding the sweet spot, a kind of Goldilocks area that lies between checking for updates too frequently or not enough. Of course, if a user attempts to access obsolete data during this gap, that gamble is lost.2. When there’s a delay in updating cached resultsThis problem overlaps a little with the previous problem. Each time a value is updated in the primary database, a message is sent to the cache, instructing it either to update the changed value or to remove it entirely. (In the latter case, the next time the value is requested, it will be accessed from the primary database and then from the cache thereafter.) Under normal circumstances, this communication happens relatively quickly and the cached item is either updated or removed in order to maintain cache consistency.Once again, however, this change requires processing power, and it takes time. Delays can be affected both by available processing speed as well as by network throughput. Should a user have the misfortune of choosing to access obsolete data between the time the message has been sent by the server to update the cache and the time when that message is received by the cache and acted upon, the result can be data that is obsolete, incorrect or both.3. When there’s inconsistency across cached nodesOf course, the larger the website or the application, the more likely the cache is stored on multiple nodes instead of just one. In addition to a primary node, there may be any number of replica nodes that are, ideally, storing identical data. From the standpoint of load balancing and performance, this often makes sense.But from the perspective of data integrity, it introduces yet another potential source of cache inconsistency. Each time the data is updated in the primary database, this change needs to be reflected in all of the replicas as well. Depending on where these nodes are located geographically, and how many there are, the updating process can take a significant amount of time. Although the updating process may be underway, it’s still quite possible that a user will access a node where the changes have yet to be made. Once again, the result, you guessed it, can be cache inconsistency.For all the benefits of a cached database, the potential for cache inconsistency is perhaps its most conspicuous drawback. But how big is the problem? Ultimately, the cost of cache inconsistency depends on the context.Some cache inconsistency can occur without much consequence. For example, if the total “likes” in your cache are temporarily out of sync with the actual total in your primary database, the brief discrepancy is unlikely to cause problems or even be noticed.On the other hand, if the cache lists that one remaining item of a particular product is still in stock, while the actual inventory at the primary database says there are none left, the resulting conflict can confuse and alienate your customers, damage your brand’s reputation for reliability, wreak havoc on the company’s transactions and accounting, and, in extreme cases, even put you in legal jeopardy.Luckily, for each of the potential sources of cache inconsistency above, there are a corresponding number of solutions.1. Cache invalidationWith cache invalidation, whenever a value is updated in the primary database, each cached item with a corresponding key is automatically deleted from the cache or caches. Although cache invalidation could perhaps be seen as a “brute force approach,” the advantage is that it requires only one costly and often time-consuming write—to the primary database itself—instead of two or more.2. Write-through cachingIn this case, rather than updating the primary database and removing the cache, with the write-through strategy, the application updates the cache, and then the cache updates the primary database synchronously. In other words, instead of relying on the primary database to initiate any updating, the cache is in charge of maintaining its own consistency and delivering word of any changes it makes back to the primary database.3. Write-behind cachingUnfortunately, there are times when two writes can actually make a wrong. One of the drawbacks of the write-through cache strategy is that updating both the cache and the primary database requires two time-consuming, processor-taxing changes, first to the cache and then to the primary database.Another strategy, known as write-behind, avoids this problem by initially updating only the cache and then updating the primary database later. Of course, the primary database will also need to be updated, and the sooner the better, but in this case the user doesn’t have to pay the “cost” of the two writes. The second write to the primary database occurs asynchronously and behind the scenes (hence the name, write-behind) at a time when it is less likely to impair performance.In addition to cache invalidation, write-through and write-behind caching can address many of the scenarios that help you achieve cache consistency. But finding the answer to a problem is not the same as implementing it.Redis Enterprise’s active-active geo-duplication allows for multiple primaries and enables you to deftly handle increasingly heavier loads. The name active-active refers to the fact that each instance of your database can accept both read and write operations on any key. Each database instance, no matter how far-flung, is a peer on your network. That means when a write occurs to any instance, that node automatically sends a message to all the other instances on your network, indicating what in the cache has been changed and ensuring all of the instances retain a consistent set of cached data.Redis Enterprise’s unique active-active geo-duplication employs sophisticated algorithms designed to deal with potential write conflicts that can lead to cache inconsistency. These algorithms are based on conflict-free replicated data types (CRDTs), ensuring that writes from multiple replicas can be merged in a way that effectively maintains consistency.Because the challenge of maintaining cache consistency becomes more complex and increasingly more consequential as your architecture grows, you need an enterprise-level caching solution to reliably deliver the consistency that your business requires and that your customers expect.Consistency may have been a hobgoblin as far as Emerson was concerned, but it’s absolutely essential when it comes to enterprise-level database caching. That’s why it’s wise to choose Redis Enterprise. You’d be foolish not to.Get the whole story of caching. Read Caching at Scale with Redis, by Lee Atchison."
570,https://redis.com/blog/bliss-memcached-cloud-over-the-paasture-powered-by-redis/,Bliss: Memcached Cloud Over the PaaSture (Powered by Redis),"December 18, 2013",Itamar Haber,"With winter storms on the attack around much of the world, it’s always nice to visualize some calming imagery, which today’s news inspired us to do. We are pleased to announce the graduation of our Memcached Cloud add-ons from beta to general availability (GA) across major PaaS(ture) providers, including Heroku, AppFog and AppHarbor.Memcached is an extremely popular open source caching solution among large enterprises and startups for a variety of use cases, and many of these developers offload their infrastructure management to PaaS vendors so they can focus their R&D resources on developing great companies.With this backdrop, our Memcached Cloud add-ons offer developers the best and easiest step up to fail-safe caching in the PaaS environment.During our PaaS beta period for the Memcached Cloud add-ons, we offered it with 1GB of free capacity. As part of our transition to the GA version, we’re now discontinuing the free version and providing several plan options with our usual pricing models. Previous users should visit their PaaS provider’s add-on pages for transition and pricing details:Heroku: https://addons.heroku.com/memcachedcloudAppFog: https://docs.appfog.com/add-ons/memcachedcloudAppHarbor: https://appharbor.com/addons/memcachedcloudUpgrading is very simple, and will ensure that you continue to benefit from the only Memcache add-on that guarantees high availability and top performance for your app. Our technology combines persistent storage and in-memory replication, so your app can hum along smoothly without fear of losing data or performance. If you’re anything like us then your probably more comfortable around code, so here’s a short pseudo-code snippet we’ve prepared for you to run through and see what’s next:Memcached Cloud offers developers the best benefits of Garantia Data’s Redis Cloud – including true high availability (storage engine, replication, and auto-failover), infinite scalability, stable top performance, and zero management – without requiring you to change Memcached protocols. As a result, you can finally rely on your application’s cache to provide your users with consistent, uninterrupted, top-notch service and performance, regardless of failure events or scale challenges. If you’re already using our Memcached Cloud service then you are familiar with its rock-steady stability and unparalleled wealth of features.Our production-proven functionality includes unique benefits such as scheduled and on-demand backups, data persistence, static endpoints and seamless scaling without any interruptions to the service. Because Memcached Cloud is tapping directly into raw Redis power, you can actually use Redis’ KEYS and MONITOR commands to see what’s stored in your Memcached, or even just scroll through the CSV backup file. If your Memcached-based application is deployed with a major PaaS provider, our competitively-priced add-on can be automatically connected with it and with each of our Memcached Cloud plans you cam to create multiple buckets under the same subscription for no extra charge.Once you sign up, you’ll be able to start using Memcached Cloud immediately without any changes to your app, and you’ll gain the most advanced, high performance, and cost-effective enterprise-class hosted Memcached service. Of course, new users who don’t use PaaS can create a Memcached Cloud account here as usual."
571,https://redis.com/blog/3-reasons-your-mysql-db-needs-redis/,Redis MySQL: 3 Reasons Why Your MySQL Database Needs Redis,"September 20, 2022",Talon Miller,"Are your applications running on MySQL performing to their optimum potential? Not so much, right? Discover three ways Redis Enterprise can help you overcome common obstacles in MySQL – with minimal code changes and zero disruption to your application. Want to dive even deeper into how Redis Enterprise can help either as a cache or a primary database? Click below to read our dedicated MySQL solution brief.Download Modernize Your MySQL Database With Redis EnterpriseMySQL is a really great, free, open source database solution until… it isn’t. After getting started, it’s very common for products and services to eventually run into issues with performance, scaling, and innovating outside of MySQL’s rigid SQL-based environment. To meet today’s user expectations, your MySQL needs real-time performance, sustained scalability, and support for MySQL use cases beyond what’s available out of the box.Redis Enterprise, the leading real-time data platform, can be used alongside MySQL to store data in-memory to greatly relieve the challenges associated with MySQL: speed, scalability, and inflexible data types. Let’s enhance and extend the life of your MySQL by enabling the sub-millisecond performance that modern use cases require.Redis Enterprise can be used alongside your MySQL database to store your application’s most critical and commonly-accessed data in-memory to deliver it with sub-millisecond speed. Let’s go into more detail on the how.Okay, look, MySQL’s speed isn’t too bad… for a relational database. But that’s not saying much for real-time use cases that require sub-millisecond speed. Yes, MySQL Query Cache is an in-memory option available with your MySQL environment, but you must actively set up and maintain this feature. You’re also stuck with the same data types and SQL statement executions. The lack of data types in this caching solution puts a very small box around what type of caching use cases you can enable.Plus, you will have to continue working with SQL statements inside of your cache, meaning you’ll constantly be changing variables and optimizing SQL statements to increase the response time of these queries. It’s a very hands-on caching solution, to say the very least! MySQL caching aside, MySQL performance on secondary indexes is infamously poor as it usually includes scanning columns and combinations of columns, making it very inefficient for quick query responses.What about those pesky secondary indexed queries? You can use Redis Enterprise to offload them, speeding up the responses to your applications and taking some of that scaling pain off of your MySQL database, reducing costs as well!Speaking of scaling issues, let’s dive into what scaling pains you’ll experience with MySQL and how you can remedy them.MySQL performance tuning is a popular topic for a reason. Sooner or later, the sheer amount of data in your MySQL database will take a hit in performance and availability. There are a number of variables to consider when scaling MySQL efficiently, ranging from hardware to software tuning.From a hardware perspective, constant monitoring and right-sizing are needed for storing your HDD or SSD, the processor (which is very expensive to upgrade), memory (RAM for caching), and network traffic.Regarding software, MySQL performance and scalability issues are sensitive to the version of MySQL you’re running. Making sure you have the most current version is highly recommended. From there, it comes down to optimizing SQL queries. Scalability in MySQL can’t handle running complex queries against large data volumes.These complex queries can be offloaded to Redis Enterprise, which is much, much faster at processing, reducing the number of resources you need in MySQL. Using Redis Enterprise alongside your MySQL database allows you to run resource-hungry queries on MySQL only when they are needed (while adding real-time speed). Most queries are handled by the in-memory Redis Enterprise, freeing up the capacity of your MySQL database for what it’s good at. Issues start when solutions are built off MySQL to do something it was never intended for!MySQL doesn’t have the diverse data needs of modern applications, like a variety of data types and data models that can be deployed quickly and easily anywhere in the world.Get the full potential from your MySQL data by supporting the diverse data types and data models that are the core necessities of modern-day applications. For example, use Redis Enterprise to build a real-time search engine. Redis Enterprise has a built-in real-time search engine that can be used with your MySQL database to significantly speed up complex queries. Reduce the time it takes to return your MySQL data to your customers and services, plus offload those expensive and complex queries. That is just the start; there are many other data types and data models that Redis Enterprise supports natively, so you can combine all your real-time needs on one platform, not just for MySQL:What about MySQL cloud? Redis Enterprise can be deployed on the cloud vendor’s versions of MySQL, such as Amazon RDS, Cloud SQL for MySQL on Google Cloud, or Azure MySQL. Almost none of these vendors provide the most performant version of MySQL, MySQL HeatWave. Redis Enterprise is the best managed-service, in-memory solution in the cloud. Taking the administrator out of the equation by not having to constantly worry about hardware, software versioning, and much more.To avoid creating a bottleneck in their application, database or network layers, many developers use Redis for the following use cases:In addition to the examples above, Redis can be used as a message broker, data structure store and temporary data store for a variety of use cases. Essentially, Redis gets your data closer and faster to your end user, while collecting their data more quickly. Taking things to the next level, Redis Enterprise offers high availability, in-memory replication, auto-scaling and re-sharding, along with leading-edge CRDT-based active-active support for distributed databases and built-in Redis modules such as Search and Query, JSON, And Probabilistic.This should be a good start on transforming your MySQL database’s speed and scalability while adding real-time performance for your applications. And there is much more than just speed, scalability, and additional use cases – how about auto scaling, enterprise clustering, and Active-Active Geo-Distribution of your most frequently accessed data with 5-9s of high availability? All of this is included with Redis Enterprise Software or Redis Enterprise Cloud.The next question you might have is, “How do I start this MySQL integration with Redis Enterprise?” We have that covered as well. CDC (change data capture) allows data to quickly and easily move your frequently accessed data and complex queries from your MySQL into Redis Enterprise, and we have a CDC tool just for that, Redis Connect.Want to learn more? Check out how to get the most out of your MySQL database with Redis Enterprise."
572,https://redis.com/blog/introducing-rediscover-magazine/,Introducing Rediscover Magazine,"August 14, 2020",Fredric Paul,"It’s my great great pleasure to introduce Rediscover Magazine. The team at Redis created this ambitious publication to highlight the concept of “rediscovery”—the idea that even though you may think you know what a person, product, technology, or idea is all about, there may be much more there than you realize.Not surprisingly, Redis itself was the inspiration for this idea. Many developers, architects, and DevOps professionals know Redis as a great caching solution—a fast and cost-effective way to optimize data infrastructure performance. But as software engineers and architects take a closer look, they’re rediscovering the full power of Redis as an ultra-fast, super-scalable, highly available (five-nines uptime!), developer-friendly tool for building applications for the digital economy. Importantly, it’s not just that Redis has grown and improved over the years, though of course it has—rather, many of those capabilities have been there for a long time, even if we didn’t always see them or recognize them.Read Rediscover Magazine right now online, download a PDF, or request a print copy. Visit Redis.com/rediscover-magazine now!I’m about to tell you about some of the informative, engaging, and surprising stories in the premiere issue, but right about now you may be thinking: “What!?! A magazine? In 2020? What are you thinking?”Well, bear with me for a moment, and perhaps I can convince you that this actually makes sense. You see, it’s no accident that we worked to capture the spirit of rediscovery in a magazine, available both digitally and in print. The rise of the web makes it easy to overlook the power and excitement of a compelling story presented in a beautiful and engaging visual environment—something that magazines have always done extraordinarily well. In fact, I spent the formative years of my career writing and editing computer and technology magazines, so I was super excited to rediscover the art of storytelling in this time-honored format.Of course, Redis is far from the only thing ripe for rediscovery. In our cover story, “The Power of Rediscovery” acclaimed writer Don Steinberg takes an incisive and amusing look at everything from WD-40 (not just a rust-prevention solvent but an all-purpose lubricant with literally thousands of uses) to Arnold Schwarzenegger (not just a body-builder, he had what it takes to become a movie star—and then Governor of California). We think you’ll be surprised by the choices, and their stories.And that’s only the beginning. Rediscover Magazine also includes important voices like Redis creator Salvatore Sanfilippo talking about the early days of Redis, the evolution of open source software, and much more. Silicon Valley legend (and long-time Sun Microsystems CEO) Scott McNealy shares tips on managing through a crisis. Leading venture capitalist Enrique Salem at Bain Capital Ventures shares a story of one of his portfolio companies that is rediscovering the power of its own technology. Analyst Rachel Stephens from RedMonk rediscovers the potential of the Markdown language. And Redis co-founders Ofer Bengal and Yiftach Shoolman offer unique perspectives on remote work and implementing artificial intelligence, while some of our company’s women leaders share management tips.But wait, there’s even more! There are also stories on the future of virtual events, how to become a real-time financial services company, and top database trends, so be sure to check out Rediscover Magazine for yourself.It’s easy and free. Just visit Redis.com/rediscover-magazine to read the magazine, download a PDF, or request a printed copy. We’re confident you’ll find it interesting, illuminating, and enjoyable!"
573,https://redis.com/blog/rediscover-real-time-data-at-these-3-redisconf-2021-sessions/,Rediscover Real-Time Data at These 3 RedisConf 2021 Sessions,"March 17, 2021",Udi Gotlieb,"Rediscover the power of real-time data at RedisConf 2021. Our annual real-time data conference is going virtual again from April 20–21, where you’ll have an opportunity to dive into the latest product experiences, get hands-on training, network with other Redis pros, and show off your skills by participating in a $100,000 hackathon.RedisConf 2021 will host more than 60 breakout sessions, with 25% led by Redis Enterprise customers. You’ll be able to learn about real-world use cases across three tracks:There’s something for everyone, and here’s your first look at three RedisConf 2021 sessions:Speaker: Mike Lee, Head of Enterprise Payment Architecture at Capital OneCapital One is building a next-generation digital payments platform, which will process billions of transactions in Automatic Clearing House (ACH), wire transfers, and more. In this session, Capital One’s Head of Enterprise Payment Architecture Mike Lee will discuss why Capital One chose Redis Enterprise for this event-driven payment-processing workflow.Mike will discuss how Redis Enterprise delivered on its real-time data performance SLAs, provided cloud native high availability at every level of the platform, and met Capital One’s business continuity needs with the out-of-box, conflict-free, Active-Active deployment across the regional footprint.Speaker: Dustin Brown, Director of Technology at SiteProSitePro was created as an end-to-end system for oil and gas customers to reduce their environmental impact and function with greater operational awareness. Its mission-critical IoT platform is responsible for controlling and monitoring oilfield infrastructure. In his talk, SitePro’s Director of Technology Dustin Brown will explain why SitePro relies on RedisTimeSeries to provide real-time actionable data for its customers.Dustin will offer a real-world case study and deep analysis of the module, explaining why it is a natural fit for supporting data collection from IoT sensors, using Redis at the edge to lower cost, and reducing processing time, as well as walk through tips and expectations for a successful production implementation.Speakers: Robert Belson, Corporate Strategist at VerizonHigh-speed 5G networks will unlock a new world of immersive experiences, but their development may create challenges within an application’s architecture. In this session, Robert Belson will discuss how you can use Verizon’s 5G Edge infrastructure to deploy compute resources geographically closer to your users than ever before using Wavelength Zones.The duo will also dive into how Redis Enterprise enables an ultra-low latency caching solution as part of your mobile edge computing architecture. Plus, Belson will deploy a cluster to multiple Verizon 5G Edge Zones and will benchmark performance to traditional availability zones in the parent region.Whether you’re brand new to Redis or a veteran, RedisConf 2021 will give you the opportunity to rediscover the power of real-time data. Don’t miss these breakout sessions and dozens more—plus exclusive new training courses and $100K hackathon—at our annual real-time data conference. Registration is open now!"
574,https://redis.com/blog/what-is-enterprise-caching/,What Is Enterprise Caching?,"February 10, 2022",John Noonan,"Buyer’s Guide for Enterprise Caching, an e-book companion with enterprise caching solutions to provide consistently high performance while scaling, is now available. Download for free below.For decades, databases worked behind the scenes driving applications and websites making digital experiences more dynamic and adaptable. But there’s been a fundamental problem with this model. These same databases have also made applications slower.That’s where caching comes in. It takes data that was stored in a database on your server’s hard disk and moves it to a temporary place where it can be accessed far more quickly and efficiently. As a result, the complex, energy, and time-consuming operation to acquire data only needs to be performed once. From that point on, the data can be retrieved quickly and efficiently from the cache.Of course, as your company gets bigger and its reach grows, the stakes get higher and your margin for error becomes razor thin. Suddenly, caching is no longer a nice-to-have—it’s a must-have. What’s convenient for a small-scale company becomes essential for a large-scale competitive enterprise. And failure is not an option.Enter the enterprise cache. Built on the solid foundation of the basic cache, it provides a suite of features that enterprises require in order to keep pace with growing demands, including high availability, genuine product support, sub-millisecond performance, fully distributed replication, and a cost-effective way of managing your complex data sets. It’s more scalable, more failure resistant, and yes, more affordable.The rationale for adopting an enterprise caching solution is simple: When you need to be able to scale and you can’t afford to fail. How can you tell when it’s time to adopt an enterprise caching solution? There are a number of factors to consider:1. Your original database won’t scale effectivelyThe humble ant is one of the most amazing creatures on the planet. It’s capable of lifting close to 5,000 times its own body weight. Over the centuries, numerous scientists (and science fiction filmmakers) have wondered what would happen if we took the immense strength of the tiny ant and expanded it to human size. Unfortunately, an ant doesn’t scale. If we produced one the size of your coworker, its legs would collapse under the weight of its own body.Although the perils of expanding a cache aren’t quite as spectacular, they share some similarities. As they expand, standard caches typically run into two types of roadblocks: storage and resource limits. The former describes the amount of space available to cache data. The latter refers to the capacity to perform necessary functions, including storing and retrieving cached data.The solutions can be straightforward, but potentially never-ending. When you reach storage limits, the standard remedy is obvious: Increase your storage. If you don’t have enough oomph to handle all your resources, increase your bandwidth and your processing power. With vertical scaling, you increase the resources allotted to your cache to operate.On-prem, this usually means replacing your current server with a more powerful one that has more RAM, processing power, network bandwidth, or all three. If your cache is in the cloud, it may mean moving to a larger instance. Another alternative, horizontal scaling, involves adding more nodes to the cluster of instances that are handling your cache without altering the size of an individual cache instance. In short, vertical scaling means increasing by size, while horizontal scaling involves increasing by number.2. The cost of caching is becoming prohibitiveSteadily expanding the size of your cache to meet a growing demand may temporarily solve your problem, but at what cost? If you’re like many people, your belongings have outgrown your house or apartment and you may have had to rent one or more storage units. If so, then you realize that in most cases, it costs the same amount to stuff a storage locker with worthless knick-knacks as it does to fill it with priceless antiques.Basic caching works in a similar fashion. Frequently used or high value data is treated identically to less common or less important keys and values. Not only that, but when you run out of cache space, the nature of that data is irrelevant. You’re out of space. Unfortunately, adding cache space can be expensive. Redis on Flash (a component of Redis Enterprise) helps to keep your cache costs in check by setting up a caching hierarchy. More actively used cache values are stored in RAM, while lesser used ones can be maintained in much larger and less expensive flash memory.3. You can no longer rely on a single masterAdding nodes to your cache can meet the demands of increased traffic, but it only tackles part of the issue. Basic caching allows for additional read replicas, a method of horizontal scaling that improves read performance by distributing the read load across multiple servers. Unfortunately, you are still limited to one master to handle all writes.Being limited to a single master can create problems if your deployments span multiple regions or use multiple providers or multiple clouds. If your application has a far-flung customer base, relying on a single master can create a debilitating bottleneck. That’s because all of your write requests, regardless of their origin, must be directed to one limited location.It’s a little like going to a carry-out restaurant where you can pick up your order from multiple windows, but there’s only one cash register open where you’re supposed to pay. With Active-Active Geo-Deployment from Redis Enterprise, any master instance, regardless of its region or its provider, can handle both read and write requests.4. High availability has gone from luxury to necessityWith a small-scale application, those occasional times when your app goes down can be an annoyance and an embarrassment. An enterprise-level outage is a game changer. A fielding error in a little league baseball game is unfortunate. A similar error in the World Series can cost millions.Likewise, a failure in availability is no longer just an inconvenience. It’s a genuine liability. In fact, depending on the SLA you have with your customers, it can put you in legal jeopardy. Unfortunately, basic caching provides no inherent guarantees of scaling, security, or high availability. Although it’s theoretically possible to build many of these safeguards atop your open-source cache, these home-grown solutions often come with their own special headaches and hidden costs.Of course, some third-party Redis caches offer 3-9s availability, but only across a single region and with no data persistence, just snapshots. If your application is limited, it’s a limited solution. But if your company and/or your customer base are international, it isn’t enough. Redis Enterprise Cloud offers 5-9s SLA across one or more regions. It supports data persistence and backup without impacting performance. In addition, it provides automatic cluster recovery and pure in-memory replication.All of these increased demands as your company expands can result in slower load times, which can alienate long-time customers and lead to widespread rejection by potential new ones. Like it or not, response time is a critical component of the online experience.According to Unbounce, 70% of users said that load time influenced their willingness to buy from online retailers. Research has shown that applications have roughly 100 ms before users get the sense they are waiting. That’s one-third of the time that it takes to blink. If your customers are able to blink while your application is loading, there’s a good chance you’ve already lost them.And it’s not just a question of purchasing decisions. A study by Salesforce found that 83% of customers considered the experience to be as important as a company’s products and services.Finally, in the age of viral media, one customer’s isolated bad experience is unlikely to remain isolated for long. When people have an unsatisfying experience on a website, they don’t usually keep it to themselves. On the contrary, according to Salesforce, 61% of customers share that bad experience with others. As a result, your application’s deficiencies can trigger a chain reaction of bad will as the news gets rapidly disseminated throughout your potential customer base.Luckily, there’s a silver lining to this last sobering set of statistics. The same Salesforce study found that 70% of customers are apt to share their good experiences with others. If your company is growing and you want to build a base of happy new customers instead of shedding old ones, enterprise caching may prove to be just what you need to lay the groundwork for potentially limitless expansion of their digital experiences.For more information, check out our complimentary A Buyer’s Guide for Enterprise Caching."
575,https://redis.com/blog/redis-on-windows-10/,Running Redis on Windows 10,"March 19, 2022",Redis,"In this guide, we will cover several topics that will help you install Redis on Windows 10.  We also address if this is indeed the best setup.You can download a moderately out-of-date precompiled version of Redis for 32-bit and 64-bit Windows thanks to Dusan Majkic from his GitHub page.After you download Redis on Windows, you’ll need to extract the executables from the zip file. As long as you’re using a version of Windows more recent than Windows XP, you should be able to extract Redis without any additional software.After you’ve extracted either the 32- or 64-bit version of Redis to a location of your choice (depending on your platform and preferences; remember that 64-bit Windows can run 32- or 64-bit Redis, but 32-bit Windows can only run 32-bit Redis), you can start Redis by double-clicking on the redis-server executable. After Redis has started, you should see a window similar to figure A.1.If you find yourself in the position of needing the most up-to-date version of Redis on Windows as possible, you’ll need to compile Redis yourself. Your best option is to use Microsoft’s official port, which requires Microsoft Visual Studio, though the free Express 2010 works just fine. If you choose to go this route, be warned that Microsoft makes no guarantees as to the fitness of their ports to Windows for anything except development and testing.You can install Redis cache on Windows 10 using Windows Subsystem for Linux(a.k.a WSL2). WSL2 is a compatibility layer for running Linux binary executables natively on Windows 10 and Windows Server 2019. WSL2 lets developers run a GNU/Linux environment(that includes command-line tools, utilities, and applications) directly on Windows.Ever since Jessica Deen explained how WSL works at my SVDevOps Meetup, I’ve recommended Redis Windows 10 users run Redis on their own dev machines. Yes, you heard right. Starting with Windows 10 (v1709 – 2017-09, Fall Creators Update), you can run at least a half-dozen flavors of Linux on the Windows Subsystem for Linux (WSL), and you can run Redis on top of them. No VM required. No Docker. No joke!To answer the question “Which version of Windows is my PC is running?” press your Windows logo key + R, type “winver,” then select OK. Starting with version 10, you’ve got a command called “wslconfig.” It lists distros you have and controls which one starts by typing “bash.” Try it out!You can run Redis on Windows 10 using Windows Subsystem for Linux(a.k.a WSL2). WSL2 is a compatibility layer for running Linux binary executables natively on Windows 10 and Windows Server 2019. WSL2 lets developers run a GNU/Linux environment (that includes command-line tools, utilities, and applications) directly on Windows.You can either follow the written steps below or watch this video where Guy Royse shows you how to install the latest version of Redis on Windows 10 using the Windows Subsystem for Linux (WSL).Follow these instructions to run a Redis database on Microsoft Windows 10.In Windows 10, Microsoft replaced Command Prompt with PowerShell as the default shell. Open PowerShell as Administrator and run this command to enable Windows Subsystem for Linux (WSL):Reboot Windows after making the change — note that you only need to do this once.Then search for Ubuntu, or your preferred distribution of Linux, and download the latest version.Installing Redis is simple and straightforward. The following example works with Ubuntu (you’ll need to wait for initialization and create a login upon first use):NOTEThe sudo command may or may not be required based on the user configuration of your system.Restart the Redis server as follows:Use the redis-cli command to test connectivity to the Redis database.NOTEBy default, Redis has 0-15 indexes for databases, you can change that number databases NUMBER in redis.conf.Now that Redis on Windows is up and running, it’s time to download and install Python.If you already have Python 2.6 or 2.7 installed, you’re fine. If not, you’ll want to download the latest version of Python 2.7, because that’s the most recent version of Python that has support for the Redis library. Go to http://www.python.org/download/ and select the most recent version of the 2.7 series that’s available for Windows in either the 32- or 64-bit version (again, depending on your platform). When Python is done downloading, you can install it by double-clicking on the downloaded .msi file.Assuming that you accepted all of the default options for installing Python 2.7, Python should be installed in C:Python27. From here, you only need to install the Python Redis library to be ready to use Redis with Python. If you’re using Python 2.6, any time the book refers to Python27, you can instead use Python26.To help you to install the Redis client library, you’ll use the easy_install utility from the setuptools package. This is because you can easily download setuptools from the command line. To get started, open a command prompt by going into the Accessories program group in the Start menu and clicking on Command Prompt.After you have a command prompt open, follow along with the next listing; it shows how to download and install setuptools and the Redis client library.Listing A.6 Installing the Redis client library for Python on WindowsStart Python by itself in interactive mode.Import the urlopen factory function from the urllib module.Fetch a module that will help you install other packages.Write the downloaded module to a file on disk.Quit the Python interpreter by running the builtin exit() function.Run the ez_setup helper module.Finished processing dependencies for setuptools==0.6c11The ez_setup helper downloads and installs setuptools, which will make it easy to download and install the Redis client library.Finished processing dependencies for redisUse setuptools’ easy_install module to download and install Redis.Now that you have Python and the Redis client library installed, read on to find out if Redis on Windows is the best setup for you.The main drawback of Redis on Windows is that Windows isn’t officially supported on Redis. Specifically, Windows doesn’t support the fork system call, which Redis uses in a variety of situations to dump its database to disk. Without the ability to fork, Redis is unable to perform some of its necessary database-saving methods without blocking clients until the dump has completed.Recently, Microsoft has contributed engineering time helping to address background saving issues, using threads to write to disk instead of a forked child process.As of this writing, Microsoft does have an alpha-stage branch of Redis 2.6, but it’s only available as source, and Microsoft makes no guarantees as to its worthiness in production scenarios.At least for the short term, there’s the previously mentioned unofficial port of Redis by Dusan Majkic that offers precompiled binaries for Redis 2.4.5. However, Redis blocks when dumping the database to disk.Get started now with a free Redis Cloud account."
576,https://redis.com/blog/database-consistency/,Database Consistency Explained,"May 27, 2022",John Noonan,"Database consistency is defined by a set of values that all data points within the database system must align to in order to be properly read and accepted. Should any data that does not meet the preconditioned values enter the database, it will result in consistency errors for the dataset. Database consistency is achieved by establishing rules. Any transaction of data written to the database must only change affected data as defined by the specific constraints, triggers, variables, cascades, etc., established by the rules set by the database’s developer.For example, let’s say you work for the National Traffic Safety Institute (NTSI). You’ve been tasked with creating a database of new California driver’s licenses. The population of California has exploded in the past ten years, creating the need for a new alphabet and numerical format for all first-time driver’s license holders. Your team has determined that the new set value for a California driver’s license in your database goes as follows: 1 Alpha + 7 Numeric. Every single entry must now follow this rule. An entry that reads “C08846024” – would return with an error. Why? Because the value entered was 1 Alpha + 8 Numeric, which is, in essence, a form of inconsistent data.Consistency also implies that any data changes to any one particular object in one table need to be changed in all other tables where that object is present. Keeping the driver’s license example going, should the new driver’s home address change, that update must be represented across all tables where that prior address existed. If one table has the old address and all the others have the new address, that would be a prime example of data inconsistency.Note: Database consistency doesn’t guarantee that the data introduced in any given transaction is correct. It only guarantees that the data written and read within the system meets all prerequisites of data that is eligible for entry into the database. To put it simpler, given the example above, you can very well enter a data transaction that meets the 1 Alpha + 7 Numeric rule, but that doesn’t guarantee that the data corresponds to an actual driver’s license. Database consistency doesn’t account for what the data represents, just its format.Consistent data is what keeps a database working like a well-oiled machine. Established rules/values that keep inconsistent data out of primary databases and replicas keep its operations smooth with:Database consistency regulates all data coming in. So although the database changes when accepting new data, it at least changes consistently and in accordance with the validation rules established at the onset. In today’s world, there are daily billion-dollar decisions made around the globe based on the perceived consistency of a database. When real-time information becomes the new status quo for modern-day digital businesses, it’s vitally important that validation rules are put in place to keep datasets clear of erroneous information, as that also increases latency, making real-time experiences not so real-time after all.What are examples of a database consistency operation in the real world? We’ve already explored one example with our NTSI scenario above. Let’s pivot to the world of banking.Say you’re transferring funds from one account to another. You’ve just transferred $1200 into an account that already has $300. You refresh, positive you’ll find a $1500 balance. Yet, this recent operation isn’t reflected in your balance. In fact, your new balance now reads $0. This technical slight is a prime example of weak consistency and will likely result in time spent troubleshooting the problem with a bank representative. Issues like these can tarnish a brand’s reputation and cost a significant amount of money. Strong consistency in database systems is becoming more and more of a non-negotiable, for developers and consumers alike.Strong Consistency means that all data in a primary, replica and all its corresponding nodes fit the validation rules and are the same at any given time. With strong database consistency, it does not matter what client is accessing the data – they will always see the most recently updated data that follows the rules established for the database.Weak consistency is a bit like the proverbial wild, wild west. There are no assurances that your data in your primary, replica, or nodes is the same at any given moment. One client in India could access the data and see information that passes the validation rules, but may not be the most recently updated data, resulting in consistency errors. They could very well be acting on information that is no longer relevant, even though at one point it may have been.Consistency levels are another set of preconditioned values that dictate how many replicas or nodes must respond with the new permissible data before it is acknowledged as a valid transaction. This operation can be changed on a per-transaction basis. So, for example, a programmer can dictate that only two nodes need to read the newly input data before it acknowledges data consistency. Once it crosses that barometer, it will be considered consistent data thereafter.Isolation levels are part of a database’s ACID (Atomicity, Consistency, Isolation, Durability) properties. ACID is a foundational concept of database consistency with SQL databases and is what certain databases follow in order to optimize database consistency. Isolation is one of ACID’s properties, and it compartmentalizes certain pieces of data away from all the information in a certain database network, keeping it from being modified by other user transactions. Isolation is leveraged to curtail reads and writes of inconsequential data produced in concurrent transactions.There are four types of isolation levels:Interested in cloud caching techniques for enterprise applications? Click below to read Lee Atchison’s Caching at Scale with Redis."
577,https://redis.com/blog/native-json-support-on-azure-cache-for-redis-enterprise/,Native JSON Support on Azure Cache for Redis Enterprise Now Generally Available,"August 5, 2022",Shreya Verma,"Find new opportunities to help create a document database using native JSON support on Azure Cache for Redis Enterprise.We are excited to announce that native JSON support on Azure Cache for Redis Enterprise and Enterprise Flash tiers is now generally available. The Enterprise tiers are developed in a joint partnership between Redis and Microsoft. They help you achieve the highest level of performance, availability, and functionality for your Redis cache databases.With this release, we now provide a real-time document store on Azure Cache for Redis that allows you to build modern, high-performing, and scalable applications using a dynamic, hierarchical JSON document model. JSON-based keys can be accessed with a dedicated set of commands. RedisJSON supports the JSONPath syntax to atomically update, read, and project elements within your document.This native support enables RedisJSON to combine with RediSearch, to secondary index, query, and full-text search JSON documents in real-time with ease. The result? Enhanced and scaled application customer experience supporting real-time use cases with low latency, JSON-oriented document database. With the addition of RedisJSON, Azure becomes the first Cloud Service Provider to offer the RSAL-licensed Document capability for their customers as a first-party service. This means that, unlike other services which claim Redis API compatibility, developers can seamlessly migrate applications built using Redis OSS or Redis Stack to Azure Cache for Redis Enterprise when they are ready to launch a fully Enterprise capable solution.With the RedisJSON module, Azure Cache for Redis Enterprise can now deliver a high-performance NoSQL document store that allows developers to build modern applications. It uses native APIs to ingest, index, and query JSON documents. RedisJSON with RediSearch provides a rich query language that can perform full-text searches, complex structured queries, and auto-complete suggestions using “fuzzy” searches.Some common use cases where RedisJSON can help include Customer360, content management, mobile app development, session management, product catalogs, and more.Below, you’ll find the steps for creating a database with RedisJSON using the Azure Portal:4. In the Advanced tab, select RedisJSON from the Modules drop-down.5. Click Review + create to create the JSON-enabled database.You can use the following AZ CLI command to create your RedisJSON database.Want more detail? Consult the complete Azure CLI documentation for Azure Cache for Redis Enterprise.RedisInsight provides built-in support for the RedisJSON, RediSearch, and RedisTimeSeries modules. With RedisInsight, you can flawlessly visualize, query, and edit your JSON data. You can also administer your Redis database using RedisInsight. For example, gain insights into real-time performance metrics, inspect slow commands, and manage Redis configuration directly through the interface.RedisInsight comes with built-in tutorials for modules to get you started.For details, please refer to this quick-start JSON tutorial."
578,https://redis.com/blog/5-things-you-didnt-know-you-could-do-with-redis/,5 Things You Didn’t Know You Could Do With Redis,"September 28, 2022",Alex Patino,"You certainly know Redis as a cache and primary database. It’s earned an excellent reputation among developers worldwide. But Redis also provides a lot of underlying technology to solve lots of business problems, and you might not realize how it might help in your own IT shop.As with any technology, its value is measured purely on the accomplishments it enables. As you see in the five mini-case studies that follow, Redis lets you achieve more than pure database functions.Speeding up financial transactions? Making video game cloud service insanely fast? Enabling instant online purchases? Let’s start with those and explore a few other examples of what’s possible with Redis.Like many other brick-and-mortar retail businesses, the pandemic had a huge effect on Ulta Beauty, from swift adoption of curbside delivery service (which ballooned six-fold in the final quarter of 2020, industry-wide) and greater dependence on digital sales. A solid e-commerce infrastructure became a non-negotiable requirement. Digital businesses that strengthened, modernized, and scaled their tech stack reaped enormous benefits – or at least survived.Ulta Beauty was among the retailers that acted quickly, moving to Redis Enterprise Cloud for its data and e-commerce strategy and execution. The results? A 40% increase in revenue to the sound of $8.6 billion.In the end, using Redis for predictive analytics, Ulta Beauty surpassed all quarterly expectations. Doing so lets them identify trends and use benchmarking to help them stay ahead of the curve. Redis also provides Ulta Beauty with retail inventory management and facilitates modern customer personalizations, experiences, and virtual product testing through machine learning.“Data was the key to help us make the right decision,” explained Omar Koncobo, Ulta Beauty’s IT director of e-commerce and digital systems. Data helps the company make suitable investments. “With data, we can be sure that we are actually delivering what our customers are looking for. And so now it is such an important piece of everything we do. And we make sure that all of our decisions and everything we do is backed by data.”Watch the fireside chat with Koncobo and Redis’ Udi Gotlieb as they cover how database innovation fuels the beauty industry.The COVID-19 pandemic accelerated the streaming space to unprecedented levels (of which you probably were a part). The growth hasn’t ceased since, with global streaming viewing time up by 14%, according to Conviva’s Q2 2022 State of Streaming report.For TELUS, a Canadian telecommunications company, that means maintaining a faultless delivery of constant streaming entertainment to over 1.5 million customers across Western Canada and Quebec through their Optik TV product (think YouTube TV meets Apple TV). The company’s technology strategy team oversees third-party integrations with Optik TV, such as Amazon Prime, Netflix, and other streaming services.TELUS built Showcase, Optik TV’s all-in-one local and streaming content hub on Redis Open Source. But with a market accelerating at breakneck speed, the demand called for enterprise-level support with real-time performance. “It wasn’t a simple dollars and cents business case,” explained Steve Allen, manager of Showcase’s development team. “It was the operational availability of having enterprise customer support and the fact that Redis Enterprise offered high availability without manual intervention.”In this TELUS case study, read about TELUS’s journey from Redis Open Source to Redis Enterprise, including how TELUS used Redis Enterprise as a cache to deliver instant content to millions of customers with Active-Active Geo-Distribution.Plivo is a Communication Platform as a Service (CPaaS) that provides cloud communication services with an integrated messaging platform and a cloud-based carrier network. The company serves over one billion API requests per month in more than 190 countries. Its Voice API platform is used by thousands of businesses across the globe.Plivo’s engineers built its communications stack on Amazon Elasticache, but they were unsure Elasticache could keep customer operations afloat in the event of a failover. They needed to account for cross-region optimization and fault tolerance. The system needs to kick in so fast that users don’t notice anything changed.According to Rajat Dwivedi, Plivo’s director of API engineering, “We wanted to ensure we could meet uptime and scalability requirements through Active-Active Redis. We tried to simulate these capabilities with Amazon ElastiCache, but realized this is something we didn’t want to solve ourselves.” The company chose  Redis Enterprise Cloud, which delivered this functionality within a fully managed solution that could handle all of Plivo’s Voice API requirements. The Plivo case study details how Plivo has sustained volume and scaled its architecture worldwide.The founders of Kipp set out to be the bridge between credit issuers and merchants in the digital payment approval process, helping to deliver a great online shopping experience for any customer.A credit card transaction that stalls or makes you repeat the payment details is annoying to consumers but a larger issue for businesses: It’s another moment to reconsider the purchase. Nobody wants to nudge anyone to consider a competitor.Kipp turned to Redis Enterprise Cloud on AWS to save this potentially enormous market from continuing to fall through the cracks in the digital payment process. Because Redis is a widely-approved and used data platform across the financial industry, the IT team didn’t need to learn to validate the system with banks and credit issuers. For more on how Redis helps Kipp deliver a modern e-commerce experience, read Kipp Eyes E-commerce Opportunity Through Real-Time Payment Approval.Knowledge management firm Yext offers a data and AI Search platform that uses machine learning to ingest, structure, and deliver data in the form of answers, largely for support-related content. For over 15 years, thousands of companies worldwide have trusted Yext to create seamless content-driven experiences at scale across search engines, websites, mobile apps, and hundreds of other digital touchpoints.With databases such a core part of the Yext platform, findability and discoverability are paramount. To have millions of answers at the ready, Yext uses the hosted version of RediSearch, one of our core modules. Because both Yext and Redis are multicloud, Redis can serve as a back-end system for Yext anywhere it has a point of presence.In Yext & Redis Help Companies Wrangle Public Data Across Multiple Clouds, Third-Party Sites & Owned Experiences, you can learn how Redis helps Plivo’s mission to always have the right answer, at any time, anywhere the customer may be.In this digital-first, real-time data landscape, Redis is helping many businesses to find solutions that go well beyond caching and database storage. Industries ranging from financial services, retail, gaming, and healthcare have turned to Redis for speed, scale, and reliability and, in the process, found new ways to cultivate modern customer experiences."
579,https://redis.com/blog/redis-enterprise-on-google-cloud-five-deployment-scenarios/,Running Redis Enterprise on Google Cloud Explained: Five Deployment Scenarios,"February 8, 2022",Gilbert Lau,"Deploying Redis Enterprise, the world’s fastest database, on the performant and secure Google Cloud will give our users the best of both worlds. This post will describe five different Redis Enterprise deployment scenarios on Google Cloud. We will go through these deployment scenarios, characteristics, limitations, and caveats for each.Redis Enterprise integrates with Google Cloud (GCP) to provide a best-in-class customer experience in two major deployment form factors, a fully managed service, and a self-managed software. Apart from these consumption models, Redis serves as a key-value store, enabling many popular use cases such as caching, session management and confidently serves as a primary database for real-time applications. It empowers many different verticals such as retail, financial services, online gaming, social media, and many more to run their mission-critical applications on Google Cloud. Redis modules such as RediSearch, RedisJSON, RedisGraph, and RedisTimeSeries provide developers with the necessary toolsets for building highly interactive applications at a lightning speed with extreme performance. Redis Enterprise will shorten the time to market for your next applications or services with linear scalability and five-nines (99.999% SLA) availability.As the term suggests, fully-managed allows customers to consume the Redis Enterprise database as a service. Redis as the provider of this service is responsible for managing the underlying infrastructure and the lifecycle of your Redis databases. It is a pay-as-you-go consumption model in which customers are billed at an hourly rate based on five predefined shard types. Shard types are based on capacity and performance characteristics, which are memory limit and throughput. This allows Redis to meet customers where they are at in terms of their use case, and stay cost-competitive . Choosing which shard types to support your Redis database deployment is opaque to the customers. Redis will determine the optimal underlying infrastructure resources to meet the SLAs set forth by the customers while minimizing the cost of running Redis on Google Cloud.Customers have the options to subscribe to our fully managed service through Google Cloud Marketplace or directly on Redis Enterprise Cloud. There are benefits to subscribe through the Marketplace, such as existing Google Cloud commit drawdown and unified billing within Google Cloud. Customers with existing Google Cloud contracts usually take this route to simplify procurement and consolidate consumption billing.The second way to consume our fully managed service is through Redis Enterprise Cloud. Here, customers will log into the Redis console directly without going through Google Cloud console. This option cannot burn down existing Google Cloud commits and does not support unified billing. However, customers can choose to deploy Redis Enterprise in three different plans: Fixed plan, Flexible plan, and Annual plan. For the Fixed plan, customers will pay a fixed monthly price according to their memory limit. Whereas the Flexible plan Redis will optimize the plan price according to the customer workloads. Behind the scenes, Redis will build an optimal, cost-effective infrastructure and Redis database configuration. Our customers will have the flexibility to change their plan options at any given time and their configuration. Price will change accordingly. Finally, the Annual plan offers our customers a discount towards the Flexible plan prices, by committing to a predefined annual consumption upfront. The annual commit applies to all the customer workloads across multiple clouds and regions. The data endpoints will be retained and the service to your application will not be disrupted. You can compare the plans’ features here.The table below summarizes the high-level differences between Google Cloud Marketplace and Redis Enterprise Cloud offerings in fully managed Redis Enterprise deployments:If fully managed is not an option for the customers’ requirements, they can deploy Redis Enterprise as self-managed software. Customers are responsible for the underlying infrastructure on Google Cloud and the lifecycle of Redis Enterprise clusters hosting their databases. This requires customers to have full knowledge of Redis Enterprise from deployment, configuration, management, operations, and maintenance perspectives.Let’s talk about the first option of the self-managed offering. This option will require procurement of the software license for Redis Enterprise. Customers will spin up their own Google Compute Engine virtual machines hosting their Redis Enterprise clusters. They are responsible for the day-to-day operations of the clusters as well as scale-in and scale-out of the clusters to achieve optimal cluster configurations from the cost and performance standpoints. They will have full control of Redis and how their Redis Enterprise clusters are deployed and secured. They are in the full cockpit to configure, manage, and operate their clusters the way they like. They will subscribe to our world-class support to keep them up and running in optimal condition.The second option for self-managed Redis is to deploy Redis Enterprise on a Google Kubernetes Engine (GKE) cluster via the Google Cloud Marketplace. Customers will be billed at an hourly rate per database shard. Similarly, they are responsible for the lifecycle of the Redis Enterprise clusters. Thus, knowing how to operate and manage the clusters on a daily basis. Since this option uses Redis Enterprise Operator to deploy a Redis Enterprise cluster and databases, any future operations of this cluster become easier with the automation already built into the operator itself, rather than running Redis Enterprise in GCE virtual machines. Assuming the underlying Kubernetes cluster has ample resources, scaling out a Redis Enterprise cluster can be done through a declarative update of the Redis Enterprise cluster’s Kubernetes resource definition. In addition, upgrading the Redis Enterprise version can also be accomplished through a declarative update of the Redis Enterprise cluster’s Kubernetes resource definition without introducing any downtime by leveraging the rolling update functionality from Kubernetes.Last, and the third option for self-managed Redis, is that customers can choose to deploy Redis Enterprise on an existing Google Kubernetes Engine (GKE) cluster. Customers will assume the responsibility of managing the lifecycle of the Redis Enterprise clusters. The customer experience for day two operations is the same as the second option above through Google Cloud Marketplace. However, the customers will not be charged at an hourly rate by shard. Instead, customers need to procure our annual software subscription license. The table below explains the major differences between the deployments of Redis Enterprise on GCE VMs, Google Cloud Marketplace for Kubernetes, and customer’s provided GKE clusters.Choosing which deployment option is totally dependent on your project needs. If you do not want to manage Redis Enterprise clusters, fully managed is an obvious choice with many great benefits. In addition, if you already have a committed contract with Google Cloud, subscribing to a fully managed service through Google Cloud Marketplace will definitely bring you very attractive cost benefits. If your company’s security or other compliance policies would not allow your data to reside in Redis’ managed VPC, self-managed is a clear choice as it will always run Redis Enterprise clusters in your own private VPC. Further, if your company has a mandate to standardize everything on Kubernetes, then deploying Redis Enterprise through Google Cloud Marketplace is a great option as it uses Redis Enterprise Operator to deploy and manage the lifecycle of your Redis Enterprise cluster and databases in a native Kubernetes way. Finally, it is not uncommon to switch from self-managed to fully managed and vice versa. Redis provides the flexibility for how our customers want to manage their Redis Enterprise clusters as well as various consumption models best suited to their financial needs.The collaboration between Redis and Google Cloud is never-ending. The two companies continue to strive to provide the best real-time data platform on Google Cloud. We are rolling out new features on Google Cloud on a regular basis. To learn more about our partnership, please visit our partnership page."
580,https://redis.com/blog/becoming-one-redis/,From Our Founders: Becoming One Redis,"August 11, 2021",Ofer Bengal and Yiftach Shoolman,"Today, we announced that Redis Labs is becoming Redis, dropping “Labs” from our company name. The change signals the maturity of the company and the Redis open source project, to which we have continuously contributed since our founding in 2011 and which we’ve sponsored since 2015. We explain the motivation for this move in the official announcement and what we aspire for “One Redis” to mean in this short video.To provide further clarity on this change you are invited to read the following Q&As.We’re excited to continue working together across the Redis community with one goal. Go Redis!Ofer & YiftachWe’re dropping “Labs” from our company name and branding, a reflection of who we are and what we deliver: Redis, pure and simple.Redis is the world’s fastest, most-loved database for building real-time applications, at scale, across every industry and use case. We’ve always believed in Redis and contributed to the evolution of open source Redis from the outset of founding the company; helping develop it from a popular caching system into the leading real-time data platform that it is today. It’s proven, scalable, and beloved by developers, and an essential component of the modern app stack. This change demonstrates the natural progression of Redis we’ve sought to deliver in making it an essential infrastructure for modern apps.Going forward we will specifically indicate if we refer to Redis the company or Redis the open-source project. Additionally, while the open-source project continues to be documented on redis.io, the company’s website will be redis.com.Our company began contributing to the open-source Redis project shortly after we were founded in 2011, which was less than 2 years after the creation of Redis. In 2015 we started sponsoring the project and Salvatore Sanfilippo, Redis’s creator, joined Redis Labs as its lead open source maintainer. Sanfilippo stepped back from being the sole maintainer of the project in June 2020, while remaining on the company advisory board.Additionally, over the years the company has developed an expanded set of data models for Redis, complemented by powerful search and programmability engines. We have also created RedisRaft, to be released with Redis 7.0, which will make Redis a strongly consistent database. These enhanced capabilities have expanded the original caching and session management use cases of Redis to a broad range of new possibilities for developers, companies, and partners to cover the real-time needs of their applications.Our sole focus is continuing to lead the evolution of Redis as the leading real-time data platform by working across developers, customers, and partners who love and want to grow Redis.Redis is deeply rooted in our DNA. We’ve been fostering and growing the open-source project for 10 years alongside Salvatore Sanfilippo, through contributions to the code base on a regular basis long before we launched our own managed service.There is no change to the way open source Redis is licensed, managed, and controlled. Redis is and will remain BSD licensed. The governance model introduced last year also remains the same. This governance has not only enabled us to effectively tackle the technical challenges that the Redis project entails, but also maintain its enduring popularity with developers.The open-source core team is doing a great job and we’ve seen progress in getting much-needed improvements incorporated into Redis and that ultimately benefits its users. The first release under their direction, Redis 6.2, was released in March 2021 and featured contributions of more than 35 community members. To put into perspective, the community participation in Redis OSS has seen a 56% YoY increase in PRs created and 156% PRs closed on the Git project. There’s also been an 86% YoY increase in Redis project Git authors.This is reflected in Redis’s continued popularity with developers. Redis was just named by Stack Overflow developers’ “Most Loved Database” for the fifth consecutive year, has been found by Sumo Logic’s analysis to be the #1 database on AWS for two consecutive years, named the #1 database technology to adopt by CNCF End User Community, and the #1 Database for Container Images and Kubernetes StatefulSets according to DataDog’s Container Report.Developers have been and will continue to be our focus. Redis was designed to solve the need for a really fast and simple in-memory database. We took it upon ourselves to continue and foster that vision through the developer community with programs and initiatives that support the open-source project such as the annual RedisConf event, Redis University, hackathons, and much more. We stay committed to this vision going forward and aren’t introducing any changes to how developers can learn, use, and build with Redis. Redis.io remains the central hub for the open-source project, and we offer more developer resources to create unique real-time apps and the ability to engage with other creators on developer.redis.com.Nothing is changing about the code or the licensing. You will see a different logo on our website and a registered company name on contracts. We will continue making investments to make Redis more accessible and easy to use.Bringing greater alignment between the Redis core technology and enterprise product suite is a win-win for our customers. It will allow us to continue building Redis into an enterprise-grade platform, and in turn deliver availability, flexibility, and even greater performance to customers’ applications, in real-time.It won’t in any way—we’re just dropping “Labs” from our name. Our partners are a valuable part of the Redis ecosystem and have helped make it the most loved and one of the most popular databases. Nothing is changing about the code or its licensing. This change will enable customers to more clearly understand a direct Redis offering versus a partner offering leveraging Redis. We are committed to driving alignment and opportunities across the developer community, customers, and partners to continue Redis’s growth trajectory."
581,https://redis.com/blog/the-top-3-game-changing-redis-use-cases/,The Top 3 Game Changing Redis Use Cases,"April 3, 2014",Itamar Haber,"Two weeks ago the Redis team had exhibited in the 2014 Game Developers Conference. As the name suggests, the conference was packed with game developers and I had great discussions about how Redis is used in building games that scale. Here’re the top use cases for Redis in the multi-player gaming industry.If there’s one Redis feature that’s a definite blockbuster with game developers it’s its sorted sets. Almost every online multi-player game sports at least one ranked list of its players, and most manage multiple such leaderboards concurrently. For example, an online FPS could rank players by the number of frags and keep a global leaderboard as well as regional and per tournament ones. Such lists are constantly hit with updates and are paged through frequently to display their contents.Redis practically lends itself to the task – to update a leaderboard just ZADD the player’s identifier. Want to get a player’s rank? ZRANK it! And getting the top n players or paging through the leaderboard is a simple matter of ZRANGE.Session management is key to any online application, and games are no different in that respect. If anything, realtime games’ session management requirements are stricter than those of the average shopping or social site. While shopping carts and social streams updates can take a couple of seconds to complete, the success of a game often depends on its responsiveness and accuracy of the players current stats (e.g. hit points, buffs, etc…).Managing sessions in Redis is a wildly-popular practice and the efficient hash data structure makes it easy-peasy to manage (i.e. HINCRBY) a player’s counters.Another piece of information that gets hit quite heavily is the player’s profile. Depending on the game, the profile consists of any number of odd ends that the player is associated with. Although the profile is updated less frequently (at least compared to player rank and session data), it is still accessed often both by the narcissist player and by his peers/opponents. Again, Redis hashes make this a breeze.I’m a huge fan of polyglot persistence and I certainly got my fix in that show. A lot of game developers that I spoke with had reported using Redis to augment the capabilities of another datastore – MySQL, Cassandra and MongoDB being the common technologies in use. The following quote from of the developers really caught my fancy: “everything numerical goes into Redis, the rest to Cassandra”.Want to share your story? Have any questions or feedback? Email or tweet me, I’m highly available."
582,https://redis.com/blog/rediscover-redis-security-with-redis-enterprise-6/,Rediscover Redis Security with Redis Enterprise 6.0,"April 30, 2020",Alon Magrafta,"Based on high demand from the Redis community and users, the newly released open source Redis 6 dramatically improves Redis security and operational safety. We are excited to announce the general availability of Redis Enterprise 6.0, which utilizes Redis 6’s important improvements, and takes Redis security to an even higher level. This release features access control lists (ACLs) and role-based access control (RBAC), allowing you to exercise fine-grained control over the security of your Redis Enterprise deployment. In addition, Redis Enterprise 6 incorporates many changes from open source Redis 6, and provides support for Redis Streams on Active-Active databases.These new capabilities bring major security benefits to Redis users and make it easier to effectively scale Redis usage within your organization.Redis, as the home of Redis, has invested countless hours working with the Redis community to take open source Redis to the next level. Let’s look at how Redis Enterprise 6.0 supports the newly released Redis 6 and takes advantage of many of its features:Access control lists allow you to control what level of access each user has in Redis. With ACLs, you can specify which commands specific users can execute and which keys they can access. This allows for much better security practices: you can now restrict any given user’s access to the least level of privilege needed.What are ACLs good for?Using ACLs lets you tailor access for specific users. Developers, administrators, and the applications themselves may be able to function without full access to the database itself, and have more limited access. For example, an application that only reads from Redis doesn’t need permissions to flush the database. It’s now possible to create a read-only user for such an application. Following the principle of least privilege in this way, ACLs allow for a significant improvement in the security of Redis deployments.ACLs in Redis EnterpriseRedis Enterprise has always provided a centralized management platform for multiple Redis databases. In Redis Enterprise 6.0, you manage ACLs for the entire cluster, so you can reuse ACL templates across users, accounts, and multiple databases to precisely scale complex security configurations with a few simple clicks.In open source Redis, you define ACLs on a per-user basis. Redis Enterprise improves upon this by letting you create roles, each with a specific set of permissions. For example, you might have a role for read-only users and another role for your site reliability engineers. You can then associate these roles with the appropriate Redis users. This is known as role-based access control, or RBAC.RBAC lets you set permissions for your databases and for the Redis Enterprise management console itself, providing a complete security-management solution for your cluster.What is RBAC good forRole-based access control lets you scale your Redis deployments while minimizing the overhead involved in managing a cluster with many databases, multiple users, and various access control lists. With RBAC, you can create a role once and then easily deploy it across multiple databases in the cluster.ACL rolesIn the diagram above, which represents Redis Enterprise 6.0, Bob, Sue, and Alice would receive access via the CachedReader role. In the diagram below, which represents OSS Redis 6.0, these users’ CachedReader permissions are set individually.ACL open sourceRedis Enterprise 6.0 adds support for the Streams data type in Active-Active databases.What is Redis Streams?Redis Streams is a Redis data structure that models an append-only log and enables you to use Redis Enterprise as a high-speed, in-memory, streaming database. Redis Streams is often used to collect and syndicate real-time data for internet of things (IoT) devices, complex event processing systems (such as fraud detection), and messaging applications.What are Active-Active databases?Active-Active is a Redis Enterprise feature that synchronizes a database across two or more geographical regions. This allows you to build globally distributed applications while guaranteeing local-latency read and write performance.Redis Streams on Active-Active databasesUsing conflict-free replicated data types (CRDTs), Redis Streams ensures both high availability and low latency while concurrently reading and writing to and from your stream in multiple data centers in multiple geographic locations. When used in a globally distributed Active-Active database, all Redis Streams data structures will be synchronized in a strongly eventually consistent manner across all regions, so data or transactions are resistant to failure events.You can use Streams with Active-Active in many situations, including:With the addition of Redis Streams, Active-Active Redis databases now support all major Redis data types, including Counters, Floats, Geospatial, Hashes, HyperLogLog, Integers, Lists, Sets, Sorted Sets, Strings, and, of course, Streams.For more information, read our deep dive into open source Redis 6. You can also sign up for a free 1:1 Ask the Expert session at RedisConf 2020 Takeaway on Tuesday, May 12.Starting today, current customers can contact Redis for information on how to access Redis Enterprise 6.0—go to our Download Center now."
583,https://redis.com/blog/introducing-redis-om-client-libraries/,Introducing the Redis OM Client Libraries,"November 23, 2021",Kyle Banker,"Today, we’re pleased to announce a preview release of four new high-level client libraries for Redis. We’re calling these libraries Redis OM: Object Mapping, and more, for Redis. Our goal is to make it as easy as possible to use Redis and the Redis modules from your applications.We’re launching with support for .NET, Node.js, Python, and Java (Spring). If you’re a software developer keen to see why we’re so excited about this launch, you can try one of our new libraries right now:Redis OM for .NETRedis OM for Node.jsRedis OM for PythonRedis OM for SpringEach Redis OM library includes a comprehensive README and tutorial to help you get started quickly.Just want a high-level overview for now? Read on!As programmers and CS geeks, we love the elegance of a good data structure. And, as we all know, Redis provides rock-solid, distributed data structures in spades. There’s a kind of Unix philosophy that runs through Redis: you can compose a well-built collection of primitives to solve any problem at hand. This composability has allowed Redis users to create a wide variety of tools, including caches, distributed locks, message queues, rate limiters, pub/sub systems, background task executors, de-duplicators, and permissions systems, to name several.The challenge is that not everyone has the time to reinvent these tools on their own. Not everyone has the bandwidth to build their own rate limiter or to figure out how to map a Redis hash to Java class (and vice versa!). We’ve built (and continue to build) Redis OM so that you can get the performance of Redis even if you don’t have time to compose your own abstractions using the core data structures that Redis provides.Redis OM (pronounced “ōm”) is object mapping (and more) for Redis. Our aim is to furnish you with a toolbox of high-level abstractions that make it easy to work with Redis data in the programming environments you call home. The first abstraction we’re providing is object mapping. If you’re modeling any domain, you’re likely representing that domain in an object-oriented fashion. The Redis OM libraries let you transparently persist your domain objects in Redis and query them using a fluent, language-centric API.For this first preview release, we focused on object mapping and fluent queries. Since Python is a sort of lingua franca for us programmers, let’s explore a few examples from Redis OM for Python to get a sense of what’s possible here.First, we can define a simple domain object. In this case, the object represents a customer:Now we create a new Customer instance like this:And then we can write this new customer instance to Redis by calling save() on it:The Redis OM libraries automatically generate a unique ID for any new object. For this, we’re using ULIDs (Universally Unique Lexicographically Sortable Identifiers). You can think of a ULID as a kind of user-friendly UUID. ULIDs are sortable, globally unique, URL-safe, and fairly human-readable when base32-encoded.Want an example? A typical ULID looks like this: 01ARZ3NDEKTSV4RRFFQ69G5FAVAnyway, once you’ve saved one of these objects to Redis, you can retrieve it by providing its unique ID. In Python, you can access that unique ID with the pk property (pk, of course, refers to “primary key”).Pass the unique ID to Customer.get(), and you’ll get your object back:With the Redis OM libraries, you can serialize your domain objects in two ways. We’re serializing this Customer object as a Redis hash, as it’s a simple object with a flat structure. But if you have objects with a large number of fields or nested objects, you’ll probably want to serialize them as JSON instead (and for this you’ll need the source-available RedisJSON module).If you’re mapping domain objects to Redis, you probably want to be able to query them. RedisJSON (and, by proxy, RediSearch) provides indexing and querying for Redis. The Redis OM libraries take advantage of these features to give you a fluent query API over your domain objects.Let’s take a quick example. With the Redis OM Python library, you can construct fluent expressions to query your data. For example, here’s a query that retrieves all customers whose last name is either “Javayant” or “Jagoda”:The beauty of these queries is that they’re always indexed, so they’re efficient by default. See our recent benchmarking blog to get an idea of the performance benefits you’re likely to see.The Redis OM libraries are a bit like a family; they’re united in their purpose of making it as easy as possible to use Redis and the Redis modules. At the same time, each library has its own language-specific emphases, all designed to delight the .NET, Node.js, Python, and Java/Spring software engineers we wrote them for. Some examples?Redis OM for Node.js is written in TypeScript, providing first-class support for TypeScript and JavaScript.Redis OM for .NET lets you query your Redis domain objects using LINQ.Redis OM Spring integrates natively with Spring, extends Spring Data Redis (to provide a familiar interface), and even adds some support for RedisBloom (which gives you probabilistic data structures).Redis OM Python integrates natively with the popular FastAPI framework. Combining FastAPI with Redis is a great way to build high-performance web services. Redis OM Python also supports both sync and async usage.We’re thrilled to announce this preview release for the Redis OM libraries, and we can’t wait to get your feedback. Pull requests and Github issues are great, but we’d also love to chat directly. Find us on the Redis Discord server. Come say hi, and let us know what you need next! As we continue to develop and improve the Redis OM libraries in earnest, we welcome your ideas, questions, and contributions."
584,https://redis.com/blog/how-mutualink-uses-redis-to-support-a-life-saving-microservices-architecture/,How Mutualink Uses Redis to Support a Life-Saving Microservices Architecture,"January 15, 2020",Sheryl Sage,"(As organizations look to modernize their applications, many are turning to a microservices architecture to deconstruct their legacy apps into collections of loosely coupled services. This profound change inspired us to reach out to Redis customers in various stages of this journey to microservice architectures. We will be telling their microservices stories in a series of blog posts in the coming weeks.)When Mutualink looked to modernize its Interoperable Response and Preparedness Platform (IRAPP), the company’s IT team turned to a microservices architecture to enable dozens of technology components to work seamlessly together. That’s important, because Mutualink’s mission is to facilitate collaboration between Federal, state, and local agencies and private entities to resolve dangerous incidents.Paul Kurmas, Director of Strategic Product Development at Mutualink, shared what a microservices model means to him: “It allows us to design components that do one thing and do it well, and the payoff is simpler software that is faster, more reliable, and easily supportable. Our microservices architecture maximizes availability and minimizes the scope of any software, system, or communications failure, while also helping us scale because we can load-balance work from various clients or internal services across a large pool of instances.”All of this requires a flexible and efficient central data store with top performance, effortless scalability, and high availability. That’s why the Mutualink team turned to Redis Enterprise to power the data layer in its microservices architecture.Mutualink’s tens of thousands users expect instant response every time they use the system to track and report location data during an incident; send and receive text, audio, or video messages; share files; and more. So IRAPP’s latency must be as low as possible, and the architecture has to maintain its performance even as clients’ needs scale. At any given time, the platform might contain hundreds of active collaborations between agencies, each of which feeds an event stream with data on who’s online where and what new content is available for subscribers.Mutualink runs Redis Enterprise on virtual machines that store upwards of 32 terabytes of data, including both user records and subscriptions to these various streams. Thanks to Redis’ ability to automatically scale data across servers, the IRAPP can maintain consistently high performance and scale dynamically by adding servers or availability zones as these collaboration volumes expand. This lets Mutualink handle a large number of concurrent data requests with sub-millisecond latency.From the beginning, Mutualink worked to enable high availability so the platform could survive the loss of a software instance, compute node, or even an entire data center. One way the team accomplishes this is through Redis Enterprise’s Active-Active replication, in which data is automatically synchronized between multiple data centers for geographic redundancy. “Our focus on instant access to data is critical. It’s what made Redis an attractive solution to be the central data store to our application,” Paul noted. “The Active-Active functionality was our deciding factor in using Redis Enterprise, and it saved my team many, many staff years’ of time trying to invent a cross-data site synchronization model.”Mutualink’s Redis data model supports near real-time, eventual consistency—if an event triggers a change to data on the company’s server in its Boston region, for example, the response to the event-triggering client is immediate, while the data is reflected on its server in its Dallas region in near real-time. Additional clients can subscribe to events in Redis in whichever region is closest.Taking a closer look, the microservice that handles signaling for voice over IP (VoIP) has to maintain thousands of telephony connections even if there’s a software failure. Mutualink exports each call state from the platform and stores it in Redis in multiple regions, so that the platform remains stateless. Then, when the next message in that session is ready, it can be delivered efficiently to any instance of the service. Thanks to Redis Enterprise’s conflict-free replicated datatypes  (CRDT), the VoIP service provides uninterrupted availability even when some data centers are completely unavailable. With this kind of resiliency, the overall application can continue to perform reads and writes even if one or more of the individual microservices fails.While Mutualink originally planned to use Redis for just one central component of its microservices architecture, the team quickly learned how easy it was to plug other services into Redis and use its simple APIs to maintain their own data. Rather than having one service (which they called “the World”) act as the gatekeeper to all of the platform’s data, Mutualink now has 15 – 20 different services directly using Redis Enterprise to serve various needs.Redis’s extensibility and data model diversity help the Mutualink team avoid vendor sprawl, since its data structures can support a wide range of business capabilities. Features Mutualink has built with Redis include:Redis helps each of these microservices transform data without requiring changes to the back-end stores. Mutualink benefits from a common, yet decentralized, data layer in Redis that all services can access. Plus, each service can use the right language, database, or other developer tools for the job at hand. Since Redis supports multiple data formats, individual services can employ key-value, graph, hierarchical, JSON, streams, search, or other data models as needed.By enabling Mutualink to cache frequently used content and evict unnecessary data (such as expired authentication tokens or aging data surrounding subscriptions, text messaging, and HTTP live-streaming services), Redis Enterprise also ensures optimal use of the company’s compute resources. For instance, Redis stores frequent location status updates and OAuth2 tokens, which are kept on a short timeout. When a client stops reporting, Redis automatically removes its subscriptions and presence reports to free memory.By using Redis as both a caching layer and primary data store, Mutualink can more quickly phase out its legacy architecture and replace it with more efficient access to data. “We have a 3 – 6 month window to move thousands of users over to our new microservices-based system,” Paul said. “Since Redis and microservices remove the constraints of our old architecture, it will increase our speed of deployment 2x – 3x within the first year.”Now that Mutualink has laid the groundwork for its modern microservices architecture, Paul looks forward to building out even more purpose-built software components. He expects Redis Enterprise to remain a key asset as his team scales to more instances, mitigates the impact of failures, and performs upgrades to individual services.Learn more about how Mutualink is taking full advantage of Redis Enterprise.Join us: Hear more about Mutualink’s journey to a microservices architecture and how Redis Enterprise is used to solve several key problems with distributed systems in our January 22 webinar.Featured image by Nicholas Jeffries, Unsplash"
585,https://redis.com/blog/fog-computing-need-redisedge/,Fog Computing and the need for RedisEdge,"October 25, 2018",Rob Schauble,"Internet of Things (IoT) solutions present a unique challenge for any database. They produce increasingly large and fast data from a very broad spectrum of IoT devices, while requiring near-instant response times. Given this, the solution’s data processing and analysis must increasingly be handled at the network edge, as close as possible to the sensors, actuators and other IoT devices. We no longer have the luxury of being able to crunch IoT data in a cloud environment, where there is seemingly limitless compute and storage resources, because the latency would be unacceptable. Thankfully, there are powerful databases and platforms tackling this challenge head-on. But before we explore the solutions, it’s important to have a solid grasp of the data requirements unique to IoT environments.Edge computing has increasingly become table stakes for IoT and big data applications, given their volume, velocity, variety and veracity (the “four V’s”) requirements, and the distributed nature of most use cases. So is there a way to maintain the data processing capabilities we have long enjoyed (and the very reason we’ve had a massive trend to cloud computing over the past ten years), if we move back to distributed computing at the edge? Can we have our cake and eat it too?Fortunately, in this case – with the popularization and growing trend of fog computing – the answer is yes. The solution is to bring cloud principals to the edge, where the fog and cloud environments operate in tandem to handle complex IoT use cases. When you have critical latency requirements (for example with smart city or connected car IoT use cases), that data must be handled by ruggedized fog nodes close to the IP cameras and other sensors, while non-latency critical data can still be synchronized to the core or cloud. In this way, data from all edge devices and fog nodes in your IoT solution can be aggregated at a core level (for example, a city block in the smart city use case), and ultimately to the cloud or data center environment for more in-depth business intelligence and other analytics.With fog computing, we refer to data communication between IoT devices, edge devices, fog nodes and the cloud as “north-south” communication, and data communication between the edge/fog nodes across the system as “east-west” communication. For this to be effective, you must have common cloud or data center environment capabilities at the edge, such as machine learning, deep learning and image recognition. This presents our next challenge: how do we handle these needs, given the distributed nature and modest storage and compute capabilities of edge devices and fog nodes? Having fog and cloud environments operating in tandem is critical. For example, with machine learning, you might need to train models in the cloud (where you have vast compute and storage resources), and deploy those trained models to fog nodes and/or edge devices (so they can be served close to IoT devices for minimal latency).The “intelligent edge” has arrived and rescued us from these seemingly unsolvable problems (or Kobayashi Maru for you Star Trek fans). The edge is the battleground for IoT today, but is there a database on the planet that can handle its massively large and high-velocity data from potentially thousands of sensors, cameras and other devices? One that can process that data in real-time, with many different database models, and with a small footprint?Fortunately, RedisEdge is a multi-model database that delivers blazing fast performance with the ability to ingest millions of writes per sec with <1ms latency at the IoT edge. It can do this with a small hardware and software footprint, so it is perfectly suited to live on fog nodes, edge gateway devices and even IoT devices in some cases. RedisEdge has many native data structures (sets, sorted sets, lists, hashes, streams, etc.), providing ultimate flexibility for IoT application developers. Furthermore, with the many modules that already exist to extend it, RedisEdge can handle very diverse workloads required at the IoT edge (time-series, graph, machine learning, search, etc.). Rather than deploying five different databases to support these needs, RedisEdge can manage them all with a single instance, tremendously simplifying your architecture.Now that we’ve shown there is a database worthy to take on the intelligent edge of IoT, you might wonder which platform is best-suited for running RedisEdge? Fortunately, the new Azure IoT Edge from Microsoft is purpose-built to take on this challenge! We are delighted to partner with Microsoft Azure on IoT edge solutions, since both Redis and Microsoft are putting significant focus and investment into accelerating intelligence and computing at the edge. With RedisEdge integrated into the Azure IoT Edge runtime environment, joint customers and partners will enjoy a fast general data store, a message broker between Azure Edge modules, streams processing, a time-series database, and in-memory processing (for machine learning model serving, graph processing, etc.) to ensure the best possible performance.Stay tuned over the coming weeks as we share more details on how the IoT community will benefit from our joint IoT edge solutions."
586,https://redis.com/blog/running-redis-on-google-colab/,Running Redis on Google Colab,"January 28, 2022",Nava Levy,"Because of the increasing use of Redis for data science and machine learning, it is handy to run Redis directly from a Google Colab notebook.  However, running Redis on Google Colab differs from how you would set it up on your local machine or using Docker. In this post, I show you how you can run Redis on your Colab notebook, in two simple steps, directly from your browser.Google Colab is a popular browser-based environment for executing Python code on hosted Jupyter notebooks and training models for machine learning (ML), including free access to GPUs. It is a platform for data scientists and machine learning (ML) engineers to help them learn and develop ML models in Python. Redis is an in-memory open source database that is increasingly being used in machine learning – from caching, messaging and fast data ingest, to semantic search and online feature stores.While Jupyter Notebooks support many languages, Colab supports only Python. To use Redis with Python, you need a Redis Python client. In this tutorial, we demonstrate the use of redis-py, a Redis Python client, which we install using the %pip install redis command.You can run a shell command in Jupyter Notebook or Google Colab with IPython by prefixing it with the ! character or % to use magic commands. A list of useful magic commands for data scientists is described in Top 8 magic commands in Jupyter Notebook.To install Redis and the Redis Python client:To start the Redis server run:That’s it! It’s that simple.Let’s now look at the commands we need to verify that Redis is running, connect to it, and read and write data.To verify that Redis is up and running, create a connection to Redis using the Python client redis-py and then ping the server:If you get a response True, then you are good to go!Once you are connected to Redis, you can read and write data with Redis command functions. In this example, we use Redis as a key value database (also called key value store). The following code snippet assigns the value bar to the Redis key foo, reads it back, and returns it:If you want to play with the commands yourself, consult the Redis with Colab notebook, which includes the code in this tutorial."
587,https://redis.com/blog/fall-events/,Fall Events,"September 25, 2017",Tague Griffith,"It’s hard to believe, but the crew here at Redis is already planning for RedisConf 2018.  Next year’s conference will take place at the end of April 2018 in San Francisco.  Sign up here and we’ll keep you up to date on all the latest news.  I’m not going to spoil the surprise, but 2018 will be even bigger and better than this year’s conference, so don’t miss out on all the fun.In the meantime, we’d love to see you at one of the many events we have planned in the coming months.September EventsConferencesRedis is a premier sponsor of the Strata Data Conference in New York City on September 26th to September 28th.  Drop by Booth 1014 to chat with our team about Redis Enterprise.  You can arrange for a meeting ahead of time.Our Developer Relations team is a Bronze Sponsor of this year’s Strange Loop 2017 from September 28th to September 30th. If you’re new to Redis or a long time user with questions about how you can use it better, stop by and chat with us.MeetupsThe San Francisco Redis Meetup is joining forces with the Bay Area Cloud Foundry Users Meetup on September 25th for two interesting talks on using Redis in conjunction with Pivotal Cloud Foundry.  Jacques Malan of Pivotal Labs will be sharing his thoughts about how Redis can be used to supplement deep Neural Network learning Systems.  Our own Kyle Davis will be talking about Fast Data Ingestion with Redis running on Pivotal Cloud Foundry.  RSVP for the meetup here.In France, François Cerbelle is running a series of new meetups demonstrating how to build Arduino based IoT devices that can send sensor data to Redis.  RSVP for any of the regional events to learn more about Redis and IoT.  François would love to have you at any one of his events.  You can register for his events from the main Redis France Meetup page.Finally, the stalwart Seattle Redis Meetup has been hosting a series of meetups demonstrating how to use the client libraries from many languages.  This month’s meetup features Nell Shamrell-Harrington demonstrating the use of Rust to move data between Redis and PostgreSQL.  Sign up for the meetup here.October EventsConferencesOur European team will be out in force this month, with events in Switzerland, Denmark and Ireland.  Redis is a Bronze Sponsor of this year’s Cloud Foundry Summit in Basel, Switzerland.  Redis engineer, Shay Nativ will be speaking about using Redis as part of a machine-learning architecture.  From there, it’s off to Copenhagen for DockerCon EU.  Uri Shachar from the Redis Cluster group will give a presentation on choosing a data store for microservices.  Finally, Dvir Volk from our engineering team will be speaking about using Redis with Apache Spark at Spark Summit Europe in Dublin, Ireland.Not to be outdone, the US team will again be at Oracle Open World.  Drop by Booth 5409 to speak with our Redis experts.  You can also catch up with our US team at the All Things Open Conference, where I will be speaking about using Redis to serve predictive models trained in Spark.October Community ActivitiesIn October, the Seattle Redis Meetup will be hosting Eric Baer and Blake Schwartz who will discuss GraphQL and how Redis can be used in conjunction with other databases that support GraphQL.  Sign up here for the October meetup.Further south, the San Francisco Redis Meetup will be talking about Redis 4.0 and Redis Enterprise and the Silicon Valley Redis Meetup will host Kyle Davis for a presentation on using Redis on Microcontrollers."
588,https://redis.com/blog/redis-provides-fast-data-ingest-no-heartburn/,"Redis Provides Fast Data Ingest, No Heartburn","September 7, 2017",Roshan Kumar,"Imagine visiting an amusement park, consuming large quantities of all four primary amusement park food groups (which are corn dogs, funnel cake, snow cones, and cotton candy of course), and then spinning around wildly on the Tilt-A-Whirl. The volume and variety (and dare we suggest velocity?) of food you’ve just consumed is likely to overwhelm your stomach, causing discomfort and forcing it to slow down and regroup in order to accommodate the day’s indulgences.But your software stack isn’t allowed the luxury of slowing, not even a fraction of a sub-millisecond; it needs to handle whatever data is thrown at it without breaking a sweat or popping a Tums. Fast data ingest and processing is an increasingly common requirement for big data use cases, and internal stakeholders and customers alike have come to expect (and rely on) the real-time decision making that it enables.Collecting, storing and processing large volumes of high-variety, high-velocity data is a task that presents several complex design challenges—especially in fields like IoT, e-commerce, security, communications, entertainment, finance, and retail.And this is where Redis has a real advantage. It tackles these challenges with an ease that is unmatched by other fast data ingest solutions, which tend to be complex and over-engineered for simple requirements.In fact, in an independent survey of Redis customers conducted by TechValidate, 74% reported using Redis for messaging and data ingest.We’ve outlined a few of the standout features that make Redis so ideally suited for messaging, data streaming, and fast data ingest.When it comes to performance, Redis has been benchmarked to handle over one million read/write operations per second, with sub-millisecond latencies on a modest-sized commodity cloud instance of one or two servers. This makes Redis the most resource-efficient NoSQL database in the market.Redis offers a variety of data structures such as Lists, Sets, Sorted Sets, and Hashes that provide simple and versatile data processing in order to efficiently combine high-speed data ingest and real-time analytics. By taking advantage of data structures that have been purpose-built to support the objective at hand (e.g. time-series analysis, spatial analysis, machine learning, etc.), analytics operations get a big performance boost as these data structures provide not only mechanisms to more elegantly store variably structured data, they also come with built-in operations that perform complex in-database analytics on the data, right in memory where it is stored.Another performance boost comes from Redis’ publish/subscribe capabilities, which allow it to act as an efficient message broker between geographically distributed data ingest nodes. Data producing applications publish streaming data to channels in the format(s) required, and consuming applications subscribe to those channels that are relevant to them, receiving messages asynchronously as they are published.For datasets measured in terabytes, Redis Enterprise from Redis is enhanced to run on a combination of RAM and Flash memory. The ability to simultaneously store keys and hot values in RAM and cold values in more cost-effective Flash drastically reduces operational costs without compromising responsiveness. For larger datasets, which are fast becoming the norm, this is the most cost-optimal way to deliver in-memory performance at the scale of big data.We recently published a white paper titled Redis for Fast Data Ingest. It overviews the challenges encountered when designing fast data ingest solutions and how Redis addresses these with very little effort. For those of you that like to dive deep, there’s a sample fast data ingest scenario to follow along with, and three different methods, complete with code (oh yeah!), for accomplishing that scenario.As your organization’s big data needs grow, remember that achieving fast ingest and real-time processing of high-volume, high-variety, high-velocity data doesn’t have to give you a headache—or your system heartburn. Just use Redis Enterprise."
589,https://redis.com/blog/oracle-openworld-what-you-missed/,6 Things You Missed If You Didn’t Visit Redis Labs at Oracle OpenWorld,"September 26, 2019",Fredric Paul,"As you are probably aware, tens of thousands of database professionals and other tech leaders and practitioners converged on San Francisco last week for Oracle OpenWorld. And you may have heard some of the news coming out of the event, ranging from another batch of controversial statements from Oracle chief Larry Ellison to the annual raft of new announcements, this time centering around Exadata, AutoML and data security, cloud infrastructure, and even a new mission statement!But unless you were there, you might have missed a golden opportunity to learn more about Redis and Redis Enterprise. Let’s take a quick look at what went down in and around the Redis booth at Oracle OpenWorld:In an exclusive sit-down with TFIR at Oracle Open World, Kyle Davis, Head of Developer Advocacy at Redis, covers all the basics about Redis and Redis Enterprise in just 10 minutes! It’s a great way to get up to speed on what we’re up to, and fortunately, you can watch the video even if you weren’t at the show:MySQL has definitely earned its popularity, Redis’ Head of Ecosystem Programs Dave Nielsen told a standing-room-only crowd, but as a fast and lightweight in-memory database, Redis specializes in things that MySQL doesn’t always do well, making them perfect complements to each other. In particular, users of MySQL databases may face challenges in areas such as:Fortunately, he said, there are several ways adding Redis can help, including:Redis is known for its performance, simplicity, and extensibility, but Madhukar Kumar, Redis’ Vice President of Technical and Product Marketing, explained that Redis supports a surprising variety of high-performance operational, analytics, and hybrid use cases. He specifically called out using:Earlier this year, Redis Enterprise delivered more than 200 million ops/sec, with less than 1 millisecond latency, on as few as 40 AWS instances. This represented a 2.6X scalability improvement in less than 15 months. And at Oracle OpenWorld, Enterprise Customer Success Team Lead Mikhail Volkov showed attendees just how we keep breaking our own speed records by maintaining close to optimal scalability (94%).Kyle talked about “how the AI sausage is made” and reminded attendees that “production is hard,” especially when it comes to serving complex artificial intelligence models. “When running AI across many services, there are lots of places for potential failure and you run the risk of losing important data. To ensure that AI predictions are properly recorded, the prediction needs to be on the same instance as the storage … Enter RedisAI!” RedisAI implements an AI serving layer as a module in Redis that’s no more complicated than a simple caching operation in Redis. You don’t have to know anything about AI, you just need to know these four commands: AI.MODELSET, AI.TENSORSET, AI.MODELRUN, and AI.TENSORGET. And RedisAI has another superpower: the ability to swap models at will to enable A/B testing, user-level configuration, interval model replacement, and optimizing models for peak periods.It wouldn’t be a trade show without t-shirts, but few pieces of swag are as cool as the jet-black GOT Redis? t-shirt our booth staff was handing out to folks who attended our informative lightning talks. But that’s not all. We also had stickers to adorn your laptop and chances fun door prizes ranging from drones to smart speakers to Star Wars LEGO kits! The only catch? You had to be present to win!Of course, these highlights represent only a small fraction of the many lightning talks, product demos, one-on-one conversations with Redis experts, and other Redis activities at the event.But don’t worry too much if you didn’t make it to Oracle OpenWorld this year. There are plenty of upcoming opportunities to connect with the Redis team at events around the world, from Chief Data Officer Day in Madrid, Spain, this Thursday, September 26 to AWS re:Invent in Las Vegas, December 2 – 6, and many more.(To stay up to date on Redis events and find the best ones to attend, subscribe to our events page now!)"
590,https://redis.com/blog/what-is-a-data-pipeline/,What Is a Data Pipeline?,"July 29, 2022",Talon Miller,"Data has become the most valuable commodity to businesses in the 21st century. From start to finish, the success of every large corporation hinges on its ability to gather, process, manage, and utilize data.To better utilize your data, read up on the most common data migration mistakes in our e-book:Top Five Benefits of Adopting a Cloud Migration Proof Data LayerData pipelines have become pivotal to getting data from one place to another and in a way that’s readily available for analysis. Data pipelines eliminate most manual steps from the process, providing you with faster, more reliable insights that will propel you closer to achieving your objectives.Below we’re going to unpack everything you need to know about data pipelines and how you can fully utilize data to give you a competitive edge over your competition.A data pipeline is several actions taken to take raw data from different locations to a place where it can be stored and analyzed. When data is gathered in its purest form, it may not be optimized for analysis. And so, a crucial aspect of a data pipeline’s utility is to transform this raw data in a way that’s primed for reporting and to be used to develop key business insights.The data pipeline process can be broken down into three different stages.The sources are essentially the locations from where data is gathered. These locations will include relational database management systems such as:In many instances, data is ingested from disparate sources, then manipulated and translated based on the needs of the business.The last stage involves transmitting the data to its intended storage location, which typically is a data lake or a data warehouse, for analysis.Batch-based data pipelines are one of the most common types and these are deployed either manually or periodically. As the name suggests, they process data in batches. Under this architecture, there might be an application that creates a broad range of data points that needs to be transmitted to a data warehouse and a location for them to be analyzed.Batch-based data pipelines operate under a cyclical process where each cycle is completed once all of the data has been processed. The time taken for it to complete a run depends on the size of the consumed data source, which can range anywhere between a couple of minutes to a couple of hours.When active, batch-based data pipelines will place a higher workload on the data source. As a result, businesses tend to deploy them at times when user activity is low to avoid impeding other workloads.They are typically used in situations where time sensitivity isn’t an issue, such as:Below is an example of what a batch-based data pipeline would look like:Living in a world where businesses generate unfathomable amounts of data every day, companies have looked to streaming data pipelines as a more optimal way of gathering, processing, and storing data. This is because streaming data pipelines operate continuously and the architecture allows them to execute millions of events at scale and in real-time.They’re used in scenarios where data freshness is mandatory and where organizations are required to react instantly to market changes or user activity. So if for example, our goal is to monitor consumer behavior on a website or an application, the data will be based on thousands of events, each with thousands of users. This can easily amount to millions of new records each hour.Should it be mandatory for an organization to respond instantly to changes seen in real-time – downtime on an app or a website for example – using a streaming data pipeline would be the only viable option.Typical use cases of streaming data pipelines include:Below is an example of what a data streaming pipeline would look like:ETL stands for ‘extract, transform, load’ and is a data integration process that makes data consumable for businesses to leverage. An ETL pipeline will allow you to extract data from one or many sources, transform it, then push it into a database or a warehouse.They are essentially made up of three interdependent processes of data integration that are responsible for transferring data from one database to another. There are three fundamental differences between data and ETL pipelines.The last stage of the ETL pipeline process is transferring the data into a database or a warehouse. This is different from data pipelines – it doesn’t always end with the loading process. With data pipelines, the loading can initiate new processes by activating webhooks in other systems.Although data pipelines transfer data between different systems, they aren’t always involved in transforming it, unlike ETL pipelines.ETL pipelines tend to be run in batches, where data is moved periodically in chunks. Conversely, data pipelines often operate as real-time processes that involve streaming computation.It’s important to highlight that the data pipeline itself is a process for transferring data from the source to the target systems, whereas the data pipeline architecture is a comprehensive system that extracts, regulates, and connects data to other different components. This entire process typically comprises four steps:Data pipelines can be broken down into repeatable steps that pave the way for automation. Automating each step minimizes the likelihood of human bottlenecks between stages, allowing you to process data at a greater speed.Automated data pipelines can transfer and transform reams of data in a short space of time. What’s more, is that they can also process many parallel data streams simultaneously. As part of the automation process, any redundant data will be extracted to ensure that your apps and analytics are running optimally.The data that you gather may likely come from a range of different sources which will contain distinguishable features and formats. A data pipeline will allow you to work with different forms of data irrespective of their unique characteristics.Data pipelines will aggregate and organize your data in a way that’s optimized for analytics, providing you with fast immediate access to reliable insights.Data pipelines enable you to extract additional value from your data by utilizing other tools such as machine learning. By leveraging these tools, you’ll be able to carry out a deeper analysis of your insights that can reveal hidden opportunities, potential pitfalls, and ways you can enhance operational processes.Easy, simple to use, and effective, Astera can extract data from any cloud or on-premises source. With this tool, data is cleaned, transformed, and sent into a destination system based on your business needs. What’s more, is that you can do this within a single platform.Hevo Data is a no-code pipeline that allows you to load data from different sources to your data lake in real-time. The tool is time-efficient and is designed to make the tracking and analyzing of data across different platforms as easy as possible.The tool helps you to automate your reporting process. All that’s required is for you to connect more than 100 disparate data sources and examine them in different formats.Integrate.io requires no code and transfers data from source A to source B through the ETL process (extract, transform, load). This user-friendly tool connects different data sources and destinations through connectors that require minimal (or no) code, thereby allowing you to shift critical business information from different sources for analytics.Data source connectivityYou’re able to draw data from over 100 different sources with pre-built connectors. This includes the integration of SaaS software, cloud storage, SDKs, and streaming services.Deploy seamlesslyIt only takes a few minutes to set up a pipeline through their simple and interactive user interface. Deployment is easy and you’ll carry out an analysis to optimize data integration calls without hindering data quality.Scales as data growsYou’re able to scale horizontally as the volume of data and speed increases. Hevo Data can take on millions of records per minute with low latency.Redis is a real-time in-memory data platform that serves low latency, high-volume, highly scalable, and reduced-cost data ingestion, data processing, data, and feature serving. The whole point of a highly performant data pipeline is to serve data as fast as possible to services used by customers or even directly to customers. The RDBMS or even some NoSQL databases aren’t capable of providing the speed and performance of an in-memory solution like Redis so including Redis inside of your pipelines to enable real-time use cases is paramount.In-memory speedRedis Enterprise can process more than 200 million read/write operations per second, at sub-millisecond latency.Cost-efficient storageRedis on Flash which extends DRAM with SSD enables cost-efficient storage and allows ingestion of very large multi-terabyte datasets while keeping the latency at sub-millisecond levels even when ingesting more than 1M items/sec.Data type flexibilityRedis is one of the most data pipeline-friendly databases out there because of all of the native data types like:"
591,https://redis.com/blog/how-redis-is-enabling-the-future-of-real-time-ai/,Guest Post: How Redis Is Enabling the Future of Real-Time AI,"October 2, 2021",William King and Taimur Rashid,"The wave of cloud-based, real-time analytics and AI-driven applications is gaining momentum across multiple industries. Organizations want to empower their businesses and teams with emerging technologies, and AI and ML initiatives are front and center for many. We’re in the golden age of AI and ML. It’s becoming a core part of businesses around the world, and its importance has been accelerated by the COVID-19 pandemic. This informative and comprehensive post by William King, CTO and Co-Founder of Subspace, is about the future of real-time AI—and how the Redis technology is a driving force behind it. Drawing from foundational examples such as ML feature stores to more vertically specific applications like NLP for scientific and medical research, there are many ways Redis is being used to bring real-time to AI/ML use cases. And this is just the beginning.Thanks for letting us share your insights, William!—Taimur————————————Real-time application performance is a perspective exercise; you could view it either from the standpoint of user experience or corporate profitability, depending on audience. The end conclusion, however, remains: The world increasingly demands immediate data processing and responsiveness, and those who can provide it will enjoy greater success.Real-time functionality is increasingly critical across industrial automation, Internet of Things (IoT) services, cybersecurity, multiplayer gaming, business communications, autonomous vehicles and aircraft, and countless other spaces. The requirements of artificial intelligence (AI) continue to apply and evolve across these areas.Arguably the greatest challenge to real-time applications is achieving instantaneous performance with seamless, potentially vast scalability. As an analogy, think about vehicle speed relative to cargo capacity.With only a passenger seat or two for capacity, a performance sports car achieves high responsiveness and speed. Could you get the same results from a loaded 18-wheel big rig?This is what Redis has been dedicated to with its open-source, in-memory data structure store (also named Redis).As the company describes it, Redis can be “used as a database, cache, and message broker” through various data structures. Redis provides for on-disk persistence, asynchronous replication, high availability, and automatic partitioning. Most importantly for this discussion, Redis achieves its real-time results by keeping datasets in memory. With traditional data architectures, a subset of the application’s total dataset—typically the “hottest” or most immediately needed data—resides in DRAM-based system memory, while the majority of data sits in slower but much less expensive disk (and/or SSD) storage. In-memory architectures swallow the bitter pill of higher costs to avoid time-intensive fetching of data from high-capacity storage media.In April 2021, the company announced its new Redis 7.0 platform, which comprises several components and technologies. For example, rather than forcing users to pick between scalability and performance or data consistency across potentially globally distributed nodes, Redis 7.0 offers the best of both worlds. Part of this functionality depends on Redis’ “Active-Active” technology, which allows applications to deploy in any cloud model or location to keep data as close as possible to customers. When combined with the company’s AI inferencing engine, RedisAI, users can deploy real-time AI applications wherever AI features are stored, resulting in improved “AI-based application performance by up to two orders of magnitude”For people who don’t work on databases every day, it’s easy to miss the significance of Redis 7. However, these advances illustrate how the world is racing toward real-time applications and highlight the technical underpinnings that make those advances possible. A better understanding of Redis can shed valuable light on the broader trends shaping tomorrow’s application environments.As noted, in-memory architecture favors real-time application performance because it keeps data in RAM, closer to the CPU. The architecture minimizes data travel time. Similarly, Redis’ Active-Active Geo-Distributed topology spans a global database across multiple node clusters. The system achieves inter-node consistency through a mesh of concurrent updating and data replication. Even if the majority of nodes in a database somehow fail, the remaining geo-replicated nodes will continue to provide uninterrupted service with full data integrity. This decentralized approach echoes the internet’s own architecture (designed for maximum network viability) while simultaneously allowing users to access data from the closest possible point and thus achieve near-local performance. Again, the idea is to minimize read/write data travel time without sacrificing consistency.Redis has been talking up its Active-Active Geo-Distributed topology since at least 2018, but each year brings iterative improvement and expansion. For example, Redis 7.0 updated the platform’s implementation not only to accept the data interchange format JSON as a supported data type but also support JSON with Active-Active Geo-Distribution. With this addition, Redis goes beyond its usual asynchronous replication. Now, Redis facilitates local operational autonomy by enabling read/write execution without waiting for commits from any single centralized master. Not waiting for permission means faster operational performance.The question remains:**Now that this active-active geo-distribution technology exists, how can it radically improve conventional application approaches? **Redis isn’t shy about touting how its ability to realize high throughput, sub-millisecond latency and global distribution is helping it oust the largest legacy database systems from their market entrenchment.As Redis explains, traditional application architecture entailed having a primary database (DB) to handle most requests unless the request was particularly time-critical. Then, the application would resort to a secondary DB (like Redis) for functions such as caching and queueing. With its latest enhancements, Redis essentially said, “Yeah, our database can also do…everything. And then do it in real-time.”The more Redis extends its database features and functionality, the more Redis will make its competition obsolete. Hence, Redis calls its approach “DBLess,” although the term is meant to go beyond databases in particular and instead convey the idea of a new, much more efficient technology than legacy approaches have produced.Redis believes that its real-time DBLess architecture offers a significant enough leap in solution value to disrupt major incumbents and potentially change markets.In parallel with this DBLess push, Redis has bolstered support for feature store functionality in RedisAI. In short, a feature is a potentially useful tidbit contained in one or more raw data points. As you might expect when dealing with terabyte- to petabyte-scale datasets, organizing features for machine learning (ML) training can be grueling and incredibly time-consuming. A feature store serves as both a repository and a system for the automated input and management of features. If that sounds like something that might suit Redis, well, of course it is. Blending the ML models of RedisAI with a lightning-fast, globally scalable platform like Redis 7.0 will give the world a new way to accelerate toward real-time AI systems based on live, geographically disparate conditions.Redis and AI also figured prominently in the company’s RedisConf 2021 Hackathon. Dr. Alexander Mikhalev was one of five Platinum Prize winners for his submission “The Pattern: Machine Learning Natural Language Processing meets VR/AR.” (Editor’s note: Check out The Pattern and other examples of what you can do with Redis on Redis Launchpad.) Mikhalev perceived a problem with the lack of proper review and vetting in modern medical sources, even referencing one interesting paper published in the American Journal of Biomedical Science & Research “claiming that eating a bat-like Pokémon sparked the spread of COVID-19.” (The purpose of the paper was to spotlight predatory scientific journals.) He used Redis AI and UX tools to turn documents into analyzed language that could then be searchable and visualized as three-dimensional graphs. The user then might explore those more intuitively in augmented or virtual realities.Another Platinum Prize winner, Dustin Wilson, used multiple Redis tools to create a live view of the Helsinki metro system. (Editor’s note: Check out this app and other examples of what you can do with Redis on Redis Launchpad.) Users could click on any transport to trace its position and delay statuses for the prior two hours. The live view, which showed transports moving in real time (similar to Uber), was accurate to within five seconds. A different view allowed users to click on any neighborhood to see the current average speed and delay time of transports in that area.The applicability of Redis to a dazzling spectrum of challenges was truly impressive. One Diamond Prize winner created an e-learning platform stocked with back-end metrics for educators. One Silver Prize winner created an RSS reader application, and another created a social network for movie buffs.And we would be remiss not to mention the Gold Prize-winning top-down shooter from Janis Vilks. Notable for being the world’s first active-active geo-distributed arcade shooter, the game’s graphics make Atari’s 1977 shooter, Combat, look like a Monet. That’s innovation for you, and this is only a proof of concept of pretty interesting things to come.While some of these applications appear simple, Redis continues to prove itself a capable platform for powering some of the world’s largest real-time applications. For instance, using a list data structure, Twitter uses Redis to store users’ 800 most recent tweets to help improve application responsiveness and scaling. Pinterest uses extensive Redis sharding to maximize resource utilization and break through caching bottlenecks found with prior systems when graphing data at scale. Malwarebytes uses Redis for security data ingestion, aggregation, and visualization, and U.K.-based energy company Utilitywise uses Redis to enhance performance and uptime for its IoT application. There are plenty of examples; the performance, scaling, and reach continue to push boundaries.Organizations have long had to make tough choices around whether to have databases for structured (SQL) or unstructured (NoSQL) data. Fortunately, Redis has been at the forefront of settling the SQL/NoSQL dilemma by delivering the best of both worlds: flexibility for today’s data processing as well as the high level of consistency needed to scale. This all-in-one suitability makes Redis a stellar fit for the diverse needs of modern AI and ML, which need to operate within the entire spectrum of modern applications, even among the industry’s most demanding, cutting-edge organizations.“The use of machine learning algorithms in simulations continues to grow to improve scientific research with efficiency and accuracy,” noted Benjamin Robbins, Director AI & Advanced Productivity, Hewlett Packard Enterprise. “By leveraging Redis and RedisAI in SmartSim, our new open-source AI framework which advances simulations that run on supercomputers, users can exchange data between existing simulations and an in-memory database while the simulation is running. The ease of data exchange helps unlock new machine learning opportunities, such as online inference, online learning, online analysis, reinforcement learning, computational steering, and interactive visualization that can further improve accuracy in simulations and accelerate scientific discovery.”This press release featured a Forrester Consulting study that found:40% of respondents believe their “current data architectures won’t meet their future model inferencing requirements.”38% of leaders are developing “roughly a third” of AI/ML models on a real-time basis.44% cite performance as a top challenge for getting models deployed.The links between architecture, performance, and achievement continue to challenge today’s business innovators. No matter how fast a cluster is or how next-generation the decentralized data framework, network latency remains an inhibiting factor when working across the public internet. Subspace created a high-performance network that operates in tandem with the public internet specifically to solve these performance bottlenecks.This type of low-level solutioneering to enable faster access to all kinds of data types appeals to both my technical and personal viewpoint. Technically, I like it when large obstacles like this start getting tackled, because it means that people are seeing the performance problems first hand and thinking about this new way the internet needs to operate.Now, quite a large group of DevOps and network engineers are picking up their tools and getting to work on solutions. What comes after these types of problems get solved is a natural adoption curve towards a new way of doing things; I think we’re in the middle of that curve right now.Personally, any organization who is working on enabling better access to this giant datastore we call the internet is doing good work, especially if they’re trying to utilize new architectures and technologies to do so. This is how innovation starts, and why it continues.The interesting part about Redis is they’re pushing the boundaries of what we can achieve in real-time computing, and enabling stable access at the same time. I’ll certainly be keeping my eye on them to see what innovations come next.See how Subspace can help you optimize and accelerate your real-time communication.————————————Ready to start your real-time AI journey? Try Redis Enterprise free."
592,https://redis.com/blog/synchronize-data-across-microservices/,How to Synchronize Data Across Microservices,"February 13, 2019",Madhukar Kumar,"The data landscape has grown increasingly demanding and crowded in the last few years, with many platforms competing to offer the best processing and storage options. Tech consumers expect companies to deliver high-speed data processing while simultaneously offering a variety of adaptable solutions that work with traditional applications but also built for modern architecture like microservices.Unlike a monolithic framework, a microservice structure enables companies to facilitate faster time-to-market for new applications or updates. With the lightweight design and agility of microservices, companies can allocate each individual service they offer to its own distinct database. But a monolithic framework does guarantee something that microservices might not: data consistency. When adopting a microservice architecture, it is important to consider how microservices share data between them, in order to prevent any particular service from creating a bottleneck and collapsing the whole system.This is where Redis Enterprise comes in. Redis Enterprise is built over open-source Redis, and is a CRDTs (conflict-free replicated data types) based, active-active database. Redis Enterprise offers companies two options for data synchronization: They can either share datasets across microservices, or transfer data between them.In the former case, a company can share datasets across microservices by relying on Redis Enterprise’s functionality as a conflict-free database. Redis CRDTs account for the possibility of multiple instances of a microservice by allowing each microservice to connect to the local instance of a distributed Redis Enterprise database. This CRDT technology ensures strong eventual consistency, which means that all data replicas will eventually achieve the same consistent state across all microservices.The other option companies have is to create an event based architecture for microservices. Redis Enterprise offers multiple ways to do this — Pub/Sub, Lists, Sorted Sets or Streams. The Pub/Sub implementation employs a simple and memory-efficient form of asynchronous communication, which requires subscribers to be active in order to receive data. The Lists data structure, meanwhile, supports a blocking call for an asynchronous data transfer, and is particularly efficient because it persists data even if a receiving microservice fails.The third technique, Sorted Sets, is ideal for transferring time-series data, but unlike Pub/Sub and Lists, it does not support asynchronous data transfers. Although Pub/Sub, Lists and Sorted Sets all offer viable ways to transfer data, Streams is effectively the culmination of the other three methods because it combines all their benefits while also supporting connectors for multiple clients and staying resilient to connection loss.Given all of these options for companies to ensure data consistency across microservices, if you were to ask me I would no doubt say that Redis Enterprise is the ideal database solution for any microservice architecture. Redis Enterprise is highly available and durable, offers multi-tenant and Kubernetes support, provides both cloud and on-premises options and — perhaps most importantly — is extremely easy to use. However, you don’t have to take my word for it. Sign up for a free Redis Enterprise database for free and try it out for yourself or read more in our documentation.If you still have any questions about how you can operationalize your microservice offerings with Redis Enterprise, or about how to achieve data consistency across your microservices, please don’t hesitate to contact us at product@redis.com."
593,https://redis.com/blog/containers-kubernetes-redis-enterprise-kubernetes-service-explained/,"Containers, Kubernetes and Redis Enterprise Kubernetes Service Explained","February 8, 2018",Vick Kelkar,"Containers are lightweight, stand-alone, portable, self contained software execution environments. Containers have their own CPU, memory, I/O and networking resources but they share the kernel of the host operating system. Containers are based on linux namespaces and cgroups. Namespaces (developed by IBM) create resource isolation for a single process while cgroups (developed by Google) manage resources for a group of processes. Containers have low startup overhead compared to that of a virtual machine running on a hypervisor. Containers are quickly becoming the basic unit of development and software packaging because they decouple applications from operating systems.Kubernetes is a popular open-source container orchestration engine used to deploy containerized applications. A Kubernetes cluster offers self-healing (restarts), scaling, scheduling and rolling updates of your containerized applications. These are some of the basic primitives that make up a Kubernetes cluster:Read more about the primitives mentioned above in the Kubernetes docs.Redis Enterprise Kubernetes ServiceRedis is an in-memory, schema-less database. It uses optimized data structures to deliver sophisticated functionality with great simplicity. Redis offers extensibility through Redis Modules and is optimized to deliver million of operations with sub-millisecond latency. Some of the Redis use cases include real-time analytics, high-speed data ingest, session storage, high-speed transactions and in-app social functionality. Redis Enterprise provides a high performance, low latency, in memory NoSQL solution to enterprises. It enhances Redis by offering data persistence, automatic failovers, backups and replication across datacenters. Redis Enterprise is offered as a fully managed cloud service or as downloadable software.At Redis, we are working on developing a Redis Enterprise Kubernetes service. The service will expose the Redis pods to the Kubernetes cluster using the LoadBalancer Kubernetes resource. The native headless Kubernetes service will take advantage of the PV, PVC and StatefulSets primitives to create a service with persistence on a Kubernetes cluster. The service will use the Kubernetes Secrets primitive to auto bootstrap Redis pods into a Redis Enterprise cluster.We added a headless Redis Enterprise service to ensure that we have an easy way to identify service pods. The advantage of a headless service is that you can reference a particular service pod in the service using the index of the service name. StatefulSets allow you to deploy the service in a consistent and reproducible way. With our Kubernetes release, we will enhance the Redis Enterprise cluster bootstrapping experience. The release will also publish our BDB endpoint in the Kubernetes service catalog on a periodic interval. In the next blog post we will spin up the newly developed Redis Enterprise Kubernetes services on a Kubernetes cluster and we will post results of a load test of the service running on a Kubernetes cluster."
594,https://redis.com/blog/building-feature-stores-with-redis-enterprise-on-google-cloud/,Building Feature Stores with Redis Enterprise on Google Cloud,"August 10, 2022",Gilbert Lau and Nava Levy,"Every day, more and more companies are building feature stores for machine learning (ML) with Redis and Redis Enterprise as the online feature store. In previous blog posts, we shared use cases and benchmarks illustrating how Redis Enterprise is the most performant and cost-effective online feature store for high throughput, low latency, or real-time use cases.We also shared tutorials on running Redis with the popular open source feature store Feast, locally, on Google Colab, and Azure—including with enterprise-grade Redis, thanks to the Enterprise Tiers of Azure Cache for Redis. In addition, we explained why Redis OSS or AWS Elasticache is often not enough and why companies that are outgrowing their OSS or ElastiCache implementations are upgrading to Redis Enterprise on AWS.In this blog post, we focus on why leading companies, such as Feast co-creator Gojek, are migrating to Redis Enterprise on Google Cloud. We also share a quickstart tutorial on how to run Redis Enterprise with Feast on Google Cloud Platform (GCP).If you are already familiar with GCP, then Redis Enterprise on Google Cloud may be your best option. It’s not only the most performant database for online feature stores on GCP (see, for example, benchmarks performed by Feast that compare Google Cloud Datastore to Redis), but it also provides you a fully managed option for your online feature store, allowing for simple management and scaling of Redis Cluster—a major benefit of Redis Enterprise over Redis OSS. In addition to the high performance and ease of management, Redis Enterprise on Google Cloud provides linear scalability and five-nines (99.999% SLA) availability, ensuring the online feature store is cost-effective at scale and experiences no downtime.For these reasons, leading companies such as Indonesian ride-hailing service Gojek have upgraded from Redis OSS to Redis Enterprise on Google Cloud. Since then, Gojek has expanded into numerous new countries, transforming into a “super app” that provides more than 20 services, including an e-wallet service, food delivery, courier service, and more. It has become one of the most successful and fastest-growing technology companies, valued at more than $10B, with its ML platform powering many of its use cases.Gojek is also the co-creator (together with Google Cloud) of open source feature store Feast, which it launched in January 2019 using Redis OSS for its online feature store.Feast is part of the Linux Foundation’s AI & Data Foundation. Feast can serve features from a low-latency online store or an offline store while also providing a central registry, storage, and serving. This allows ML engineers and data scientists to discover the relevant features for ML use cases and serve them in production.Today Feast has become the most popular open source feature store. It is deployed together with Redis as its online store in leading companies such as online mortgage company Better.com, American FinServ company Robinhood, Indian B2B wholesale retailer platform Udaan, digital consulting company Publicis Sapient, and many more—including, of course, Gojek itself. (For more details on Feast and its components, check out this Feast with Redis overview, as well as the Feast documentation on Feast.dev.)Since the launch of Feast, Gojek and its feature store have grown substantially in terms of scale and the number of use cases, to the point where the company has outgrown its Redis OSS implementation and moved to Redis Enterprise on Google Cloud instead. Gojek can now enjoy all the goodness of Redis OSS together with the benefits of Redis Enterprise: a fully managed cluster, five-nines availability, linear scalability, and other Redis Enterprise features such as Redis modules and enterprise-grade security.Now for a short overview of the quickstart tutorial on running Redis Enterprise on Google Cloud with open source Feast. Detailed explanations are available inside the tutorial itself on Google Colab.The tutorial provides a step-by-step guide that walks you through using Feast with Redis Enterprise as its online feature store for ML on GCP. It’s based on the Feast Quickstart tutorial, but instead of using the default online store, it uses Redis Enterprise as its online store to deliver real-time predictions at scale. If you’re unfamiliar with Feast or Redis Enterprise on Google Cloud, then the fastest way to get started is by taking this helpful tutorial.You can run the tutorial on Google Colab by following the steps described in the Colab notebook.To run this tutorial, you will need a Redis Enterprise database instance from Google Cloud Marketplace. If you do not have an existing Redis Enterprise subscription through Google Cloud Marketplace, you can claim your FREE Google Cloud Marketplace $400 credit.In this tutorial, we use feature stores to generate training data and power online model inference for a ride-sharing driver satisfaction prediction model. In the demo data scenario, we surveyed some drivers to determine how satisfied they are with their experience using a ride-sharing app. The goal is to generate predictions for driver satisfaction for the rest of the users so we can reach out to potentially dissatisfied users.Tutorial steps:In this blog post and accompanying Colab tutorial, we introduced you to Redis Enterprise as an online feature store on Google Cloud. We provided a quick introduction to the popular open source feature store Feast, as well as a step-by-step tutorial on how to set up Redis Enterprise on Google Cloud as the online store for Feast. To learn more about Redis Enterprise, check out the resource section of Redis.com for additional blog posts, ebooks, webinars, and more.Go experience the speed and scalability of Redis Enterprise as the online feature store for your Feast deployment!"
595,https://redis.com/blog/building-feature-stores-with-redis-introduction-to-feast-with-redis/,Building Feature Stores with Redis: Introduction to Feast with Redis,"November 9, 2021",Nava Levy and Guy Korland,"A feature store is a centralized place where data scientists from different teams across your organization can share features for machine learning. The feature store allows them to search, reuse, and serve features in production at scale. As MLOps matures, feature stores are becoming a cornerstone of machine learning platforms for a few reasons:When companies need to deliver real-time, ML-based applications to support high volumes of online traffic, Redis is most often selected as the foundation for the online feature store, thanks to its ability to deliver ultra-low latency with high throughput at scale. We’re seeing ML cloud platform providers offer feature stores using Redis, as in the recent examples of H2O AI Feature Store and Microsoft’s Azure Feature Store. And we’re also seeing a host of build-your-own implementations of feature stores using Redis at a variety of companies including Wix, Swiggy, Comcast, Zomato, AT&T, DoorDash, iFood, and Uber.These companies have huge datasets, with hundreds to thousands of features feeding ML systems of massive scale. Their Redis-backed feature stores support time-sensitive, mission-critical business applications, many for recommendation and fraud detection, using real-time data.Open source feature store implementations such as Feast are also choosing Redis. Redis was selected as Feast’s online store for real-time use cases at scale, first by the Indonesian ride-hailing company Gojek (who was the initial creator of Feast in collaboration with Google), and later by Feast’s early adopters, such as Udaan and Robinhood. In addition to that, Redis was selected by Microsoft for the online store of its new Azure Feature Store with Feast (stay tuned for more details on that soon!).To provide a better understanding of how feature stores work, and why Redis is such a key feature store component, we’re going to use the rest of this article to introduce Feast and show how you can use it to build your own feature store with Redis.Feast (Feature store) is an open source feature store that’s part of the Linux Foundation’s AI & Data Foundation. Feast can serve features from a low-latency online store or from an offline store, while also providing a central registry, storage, and serving. This allows ML engineers and data scientists to discover the relevant features for ML use cases and serve them in production.Feast is built in a modular way so that you can adopt all or some of its components. Because Feast is open source, you can deploy a Feast Feature Store and customize it for your own needs, without having to start building a feature store from scratch. Companies who choose  Feast with Redis for their feature store have significantly shortened development time and effort, as compared to building out their own feature store. In the next section, we’ll go over Feast’s key components.If you look at the Feast architecture diagram below, you’ll notice several key components:Feast registry: Feast registry is an object store-based registry, which is a central catalog of all of the feature definitions and their related metadata. This registry allows data scientists to search, discover, and collaborate on new features. The registry also allows for programmatic access through the Feast SDK.Feast Python SDK/CLI: The SDK is the primary user-facing tool for managing version-controlled feature definitions, materializing (load) feature values into the online store, building and retrieving training datasets from the offline store, and serving online features.Online Store: The online store is a database that stores only the latest feature values for each entity. The online store provides low-latency online feature value lookups. Feast allows users to load or materialize their feature data into an online store in order to serve the latest features to models for online prediction.Offline Store: Offline stores maintain a record of historic time-series feature values. The offline store persists batch data that has been ingested into Feast. This data is used for producing training datasets. Feast does not manage the offline store directly, but runs queries against it.The high-level architecture diagram above describes the following flow, as an example:You can find more details on Feast, including its concepts, architecture, and releases, at feast.dev. Next, let’s see how to quickly get started with building your own feature store using Feast with Redis.Choosing Redis as the online store for Feast (for Feast versions >= v0.11) takes just a couple of lines of configuration: Define the online_store in the Feast YAML configuration file, setting the type and connection_string values in feature_store.yaml as follows:By adding these two lines for online_store (type: redis, connection_string: localhost:6379) in the YAML configuration file, Feast will use Redis as its online store.We just showed how to connect a single Redis instance to Feast. If you’re using a Redis open source cluster with SSL enabled and password authentication, you’ll need to use a different connection_string value.See the Feast documentation for a full listing of the configuration options for Redis. You can also consult the Feast online store format to better understand the data model used to store feature values in Redis.At Redis, we’re committed to making Feast faster and more reliable for delivering real-time ML use cases at scale. For the recent Feast v0.14 release, we were thrilled to help the core online serving path become 30% faster!For next steps, we recommend learning about how and for which uses cases companies are using features stores with Redis for the online store (Wix, Swiggy, Comcast, Zomato, AT&T, DoorDash, iFood, Uber), and specifically how they’re using Feast with Redis (Gojek, Udaan, Robinhood). We also recommend reading about the new Azure Feature Store for Feast and checking out the quick-start tutorials on Azure GitHub repo.We hope you’ve enjoyed this introduction to using Feast with Redis. We’ll be posting more resources soon, but if you have any feedback or questions, please don’t hesitate to contact us."
596,https://redis.com/blog/feature-store-summit/,Key Takeaways from the First Feature Store Summit,"October 27, 2021",Taimur Rashid and Nava Levy,"Two weeks ago was the first Feature Store Summit event, filled with talks, panels, and a lively Slack channel with more than 800 active members. The summit brought together industry thought leaders and practitioners from over 25 organizations—all focused on feature stores! The size of the summit and enthusiasm of the attendees shows the growing importance of feature stores and the key role they play in machine learning operations.The event was held virtually and was hosted by featurestore.org both on the conference side (thanks to Jim Dowling) and on the Slack channel (thanks to Helena Paic), including a Slack Q&A after each talk. Over two days, the 21 talks, 4 panels, and multiple polls covered key themes, including: 1) solving the hardest problems at scale, 2) who benefits from feature stores, 3) the future of feature stores, and 4) whether it’s better to build or buy.In addition, Taimur Rashid, our Chief Business Development Officer, delivered a presentation with Davor Bonaci and Dr. Charna Parkey from Kaskada called Creating and Operating ML Models from Event-based Data Using Feature Stores and Feature Engines. In it, they also covered the importance of having a scalable, reliable, low-latency online store for serving features in production. More details on their presentation are provided under point three below.Here are our four key takeaways from the event.A large and growing portion of Machine Learning (ML) use cases rely on real-time data. And a recurring theme at the Feature Store Summit was delivering ML use cases using real-time data with low latency—and the importance of the online store in delivering it. Consistently, reliably, and quickly serving fresh data for ML models for online predictions is hard and complex, especially for real-time applications that require very low latency.For example, 100ms end-to-end latency is required for most real-time applications, with some use cases, like AT&T fraud detection, requiring less than 50ms latency. This is especially complicated at the scale companies like AT&T, Twitter, Zomato, Spotify, and DoorDash operate at, with thousands of features, multi-terabyte-sized and sometimes petabyte-sized datasets, and 100s thousands of predictions per second. If we look at the scale of DoorDash as an example, then the total number of feature-value pairs exceeds billions. The throughput required for high volume use cases at DoorDash (like store ranking) reaches tens of millions of reads per second!We found the slide above from Hopswork’s presentation to be very perceptive and useful in explaining why companies are working so hard to provide real-time AI at scale to solve that hardest problem. As illustrated by the utility function in the slide, the lower the latency, the greater the business value for the company and its users—and this correlation is exponential! Note the difference between real-time ML and operational ML: While predictions are done online for both (e.g. while the user is interacting with the website), operational ML (and all the other cases below it) uses data from batch sources. Only real-time ML delivers online predictions while at the same time consuming fresh data, i.e. real-time features from streaming sources.During the summit, a poll was conducted on the best strategy for feature stores. Should you build a feature store, or buy one? Here are the results:Despite the growing maturity of feature stores as a category, 50% of the surveyed companies prefer to build their own feature store rather than buy a commercial off the shelf solution, while only 35% prefer to buy, and 15% are still deciding. Companies who adopted a build strategy include Spotify, Wix, Uber, and DoorDash.There’s also a hybrid approach using open source solutions like Feast or Hopsworks. This way, the company doesn’t need to reinvent the wheel, but can leverage all the work that was put into the open source project. At the same time, the company can still customize the open source solution, giving it the freedom and flexibility to adapt the feature store to their own unique needs. This can save a lot of time and development efforts compared to starting from scratch. Companies who choose this hybrid approach include Salesforce, Robinhood, Wildlife Studios, Twitter, and Udaan.Build vs buy decisions depend on a number of internal factors, including where you are in the ML journey, the maturity of your ML platform and your team’s skills, the size of your scale, your unique needs, etc. However, as the category evolves and matures for both open source and commercial off-the-shelf solutions over the next several years, we expect more and more companies will opt to buy a feature store off-the-shelf or use a managed feature store in the cloud rather than building one on their own.While a lot of emphasis is placed on the centralized nature of managing and storing features, modern feature platforms have additional capabilities including standardized communication and governance layers between teams across data engineering, data science, and ML engineering. This was evident through many stories shared at the summit. For example, Salesforce discussed how their company leverages the feature store to build a collaborative environment across teams, while Twitter described how they solve the challenges of collaboration and feature shareability across teams by centralizing the feature store.In addition to collaboration, robust feature engines that ensure proper data preparation and minimize (or ideally prevent) challenges like data leakage are also hallmark capabilities required of feature platforms. Vaso Bank shared how their centralized feature store allows them to build a fraud detection system that avoids training and inference skew. This helps them maximize reusability, discoverability, and ensure consistency. Kaskada and Redis shared how they combine an extensible feature engine with time travel and feature storage with low-latency in-memory Redis to operationalize ML models from event-based data (see the slide below).Modern feature platforms provide a solution that closes the entire loop. First, you have an online feature store for low-latency serving, then an offline feature store for training and batch inference. Model binaries get stored in a model store, and as the model is served for predictions, model monitoring is used to determine model effectiveness. This is captured into an evaluation store, which can then be fed back to the feature engine so that models can be updated when the features are no longer predictive (as referenced above). This architecture goes well beyond storage, and ensures ML features are live, fresh, and fast, enabling them to support low-latency serving, augmented vector predictions, and continuous re-training at low latency.Another recurring theme throughout the summit was how to drive user adoption. Once you buy or build a feature store for your ML operations, how do you convince the data scientists and ML engineers to change their behavior and start using the feature store? To invest time in registering each feature? To share features that they would later need to maintain and support?Introducing “killer features” for fast iterations like point-in-time joins and automated backfills is key, as explained in Spotify’s presentation, as well as making sure the feature store can address a wide range of ML use cases from batch scoring to near-real-time to real-time. But even more than that is instilling trust in the feature store. This trust is instilled, first and foremost, through feature store consistency. Despite the dynamic nature of the feature store environment (including the data, the features, and the models), it is critical that the feature store ensures consistency.David Liu of Twitter talked about four levels of consistency, and that at the foundation level, the most important level (and the most talked about) is offline to online consistency, as depicted in the slide below by Twitter. The offline/online consistency is ensured at Twitter via a standardized schema and structure that is very rigid in the offline and online stores, so a reliable transfer of feature data is possible.Other important aspects of feature stores that instill trust are:Feature store as a category didn’t even exist just a few years ago, but it’s now a critical piece of data architecture that links the past with the present by combining data science models and real-world data. When you consider the incredible traction and advances it has made so far, it’s quite clear that the emerging concept of feature stores is here to stay.Feature stores solve some of the hardest problems for machine learning operations, and are often mentioned as the cornerstone of MLOps. Looking into the future through Feature Store Summit, we saw interesting directions for features stores, and we look forward to seeing how these are going to materialize in the near future.This wraps up our key takeaways from the first Feature Store Summit. We learned a lot from it and enjoyed co-presenting with Kaskada. We look forward to the next Feature Store Summit! Huge thanks for featurestore.org for organizing the event and see you next year!—Ready to test out Redis as a data store for your feature stores? Try Redis Enterprise Cloud for free!"
597,https://redis.com/blog/top-9-takeaways-from-redisconf-2021/,Top 9 Takeaways from RedisConf 2021,"April 30, 2021",Mike Anand,"Every year Redis enthusiasts from around the world gather at RedisConf 2021, our annual real-time data conference. Last year was our first virtual RedisConf, and this year was bigger and better—more than 12,000 developers, architects, and business and technology leaders from 122 countries registered for keynotes, fireside chats, 60+ breakout sessions, and training from April 20–21.RedisConf 2021 was all about rediscovering the power of real-time data. We brought 10 keynote speakers to the stage to discuss how Redis has grown over the past year, what’s in store for the future, and how their businesses leverage real-time data to meet customer demands. You can get the full experience by watching the keynotes (and all breakout sessions!) on demand for the next month, but here’s a quick digest on the key takeaways from RedisConf 2021:Across our keynotes, we consistently heard the same point: Real-time experiences are no longer nice-to-have, they’re an expectation in every digital business, with customer-facing front end apps. Redis as a real-time data platform offers more capabilities and choices for your needs—along with the high performance it’s known for. From strong consistency to integrated data models to artificial intelligence (more info on each below!), Redis has what you need to deliver the real-time, digital experiences that today’s customers expect.JSON is the lingua franca of the internet. With billions of JSON documents part of each and every digital service, being able to retrieve information from JSON at the speed of Redis and help deliver real-time experiences has been a project we’ve been working on for the past year—and the combination of RedisJSON with RediSearch is going to be in preview next month! This new capability allows indexing, querying, and search in JSON documents stored in Redis at a speed 3 to 37 times faster than other key document databases.Plus, later this year you will be able to deploy and run a fully managed RedisJSON+RediSearch database across multiple regions/clouds or hybrid deployment—closer to where your users are—using Active-Active Redis Enterprise. Check out Yiftach Shoolman’s day one keynote to see our brand new retail demo application, RedisMart, with all those new capabilities in action!Redis has established its reputation as one of the most developer friendly databases—with the set of supported data structures packaged with Redis, developers can spend less time on data manipulations and more time on app logic. With the set of Redis modules—RedisGraph, RedisBloom and RedisTimeSeries—developers can add more advanced use-cases while using the same set of familiar Redis interfaces, and get more code developed faster! We’ve seen strong adoption trends for its Redis modules with a YoY increase of 60%—and for the first part of this year, the increase has been 120% compared to the same time last year.Be sure to check out the developer (Build with Redis) track sessions and the RedisMart demo retail app we introduced in Yiftach Shoolman’s day one keynote, all available on demand for the next month!Last year we introduced our new community governance model—made of a core team from Redis, AWS, and Alibaba—taking his place. The team announced the release of Redis 6.2 earlier this year, and they plan to increase the cadence of major Redis releases to two per year, with Redis 7.0 planned for Q3 of this year. Learn more about these releases in Yiftach Shoolman’s day one keynote and the breakout sessions on Redis 6.2 and 7.0.Redis adoption as a primary database is on the rise with more than 66% of Redis customers relying on Redis as their main database, and we want to extend the functionality and make Redis suitable for use cases where strong consistency is a requirement.RedisRaft—a new strong-consistency deployment option first introduced at RedisConf 2020—will be generally available with Redis 7.0.There’s no question that real-time capabilities are becoming increasingly important for supporting AI use cases in application stacks. At the heart of AI reference architectures is the feature store, the bridge between your data and machine learning models. In the past year companies such as Netflix, Uber, Doordash, Airbnb, and Spotify have published their AI stack architecture, where Redis consistently serves as the online feature store, and combined with a model store and RedisAI inference engine, addresses the need to deliver real-time AI services.We’ll be gradually releasing RedisAI online feature store specific capabilities on Redis Enterprise Cloud, our fully managed cloud service, in the second half of 2021.In addition to the existing Google Cloud offering of Redis Enterprise as a fully managed Database-as-a-Service (DBaaS), we’re now adding a self-managed option of Redis Enterprise with Anthos on GKE. Anthos extends the use of GKE to other platforms outside the Google Cloud and allows standardized and rapid deployment on other clouds and on premises. To learn more about this newly announced support, tune in to Jeff Reed’s day two keynote or visit our Google Cloud page.Last September Redis was named an Advanced Technology Partner of AWS. Now there’s even greater support for Redis customers on AWS, with the availability of all Redis Enterprise Cloud in AWS Marketplace. AWS customers will be able to find Redis Enterprise Cloud on AWS’ public listing, use AWS credits towards Redis Enterprise on top of AWS, and benefit from simplified and consolidated billing. Check out our AWS page to learn more about this announcement.Real-time experiences are now a part of every digital business. This year we hosted three key Redis customers on the main stage and asked them to share how they make real-time services a reality in their respective businesses. Mike Lee, Head of Enterprise Payment Architecture at Capital One, took the audience through the transformation taking place in the payments space with fintech disruptors, crypto currencies, and new regulations and how he built an architecture to deal with today’s and tomorrow’s real-time needs. You can tune into Mike’s segment on our day one keynote. Dave Anderson from Genesys, a contact center solutions provider, gave perspective on the criticality of real-time experience in the voice application in the call center, and Alex Curtin from Unity—one of the largest technology providers for the gaming industry—shared his story on accelerating gaming developers onboarding with Redis Enterprise on GKE. Tune into Dave’s and Alex’s segments on our day two keynote.Get the full story on these takeaways by checking out the entire keynotes, available to view on demand for the next month on the RedisConf platform. If you’d like share your feedback on the event or the keynotes, don’t hesitate to reach out to us on Twitter with the hashtag #RedisConf2021."
598,https://redis.com/blog/21-redis-experts-to-follow-on-twitter/,20 Redis Experts To Follow on Twitter,"September 14, 2022",Redis,"Whether you’re brand new to Redis or a seasoned user, there’s always more to learn. To help you get started, we compiled this list of our favorite Redis experts to follow.Whether you’re brand new to Redis or you are a seasoned user, there’s always more to learn about the open source, in-memory data store used by millions of developers. We publish tons of informative blog posts, white papers, and other resources to help you develop your Redis knowledge. However, sometimes the best way to learn is directly from individuals: the geeks, nerds, developers, boffins, and Redis mega-fans who live and breathe technology, with specific attention to Redis tools and innovation.The best way to learn from and interact with these folks is through social media. To help you get started, we compiled this list of Redis experts to follow on Twitter, listed in alphabetical order. Get ready to become a full-fledged Redis geek yourself with a daily stream of important Redis news and updates, developer tips, technical thought leadership, and community conversation.Bobby Calderwood is the founder of Evident Systems and creator of the event modeling platform oNote. He’s an expert on microservices architectures and a former distinguished engineer at Capital One. Bobby contributed Redis Streams support to Carmine, the Redis client and message queue for Clojure. You can watch his RedisConf talk, Using Redis Streams to Build Event-Driven Microservices and User Interface in Clojure, on YouTube.Follow Bobby on Twitter @bobbycalderwood.Nick Craver is a principal software engineer at Microsoft working on Azure and was formerly the architecture lead for Stack Exchange/Stack Overflow. He describes his work this way: “I design and build very fast things in hopes of making life easier for millions of developers.” A strong advocate of open source, Nick maintains several OS projects, including StackExhange.Redis.Follow Nick on Twitter @Nick_Craver.Yossi GottliebYossi Gottlieb is an experienced technology leader who today serves as chief architect at Redis as well as co-lead of the Redis open source project. He has worked as a software developer and cybersecurity researcher, and is the former CTO of a system integration firm. Yossi contributes to the Redis blog from time to time, such as a Q&A about Redis’ architecture principles.Follow Yossi on Twitter @yossigottlieb.Marc Gravell is a Microsoft developer and a Stack Overflow alumnus. He’s also an author of StackExchange.Redis, a C# client for Redis, and he blogs regularly at blog.marcgravell.com. Marc is a frequent commentator on the latest things happening in the developer world.Follow Marc on Twitter @marcgravell.Rui Gu is a long-time Redis advocate and creator of Reddison, the open-source Redis Java client with features of an in-memory data grid. Learn more about the development of Reddison from this interview with Rui: Building an Open Source Enterprise Redis Client.Follow Rui on Twitter @redisson_ruigu.As Redis’ first technology evangelist, Itamar Haber knows Redis inside and out. As he says, “I live and breathe Redis.” He regularly speaks at various tech conferences and contributes to the Redis blog on topics such as Redis Namespace and Other Keys to Developing with Redis. Keep up with Itamar online for technology commentary, insights, tutorials, and exciting product updates.Follow Itamar on Twitter @itamarhaber.A self-professed “Redis geek,” veteran software developer Carlos Justiniano is acting CTO at Pro-Tect-Me and former senior VP of technology at F45 Training. He wrote the Hydra framework, a Node.js package that leverages Redis to facilitate building distributed applications, including microservices architectures. A three-time RedisConf speaker, Carlos frequently writes on Medium.Follow Carlos on Twitter @cjus.Guy Korland is the CTO of incubations at Redis. He has many years of experience in software development. Guy is the former CTO of SelfPoint (where he built a high-end ecommerce SaaS solution) as well as the founding CTO of Shopetti, an online shopping application. Read Guy’s posts on the Redis blog.Follow Guy on Twitter @g_korland.Danni Moiseyev is a principal software engineer at Redis, where he leads the development of a time-series Redis module and also works on Redis Enterprise. You can watch a recent RedisConf presentation Danni gave on RedisTimeSeries.Follow Danni on Twitter @dannydevmo.Savannah Norem is a developer advocate at Redis. She’s written about probabilistic data structures on the Redis blog and live codes with Python and Redis every Tuesday, which you can watch on YouTube. Savannah tweets regularly on all things technology (as well as cats and crafting for good measure).Follow Savannah on Twitter @sav_norem.Madelyn Olson is a senior software development Engineer at AWS and a Redis core member, serving as a maintainer of the Redis open source project. As veteran tech columnist Matt Asay writes, “If you’ve used Redis, you’re a direct beneficiary of the ‘small fixes all over the place’ Olson contributes to make Redis great.” She recently spoke at RedisConf 2021 about building for high availability and performance with Redis Cluster and AWS. She may not tweet often, but Madelyn is one of the most knowledgeable people about Redis outside of Redis.Follow Madelyn on Twitter @reconditerose.Mark Paluch is the Spring Data engineer at Pivotal and project lead of the Lettuce Redis driver. A self-professed “software craftsman,” he has extensive experience in software engineering. Follow Mark online for developer news and commentary, and check out his blog for his more in-depth thoughts.Follow Mark on Twitter @mp911de.Dmitry Polyakovsky is a principal software engineer at Oracle Cloud. He has spoken at various Redis events, such as this RedisConf 2020 presentation on using Redis with Python to analyze Covid-19 data, and writes frequently about Redis on his blog.Follow Dmitry on Twitter @dmitrypol.Simon Prickett has been coding since the 1980s and today serves as the principal developer advocate at Redis, where he leads the team that develops instructional materials for Redis University. He blogs regularly on his own website simonprickett.dev, contributes to the Redis blog, and has created many entertaining technical video tutorials.Follow Simon on Twitter @simon_prickett.A self-professed “geek, graybeard, and gamemaster,” Guy Royse is a developer advocate at Redis with more than 25 years of developer experience. He speaks regularly at tech conferences and blogs at his own website guyroyse.com as well as the Redis blog. You can watch his entertaining dev tutorials on YouTube and follow him online to read his thoughts on everything from coding to Dungeons & Dragons to the search for Bigfoot.Follow Guy on Twitter @guyroyse.Brian Sam-Bodden is also a developer advocate at Redis who strives to “push the envelope of technology and fill the world with better, more passionate programmers.” He is a frequent speaker at user groups and conferences and is the author of two books on Java. You can watch Brian’s dev tutorials on YouTube and follow his work on GitHub.Follow Brian on Twitter @bsbodden.You can’t talk about Redis without mentioning its creator, Salvatore Sanfilippo, who developed the first version of Redis back in 2009 in an attempt to improve the scalability of his startup. After overseeing the open-source Redis project for 11 years, Salvatore stepped down in 2020, but he continues to serve as a member of the Redis technical advisory board. Salvatore frequently blogs about his latest projects, including his new science fiction novel, at antirez.com.Follow Salavatore on Twitter @antirez.An experienced technologist who has been coding since he was 14 years old, Yiftach Shoolman is co-founder and CTO of Redis, which he started as Redis Labs back in 2011 with CEO Ofer Bengal, eventually teaming up with Salvatore Sanfilippo, original founder of the Redis open-source project. Yiftach speaks frequently at Redis events, such as this keynote presentation he gave at RedisConf21 about building, deploying, and running real-time applications with Redis.Follow Yiftach on Twitter @yftachsh.From Hangzhou, China, Zhao Zhao is a senior engineer at Alibaba Cloud as well as a member of the Redis core team. He focuses on improving Redis performance and data replication consistency. While Twitter is not Zhao’s main platform, you can follow his work on GitHub.Follow Zhao on Twitter @soloestoy6.We hope this list of Redis experts to follow on Twitter helps turn your daily feed into a steady stream of Redis-themed knowledge, insight, and inspiration. For more resources, be sure to regularly check out the Redis blog, visit the Redis Developer Hub, or take a free online course at Redis University.Oh, and don’t forget to follow our own Twitter account, @Redisinc!"
599,https://redis.com/blog/how-to-reduce-latency-and-minimize-outages/,How to Reduce Latency and Minimize Outages,"February 9, 2022",André Srinivasan and Robert Belson,"It’s not just latency that’s the problem when we discuss the maximum delay between the time a client issues a request and the time the response is received. How do you know the latency, or the slowness, isn’t actually an outage? Or that it doesn’t result in user abandonment, another form of “outage”?I’ll use the examples of real-time inventory and fraud detection to illustrate how you can reduce latency and, by extension, eliminate outages.First, think about real-time inventory specifically in the context of the pandemic. We evolved our shopping habits during COVID-19; when we had to go into the store, we thought long and hard about it, and we made sure that what we needed was actually in stock. Our plan was to go to one store, get in, get our stuff, and get out.But if the store’s inventory system or the public view of it is slow, I may falsely assume what I’d like is available and simply go to the store. If what I’d like isn’t there, I’m going to leave; after all, I was there for a specific reason. I may also stop by customer service to complain not just about the slow system but also the lack of inventory. The result is lost revenue as well as lost time complaining. In this case, latency in the inventory update is effectively an outage for that system.Now let’s move on to fraud detection. In a nutshell, fraud systems operate on relatively static data and were developed before there was a need to incorporate real-time data as part of the risk calculation. If we think about a digital identity or user profile, this idea of verifying the customer’s identity can be thought of as a mix of static data (e.g., mailing address) and dynamic data (e.g., recent purchases). Most likely, that static information isn’t so secret due to data breaches. Bad actors know this and can develop strategies to defeat common fraud detection strategies.The fraud may eventually be detected but only after the transaction is already completed. In effect, that latency, that inability to incorporate real-time data, means the fraud detection system was unavailable. It suffered an outage.At this point, it would be perfectly relevant for you to say, “Wait a minute. We keep modernizing our architectures (as Figure 1 below indicates). How can this still be an issue?”I would suggest that, as we’ve evolved architectures, we’ve solved for availability and delivering on an SLA of five nines. We started out with relational databases, then broke up the data into multiple relational databases for performance, and then introduced data-type-specific stores to further improve performance.But multiple data stores also introduce multiple copies of data and create consistency challenges. We were able to address this with event-driven architectures using a message bus such as Kafka. We, therefore, accomplished our five-nines of availability, and we’re mitigating the challenge of data consistency across all the systems at the cost of complexity. But we haven’t addressed latency overall. Meanwhile, consumers are more frequently using mobile devices to better inform themselves, all of which points to a need for speed more than ever before.If we turn our focus in this event-driven microservices architecture to the data store, I would suggest there are many tools to solve the latency problem. As a Solutions Architect at Redis, I can speak to the tools I use that operate at sub-millisecond latency in this type of architecture. Redis Enterprise is an in-memory database platform that maintains the high performance of Open Source Redis and adds enterprise-grade capabilities for companies running their business in the cloud, on-prem, and with hybrid models. This blog post provides further details about how Redis achieves real-time performance with linear scaling to hundreds of millions of operations per second.For instance, we can reduce the overall data store complexity if we take advantage of Redis Enterprise in our event-driven microservices architecture (see Figure 2).Now as a highly performant system, we address our need for real-time data by incorporating dynamic information into the risk calculation without compromising the need to complete this step inline with the transaction. Similarly, we can return to the is-what-I-need-at-the-store problem with real-time inventory, and, assuming there is only one store or everything is centralized, we can be confident that latency will not create false results.Which takes us to the next level: What happens when there is more than one store and there is no centralization but instead multiple inventory systems? If we do nothing, we have a new level of consistency challenge. Fortunately, we can address this with Active-Active Geo-Duplication and create master data across the entire set of systems. Active-Active, implemented as a conflict-free replicated database, is illustrated in Figure 3. It refers to using at least two data centers that each can service an application at any time and deliver application accessibility even if parts of the network or servers fail unexpectedly.  See Active-Active Geo-Distribution (CRDTs-Based) for a deeper dive.Unfortunately, if my applications are not co-located with the data, I’m still facing an average of 100ms of latency between where the data is needed and where the data lives (see Figure 4 below); latency can still be the source of an outage.We can solve this challenge by bringing the data closer to where it is being used. If the public cloud is too far away at 100ms, then I leverage network edges like Verizon 5G Edge and AWS Wavelength Zones and reduce the logical distance between the application and the data to 50ms (see Figure 5 below).Our distributed system is now modernized with an event-based microservices architecture where we have reduced the data store complexity, created data consistency with Active-Active Geo-Duplication, and have reduced latency to meet real-time requirements. Furthermore, I can bring that architecture closer to where it’s being consumed and reduce the impact of latency as a source of an outage. I can leverage network edges with Active-Active Geo-Duplication so that I am now consistent in ensuring everywhere the data lives is close to where it is being used.Circling back to where we started with real-time inventory and fraud detection, this translates into happy customers who have confidence the items they want to buy will be in the store when they visit. It means businesses keep their customer retention costs down with reduced support interactions, and financial institutions mitigate losses from fraud, as well as manage their costs from real-time transaction analysis. Latency—as well as the risk or perception or reality of an outage—is minimized.Want to try this out? This 5G Edge Tutorial is a good starting place. Interested in learning about more modern approaches to reducing latency in complex, distributed systems? Read our ebook linked below."
600,https://redis.com/blog/5-industry-use-cases-for-redis-developers/,Redis Use Case Examples for Developers,"July 12, 2022",Ajeet Raina,"Redis has a great reputation – but where’s it used? Developers rely on Redis Enterprise for critical use cases across several industries. Learn several scenarios where Redis has made a difference in application development for gaming, retail, IoT networking, and travel.How are developers using Redis for their database needs? We began as Redis open source, but in the subsequent 13 years, the must-haves for any growing digital business have shifted, and now what’s required is more of everything. That means more availability, more persistence, and never any room for lags in performance, so add backup-enabled and instant failover to the mix of required features.Painless business scalability is an objective goal for any programming team. Building applications with an in-memory Redis cache and Redis database cuts through complexity and latency since Redis Enterprise provides dual support in a single system.Everything comes down to speed. It means faster application interactions (such as quick data retrieval and a balanced load of backend services with caching) and software that scales on user demand (build low latency microservice architectures with Redis’ multi-model database).Let’s explore some popular Redis uses and customer real-world Redis performance examples.Fraud costs money, and the cost is only going up. Opportunists follow the growth, leading them to the digital space, with retail, gaming, and finance among the verticals hit hardest by fraudsters. “Every $1 of fraud now costs U.S. retail and e-commerce merchants $3.75,” a LexisNexis report asserts; that’s up 19.8% since 2019)BioCatch is an Israeli digital-identity company that uses groundbreaking biometrics tracking to stay ahead of fraudsters. As the company’s business rapidly grew to 70 million users, 40,000 operations per second, and 5 billion transactions per month, the BioCatch team needed a way to deal with significant database scaling issues.This isn’t a unique challenge. Online transactions soared as a result of COVID-19. According to Morgan Stanley’s global e-commerce growth forecast 2022 report, the market is estimated to soar from $3.3 trillion today to $5.4 trillion by 2026. With that growth comes cybersecurity dangers: digital identity threats, cybercrime, and customer fraud. Phishing and counterfeit pages increased by 53% in 2021, reports Bolster.AI.Your data layer needs lightning-fast speed to build finely-tuned fraud detection algorithms that respond in under 40 milliseconds before anything can negatively influence the customer experience.Data breaches have become their own epidemic. IBM reports that 83% of organizations have had multiple data breaches. The average cost of each instance is approximately $4.3 million. The United States earned the top spot for the 12th year in a row for countries with the highest average total cost of a data breach.The increased complexity, volume, and sophistication of threats require more advanced fraud detection methods to keep up with malicious actors and build more substantial fortifications against them. Because traditional data platforms often struggle to keep up with modern online transactions’ speed, scale, and complexity, it is difficult to detect and stop fraud in real-time.In need of blazing performance, high availability, and seamless scalability from its data layer, BioCatch turned to Redis Enterprise to decouple the compute from the data. After initially considering Redis Enterprise as a cache, the team soon realized that Redis would also make a great system configuration NoSQL database.BioCatch uses Redis features, and various data structures to create a single source-of-truth database that serves mission-critical information across the entire organization. BioCatch captures behavioral, meta, and API data during active user sessions. It also creates user behavior profile subsets and predefined fraudulent-behavior profiles.With 3 petabytes of data, 300 million keys, and 40 databases running on Microsoft Azure, BioCatch relies on Redis Enterprise to serve data for all its microservices. Since operating with our enterprise-grade Redis cache, BioCatch has had zero downtime and no operational hassles, giving its team breathing room to focus on the strategic projects that serve its core mission.Learn more about real-time fraud detection and exabyte analytics in our Data Economy podcast vlog.In 2021, the global gaming market topped $198.40 billion and is expected to reach a value of $339.95 billion by 2027, according to Mordor Intelligence. This massive estimated growth is due in large part to mobile gaming.Successful mobile games require a great user experience, which can post significant infrastructure challenges, especially for real-time multiplayer games. Users must be able to launch the game, connect to a server, and collaborate with other players; any lag or hiccup can ruin the experience. The gaming experience also includes transactions in real-time, sometimes with real money involved. For the customer, the expectation is an immediate transaction, with personal payment details cached at the ready.Developers rely on Redis’ low latency to deliver high performance and virtually unlimited scale critical in gaming situations where large volumes of data arrive at high speed. Take fantasy sports, for instance, which is estimated to become a $48 billion market by 2028. American football is the most popular fantasy sport in the United States, with 35 million players. But that pales in comparison to India’s fantasy cricket leagues, which stand at around 100 million players, according to a study by the Federation of Indian Fantasy Sports (FIFS).In India, teams release their game rosters before a match, and online players only have 10-15 minutes to update their respective fantasy teams. This is a massive amount of data to intake, and it should not impact the customer’s experience, especially when time is of the essence.For game developers, serving game elements such as graphics, pictures, thumbnails, and music requires a robust caching solution that can reduce the load on datastores operating on a relational database such as MySQL while ensuring blazing fast response times.Caching helps provide a responsive user experience with minimal overhead. Case in point:  Scopely.Scopely, which makes mobile games like The Walking Dead: Road to Survival, relies on Redis Enterprise for various needs, including leaderboards, API management, and queue workload management.Scopely needed to support a variety of data structures and features, such as customizable expiration, eviction, intelligent caching, request pipelining, data persistence, and high availability. These needs can’t be fulfilled with a SQL database, not without the need for a complex load balancing cluster.Discover more detailed information in our e-book, Why Your Game Needs a Real-Time Leaderboard.Creating a comprehensive digital presence as an online business is a massive endeavor with a steep learning curve. An excellent digital business needs a backend that ensures store pages are available, an inventory management system, a rapid-speed cache for autocomplete functions to the site, a search engine for products, and machine learning technology for creating personalized customer experiences in real-time. And it all must be performant; according to a 2020 YOTTA study, 90% of shoppers say they will abandon a site if it is too slow.However, modern multi-channel retailers are turning to real-time inventory systems to optimize their inventory, yield, and supply-chain logistics, with the aim of improving the customer experience and supply chain. Building and maintaining these complex systems is daunting for application developers.Here too, performance is critical. Delayed or inaccurate inventory information can frustrate customers, leading to shopping cart abandonment (an $18 billion problem of its own) and order cancellations, lost revenues, higher costs, and brand damage.Apparel retailer Gap Inc. wanted to give its e-commerce customers real-time shipping information for each item shoppers added to their carts. The company faced issues with delays and inaccurate inventory information.This issue created a poor customer experience that inflated costs and eroded brand loyalty.Application developers at Gap Inc. found Redis Enterprise’s linear scalability and sub-millisecond performance at a massive scale to be a huge assist, particularly for seasonal Black Friday peaks. In microservice environments, fast and flexible data models protect from overprovisioning parts of the infrastructure that are not used during slower periods.Availability, speed, performance, and experiences: a true 360° omni-channel journey keeps these balls in the air and never lets them drop.In the era of big data, businesses require software that instantly collects, stores, and processes large volumes of data. Yet many of today’s solutions supporting fast data ingest are complex and over-engineered for simple requirements such as streaming real-time data from the internet of things (IoT) and event-driven applications.In these applications, data must be analyzed quickly to make rapid business decisions. D ata loss is typically not permissible for these use cases.However, data loss does occur, predominantly when working with a relational database. A SQL database is usually created around a known use case at the onset. Introducing another data structure or data model into a SQL stack can bog down the system with slower speed, slower ingestion, and lost data, as the data has to be altered to fit the database’s chosen model.Lost data equals lost opportunities. Any lost data could be the gateway to an entirely new revenue stream.A notable Redis usage example of fast data ingestion is Inovonics, which provides high-performance wireless sensor networks with more than 10 million devices deployed worldwide. For most of its 30-year history, Inovonics considered itself primarily a wireless technology provider. But the rise of big data helped the company realize that the unique data sets collected by its wireless devices and sensors could also have tremendous value.Inovonics’ edge platform required robust data platform capabilities for resilience and performance while minimizing the operational footprint and operating costs. With the application of Redis Enterprise Cloud, a fully automated Database-as-a-Service (DBaaS), Inovonics centralized all its data on Google Cloud, opening up new product offerings in the form of insightful, easy-to-access data analytics.Inovonics uses Redis Enterprise on its IoT edge devices to push data to its gateways and to the company’s virtual private Google Cloud from those gateways.On Google Cloud, the application of Redis Enterprise is used for data ingestion to store the millions of daily messages coming from Inovonics’ sensor networks and to provide a central, aggregated view from which to analyze the data. Redis Enterprise also stores the application data model so incoming messages can correlate with representational information such as sensor location.In addition to challenging in-store retail, the COVID-19 crisis has forced technology vendors to re-calibrate and customize their operations and application delivery models. To maintain business continuity at scale without any downtime, businesses need the right tools and techniques to scale their infrastructure and accelerate their application response times.For example, consider Freshworks, which builds cloud-based suites of business software. Due to extraordinary growth over the past six years, the company was straining the capabilities of its application architecture and development operations. As the company’s database load grew, it struggled to maintain performance. Looking to dynamically scale its cluster without compromising availability, the team also wanted to reduce the burden on Freshworks’ primary MySQL database and speed application responses.After evaluating NoSQL in-memory databases like Aerospike and Hazelcast, Freshworks chose the high performance and flexibility of Redis. Ultimately, the team chose Redis Enterprise Cloud to ensure high availability and seamless database experience as an infrastructure service for developers.In addition to using Redis Enterprise as a frontend cache for its MySQL database, Freshworks uses Redis Enterprise’s highly optimized hashes, lists, and sorted set data structures and built-in Redis commands to meter the API requests coming into its Freshdesk software.Redis Enterprise also serves as a persistent store for background jobs stored on disk. And as Freshworks transitions to microservices, the company has started to separate critical workloads out of its monolithic Ruby on Rails web application framework. One of the first microservices to result from this effort is dedicated to authentication and uses Redis Enterprise as a session store.Finally, Freshworks leverages Redis Enterprise’s powerful data structures, including HyperLogLog, bitmaps, and sets, as a frontend database for user analytics.The case studies above are merely a sample of the use cases where Redis Enterprise is an optimal choice. But there are a few features worth highlighting.A caching layer stores repeatedly requested data. Ideally, that data is served with a sub-millisecond response time that enables faster loads and eases backend costs.At an enterprise-grade level, an in-memory cache scales linearly across clouds and suffers no performance degradation. Quick data retrieval equals a faster response time for the user. A fast cache also balances backend service loads, allowing existing hardware to operate at peak performance.(Note: Some Redis service providers, such as Amazon ElastiCache, support Redis only as a cache and not as a primary database.)Robust messaging solutions are vital in a microservices architecture. These various collections of services need to communicate with one another in a loosely connected environment.Messaging protocols such as Pub/Sub aid live broadcasting notifications and are extremely useful for message dispersal when minimal latency and massive throughput are essential. These protocols make a difference. A Sorted Set and Hashes power chat rooms, social media feeds, real-time comment streams, and server intercommunications. Another structure, Lists, can help create lightweight messaging queues.Session management is all about personalization. Applications need to handle high volumes of data, all while caching and retrieving user-profiles and session metadata. With a reliable session store, it’s possible to scale to billions of field-value pairs with sub-millisecond response times and to handle unexpected spikes in traffic.A session store manages session data and improves usability, authentication, user profiles, and logging performance by caching IDs and tokens. This reduces the load on the primary database and computes resources, saving money in the end.Geospatial data can integrate location-based features in an application. Common examples including estimating distance, arrival times, and nearby recommendations.With a geospatial index, it’s possible to store and search for coordinates using a geospatial command, such as GEOADD (to add one or more geospatial items using a Sorted Set, or GEODIST, GEOHASH, and many others.See what Redis geospatial indexes can do in the video below.Developers have to scramble to deliver new applications with compelling features. That keeps them busy, as it’s hard to match the expectations.To speed time to market, development teams must unlock sub-millisecond response time performance and find easy ways to apply multiple data models to get the freedom they need to build software the right way.What way will you go when you start building tomorrow’s next great application? Explore these impactful ways to use Redis and build powerful applications with speed and performance."
601,https://redis.com/blog/how-three-redis-community-members-rediscovered-redis/,How Three Redis Community Members Rediscovered Redis,"September 21, 2020",Haley Kim,"We are constantly inspired by the creative and powerful ways the Redis community uses Redis to power innovative applications. In the premiere issue of Rediscover Magazine, we showcased a trio of community members who have rediscovered Redis to help them conquer their data challenges.Now, here’s your chance to get to know Carlos Justiniano, Matthew Goos, and Dan Pipe-Mazo a little better and learn how they use Redis in biotech, medtech, and robotics respectively. Check out “Meet the Redis Stars” in Rediscover Magazine, then read on here to learn their favorite Redis features and check out their RedisConf sessions and social media!Twitter: @cjusGitHub: @cjusMedium: @cjusVeteran software developer Carlos Justiniano is Chief Technical Officer at Skafos.ai. Previously he was Vice President of IoT and Cloud Platforms at Zenerchi (he’s still an advisor at the company), where his team used RedisGraph to power its BioGraph project, a navigable model of human physiology. He’s still new at Skafos, but has already proposed using RedisGraph and RedisAI to power its next-generation AI visual product search technologies. Carlos also wrote the Hydra framework, a Node.js package that leverages Redis to facilitate building distributed applications, including microservices architectures.Favorite Redis feature: “My favorite feature of Redis has to be the ability to create a memory based key space, coupled with rich data structures.”RedisConf 2020 Takeaway session: Creating a Model of Human Physiology using RedisGraphThis was Carlos’ third RedisConf appearance, and his presentation focused on modeling human physiology with Internet of Things and wearable tech devices using RedisGraph. Watch him demonstrate how RedisGraph is powering Zenerchi’s biotech platform, which produces advanced 3D, VR, and AR visualizations.Twitter: @m4g005LinkedIn: @matthewgoosMatthew Goos has more than 25 years of technology experience, and is the Co-Founder and CTO of MDmetrix, an interactive data analytics platform that helps clinicians and institutions understand patterns in their data. MDmetrix built its Mission Control application to combat the spread of COVID-19 by providing analytics to institutions and physicians around the country to improve patient care and optimize utilization of resources. MDmetrix uses two Redis modules: RedisJSON for storing data on users and RedisGraph for storing data for analysis.Favorite Redis feature: “If you consider a module a feature, then our favorite thing is RedisGraph. It allows us to store our customer data flexibly so that our analytics engine can quickly process it.”RedisConf Takeaway 2020 interview: Afternoon KeynoteCatch Matthew in conversation with Howard Ting, formerly CMO at Redis, as he explains more about how he and his team build Mission Control. MDmetrix was one of the winners of our Rediscover Redis competition!Twitter: @dpipemazoGitHub: @dpipemazoLinkedIn: @dpipemazoDan Pipe-Mazo is the CTO of Elementary Robotics, whose mission is to create more affordable and accessible robot assistants. Dan has spoken both at Redis Day London 2018 and RedisConf 2019 about Atom, the Redis Streams-based microservice software development kit that the robotic software stack is built on. Atom 2.0, currently released in beta, also uses RedisTimeSeries and Grafana. All Atom users get full dashboards and performance specs of all their microservices with zero additional work, for free. Elementary Robotics has been running 100% of its robotic data on Redis Streams for the last two years!Favorite Redis feature: “Redis Streams are a game-changer in robotic software as the same action on the publisher side, (XADD), can be interacted with on the client side in either a typical Pub/Sub fashion (XREAD) or a last-value-cache fashion (XREVRANGE). This is perfect for robotics in which we have several high-frequency data inputs (motor position, temperature, etc.) being injected at very high frequencies (> 1kHz).”RedisConf Takeaway 2020 session: Build a Message Bus with Redis Streams and FastAPILearn how to build a message bus—a way for producers to communicate with receivers by adding messages to a persistent data structure—in this session with Kyle Bebak, Web Architect at Elementary Robotics. He demonstrates how you can do this with Redis Streams and FastAPI, a Python web framework.Read more about Carlos, Matthew, and Dan in Rediscover Magazine, available free (online and in print) at Redis.com/rediscover-magazine. The premiere issue features more than a dozen stories on rediscovery, the power of data, real-time financial services, database trends, serving artificial intelligence, and managing remote workers, plus exclusive interviews with Redis creator Salvatore Sanfilippo and more tech leaders."
602,https://redis.com/blog/redis-lightweight-scalable-database/,Redis: the Lightweight and Scalable Database,"April 27, 2018",Daniel Jones,"We are FreeWheel, a Comcast company providing solutions to the Media Advertising industry to enable efficiency and insight into all aspects of campaign workflows, across media for media buyers and publishers.Our system is based on a microservices architecture. The ecosystem of products that support the workflow has had to evolve significantly over the last 15 to 20 years in terms of tech stack and infrastructure. Part of that evolution was ensuring a robust and quick-caching mechanism to support shared data storage within our ecosystem and across environments.A few years ago, our system’s caching mechanism was a simple per-server-per-application store, utilising the built-in technologies available on Microsoft IIS and running .NET web applications. This was sufficient for isolated domain data with no shared concerns outside of a single request. But as the ecosystem grew, so did the need to share that data with processes running in a different application or a different server pool. Problems with stale caches surfaced. Rolling out our own orchestrations that attempted to ensure an up-to-date store resulted in unmaintainable and hard-to-scale code. We looked at many solutions, including memcache and various derivatives such as Couchbase, but in the end we settled on Redis.Redis brought with it a growing reputation as the standard bearer in lightweight data storage, performance and reliability. In short, it just worked, and we weren’t required to waste time troubleshooting the underlying technology. Instead, our time was spent fine tuning a clean implementation, which ensured failover support and handling of flexible cache invalidation policies (depending on the use case). We could also be sure that the data we stored in Redis persisted quickly to a farm of servers and was available to many different deployed applications so that users were always getting the correct data.Moreover, there is good community support and documentation for Redis development, with supporting libraries such as StackExchange. Redis gave us the right amount of abstraction to achieve what we need. We were also able to easily integrate Redis to address concerns such as health monitoring against our existing stack. This was a fundamental plus point, as we have a mature yet quickly growing product with new releases every two weeks. We now deploy Redis Enterprise to our production servers, both bare metal and cloud based, supporting users in Europe, US and APAC.The transition to Redis Enterprise has been a positive experience. Using Redis Enterprise has made the system more stable and most importantly, we have identified more use cases where Redis can be utilized, which will improve the user experience further. It will allow us to achieve things that may have otherwise been more challenging. Right now, we are designing the infrastructure needed for automation of some internal processes that will facilitate Continuous Integration scenarios, which are key to our success and allow us to move quickly.  Building these solutions around Redis will allow us to be more sophisticated in our approach and give us more scalable results.This is a guest post by Daniel Jones, a Technical Architect at FreeWheel. He can be reached through his LinkedIn page https://www.linkedin.com/in/daniel-jones-5b93774/"
603,https://redis.com/blog/data-ingestion-speed-up-your-application/,Data Ingestion: 6 Ways to Speed Up Your Application,"August 3, 2022",Talon Miller,"Today’s world requires real-time responses – latency is the new outage, and your customers’ expectations for speed have only gone up. Real-time use cases are only possible with real-time solutions. An RDBMS isn’t capable of providing the speed and performance an in-memory database like Redis Enterprise can provide.Read our White Paper “Latency is the New Outage.”Legacy and traditional SQL databases aren’t designed for speed at scale, so for that reason, a cache is commonly used to store copies of lookup tables to reduce latency and increase throughput. This allows that front-end DBMS to scale easily while always being available.Caching user session data is crucial for building a scalable and responsive application. Storing every user interaction requires access to the session’s data, and keeping that data in the cache speeds up response time to the application user. Caching session data in this ingestion process is also very important on a larger scale for modern microservices architectures as the session data is typically used as the source of truth, helping with data quality. This ensures microservices state updates are fast and scalable.Modern application APIs are very busy and are the source of a lot of latency if not handled with performance in mind. Caching APIs ensures responses for the application are always real-time.Let’s take a look at six different ways that data ingestion of part of your data lake into Redis Enterprise can transform your data layer so that you can keep that competitive edge and enable these real-time use cases.Applications simply cannot take on the many risks of data migration, like unexpected (and usually longer than expected) outages. The less risky route that enterprises commonly rely on is blue-green deployments to ingest data in which the application continues to use the “blue” legacy database while a new “green” cloud-native database is deployed in parallel for live-production testing and cut-over of this new data pipeline with the assurance of rollback. Redis Connect is our data ingestion tool used to maintain consistency between the legacy and new cloud-native databases (Redis Enterprise) until a level of confidence is reached with the new deployment.Mission-critical applications that are too complex and/or contain large amounts of big data are not appropriate for data ingestion blue-green deployment. They require a multi-phase data ingestion migration plan that can span months or even years to complete. Typically, these projects incrementally migrate small workloads to newly created modular applications or more recently to microservices. This application’s legacy database will often act as the system of record or data warehouse for the duration of the project, which makes consistency a challenge with the new database(s) that support the microservice architecture. Redis Connect can be used on a single, or small set of tables, to maintain consistency between the legacy system of record and new databases, sometimes even data ingest bi-laterally.It is common for businesses to keep their legacy databases as a system of record to support their existing operations while, at the same time, leveraging read-replicas, or cache prefetching, ingest data in order to enable these real-time solutions. Redis Connect is used for streaming data indefinitely from the legacy database to Redis Enterprise. A very easy and nearly zero-risk way to finally enable real-time data use cases.Legacy enterprise architectures, compliance risks, and operational concerns often act as barriers to migrating to the cloud. To overcome these challenges, enterprises have adopted a hybrid cloud architecture that splits deployments between on-premises and public clouds. Commonly, a stateless application is hosted on the cloud while operational data remains on-premises acting as the legacy system-of-record database. Redis Connect can be used for streaming data from the on-premises database to Redis Enterprise, which supports bi-lateral replication between its on-prem and public cloud replicas.Many industries are seeing unprecedented increases in transactions and high expectations for speed and availability. Businesses need to elastically provision, burst to the cloud, infrastructure to handle seasonal traffic peaks, expand data warehousing/analytics, improve disaster recovery objectives, or maintain operational business continuity in the event of a data center failure. The common solution involves leveraging the cloud for geo-distributed, on-demand infrastructure. Redis Connect can be used for streaming data from multiple sources to Redis Enterprise, which is capable of active-active geo-distributed hybrid cloud deployments.Microservices architecture adoption continues to grow as application modernization and cloud migration strategies accelerate. One of the most popular microservices design patterns is CQRS. In this pattern, different data structures (commonly supported by different databases) are used to independently optimize for writes (command) and reads (query). Implementing this pattern can be complex within a microservices architecture since consistency between command and query must be maintained. Redis Connect can be used to implement CQRS by streaming and transforming changed-data-events (CDC) from the command database to a read-optimized data structure in a query database/cache.The six ways to transform your data layer with data ingestion discussed above provide easy-to-implement and enterprise-hardened ways to help your business stay efficient and competitive. Redis Enterprise is a great place to enable your data to accomplish those modern use cases and provide sub-millisecond responses. There are far more use cases possible than just the caching ones we covered today like leaderboards, message brokers, fraud detection, and more."
604,https://redis.com/blog/the-many-flavors-of-multi-cloud/,The Many Flavors of Multi-Cloud,"March 5, 2020",Fredric Paul,"As Amazon, Microsoft, and Google fight it out in an epic struggle to dominate the cloud, a lot of companies and analysts are tossing around the term “multi-cloud.” But as Inigo Montoya immortally states in the movie Princess Bride, “You keep using that word. I do not think it means what you think it means.”Multi-cloud actually comes in a wide variety of flavors, intended to solve a wide variety of business and technical issues. And the most common multi-cloud implementations may not be the ones most people are thinking of when they hear or say “multi-cloud.”This is much more than an academic discussion. As these proof points show, multi-cloud is a very big deal:With that kind of impact, it’s important to understand the wide variety of approaches and architectures that can fall under the “multi-cloud umbrella.” Let’s take a look at some of the common ways the term is applied.The most common usages of the term “multi-cloud” often seem to focus on running a given workload on multiple different clouds—each providing parallel capabilities. That definitely happens, perhaps for load balancing or to avoid vendor lock-in or for increased resiliency in case one cloud suffers a massive outage. It may also be a way to host data closer to the user, either to improve performance or to comply with regulations about keeping data local, often referred to as data sovereignty. (The global scale of the big three cloud providers, however, means that they each have multiple availability zones and are likely to be able to provide local coverage almost anywhere you need it.)The issue, of course, is that if your application has to work in every cloud, it may not work optimally in any cloud. It may not be able to take advantage of an individual cloud’s competitive advantage—not to mention the additional IT overhead of maintaining the expertise to understand and manage multiple cloud infrastructures and vendor relationships.Alternatively, a single application or service could run in multiple clouds because specific parts of the application need to take advantage of specific services from a specific cloud provider. You might run most of a particular application on Amazon Web Services, for example, but put some portion on Google Cloud (perhaps the AI processing) to take advantage of that service’s technology.But while those approaches may be useful in some situations, they’re actually less-common implementations of a multi-cloud strategy. More common by far are companies where a multi-cloud strategy means running different workloads in different clouds. That can be on purpose, to let one application take advantage of specific proprietary services and features in a given cloud offering while other applications take advantage of other services from other providers.This kind of multi-cloud can make it easier for a company to maintain ongoing relationships with multiple cloud providers, so it can move quickly in any direction at any time. In his post on Why Organizations Choose a Multi-Cloud Strategy, Laurence Goasduff quotes Gartner VP Analyst Michael Warrilow explaining that the dominance of the mega-vendors in the public cloud services market is the main driver for enterprise buyers to choose multiple cloud providers: “Most organizations adopt a multi-cloud strategy out of a desire to avoid vendor lock-in or to take advantage of best-of-breed solutions. … We expect that most large organizations will continue to willfully pursue this approach.”In many cases, however, running separate workloads in different clouds isn’t actually a strategy at all, but a relic of past technology and business decisions. For example, a company standardized on AWS might acquire another company that’s a dedicated Microsoft shop running a .NET environment, and thus deeply committed to Azure. Similarly, the ongoing prevalence of shadow IT practices often lead to multi-cloud environments, whether or not the CIO approves or even knows about it.Multi-cloud isn’t “free,” of course. On the one hand, it’s sometimes possible to play cloud vendors off against each other to lower costs. But cloud vendors typically offer significant volume discounts, so splitting workloads across multiple clouds can lead to increased cloud costs. Dealing with multiple cloud vendors can also make it harder to keep track of and optimize your total cloud spending.More significant for many companies are the costs and hassles of managing multiple providers and multiple technologies. Every cloud vendor has its own constantly evolving set of services, interfaces, APIs, and preferred ways of connecting, even for similar services. And that can add complexity to your applications, as well as make life more challenging for engineers who have to deal with multiple cloud providers and keep track of how to interact with each one.Not surprisingly, the more dominant a cloud vendor, the less likely they are to help out with multi-cloud implementation. In fact, in Spiceworks’ Public Cloud Trends in 2019 and Beyond survey, 15% of enterprises say they need more help from cloud vendors in ‘Managing multiple cloud solutions’—while 22–23% of SMBs share that concern. Finally, that extra layer of management complexity only gets worse if multi-cloud adoption occurs on an ad hoc basis instead of as part of a carefully planned strategy.Fortunately, there are ways to make multi-cloud environments easier to work with. You can’t talk about multi-cloud in 2020 without considering the growing impact of microservices architectures and containers. Because containers package and isolate apps with their runtime environment, the rise of containers (Docker) and container orchestration (Kubernetes) helps solve a number of portability issues and accelerates application deployment to make it easier to move containerized apps among different clouds while retaining full functionality. The accelerating trend away from monolithic applications toward microservices—which breaks down applications into a collection of smaller, independent components—also helps make applications easier to deploy in multi-cloud environments.You may have noticed something missing from this multi-cloud discussion so far: What about the hybrid cloud? The idea behind the hybrid cloud is combination of at least one public cloud with at least one private cloud and/or an on-premises data center. I left hybrid cloud for the end of this post because the cloud community can’t quite seem to agree on exactly how the two concepts fit together.Many observers say the two strategies are related but different. In the blog post mentioned above, Gartner describes multi-cloud as a subset of hybrid cloud. And still others consider hybrid cloud just another flavor of a multi-cloud strategy. However you fit it in, even as the public cloud continues its meteoric rise, the long shelf life and huge sunk costs in legacy on-premises infrastructures means that hybrid cloud architectures will always have a place linking the two, especially in highly regulated industries like finance and healthcare.Multi-cloud strategies continue to gain momentum in the enterprise, but it’s hardly a one-dimensional concept. If you’re fixated on a single kind multi-cloud, you may be missing the point. There are almost as many flavors of multi-cloud as there are companies and use cases.In fact, as cloud vendors work to differentiate themselves by optimizing for particular situations, services, and types of customers, multi-cloud approaches are expected to become even more widespread and complex—we’re likely to see even more flavors of multi-cloud. The issue will increasingly become deciding on what cloud features you really need, and then choosing the fastest, simplest, least expensive, most resilient, most secure cloud—or combination of clouds—to meet your requirements.Whatever flavor of multi-cloud you choose, as a cloud-agnostic database Redis Enterprise offers the best Redis experience for all types of multi-cloud and hybrid cloud deployments. Try it now free in the cloud!"
605,https://redis.com/blog/redis-enterprise-operator-for-kubernetes/,Redis Enterprise Operator for Kubernetes,"September 9, 2022",Redis,"To streamline the management of a Kubernetes layer, we developed our own Kubernetes controller, the Redis Enterprise Operator for Kubernetes. Unlock the cloud-native data layer by downloading our e-book below.Unlocking the Cloud-Native Data LayerAs companies work to modernize their legacy systems, they are shifting architectures to take advantage of containerized applications. The initial adoption of container technology was driven by developers who needed flexible deployment options and simplified software stack management. According to Gartner, by 2027, more than 90% of global organizations will be running containerized applications in production, a significant increase from fewer than 40% in 2021.Infrastructure and operations leaders searching for agility and portability benefits within the infrastructure have embraced Kubernetes as the de facto standard platform for container scheduling and orchestration.Below you can see the typical architecture of a Redis Enterprise Cluster regardless of whether you deploy it as nodes or as containers. There is an inherited complexity in the administration of resources for scalability and availability of the cluster.Deploying Redis Enterprise on Kubernetes increases automation and ease of management; the Redis nodes become Kubernetes pods, keeping all the benefits of Redis Enterprise. Its shared-nothing architecture is an ideal platform for a Kubernetes deployment taking advantage of persistent volumes for storage. These volumes enable containers to outlive their typical lifecycle and offer data persistence.In short: You don’t need to worry about the scalability or availability of your stateful applications.To simplify and automate the management of the Kubernetes layer, we developed our Kubernetes controller, better known as a Kubernetes Operator, which deploys a Redis Enterprise database service on a Kubernetes cluster.The Redis Enterprise Operator for Kubernetes delivers the knowledge we acquired over years of deploying millions of clusters into a controller that extends the functionality of the Kubernetes layer and understands the Redis Enterprise capabilities to create, configure, and manage the database and the overall cluster.For example, the Kubernetes Operator constantly monitors the state of a cluster against its ideal state. If the cluster deviates from that state, the controller takes action to correct the problem; this is how a Kubernetes deployment can automate the scaling of the cluster or the recovery of a failed node.Why Operator?Let’s start by answering the question: “Why do you need an Operator?” Since 2016, CoreOS (now a part of Red Hat) has been advocating a need for a controller in Kubernetes that understands the lifecycle of an application. Although Kubernetes is good at scheduling resources and recovering containers gracefully from a failure, it does not have primitives that understand the internal lifecycle of a data service. As such, adopting the Operator paradigm for Redis Enterprise is a natural progression of our Kubernetes work. Operator not only offers the benefits of a typical controller but also allows you to describe failure recovery using domain and/or application knowledge.So what does the Redis Enterprise Operator actually provide?The Operator framework takes advantage of Custom Resource Definitions (CRDs) to manage and maintain domain-specific objects in Kubernetes. By capturing domain or application-specific lifecycle management logic inside the Operator, the Operator aims to reduce deployment and maintenance complexity. Thus, by making it easier to expose the Operators to specific namespaces and/or track instances across namespaces, the Operator will lower the operational burden for our customers. The following are our design goals for the initial release of Redis Enterprise Operator:You know the power of Redis Enterprise. Now imagine how much you can simplify your cluster administration by adding automated scalability and zero downtime upgrades by using the power of the Redis Enterprise Operator for Kubernetes.On top of those benefits, add unprecedented flexibility to your deployment with seamless operations across on-premise infrastructure and public cloud, or any combination, thanks to the portability and compatibility offered by the Kubernetes layer.Do you want to start on-prem and move to the cloud? Maybe you are already running in a private cloud. You can deploy Redis Enterprise on OpenShift, VMware Tanzu, Rancher Kubernetes Engine (RKE), or Community Kubernetes (kOps).Suppose you use a Kubernetes distribution offered by major cloud vendors because of regulations or your business strategy. In that case, you can do it with Azure Kubernetes Service (AKS), Google Kubernetes Engine (GKE), or Amazon Elastic Kubernetes Service (EKS).Adopting the hybrid cloud or multicloud has never been easier. Start with an on-prem deployment and move to any infrastructure as your needs change. Download the e-book below to help you get started."
606,https://redis.com/blog/data-economy-podcast-modernizing-apps/,"The Data Economy: Modernizing Apps for Real-time Analytics, Streaming, IoT, and AI","March 22, 2022",Isaac Sacolick,"The Data Economyis a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.Here’s a way to collaborate and have some fun with your technical and business colleagues. Ask everyone to identify the top experience, architecture, security, and data issues that characterize legacy applications and other technical debt.It’s likely to be a long list covering everything from frontend user-experience issues to the underlying platform and infrastructure deficiencies. But there’s one often overlooked characteristic of legacy applications that is a sure, tell-tale sign of an application that requires modernization.That characteristic is whether applications were designed for the slow “batch world” where the end-users waited for data processing, infrastructure constraints, and architecture limitations limited experiences, analytics, and machine learning capabilities.The transition from batch processing to real-time streaming flow and streaming analytics was the discussion topic during episode two of The Data Economy, a podcast presented by Redis and hosted by Michael Krigsman of CXOTalk. Watch the episode to hear from guests Mike Gualtieri, VP and Principal Analyst at Forrester Research, and Norm Judah, a former enterprise CTO at Microsoft.Mike and Norm lay out a compelling argument for why more businesses in non-tech industries and even SMBs can gain significant business advantages by changing to real-time paradigms and simplifying app development with a robust multi-model data platform.Real-time data processing is not just for Wall Street trading platforms or large-scale tech platforms like Uber. In the podcast, Mike and Norm share several examples of streaming flow applications where the data from a transaction in one system is shared in real-time with other systems. One example is travel, where a booked plane ticket should update loyalty systems in real-time because top flyers will check their point status right after booking.More complex use cases require tracking state or enriching transactions with reference data. Tracking state is important for anomaly detection, such as monitoring temperature fluctuations in industrial systems, or for optimizing fraud detection in financial applications.Many internet of things (IoT) use cases start with monitoring sensors, but real-time decisions and analytics are only possible after enriching data with reference data. For example, an IoT streaming flow from a fleet truck might include a device ID and location, but predicting optimal driving routes can only be computed after enriching the data with the truck type, driver, work schedule, and other information.Shifting to real-time often creates differentiating capabilities. For example, finding out whether your mortgage application was approved is often considered a slow decision process, but companies that develop real-time analytics and decision capabilities may be able to win business by speeding up to a faster, more real-time response.Many of these examples revolve around making quicker, more accurate predictions and aiding people in making real-time decisions. Smarter and faster decision-making is a competitive currency, and IT leaders must consider that latency is the new outage.In the podcast, Mike suggests two questions that can help leaders identify opportunities. He recommends asking, “Is there something I could predict here to make this a more intelligent process, bypass a step, or make a better-automated decision? And is there something I could do quicker about this process?”Norm and Mike go really deep into some of the technical challenges facing architects when designing real-time capabilities, and why building on top of legacy architectures is a nonstarter.Consider how many applications were developed before multicloud, microservices, in-memory databases, caching solutions, and streaming flow capabilities became readily available.Developers often built legacy monolithic applications with a single-use case in mind, but today’s data streams must support many opportunities. “You actually don’t really know all the possible end-user cases,” says Norm. “The data analyst or the storage architect has this terrible dilemma of trying to understand the possible scenarios.”Architects face several questions: Where should data and processing be performed in the cloud versus on the edge? What reference data is stored in memory to enable enriching?  How should databases store real-time versus historical data?And when you’re working with multiple streams of real-time data, Norm shares another key design criteria architects must consider: “The sequence of the data is incredibly important, and it’s easy to say but much harder to do. When I have a data source sending data to multiple places, you want to make sure that they all get the same data in the same sequence.”But Mike lays out the key business technology challenge around machine learning, especially when applied to real-time data sources. According to Mike, “The nasty thing about machine learning and investment is that you don’t know if it works until you try it.”You can take his statement as a warning or as a technology challenge. It’s both. The technology challenge is in creating versatile, real-time data processing architecture that serves multiple business needs. The business reality and warning is that leveraging machine learning and real-time analytics will require ongoing experimentation to develop the algorithms, deliver applications, and support ongoing enhancements.The key for architects is to establish a versatile, real-time data platform that runs at any scale that the business demands. It requires building multicloud or hybrid cloud platforms for long-term business opportunities and working with partners that accelerate a seamless migration.Because once the race begins and competitors create differentiating capabilities with real-time data platforms, it will be very hard for lagging businesses to catch up.Watch more episodes of The Data Economy podcast."
607,https://redis.com/blog/signs-youve-outgrown-elasticache/,"Struggling with Performance, Scale, or Cost? Learn the Signs You’ve Outgrown ElastiCache","May 6, 2022",John Noonan,"ElastiCache is Amazon’s popular managed Redis service. It has seen wide adoption over the past decade as businesses have recognized the incredible potential of two emerging technologies: AWS (Amazon Web Service) for cloud computing and Redis as an in-memory data store. Both AWS and Redis have transformed the way we use technology, making faster, more scalable, and more resilient digital experiences the norm.Organizations that concurrently moved to the cloud and sought to speed up their applications naturally turned to AWS’ ElastiCache service when they had outgrown open source Redis and were seeking a managed service. ElastiCache remains a solid choice for smaller startups building from the ground up on AWS and seeking the benefits of Redis without the expense or hassle of managing it.But successful businesses grow, and the past decade has revealed that as businesses scale they eventually outgrow ElastiCache’s abilities. AWS ElastiCache is a managed service built on open source Redis and it lacks much of the enterprise-grade functionality required by businesses operating at scale.So how do you know if you may be outgrowing ElastiCache? Here are a few key signs:Redis is at the core of our business – it’s who we are and it’s what we do. So it’s no surprise that we are far ahead of other Redis services in providing innovative ways to keep costs low while maintaining performance and resilience. One such way is our quorum concept. Most NoSQL databases use three replicas to ensure high availability. The first is used to store data, the second as a failover, and the third as the tiebreaker to determine which is correct should the primary and replica have inconsistent data. But DRAM is expensive, and maintaining three replicas of a dataset is incredibly expensive. Redis Enterprise is able to provide a highly available system with only two replicas, where your tiebreaker is determined at the node level by using an uneven number of cache nodes in a cluster, saving you nearly 33% out of the box. Learn more.Check out our Redis Enterprise vs. ElastiCache Datasheet with a full feature-by-feature technical comparison and see if it’s time to upgrade to Redis Enterprise."
608,https://redis.com/blog/data-economy-podcast-three-ways-fico-optimizes-its-machine-learning-models/,The Data Economy: Three Ways FICO Optimizes Its Machine Learning Models for Real-Time Financial Services,"May 3, 2022",Isaac Sacolick,"The Data Economyis a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.You can’t just put your machine learning (ML) models in the cloud when your algorithms must process thousands to tens of thousands of transactions per second within 10 and 20 milliseconds.And you can’t sacrifice accuracy or explainability when your ML models impact roughly 80% of all credit card transactions.These are some of the requirements Scott Zoldi, FICO’s Chief Analytics Officer, shares in his recent interview for The Data Economy podcast. And Zoldi isn’t shy about sharing challenges, insights, and methodologies on how a 60-year-old company like FICO pushes the boundaries on the R&D, infrastructure, and ongoing management of its real-time machine learning models.Scott, with over one hundred authored patents, shares the following insights, but you’ll have to watch the podcast to get all his detailed nuggets of wisdom.Scott simplifies the latency requirements from a customer perspective: “Can you imagine if you’re at a point of sale and it takes two seconds for you to wait for your credit card to clear? No one has patience for that.”It’s not just latency that drives FICO’s requirements, because making the wrong decision around fraud and credit decisions hurts consumers, merchants, banks, and other financial institutions. Zoldi shares insights on the tradeoffs between latency, analytics accuracy, costs, and other constraints when FICO architects its customer solutions.In the podcast, CIOs, CDOs, and IT leaders will learn how FICO creates new business opportunities, protects its reputation, and gains a competitive edge through its data science and technology methodologies. As Scott says, “For FICO, what’s unique is that we’ve been real-time at scale for more than thirty years. It’s really part of our bread and butter.”For data and IT leaders, selecting enterprise real-time data platforms and operating them in multicloud environments is the foundation for building, testing, integrating, and deploying machine learning models.In a real-time streaming environment, knowing the data sequence is critical for decision-making. For example, you probably hope FICO sees that your email was changed seconds before a large transaction and then flags the merchant about a highly likely fraudulent transaction—all in real-time.The speed and accuracy of ML models go into FICO’s architecture decision on what parts of the data processing and model run on edge computing and other elements that operate in the cloud. “If I chew up milliseconds, just getting to the cloud and back, that adds overhead to the overall value proposition,” Scott stresses.And it’s not just an infrastructure constraint. Scott shares this key insight around why his team researches and develops ML models before readying them for production use cases: “Some of the commodity functionality you find in a cloud-based environment may not be really meant for real-time computing.”It’s a key insight for data and IT leaders debating their architectures, especially around optimizing hybrid and multicloud architectures, scaling the data layer (hint: FICO looks well beyond traditional relational databases), or managing low latency and high quality of service.A key strategy for data and IT architects is to develop flexible data structures such as key-value data stores, geospatial indexes, and data streams to support managing, indexing, and querying a variety of real-time data sets.While Scott oversees FICO’s analytics functions, it requires a strong collaboration between data scientists, software developers, and cloud engineers to integrate models into real-time production environments. “We are part product managers because we have a view of what we need to meet the business objective,” says Scott. “We have to advance the state of the art of how our software deals with real-time constraints. We have this tight interlink between the role of us, a data scientist, a product manager, and a software developer.”For FICO, developing and supporting full life cycle support starts with R&D teams that only explore fully explainable ML models. The company also doesn’t throw all its data resources at every problem, and it doesn’t want to impute bias by bringing more data than required. Scott recognizes that “each data element brought into a solution adds liability to the decision.”The team looks beyond explainable AI, and in the podcast, Scott shares his definitions of “humble AI,” “responsible AI,” and “interpretable AI.” He believes that in the future, consent on whether, how, and when customers permit their data to be used in models will require a new wave of compliance and innovation.Please tune in to the podcast to hear more of Scott’s insights on developing and supporting ML models for real-time financial services.Watch more episodes of The Data Economy podcast."
609,https://redis.com/blog/reinvent-your-data-layer/,Looking to re:Invent? It’s Time for a New Approach to Your Data Layer,"January 1, 2022",Mike Anand,"It’s the time of year when companies take stock of the last twelve months—and lay the groundwork for the next year. It’s a time for big ideas, when you’re not just strategizing about minor improvements and optimizations, but looking for those monumental shifts that propel your business to an entirely new level.It’s also the time for the AWS re:Invent show in Las Vegas. A lot has changed since Redis last appeared in-person at re:Invent in 2019. Actually, that’s probably a bit of an understatement. Massive shifts in the digital economy and consumer habits have permanently altered how we approach technology. During that same period, Redis has grown and changed an incredible amount, including launching RedisJSON (Powered by RediSearch) into public preview, bringing the community together with the One Redis initiative, and holding the most successful RedisConf to date earlier this year.So this year, we’re switching up how we do things. Because the world has changed, and we all need to break through our assumptions and reinvent how we approach our customer experience.Organizations are looking at digital transformations in an entirely different way. Fundamental shifts in customer behavior and business interactions mean that business as usual just isn’t enough to stay competitive. Doctor visits happen via telehealth, grocery shopping is done on your mobile phone, and your day-to-day meetings with colleagues are done over Zoom. Our daily lives are happening digitally, and we expect these experiences to feel like in-person interactions, with no interruptions and a seamless, real-time flow.None of us can afford to look at our businesses the same way, and that same principle applies to our applications. Every time a customer interaction is slow, stalled, or returns the wrong data, we run the risk of losing that customer. The interconnectedness of customers through social media and other forums multiplies the risk, turning a handful of poor experiences into a landslide of lost customers.The current thinking tries to improve customer experiences by combining a primary database, such as PostGres or MongoDB, with a secondary caching database, like Redis, to add a layer of real-time acceleration. But Redis Enterprise shatters this way of thinking. With Redis Enterprise you can employ a primary database and enterprise cache within the same solution, all with real-time responsiveness.It’s the difference between accelerating your application and reinventing it.There’s a reason why we use the word reinvention. With reinvention, we fundamentally break down assumptions at a core level, keep what works, change what doesn’t, then rearchitect everything around a new, more powerful, groundbreaking core.We’ve all seen the power of reinvention with electric cars. On the surface, an electric car looks like an internal combustion engine car. But the second you get in, you know it’s a completely different experience. Getting there wasn’t a simple lift and shift of the engine. It took examining established limitations, then breaking through them to accomplish what was previously thought impossible.Redis is reinventing how you approach data. We’re challenging the assumption that you need separate data platforms for your cache and primary database, or that you need a complex platform with separate services for each data model. We don’t believe in limiting yourself to a separate data platform for every infrastructure, so we give you the flexibility to deploy in any cloud, hybrid, on-premises, or multicloud environment. And we’re shattering the illusion that global data layers have to be siloed and slow by giving you uninterrupted high availability (99.999%), diskless replication, instant failure detection, and single-digit-second failover across racks, zones, and geographies.We’re also defying the belief that greater performance comes with greater costs. Because Redis Enterprise provides all that while still being the most cost-effective Redis. With Redis Enterprise, you can fundamentally change how you approach your data layer and consolidate it from disparate, siloed data platforms into a single, unified real-time source. All while cutting down on technical debt, optimizing your total cost of ownership, and allowing your developers to focus on innovation instead of keeping the lights on.That’s the power of reinvention. It means harnessing a single data platform to create a real-time data layer that increases agility, accelerates time to market, and gives your customers an experience like never before.We’re challenging our own assumptions and taking our AWS data layer show virtual this year. Rather than silo our training and inspiration to Las Vegas, we’re holding a virtual event to help anyone around the globe reinvent their cloud data layer.Join us this week and for the next couple months as we roll out:And there’s more on the way. Check out our virtual event page and be ready to break the data matrix."
610,https://redis.com/blog/data-economy-podcast-fraud-detection-and-exabyte-analytics/,The Data Economy: Real-Time Fraud Detection and Exabyte Analytics,"March 15, 2022",Isaac Sacolick,"The Data Economy is a video podcast series about leaders who use data to make positive impacts on their business, customers, and the world. To see all current episodes, explore the podcast episodes library below.Some organizations have big data, others require real-time data processing, and a few manage the world’s most personal data. Here’s what you can learn from Experian, a company with all three data challenges.How big is big data? How fast is real-time data processing? And how smart and innovative must data-driven organizations aspire to be when addressing business risks or pursuing new opportunities? Eric Haller, EVP & GM in Identity, Fraud & DataLabs at Experian, shares business and technical insights in the first episode of The Data Economy, a podcast presented by Redis and hosted by Michael Krigsman of CXOTalk. If you don’t know Experian, chances are they know a little bit about you. They are one of the three major U.S. credit bureaus, operates in over 40 countries, and has more than 18,000 employees.In his role at Experian, Eric oversees the detection algorithms used to pinpoint fraudulent transactions that occur less than 1% of the time. For the majority of transactions, Experian must provide a fast response that minimizes the customer experience impact. How fast of a response? In retail, fraud detection algorithms must respond in under 200 milliseconds and, for some use cases, in under 40 milliseconds. In other words, really fast!Eric also oversees DataLabs, Experian’s R&D team that analyzes over an exabyte of data across their four labs. One exabyte is one thousand petabytes, or 50,000 years’ worth of DVD-quality video—in other words, very big data.And Eric reminds listeners of Experian’s data processing challenges. “We don’t want to stop a transaction unless we’re absolutely convinced there’s a significant risk, and most often, the happy path is a successful purchase,” he says.But on the other hand, Eric acknowledges that “the data you don’t see is what usually catches you.”So, Experian is always on the hunt for new data sources, signals to consider, machine learning features, and algorithm enhancements that improve factors such as accuracy, performance, transaction types, or geographies where it provides services.In fact, Eric shares a story of how Experian studied the economic impact of COVID-19. In the United States, the challenge was to find the right signals that signified a local outbreak, such as a spike in people buying cough syrup. For other hotspot regions, such as Brazil, the company partnered with the United Nations, the World Health Organization, Microsoft, Amazon, and educational institutions such as the University of Chicago to aggregate new data to track virus spread. Eric conveys the cultural sentiment at Experian: “The world needs help right now, and we should do our part.”Therein lies two lessons Eric shares in the podcast. He says, “Let the data show you how to solve the problem,” and acknowledges that Experian has significant talent, infrastructure, and experience pursuing challenges and opportunities for global and social good. For other companies, he recommends, “Be ready to work with a lot of other stakeholders that share the same dream and vision.”It’s hard to imagine now, but it was incredibly challenging just 10 years ago to aggregate and manage extraordinarily large data sets and process super-fast, high-volume data streams for real-time decision-making. Most companies were just starting to migrate to the cloud, had few tools to centralize data, struggled with SOA implementations, and developed a lot of technology scaffolding on their own.Today, organizations are processing time-series data, performing full-text searches on JSON documents, and are building applications with database-less architectures. Technology leaders recognize the importance of simplifying app development with a robust multi-model data platform and scaling globally while retaining local response times.Eric shares insights on Experian’s architecture in the podcast and his perspectives on optimizing multicloud architectures, leveraging graph databases, and centralizing multi-business-unit data sources. He also discusses information security, meeting regulatory requirements, and the training Experian requires to ensure data scientists are compliant.But with all the technology and compliance challenges, solving them is not the company’s primary objective. Eric shares his secret and says, “You would love to have the bandwidth to let your heart go wild on a data set and see what you come up with, but if you do that, you might get the one in a million needle in a haystack. You’re more likely to find winners by chasing a particular problem to solve.”That’s great advice for developers, data scientists, and engineers, who should focus on solving the business problems and challenges first—and then seek technology partners, commercial solutions, and open source options that enable efficient technology implementations.Watch more episodes of The Data Economy podcast."
611,https://redis.com/blog/microsoft-partnership-redis-enterprise-cloud/,Deepening Our Partnership with Microsoft to Grow Redis Enterprise in the Cloud,"May 12, 2020",Ofer Bengal,"I’m excited to announce that we are expanding our partnership with Microsoft Corp., to provide Redis Enterprise as two new tiers of Azure Cache for Redis. Our collaboration with Microsoft began in 2014 with the launch of Redis Cloud on Azure. Since then it has evolved into a unique and strategic partnership for both companies.Microsoft has co-developed offerings with various open source software companies, and this new initiative is unique in that it integrates two existing offerings. The service, currently in private preview, provides two new Enterprise tiers to Azure Cache for Redis, to enable both DRAM and SSD support. These new tiers are powered by Redis Enterprise, providing customers with the most feature-rich, highly available, secure Redis deployment on Azure.Over the last year we’ve worked closely with the Azure engineering and leadership teams, and we appreciate their invaluable support and effort to make this integration and partnership a success. We are pleased to have Julia Liuson, CVP of Microsoft’s Developer Division, join us at RedisConf 2020 Takeaway to talk more about our partnership.The timing of a project of this scale and complexity certainly presented some obstacles. The spread of COVID-19 meant our engineers could not relocate from Tel Aviv to Redmond as planned. Instead, we moved all business, engineering, and product collaboration online. Together, we adapted, as many companies across many industries are doing to meet the changing needs of their customers. The global pandemic has put greater pressure and expectations on our customers’ ability to operate at scale, making it all the more critical for us to quickly develop and innovate on their behalf.Throughout the development process, three key customer drivers were consistently top of mind: improve developer productivity, ensure operational resiliency, and ease cloud migration. Teams at both organizations were committed to building an integration that delivers these values to our customers. With the announcement of Redis Enterprise integration into Azure Cache for Redis, we meet these needs.Used by millions of developers worldwide, Redis is one of the most widely adopted developer tools, and is helping power some of the world’s largest enterprises. As the most beloved data store in the world, developers are at the core of the vibrant and growing Redis community.This service makes it easier than ever for developers to leverage the functionality in Redis Enterprise on Azure. By integrating into the existing Azure Cache for Redis developer experience, users can spin up a Redis cluster in a matter of minutes. Additional Redis Enterprise capabilities allow developers to scale elastically, replicate globally, and reduce overhead involved in cluster management. As a fully managed Azure service, developers can focus on building applications, not managing infrastructure. With added support of Redis modules (RediSearch, RedisTimeSeries, RedisBloom), developers can address a wide variety of use cases with a single technology. Ultimately, with increased developer agility, enterprises can build and iterate software faster, delivering business value to their customers and establishing a competitive edge.Redis Enterprise on Azure Cache gives organizations the ability to quickly build, deploy, and scale applications using Redis while reducing security risks and downtime. Microsoft has spent years investing in the security of Azure. With more than 90 compliance certifications globally, Azure meets compliance requirements according to both geography and industry.The new Redis Enterprise tiers will provide the same enterprise-grade SLAs and Azure network isolation. Plus, when implementing geo-replication, organizations can confidently and securely scale their Redis caches while ensuring even greater levels of availability.As part of their cloud migration and application modernization journeys, organizations increasingly need access to exclusive tools offered by Redis within the native Azure environment. For industries like financial services, ensuring zero data loss during cloud migrations is non-negotiable. The Redis Enterprise tiers in Azure Cache for Redis enable active geo-replication, allowing customers to seamlessly replicate data and eventually migrate legacy data workloads from on-premises data centers to Azure in a risk-free way.The new Enterprise offering for Azure Cache for Redis is currently in private preview. To learn more, visit the Azure Marketplace, where you can request access to the preview service. You can also watch a demo from RedisConf 2020 Takeaway and read Microsoft’s blog post from Julia Liuson. For urgent requests, please email us at msft-at-redis-labs@redis.com."
612,https://redis.com/blog/redis-hosting-option-for-heroku-users/,A Redis Hosting Option for Heroku Users,"September 8, 2022",Raja Rao,"Heroku may have discontinued its free platform support for Redis, which may discourage those who have depended on it. But not to worry! Redis has several ways to add continued support, including a starting plan far cheaper than Heroku Redis.Heroku is a fantastic product. For years, developers have praised this pioneer in the Platform as a Service (PaaS) category for the platform and the developer experience the tool provides. However, Heroku recently announced that it would discontinue its free platform tiers, Heroku Data for Redis and Heroku Postgres.This is not the first time we’ve seen a third-party hosted provider that hosts Redis open source abruptly change or discontinue its service. For example, RedisToGo, which also offered a hosted Redis, recently announced its closure.Now, imagine you are using Redis in production, an easy-to-imagine scenario. You’re suddenly given a rush project: purchase a new service, set up a new service, upgrade your server, migrate live production data, and so on. It can be a nightmare. You deserve a service you can depend on and one you can be confident will grow with you.Here at Redis, we want to assure you that we have your back!We are the driving force behind the popular Redis open source project and the ever-reliable, predictable platform for our customers with our commercial version, Redis Enterprise.Our cloud offering, Redis Enterprise Cloud, includes free and paid plans. The latter has enhanced developers’ and DevOps capabilities and costs less than any other Redis provider.Here is a quick comparison of Heroku Redis and Redis Enterprise:Whether you are a student who just wants to learn Redis, a startup that wants all the enterprise features at an affordable cost, or a large enterprise organization with terabytes of data and a multicloud strategy, look no further.We have your back!Unsure if this is a good deal for you? Try us out.If you sign up directly, you can get the same plan for $7/month and get $200 in credit that you can use for up to three months.Here’s how. After you sign up, choose a paid plan, and apply the coupon offered. Then, copy the endpoint URL and password to your application’s environment variables. It will look something like the one below. (If your environment variables are different, use them instead.)REDIS_ENDPOINT_URL = ""Redis server URI""REDIS_PASSWORD = ""Password to the server""Alternatively, to use the Redis Enterprise addon directly via Heroku, run the following:heroku addons:create rediscloud:30 //30MB plan @ Freeheroku addons:create rediscloud:100 //100MB plan @ $10/monthAre you a larger company with substantial custom Redis needs? Please contact us, and we’ll ensure you get the best Redis experience anywhere."
613,https://redis.com/blog/migrate-redis-data/,How to Migrate Data from Redis Open Source to Redis Enterprise in Under 5 Minutes,"July 7, 2022",Talon Miller,"Migrating data from one data source to another creates a great risk for a business. There is a chance for a number of things to go wrong like data loss, dataset schema changes causing semantics risk, extended (unexpected) downtime, and data corruption, to name just a few. Regardless of the risks, businesses need to innovate to survive and that means taking data from old or legacy databases and putting some, if not all, data into new databases to take advantage of all of the new and necessary potential. Such is the case with the companies moving more of their data into in-memory solutions like Redis to take advantage of the power of real-time data.Redis Open Source is a really great way to incorporate real-time capabilities into your app. With Redis Enterprise, you can bring all of the things you love about Redis on top of infinite linear scalability, five 9s of true high availability, security built-in, and a way to keep the costs low with Redis on Flash. Once you start to outgrow Redis Open Source, the question becomes: how to safely and most efficiently migrate that Redis data to an enterprise-hardened Redis? The answer: Redis Enterprise.As an architect, your customers will want new capabilities and reliable performance that you simply cannot get in Redis Open Source without building them from the ground up. As an operator, you will want a simple set-up and a reliable and easy way to operate this critical data pipeline without all of the manual overhead and maintenance. Redis Enterprise is the easiest and most straightforward way to migrate Redis data from Redis Open Source to the version of Redis built for enterprise solutions. Let’s dive in.In order to understand how we can perform a database migration with zero downtime deployment, we need to understand the features behind Redis Enterprise that help us execute this data migration. Redis Enterprise has the ability to provide Active-Passive Geo-Distributed Replication to applications with read-only access to replicas from different geographical locations. We call this Replica Of.In the configuration of a Redis Enterprise database, we can assign a database as a replica (destination) of one or more (max 32) databases (sources). After the initial load from source to destination is completed, all write commands are synchronized from the sources to the destination. This enables you to perform a database migration with zero downtime as the replication bridge between the destination database and the sources can stay connected for as long as needed, even indefinitely.Replica Of lets you distribute the read load of your application across multiple databases or synchronize the database, either within Redis Enterprise or external to Redis Enterprise, to another database.You might be wondering about write access – for that, Redis Enterprise has Active-Active Geo-Distribution (CRDB) that provides write access to all of the database replicas on top of the other benefits during database migrations.Below, you will see a lightning demo of how we set this up in less than five minutes. But before you watch that, let’s cover the data replication process. When our database is defined as a replica of another database, all of its existing data is deleted and replaced by the data that is loaded from the source database. In our lightning demo, we will start with a fresh Redis Enterprise database with zero data so we don’t risk losing data. Once the initial load is completed, an ongoing synchronization process takes place to keep the destination database always synchronized with its source.With security in mind, Replica Of supports encryption for a unidirectional replication between source and a destination Redis cluster utilizing TLS 1.2 encryption.But what if your source database is sharded? The entire database is treated as a single source for the destination database. If your destination database is sharded then the destination database’s hashing function is executed to determine to which shard(s) the command refers.Well, that covers what you need to know about migrating data from Redis Open Source to Redis Enterprise, so let’s jump into the lightning demo on how to do this with your Redis database in order to take advantage of not only real-time power but real-time power with enterprise-grade capabilities."
614,https://redis.com/blog/the-impedance-mismatch-test/,The “Impedance Mismatch Test”: Is Your Data Platform Simple or a Complex Mess?,"August 6, 2021",Raja Rao,"“Simplicity is the ultimate sophistication”—Leonardo da Vinci“Most information is irrelevant and most effort is wasted, but only the expert knows what to ignore”—James Clear, Atomic HabitsYou have a fancy data pipeline with lots of different systems. It looks very sophisticated on the surface, but it’s actually a complex mess under the hood. It might need a lot of plumbing work to connect different pieces, it might need constant monitoring, it might require a large team with unique expertise to run, debug and manage it. Not to mention, the more systems you use, the more places you are duplicating your data and the more chances of it going out-of-sync or stale. Furthermore, since each of these subsystems are developed independently by different companies, their upgrades or bug fixes might break your pipeline and your data layer.If you aren’t careful, you may end up with the following situation as depicted in the three-minute video below. I highly recommend you watch it before you proceed.Complexity arises because even though each system might appear simple on the surface, they actually bring the following variables into your pipeline and can add a ton of complexity:The variables such as the data format, schema and protocol add up to what’s called the “transformation overhead.” Other variables like performance, durability and scalability add up to what’s called the “pipeline overhead.” Put together, these classifications contribute to what’s known as the “impedance mismatch.” If we can measure that, we can calculate the complexity and use that to simplify our system. We’ll get to that in a bit.Now, you might argue that your system, although it might appear complex, is actually the simplest system for your needs. But how can you prove that?In other words, how do you really measure and tell if your data layer is truly simple or complex? And secondly, how can you estimate if your system will remain simple as you add more features? That is, if you add more features in your roadmap, do you also need to add more systems?That’s where the “impedance mismatch test,” comes in. But let’s first look into what an impedance mismatch is and then we’ll get into the test itself.The term originated in electrical engineering to explain the mismatch in electrical impedance, resulting in the loss of energy when energy is being transferred from point A to point B.Simply said, it means that what you have doesn’t match what you need. To use it, you take what you currently have, transform it into what you need, and then use it. Hence there is a mismatch and an overhead associated with fixing the mismatch.In our case, you have the data in some form or some quantity, and you need to transform it before we can use it. The transformation might happen multiple times and might even use multiple systems in between.In the database world, the impedance mismatch happens for two reasons:The goal of the test is to measure the complexity of the overall platform and whether the complexity grows or shrinks as you add more features in the future.The way the test works is to simply calculate the “transformational overhead” and the “pipeline overhead,” using an “Impedance Mismatch Score” (IMS). This will tell you if your system is already complex relative to other systems, and also if that complexity grows over time as you add more features.Here is the formula to calculate IMS:The formula simply adds both types of overheads and then divides them by the number of features. This way, you’ll get the total overhead/feature (i.e. complexity score).To understand this better, let’s compare four different simple data pipelines and calculate their scores. And secondly, let’s also imagine we are building a simple app in two phases, so that we can see how the IMS score changes as we add more features over time.Say you are getting millions of button-click events from mobile devices and you need an alert if there is any drop or spike. Additionally, you are considering this entire thing as a feature of your larger application.Case 1: Say you just used a RDBMS to store these events, although the tables might not fit.Case 2: Say you used Kafka to process these events and then stored them into the RDBMS.Case 3: Say you used Kafka to process these events and then stored them into KsqlDB.Case 4: Say you used Redis Streams to process these events and then stored them into RedisTimeseries (both are part of Redis and work natively with Redis).We compared four systems in this example and found out that “Case 3” or “Case 4” are the simplest with an IMS of 1. At this point, they both are the same, but will they remain the same when we add more features?Let’s add more features to our system and see how IMS holds up.Let’s say you are building the same app but want to make sure they come from only white-listed IP addresses. Now you are adding a new feature.Case 1: Say you just used RDBMS to store these events, although the tables might not fit and they used Redis or MemCached for IP-whitelisting.Case 2: Say you are using Redis + Kafka + RDBMS.Case 3: Say you are using Redis + Kafka + KsqlDB.Case 4: Say you are using Redis + Redis Streams + RedisTimeSeries.When we added an additional feature,So in our example, Case 4, which had one of the lowest IMS scores of 1, actually got better as we added the new feature and it ended up at 0.5.Please note: If you add more or different features, Case 4 may not remain the simplest. But that’s the idea of the IMS score. Simply list all the features, compare different architectures, and see which one is the best for your use case.To make it even simpler to use, we are providing you a calculator that you can implement in a simple spreadsheet to calculate the IMS score.Here is how you use it:Data Pipeline 1Data Pipeline 2It is very easy to get carried away and build a complex data layer without thinking about the consequences. The IMS score was created to help you be conscious of your decision.You can use the IMS score to easily compare and contrast multiple systems for your use case and see which one is really the best for your set of features. You can also validate if your system can hold up to feature expansions and continue to remain as simple as possible.Always remember:“Simplicity is the ultimate sophistication”  — Leonardo da Vinci“Most information is irrelevant and most effort is wasted, but only the expert knows what to ignore” — James Clear, Atomic Habits"
615,https://redis.com/blog/how-to-migrate-dynomite-database-to-redis-active-active-database/,How to Migrate Your Dynomite Database to a Redis Enterprise Active-Active Database,"April 15, 2022",Helene Brulin,"In Part I of this article, “Why Migrate a Dynomite Database to a Redis Enterprise Active-Active Database?,” we compared Dynomite and Redis Enterprise’s architectures and features. We’ve shown how Redis Enterprise can help you geo-distribute Redis Enterprise in a feature-rich, easily manageable way, and not worry about conflicts between concurrent writes.Part II will describe the migration options available to move from Dynomite to Redis Enterprise.Note that hereafter Redis Enterprise’s self-managed offering will be referred to as “Redis Enterprise Software” and the managed offering will be noted as “Redis Enterprise Cloud” or “Cloud subscription.”Let’s get practical and see how we can run two types of migration:For the purposes of illustration, let’s assume that we have a Dynomite cluster spanning two datacenters: dc-a and dc-b. Each datacenter has one rack, and each rack is composed of two nodes, between which the dataset is distributed.If we remember our description of Dynomite’s architecture, we know that each Dynomite rack contains the full dataset.Therefore, we can limit the scope of our migration, whether it be Import/Export or Active-Passive, to a single rack within our Dynomite setup.Let’s pick rack-1-dc-a and assume that the IPs of its two nodes are as follows:For clarity purposes, here is the yaml configuration for our Dynomite setup:A few observations on this configuration and the setup used for testing this tutorial:Now that we understand our setup and have decided which rack we will use for our migration, let’s create a Redis Enterprise database.As the purpose of this article is not to explain how to set up your cluster or create your database, please refer to the documentation below to get your Active-Active database up and running:To test our two migration scenarios, I have created an Active-Active database spanning two Redis Enterprise Software clusters – one in Europe and one in the U.S. Each cluster is composed of three VMs running Ubuntu 18.04. Know that if you create a database without Active-Active, the migration steps will be the same, unless specified otherwise in this article.Let’s go ahead with our first type of migration.Redis OSS provides a persistence option called Redis Database Backup Files, or RDB, which performs point-in-time snapshots of your dataset, either at specified intervals or when triggered by the SAVE or BGSAVE commands.Those snapshots are saved in .rdb files, hereafter referred to as RDB files. We will export them out of our Dynomite servers and import them into our Redis Enterprise database. With this solution, note that a delta migration isn’t possible and that the import might take a while, depending on the size of the data.IMPORTANT: There is a big difference between Redis Enterprise’s non-geo distributed databases and Active-Active databases:The way to migrate data from Dynomite to Redis Enterprise with RDB files is as follows:Let’s see the above steps in more detail.Redis OSS instances running on your Dynomite nodes have their configuration files located by default in /etc/redis if you’ve installed it with “apt-get”, or in your Redis folder if you have built Redis OSS yourself. This file is called “redis.conf”.Open this file with your favorite text editor and search for the “dbfilename” directive. Change the name of the file on each node, such asThis ensures that when we export our RDB files to external storage, they don’t have the same name. You can skip this and change their names after taking the snapshot if you prefer.Optionally, you can also:Note that after editing the Redis OSS configuration files, you need to restart the Redis OSS server so that your changes are taken into account.Now stop the traffic coming into your Dynomite database through port 8379. Again, if you are importing to an Active-Active database and have planned your migration carefully so as not to risk any accidental overwrites during the import, you can cut over the traffic to your Active-Active database.Launch redis-cli. Don’t use port 8379, Dynomite’s listening port. Instead use port 6379. This is because we need to connect to the Redis OSS instance running on our node and not to our Dynomite cluster, which doesn’t support the SAVE command. You can just run redis-cli without any command-line argument.On each node, run the DBSIZE command. You will get the number of keys stored on each instance of Redis OSS. The total should be the number of keys in our Dynomite database.Run the SAVE command and check that your RDB files have been created in /var/lib/redis – or any directory you have specified.We are now ready to export our two RDB files to external storage.For this tutorial, I’ll export the files to Google Cloud’s Cloud Storage, but you can also use other external storage options such as an FTP Server, another Cloud Service Provider storage solution, or an external disk accessible from your Redis Enterprise cluster. You can find more information about those options below:In Google Cloud, I have created the following:Now for each node, we’ll run the following command:We can now see our two RDB files in our Google Cloud bucket:We are now ready to import them to our Active-Active database.Log into the Redis Enterprise UI and select your Active-Active database. If like in this tutorial, you have created a Redis Enterprise Active-Active database spanning several clusters, you can connect to the UI through whichever cluster you like. In this tutorial, we’ll use our Europe (EU) cluster.If you’re using a Cloud Active-Active database, simply connect to the Cloud UI and select your database.Let’s navigate to our database’s configuration page and click the Import button. Select the appropriate Storage Type. In our case, this will be Google Cloud Storage.We can now add the Cloud Storage path of our two RDB files such as:We also need to add the following information:This information can be found in the JSON key file you downloaded when creating a key for your Google Cloud Service Account.Note that the private key is weirdly formatted in the JSON file; it has quotation marks and newlines. To quickly format it in a way that the Redis Enterprise UI will accept, just launch a python interpreter and print it:We now have the following import configuration:Click Import and wait for the import to be finished which will be dependent on the size of the database.With redis-cli, connect to the endpoint of your Redis Enterprise database. Try to read some keys and run the DBSIZE command to check that you have the correct total number of keys.Don’t forget to check the Active-Active Geo-Duplication as well! Just connect to the other cluster’s database endpoint, in our case here, the U.S. one, and check the number of keys that you get.Your migration is now over. You can cut over the traffic to your database if you have not done so already.Now let’s run a continuous migration.The Redis Enterprise feature ‘ReplicaOf’ (also called Active-Passive in the Redis Cloud UI) allows us to continuously replicate data between two Redis databases. The main advantage is that it replicates the deltas after the initial synchronization is done, which means nearly no observed application-side downtime.The steps are:ReplicaOf is intended to be used in an Active-Passive way. That means the target is assumed to be passive, and it needs to be tolerated so that the target gets fully re-synchronized (flush of the target database + sync from the source database).Before starting the migration, let’s discuss a few security aspects.First of all, you need to add an inbound rule for custom TCP with 6379 port for the network in which your Dynomite rack lives.Secondly, Redis OSS configuration files on both Dynomite nodes need to be updated. By default, Redis listens only on the IPv4 and IPv6 (if available) loopback interface addresses. This means Redis OSS will only be able to accept client connections from the same host that it is running on. We need to update Redis OSS’ “bind” directive so that Redis OSS can listen to connections from our Redis Enterprise cluster host.There are two ways of doing this:Let’s discuss those two options in detail.The first option is to peer the VPC in which your Dynomite rack lives with the VPC in which your Redis Enterprise cluster lives. Please note that, if like us, you’ve created an Active-Active database, it can live in any cluster. As before, we’ll use our Europe (EU) cluster for demonstration.Once you have peered your networks, you just need to edit the “bind” directive in the redis.conf file: Add the private IP of your Dynomite machine after the default loopback interface addresses.Do this for all nodes in the rack and that’s it! Don’t forget to restart your Redis OSS instances.If you can’t or don’t want to peer your networks, then you need to update the Redis OSS configuration file in the following way, on each node:IMPORTANT: This last step is required as Dynomite does not support Redis’ OSS AUTH command, which prevents us from setting a database password. Therefore, if you don’t use a firewall to control who connects to the ports in use, anyone can connect to the Redis OSS instance and access/change/delete its data. Open port 6379 only to the host of your Redis Enterprise cluster.If you really wanted to use a password, you could. But it would make it impossible to run a continuous migration, as you’d need to do the following:One more security consideration and we’re ready to start our migration!To prevent unauthorized access to your data, Redis Enterprise supports the TLS protocol.If you’re using Redis Enterprise Software, you can specifically configure it for ReplicaOf communication. If you’re using Redis Enterprise Cloud, you can enable TLS in general.In the Redis Enterprise UI, let’s navigate to our Active-Active database configuration page and click Edit.We have the option to enable Active-Passive/ReplicaOf. Once we do, we can add sources in the following format :Please note:In our case, with VPC Peering, this is what our sources look like:Now let’s take the following steps:Like before, connect to your database with redis-cli and check that your data has been migrated. Check Active-Active Geo-Duplication as well by connecting to other clusters/other local endpoints.Redis has been named the most loved database by developers for many years. If you’re using Dynomite, it’s probably because you love Redis as well. At Redis, the home of both Redis OSS and Redis Enterprise, we can help your organization geo-distribute Redis in a more manageable way, while keeping up with the highest academic standards for conflict resolution."
616,https://redis.com/blog/why-migrate-dynomite-database-to-redis-enterprise-active-active-database/,Why Migrate a Dynomite Database to a Redis Enterprise Active-Active Database?,"March 29, 2022",Helene Brulin,"Since its creation in 2009, Redis OSS has had a very vibrant open source community. Many tools and utilities have been developed around it and Dynomite, a peer-to-peer geo-distribution layer for non-distributed datastores, is one of them.Dynomite was developed by a team of engineers at Netflix and released as open source. Although it has provided good solutions for specific needs, it hasn’t been effectively maintained in the last few years. Furthermore, some of the functionalities, commands, and data types of Redis OSS (e.g. Pub/Sub or Streams) are rendered unavailable or limited by Dynomite’s distribution model of Redis OSS instances.For this reason, we have been helping organizations with migrating their Dynomite databases to a Redis Enterprise cluster.Let’s start our comparison with a quick description of both Dynomite’s and Redis Enterprise’s architectures.A typical Dynomite cluster can be described as follows:A Redis Enterprise cluster also distributes data across different Redis instances, or shards, but there are two main differences:Another important part of a Redis Enterprise cluster is what is called the “management path;” it includes a Cluster Manager, a Proxy, and a REST API/UI. The Cluster Manager is responsible for orchestrating the cluster, placing the database shards in highly-available nodes, and detecting failures. The Proxy hides the cluster topology of Redis Enterprise from applications by providing a single, never-changing endpoint, for each database within a cluster. It also helps scale client connections by multiplexing and pipelining commands to the shards.Here is an illustration of a typical Redis Enterprise Cluster:With Redis Enterprise’s Active-Active feature, you can create a global database that spans multiple clusters. Those clusters typically reside in different data centers around the world. An application that writes to an Active-Active database connects to a local instance endpoint. All writes by the application to a local instance are replicated to all other instances with strong eventual consistency.Active-Active replication provides many advantages as a geo-distributed solution, one of which is seamless conflict resolution for simple and complex Redis Enterprise data types, which we’ll discuss below.Now that we have an idea of the topologies of Dynomite and Redis Enterprise, let’s see what they entail for developers and for DevOps within an organization.Aside from Dynomite not being actively maintained, there are three main reasons why an organization would want to migrate to Redis Enterprise from a developer’s point of view :Let’s see the first two in more detail.As you probably already know, Redis OSS is not a plain key-value store, in the sense that it does not only let you associate string keys to string values. Redis OSS is a data structures server supporting different kinds of values such as lists, sets, hashes, or streams. We call those the “core data types” of Redis OSS.Redis OSS is also extensible through dynamic libraries called “modules”. Modules allow you to rapidly implement new Redis commands with features similar to what can be done inside the core itself. Among the most popular modules are RediSearch, which provides querying, secondary indexing, and full-text search, and RedisJSON, which turns Redis OSS into a powerful document store.As discussed in the introduction, some of Redis OSS commands and data types are rendered unavailable or limited by Dynomite. Here is a non-exhaustive comparison:You can find a complete list of supported and unsupported commands with Dynomite here.On the other hand, Redis Enterprise, which is maintained alongside Redis OSS, allows for multi-model operations using modules and for core Redis OSS data structures to be executed in a fully programmable and distributed manner.Dynomite is an AP system and gives you three options for consistency. Whichever option you choose, it is important to be aware that Dynomite resolves the asynchronous write conflicts by applying the Last Write Wins strategy. This can lead to lost updates due to irrelevant timestamps, especially in the context of geo-distributed writes.On the other hand, Redis Enterprise’s Active-Active architecture is based on an alternative implementation of Redis OSS commands and data types called Conflict-Free-Replicated-Data-Types, or CRDTs. CRDTs use vector clocks for event ordering. They make sure that when any two replicas have received the same set of updates, they reach the same state, deterministically, by adopting mathematically sound rules to guarantee state convergence. Additionally, one can enable causal consistency as well.Therefore with Redis Enterprise:If you’re up for it, you can find the rules which are implemented as well as examples of conflict resolution by following this link.Now let’s compare Dynomite and Redis Enterprise from a DevOps point of view, by discussing the following topics:When a node fails within a Dynomite rack, writing and reading to the rack becomes impossible. This means that an application writing locally needs to handle the failover to another rack by itself. Note that, if your application is developed in Java, Netflix’s Dyno client can handle failovers to remote racks when a local Dynomite node fails.Furthermore, when the node comes back up, any data that has been written on remote racks during the failure will be missing from the failed node. If you deploy within AWS autoscaling groups, you can use Netflix’s Dynomite Manager, which does node replacement and node warm-up within an AWS auto-scaling group.What about Redis Enterprise’s high availability?When a node fails, a single-digit-seconds failover happens for all primary shards living on that node, and their replicas are promoted to primaries. This auto-failover mechanism guarantees that data is served with minimal interruption.Based on this :These mechanisms allow Redis Enterprise to guarantee 99.99% uptime and 99.999% uptime for Active-Active deployments.Dynomite allows you to scale Redis OSS while maintaining good performance in terms of latency. You can check the benchmarks but, if you’re using Dynomite, you probably already know that.When it comes to scalability, the main differences between Dynomite and Redis Enterprise are:If you’re not using Dynomite Manager within an AWS autoscaling group, adding hosts to a running Dynomite rack typically requires:On the other hand, Redis Enterprise allows you to:Here is a benchmark that Redis did a few years ago, in which Redis Enterprise delivered over 200 million ops/sec, with sub-millisecond latency, on 40 AWS instances.When we talk about connection management with Redis, what first comes to mind is how Redis clients, like Jedis or Lettuce, handle connection pooling and pipelining. Netflix was no exception to this and implemented these features for their Dyno client.Redis Enterprise provides such features out-of-the-box. The Proxy itself establishes persistent connections to shards in the cluster, and those connections are shared by clients. It also applies performance optimizations by scheduling requests on a number of persistent connections to the shards, doing multiplexing, and pipelining on the side of Redis.Plus, the proxy is multi-threaded and will automatically scale to handle bursts in client connections.Firstly, let’s remember that within a Redis Enterprise cluster one machine does not equal one Redis instance and that each primary shard can only have up to one replica.Secondly, Redis Enterprise is multi-tenant. That means that a single Redis Enterprise cluster can serve hundreds of completely isolated databases. This would be far from trivial to do using Dynomite. It is worth noting that Redis Enterprise’s multi-tenancy fits well with its multi-model capabilities. In the context of microservices, one could imagine running different fit-for-purpose databases in a single set of nodes, each with their own replication, scaling, persistence, and module configuration.Finally, Redis Enterprise has a feature called Redis On Flash (RoF). RoF enables your databases to use both RAM and dedicated flash memory (SSD/NVMe)  to handle much larger datasets with RAM-like latency and performance, but at >70% lower cost compared to an all-RAM database.Dynomite can be deployed in containers or machines running Ubuntu, RHEL, and CentOS. Deployments are always self-managed, in the sense that you need to provide infrastructure and resources to manage your cluster. Furthermore, as discussed before, if you want to take advantage of the features of Dynomite Manager, you need to deploy Dynomite within an AWS autoscaling group.On the other hand, Redis Enterprise has many deployment options, which can be divided into two groups:"
617,https://redis.com/blog/introducing-redis-stack/,"Hello, Redis Stack","March 23, 2022",Yiftach Shoolman,"Today we’re thrilled to announce Redis Stack. Redis Stack consolidates the capabilities of the leading Redis modules into a single product, making it easy for developers to build modern, real-time applications with the speed and stability of Redis.At Redis, we’re building a real-time data layer to meet the universal demand for responsive, low-latency applications and services.To build applications that provide real-time experiences, you need a database that can process any request, whether a simple object retrieval, a search, or a complex aggregation, with the fastest possible response times, preferably in less than a millisecond. The rationale is simple: in a typical application, every user interaction generates multiple calls to the database, which can result in significant overhead; if you add to that the round trip latency of the network between the end-user and the app, every extra millisecond spent in the database makes it more difficult to deliver a real-time end-user experience.This holds no matter the data model, be it a key/value, document, stream, graph, time series, or probabilistic data structures.As the trend of the last 24 months at DB-Engines makes clear, the fastest-growing data models are key-value, search, document, graph, and time series. This trend also shows that fewer and fewer developers are choosing to model their applications with relational databases.As it turns out, we’ve spent the past four years building several dedicated data engines that extend the core key/value data-structure functionality of Redis with modern data models and data processing capabilities, such as search, document, graph, time series, and probabilistic data structures.Implemented as Redis modules, we’ve built these engines from the ground up using the same design principles as open source Redis, with an in-memory architecture and an efficient codebase written in C or Rust, allowing developers to run a wide variety of data workloads with the lowest latency possible.After listening to our community and customers, we realized that we needed to simplify the developer experience of the leading Redis modules and the capabilities they provide, along with their documentation and clients that support their functionality. We wanted to help developers be fully productive from the moment they started using Redis.This is why we created Redis Stack.Redis Stack unifies the leading Redis modules in a single product. This makes it easy to start building with our Redis-based search, document, graph, and time series capabilities.Redis Stack is a suite of three components:Redis Stack is now generally available for Redis 6.2, and we also have a release candidate for Redis 7.0.We’re providing several ways to quickly get started with Redis Stack:Once you have Redis Stack Server up and running, you can immediately leverage RedisInsight to visualize, analyze and optimize your Redis data. RedisInsight includes a series of guides that walk you through several Redis Stack use cases.On the client side, we’re supporting Redis Stack in several leading Redis clients  — Jedis (Java), redis-py (Python), and node-redis (JavaScript) — and with our new object mapping libraries (redis-om-spring, redis-om-python, redis-om-node, redis-om-dotnet).You can clone an example repository for each language we support to start developing your freshly created database.We’re excited about the universe of real-time applications that we believe Redis Stack will make possible. But we’d like to be very clear that Redis Stack is not a replacement for Redis.Redis is a core, open source technology, and our focus on its continued development is not changing. You’ll always have the option to download, build, install, and run open source Redis.When you’re ready to run Redis Stack, you can easily migrate your data using the Redis replication mechanism or by loading your RDB or AOF files.All the codebase components of Redis Stack are open and free for everyone to use, but we still want to be very clear about the Redis Stack licensing model since we updated our licenses as mentioned in this blog:We’re committed to continuing to develop Redis as an open-source project, supporting one of the largest developer communities on earth, and cooperating with our growing number of active contributors to the project.We will continue adding capabilities to Redis Stack, to allow developers to quickly and easily develop modern applications for the real-time era that are entirely based on Redis.To learn more about Redis Stack and the capabilities it provides, tune into deep-dive sessions on Redis Stack delivered at our RedisDays event available now on-demand.We’ve also put together quick answers to anticipated questions –  on the short FAQ section.Finally, we’d love to get your feedback on Redis Stack. Send us a note on the Redis mailing list or join the Redis Discord server to let us know what you think.Redis Stack is a single package that includes open source Redis with the leading Redis modules (Redis Stack Server), and RedisInsight.For the initial release of Redis Stack Server, we’re including five modules: RedisJSON, RedisSearch, RedisGraph, RedisTimeSeries, and RedisBloom.Redis Stack is supported by official Redis client and object mapping libraries and allows developers to easily use the advanced Redis Stack capabilities with several application frameworks, including Spring, ASP.Net Core, Express, and FastAPI.Redis Stack lets developers:We’ll consider adding new capabilities or even modules to Redis Stack if:RedisGears adds database triggers, stream processing, distributed functions, and full programmability to Redis.We will add RedisGears to Redis Stack once the support for JavaScript is GA, later this year.Yes, you can!The Redis object-mapping libraries provide a level of abstraction above the Redis command API, much like an ORM does for a SQL database. Let’s distinguish the core Redis client libraries from Redis object mapping libraries.These object-mapping libraries always depend on one or more core Redis libraries.Currently, the recommended client for .NET developers is StackExchange, which is not formally supported by Redis, Inc. You can extend the client with NRediSearch for RediSearch, NRedisGraph for RedisGraph, and NRedisTimeSeries for RedisTimeSeries. You can also use the redis-om-dotnet library that builds on top of StackExchange.RedisInsight is not yet available on Redis Enterprise Cloud.  However, you can connect with the RedisInsight application to your cloud database. We plan to add RedisInsight to the cloud later this year.Yes, you can leverage our Replica-Of solution to migrate your database to our fully managed cloud service without any downtime."
618,https://redis.com/blog/redis-enterprise-5-4-supports-redis-streams-ga/,Redis Enterprise 5.4 supports Redis Streams GA!,"December 14, 2018",Paz Yanover,"We are happy to announce the launch of Redis Enterprise 5.4, the latest version of our high performance, in-memory database platform.The new version contains the following major enhancements:Redis Streams, introduced  in Redis 5.0, is one of the most popular products today in the Redis ecosystem. Redis Streams is a new data type that models a log data structure in-memory and implements additional powerful operations, such as:Redis Enterprise 5.4 adds support for Redis 5.0 (GA version 5.0.2) and allows our customers to enjoy the highly available, high-performance and secure database  with the new Redis Streams data type.Redis Enterprise Modules are installed with Redis Enterprise by default so that it is easy for you to deploy and use these powerful modules. With the release of Redis Enterprise 5.4, Redis Graph —  a new Redis Enterprise Module that introduces the world’s fastest graph database —  is an integral part of the Redis Enterprise package.Redis Graph joins the previous popular Redis Enterprise Modules:Redis Enterprise Modules are only available in Redis Enterprise Software and Redis Enterprise VPC deployments.Redis Enterprise 5.4 increases the flexibility of Active-Active database (CRDB) creation with the following creation options:When a master shard fails, a replica shard is automatically promoted to a master shard to maintain data availability. This creates a single point of failure until a new replica shard is manually created.Redis Enterprise 5.4 expands the high availability capabilities by adding the ability to automatically avoid this single point of failure, thereby configuring the cluster to automatically migrate the replica shard to another available node. In practice, replica migration creates a new replica shard and replicates the data from the master shard to the new replica shard.*Please note that just as is the case with the Redis open-source project, Redis is in the process of changing the “master-slave” terminology to “master-replica” everywhere, including within our documentation.In addition to all of the above updates, Redis Enterprise Software (RS) 5.4 also adds support for a new operating system — Ubuntu 18.04 64-bit.If you’re interested in a more detailed view of what else is new in Redis Enterprise 5.4, please take a look at our technical documentation or check out our release notes.If you have any questions, don’t hesitate to drop us a line at pm.group@redis.com!"
619,https://redis.com/blog/serverless-development-with-aws-lambda-and-redis-enterprise-cloud/,Serverless Development with AWS Lambda and Redis Enterprise Cloud,"December 15, 2020",Tugdual Grall and Chinmay Kulkarni,"In this blog post, you will learn how to integrate AWS Lambda and Redis Enterprise Cloud. Using a sample movie-database application, you will discover how to build and deploy two Lambda functions, one in Node.js, one in Python. These two Lambda functions are used to interact with the Redis database to insert, update, delete, and query. The application uses the RediSearch API that provides rich query and search functionalities. Serverless, using AWS Lambda, fits into the growing trend towards microservice architectures as it allows developers to reduce the scope of a business “service” into a small project that can be implemented using the programming language of their choice. To learn more, watch the video below and read on for a quick overview of AWS Lambda and a deeper dive into how to  build an application using Redis Enterprise Cloud and Lambda:If you’re not already familiar with AWS Lambda—the company’s serverless compute runtime, also known as a Function-as-a-Service (FaaS)—here are the basics you need to know. (Lambda experts can jump ahead to the next section.) AWS Lambda allows applications to run a function on demand in response to a particular event, making it a good way to build event-driven architecture (EDA) applications. AWS Lambda functions, fully managed by AWS, can be created using various programming languages. Perhaps the best part of AWS Lambda is that developers don’t usually need to fully understand the application lifecycle to use it.AWS Lambda can be invoked using several different methods: directly from the AWS Console, using events from other AWS Services such as SNS, SQS, or Kinesis, or even from the AWS Console, CloudWatch, or S3 events. In our sample movie-database application, the Lambda functions will be invoked using an HTTP REST endpoint, this is done using the AWS API Gateway.  (You can find more about AWS Lambda function invocation in the documentation.)AWS Lambda HTTP execution processSpecifically, requests via HTTP are routed to AWS’ API Gateway management tool, which parses the body, header, and parameters, and then triggers our Lambda function with this payload:As a developer, you simply write code to expose the REST endpoint and configure the deployment to expose it inside the AWS API Gateway. (We’ll discuss this further in the next section covering our sample movie-database application.)State and data management in serverless applicationsAWS Lambda is a stateless environment. In many applications, though, you still want to share state between services or calls, and Redis can help. For simple state management, AWS developers often use ElastiCache, but many applications require more than state management, they also need persistence, rich data, high performance, and a query model. Redis Enterprise Cloud provides a fully managed service on AWS (Google Cloud and Microsoft Azure are also supported).Now we’re ready to look at our sample movie-database application to see the key steps to build an application using Redis Enterprise Cloud and AWS Lambda.The application uses the dataset that has been documented in the RediSearch Getting Started tutorial, which consists of a movie catalog, made of Redis Hashes. As shown in the chart below, the frontend is built using Vue.js, which calls REST endpoints to:As mentioned above, the application leverages the AWS API Gateway, AWS Lambda, and Redis Enterprise Cloud for the datastore. In addition, the Python service uses AWS Key Management Service to store and encrypt the Redis database password.To get started, you’ll need a few prerequisites:Once you have everything assembled, let’s walk through the process of installing and running the sample application.Step 1: Get the Redis Enterprise Cloud database informationIf you have not yet created a database on Redis Enterprise Cloud, please do so using the information in our Get Started with Redis Modules guide.When you are connected to the Redis Enterprise Cloud, you can find the database connection information in the web console; be sure you add the module “RediSearch 2” to the database.You will need the following properties to connect the Lambda functions to your Redis database:Step 2: Get the project from GitHubClone the repository to your local environment, and move it to the project directoryThe project directory structure is shown here:Step 3: Import the movie-database dataset into your applicationThe file aws-redis-cloud-demo/movies-list-microservice/import_movies.redis contains all the Redis commands to insert movies into the database. The commands used are:To import the dataset, open a terminal and run the following command:Step 4: Configure the application to use your Redis Enterprise Cloud instanceBefore testing the application, you must configure the Node.js and Python services with your Redis Enterprise Cloud database instance. Open these files:Then set the Redis host, port, and password: (REDIS_HOST, REDIS_PORT, REDIS_PASSWORD)Step 5: Build and run the movie microservice (Node.js)Follow the steps listed here to build and run the project. (You can also find all the information in the project’s Readme file.)2. Go to the movies-list-microservice directory3. Install dependencies4. Run the Lambda function locally5. Test the service by opening a browser call to the REST service: http://localhost:3000/api/movies/16. Deploy the service to AWS, by running the following command to deploy the function to your AWS environment:7. Test the API by pointing your browser to https://<xxx>.execute-api.<reg>.amazonaws.com/api/movies/1Note: if you receive an error, check the functions log in AWS CloudWatch to see what happened.A deeper look at the code:Step 6: Build and run the comments microservice (Python)Here are the steps to build and run the project. (You can also find all the information in the project’s Readme file.)2. Install the dependencies:3. Set up the  AWS environment, run the following command, and configure your ID and Secret:4. Deploy the service to AWS, by running the following command to deploy the function to your AWS environment:A deeper look at the code:Optionally, you can store the Redis database password in AWS Key Management Service. You can find the configuration steps in the project documentation.Step 7: Run the frontend application2. Edit the .env.development file to set the URL of the movie and comment services:3. Run the application:4. Open your browser, and go to http://localhost:8084You can now navigate in the application, update and search movies, and add/delete comments.Optionally, you can use S3 and CloudFront to deploy the Vue application in your AWS environment and serve it publicly to your users. This is explained in the project documentation.Working with AWS Lambda and Redis Enterprise Cloud simplifies the deployment of your services. Using Redis Enterprise Cloud with RediSearch, you can easily query Redis data using values, allowing you to use Redis as the main database of your services.Redis Enterprise Cloud is compatible with Redis, allowing you to easily migrate your existing Redis deployments, both OSS and managed services. You just need to change the connection parameters (such as database endpoints). There are multiple ways to perform a live migration, including:In addition to RediSearch, Redis Enterprise Cloud lets you use other promising data models such as graph, JSON, time series, and Bloom filters, and offers a wide variety of other database features critical in production environments, including high availability, scalability, persistence, security, and Active-Active geo-distribution.Want to learn more? Check out our AWS re:Invent home page and read these tutorials and blog posts:"
620,https://redis.com/blog/3-things-you-didnt-know-about-cloud-migrations/,3 Things You Probably Didn’t Know About the Cloud,"December 6, 2021",John Noonan,"The cloud is no longer a disruptive new concept. It’s been around for well over 10 years, which may seem short but is an eternity for technology. When AWS’ nascent S3 service launched we were still doing things like calling in food delivery with our flip phones and renting movies from Blockbuster Video. Think about how much our lives have changed since then. Much of that change is thanks to the exponential innovation unlocked by the cloud.But if the cloud has been so transformative and been around for so long, how can migration still be speeding up? How do organizations still struggle with cloud operations? How can 1 in 3 migrations fail?Part of the reason for this is because cloud adoption follows a curve. There’s no defining moment when an organization is officially in the cloud. Typically, organizations move along a maturity curve from their first foray and proof of concept, maybe with a simple lift-and-shift migration, into more sophisticated adoption models as they move more applications. They may have mastered how to lift-and-shift, but are struggling with replatforming or modernizing in the cloud.Another element is data. With all this technological innovation and scale comes a lot of data that needs to be moved, stored, used, and analyzed—often in real time by a multitude of applications, across different clouds, with varying architectures, all sending data to and from users across the globe.We decided to compile all of our findings into an infographic we could share with the Redis community: “How to Migrate to the Cloud Without Disrupting Data.” Please feel free to share it, too!Here are a few of our more interesting findings:1 in 3 cloud migrations fail to deliver any tangible value. That is a staggering number when you consider the time, expense, and opportunity cost of undergoing a cloud migration. This number highlights not only the challenge of moving applications and data to the cloud, but also the difficulty in making a cultural, business, and operational shift to a new cloud model.Long gone are the days when companies would vet, select, and migrate to a single cloud provider. Today, multicloud is the new norm. Modern companies seek the operational resilience and freedom to select best-in-breed technologies from among public cloud providers.But in doing so, businesses are fragmenting their data across the multiple clouds that they are built on. This presents a serious challenge that typically leads to a sacrifice in either speed, consistency, or operational simplicity.Those companies that survive migration and realize the tremendous value of the cloud are evolving, abstracting more of the infrastructure and operational burden of their tech stacks, and maturing from IaaS to fully managed cloud services and optimized use of cloud technology (including Database-as-a-Service).These managed services unlock the power of specialization, allowing companies that are the best at operating infrastructure to handle the infrastructure and those that are best and most innovative at data to handle the cloud data layer—all while freeing the customer to focus on what they know best: their business and their applications."
621,https://redis.com/blog/how-to-build-a-rules-management-system-using-redis/,How to Build a Distributed Business Rules Management System Using Redis,"April 26, 2022",Redis Growth Team,"Speed and efficiency are the hallmarks of excellence in today’s fast-paced business environment.Organizations that are entangled in manual, rigid processes bear the weight of a cumbersome architecture and will forever be left flat-footed as business opportunities pass them by.But those who have their processes automated will be more productive, more agile, and far more likely to experience growth. And that’s why organizations need a distributed business rules management system (DBRMS).Looking to make an impact on the business community, Vishrut Kohli built his very own DBRMS, Bonsai. By using Redis, Bonsai guarantees users an application that can automate processes in real-time, maximizing productivity levels by reducing the likelihood of lags from occurring.Let’s take a look at how this application was made. But before we go any further, we’d like to point out that we also have an exciting range of innovative apps for you to check out on the Redis Launchpad.So if you enjoy this post, make sure to have a browse after!You’ll build a distributed business rule management platform that will help businesses to create, manage and implement scalable business rules across the enterprise. Many business tasks can be automated and free up additional resources that could be invested elsewhere in the business.This application will enable organizations to specify, deploy, and manage business decision rules and decision logic so that apps can consistently make intelligent decisions with speed and without human intervention.Below we’ll highlight what components you’ll need to bring this application to life as well as walk you through each step of the implementation process.Ready to get started?OK, let’s get straight into it.Ensure that you have Redis Stack database up and running:This project was bootstrapped with Create React App.yarn installRuns the app in the development mode.Open http://localhost:3000 to view it in the browser.The page will reload if you make edits.You’ll also see any lint errors in the console.Launches the test runner in the interactive watch mode.See the section about running tests for more information.Builds the app for production to the build folder.It correctly bundles React in production mode and optimizes the build for the best performance.The build is minified and the filenames include the hashes.Your app will now be ready to be deployed.See the section about deployment for more information.Note: this is a one-way operation. Once you eject, you can’t go back!If you aren’t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project.Instead, it will copy all of the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point, you’re on your own.You don’t have to ever use eject. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However, we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.git clone: ​​https://github.com/redis-developer/bonsaiKeywordsThe most logical separation of rules is on the basis of namespace. Namespace is one set of rules which are to be evaluated on a set of data. For example, later in this post, you’ll build a namespace for a loyalty management system and all of the rules regarding that will be saved under that namespace.Here namespace is: tax_system.An entity refers to the input of a namespace and will in turn provide an output after evaluating the rules present in that namespace.Say for example we wanted to create a rule that determines the tax value of a citizen who lives in the province of Ontario and in the city of Toronto. In addition, this person pays 35% income tax. So we have two variables – province and city but one output tax_rate.So in this case, one example predicate(input) of a province is Ontario can be a predicate consisting of 4 parts :Each rule is a command which decides what should be the output to any particular input (entity). Every rule will get saved into a namespace via the following format: Ontario and the city is Toronto the tax rate will be 35.This is the rule object:Here namespace is: loyalty_system and rule_id is: 123456.On the home screen click ‘Create.’Next, fill in the different fields to complete creating the rule.After creating the rules, you’ll be able to easily visualize them in the form of a flowchart.The evaluation flow contains all of the attributes that have been used to create the rules. The application has a code that uses pattern matching algorithms to see which rule best fits the entity as well as emitting the order in which rules are executed.RedisTimeSeries allows you to visualize all of the metrics and help you to make data-driven decisions based on the performance of all the rules in their namespace.TS.ADD ruleId * 1DBRMS platforms can liberate employees and businesses from having to carry out time-consuming, tedious tasks that are essential to the organization. Automating these tasks via a DBRMS platform can completely revitalize an organization by directing each of its employees’’ attention towards less menial tasks.However, for a DBRMS system to be effective, it needs to be powered by a real-time database to consistently guarantee an elite level of performance. A mere lag can stagnate process efficiency, thereby negatively impacting productivity.Thanks to Redis, all business rules were able to be managed, updated, and maintained in real-time. This allows you to gain real-time insights on performance and make instant data-led decisions that will help you capitalize on business opportunities.If you want to discover more about how Bonsai was created then make sure to watch Vishrut’s YouTube video here. But that’s not all… we also have a whole range of innovative applications for you to check out on the Redis Launchpad.Programmers from all around the world are tapping into the magic of Redis to make an impact on everyday lives.Head on over. Be inspired. And have fun with Redis.Vishrut is a full-stack developer and data enthusiast who’s proficient in Python, MATLAB/octave, and machine learning algorithms. If you want to keep up to date with all of his projects then make sure to follow him on GitHub."
622,https://redis.com/blog/how-to-create-a-real-time-mobile-banking-application-with-redis/,How to Create a Real-time Mobile Banking Application With Redis,"March 29, 2022",Growth Team,"Fast, accurate, and up to date, RedisBank provides you with instant access to your finances with just a few swipes on your smartphone. Since mobile banking has propelled itself forward from the financial fringes and into the mainstream, we’ve all gotten used to the idea of having our entire banking information in our pockets wherever we go.In response to this transition towards digital, Lars Rosenquist has built an application that allows you to create your very own mobile banking application. By using Redis, all of your financial information will be available in real-time, allowing you to monitor, record, and plan your finances accurately.Let’s take a look at how Lars managed to put this application together. But before we go into the nuts and bolts of this app, we’d like to point out that we have an exciting range of applications for you to check out on the Redis Launchpad.So make sure to have a peak after this post!You’ll build an online banking application that will provide you with real-time updates on your bank account. With RedisBank, you’ll be able to keep a close eye on your spending and adopt  a healthier relationship with money.Below we’ll walk you through each step of the building process and highlight all of the required components to bring this application to life.Ready to get started? Ok, let’s dive straight in.This app also uses a range of Redis core data structures and modules. These include:This is a SpringBoot application that consists of APIs and a front end.ResultsNavigate to http://localhost:8080 and login using the following credentials:Please refer to this file for the credentials as these values are hard-coded for demonstration purpose.Below is a screenshot of what the login page should look like.Once you log in, you’ll land on the main portal page (see below).From here, you’ll be able to get a tighter grip on your finances through a number of different portal features. These include:Redis Streams is an append-only log. The data generation app will put transactions on the Stream and the application will subscribe to this Stream and will consume any income transactions and send them to the frontend over a Websocket connection.The first two interfaces allow you to implement the afterPropertiesSet()method and the destroy()method which you’ll use for the setup and cleanup of your subscription to the Redis Stream.The third one makes you implement the nMessage(MapRecord<String,String, String> message) which is the callback that will be executed whenever a new bank transaction comes in via the Redis Stream.At this point you’ll be subscribed to the Redis Stream. Whenever a new message arrives on the Stream, the onMessage(…) method of this class will be called. You’ll still need to carry out this method, so let’s look over how this is done.Up to now we’ve created a MessageListener and subscribed to Redis Streams of BankTransactions and, for each incoming BankTransaction, forward the BankTransaction to the Stomp/Websocket topic.Now let’s see how this works in our application. Firstly, run the data generation app in a terminal window. After that one is running, in a separate terminal window, build and run the app:Both should be running at this point, so navigate to http://localhost:8080 and log in using username/password. You should have access to an overview of transactions as well as a few non-optimized areas of the app (we’ll hop onto this shortly).You should see new transactions appearing approximately every ten seconds. If not, check your source code and see whether you’ve missed any annotations. Now let’s go over how you can add the search feature.Note: When adding imports, choose the com.redislabs imports.The first statement creates a RediSearch connection that uses the RediSearch client library. The second one creates SearchOptions based on your preferences. The third statement executes the search and returns the results.The data generation app also populates a TimeSeries for the bank balance of each account. This is updated every time there’s a new transaction. You can query this TimeSeries and pass the result to the user interface so it can show the balance over time in a visual diagram. Now let’s get started:What you’re essentially doing here is asking for the time series values stored under the key of BALANCE_TS between now and (now -1 week). You’ll then need to copy the results into a Balance array and return it.You may notice that you’ll be using the Principal name here to suffix the TimeSeries key. This means that if you log in with a different user, it will be a different key.Note: in this example we didn’t use an additional client library for a reason. Instead, we extended the Lettuce library and provided an interface that we injected. Extending Lettuce is a fantastic way of adding Redis Module functionality into your app whilst limiting the amount of dependencies.Adding this feature onto the app will show the biggest deductors from your bank account. Because of this the data-generation app is populating a Sorted Set. For every transaction, it will add the value of the transaction to a member of the sorted set of which the key is the counter-account’s name as well as the total amount of transactions for that account.This will retrieve the members of the Sorted Set by their score from 0 up to Double.MAX_VALUE. You’ll then put the result in a BiggestSpenders object.You should now be running, so navigate to http://localhost:8080 and log in using username/password. At this point you’ll now see an overview of transactions.Redis StreamsThe data generation app generates a bank transaction every 10 seconds or so and puts it in a Stream. You can query the stream using:This will provide all of the entries on the transactions_username Stream. Remember that your app subscribed to this Stream via Spring Data, so every time a new element is added to the Stream it will be processed by the app and sent to the user interface.Sorted SetThe data generation app also adds every transaction to a Sorted Set. This will add the transaction amount to the score, adding up the totalled transaction amount per account name and storing it in a sorted set. You’ll then be able to identify the biggest spenders by simply querying the Sorted Set like this:TimeSeriesThe data generation app also adds the bank accounts balance to a TimeSeries every time there is a new transaction being generated. This allows you to view the balance over time by using a query such as:HashesEach transaction is stored as a Hash in Redis so it can be indexed by RediSearch. You’ll be able to search for each transaction just like in the example below:Session DataSession data is also stored in Redis. This requires no code from your end whatsoever – adding the dependencies is enough.Use the below code to get the application logins:Note: The project is compiled with JDK11 since it’s the max LTS version that’s supported by Azure Spring Cloud. The project will also be able to run locally or on other platforms up to JDK16.Below are some known issues that you may come across:Mobile banking is the new norm and it has to be fast, easy, and accurate for it to be effective. The digital era has made us all accustomed to seamless digital interactions that are updated in real-time, and failing to meet these expectations will create a sub-optimal application that’s destined for the rocks.But through its advanced capabilities and low latency, Redis is able to guarantee real-time data transmission, eliminating the likelihood of any lags from occurring. From just a few taps on their mobile device, users can access their banking information and gain a holistic and accurate view of their finances.To gain a more visual insight into how this application was created, make sure you check out Lars’s YouTube video here.And if you’ve enjoyed this post, make sure to visit the Redis Launchpad where you’ll have access to a whole variety of different applications that are making an impact on everyday lives.Check it out. Be inspired. And join in the Redis fun!Lars has a broad range of experience in the software industry but currently works as a solution architect manager to help others reach the best of their abilities. Make sure to follow his GitHub page to stay up to date with all of his projects."
623,https://redis.com/blog/how-to-build-an-app-that-organizes-and-shares-your-movie-collection-with-friends-using-fastapi-and-react-js/,How to Build an App that Organizes and Shares Your Movie Collection With Friends Using FastAPI and React.js,"February 21, 2022",Growth Team,"Search for movies. Organize your collection. And share with your friends. Movify opens the door to exciting movie experiences by creating a social media-like platform where interactions are solely film-based. Here you can view the activity of people in your network and see what movies they’re watching, liking, and sharing.Redis’ advanced capabilities maximized the user’s experience through its ability to transmit data between components at lightning speed. Achieving this consistently optimized the application’s functionality by keeping users engaged. This allowed commands and social interactions on the platform to be carried out in real-time, mirroring the pace of real-life events.Let’s take a look at how this application was made in more detail. But before we go any further, we’d like to highlight that we have a great variety of innovative apps for you to discover on the Redis Launchpad.So if you enjoy this post, make sure to have a browse after!You’ll build a social platform that allows people to connect all around the world based on movie preferences. This involves managing your own movie collections, liking other people’s movie activity as well as creating a fun and easy way to discover movies that are hidden from the mainstream spotlight.Below we’ll highlight what components you’ll need to bring this application to life as well as walk you through each step of the implementation process.Ready to get started?OK, let’s get straight into it.What will you need?Install the below software on your local machine:To launch the app locally, you’ll need a docker-compose. For security reasons, backend/.env is not on this repo. Since we rely on Google Sign-in for authentication, you’ll need to create a Web client and register http://127.0.0.1:8000/api/auth as an authorized redirect URI.Next you’ll have to generate a random string for the encryption openssl rand -hex 16. And finally you’ll need to create a TMDB account and get an API key.You can now fill the .env file as follows:The Docker compose file defines the services that make up your app. In this case, there are 3 services – frontend, backend, and Redis.The script run.sh will build the images and start the containers.This project was bootstrapped with Create React App.In the project directory, you can run the following commands:This runs the app in the development mode.Open http://localhost:3000 to view it in the browser.The page will reload if you make edits. You’ll also see any lint errors in the console.This command launches the test runner in the interactive watch mode. You can visit the section about running tests for more information.Builds the app for production to the  build folder. This correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. At this point, your app will be ready to be deployed!See the section about deployment for more information.Note: This is a one-way operation. Once you eject, you can’t go back!If you aren’t satisfied with the build tool and configuration choices, then you can always eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them.All of the commands except eject will still work. However, they will point to the copied scripts so you can tweak them. At this point, you’re on your own.What’s also important to know is that you don’t ever have to use eject. The curated feature set is suitable for small and middle deployments, so you shouldn’t feel obligated to use this feature.Use the TMDB API to search and retrieve new movies that are hidden amongst a huge collection of films. The results are cached by the server with an LRU policy. User-generated data is stored in a graph and in JSON clusters. Here, everything can be updated with the JSON.There are three types of nodes in this data model:There are three types of edges:To create those classes we made the abstract nodes and upside edges which both make it very simple to work with CRUD operations.Having these collections will provide you with flexible and efficient querying. The metadata associated with users and collections is stored as JSON.Below we have abstract classes for CRUD operations. This makes it very simple to create certain types of nodes and get all of the CRUD operations in a single line of code.However, for queries we prefer to use cypher.Everything has been timestamped. This was very handy when we wanted to add the friends’ activity view – which was the addition of a simple cypher query on our already timestamped relations.The homepage is where a friend’s recent activity is displayed. Here you’ll have a share link to connect with other users on the platform.Movies are organized in collections which are then shared with your friends.Searching for movies is done by using external APIs and responses are cached using an LRU eviction policy.To get additional information about a movie, you may need to make a couple more calls. Here we’re using TMDB API to request information about a movie. Another one is made gather information about the crew and cast members.One of the most frustrating endeavors any movie fan can relate to is trying to discover captivating movies that are concealed from the spotlight. These hidden gems are hard to locate and often require a lot of time investment to dig them out from underneath the mainstream surface.But with Movify, fans can navigate more efficiently to their favorite movies by connecting with people with  similar POVs and cinematic tastes, enabling users to share. Their activities and see what films others are liking, sharing, and watching.From a performance perspective, Redis’ powerful caching capabilities guaranteed low latency, allowing users to interact with each other in real-time. This removes the likelihood of any lags from occurring and contaminating the user experience, enabling users to seamlessly interact with other movie fans seamlessly.If you want to discover more about how this application was made, watch the YouTube video here. But that’s not all. We also have a whole range of innovative applications for you to check out on the Redis Launchpad.Programmers from all around the world are unlocking their creative potential by using Redis and you can too.Head on over. Be inspired. And have fun with Redis.Despite still being a student, Matthias has a whole range of experience in programming, artificial intelligence, machine learning, and much more. To stay up date with all of the projects he’s involved in, check out his GitHub page here."
624,https://redis.com/blog/feast-with-redis-tutorial-for-machine-learning/,Getting Started on Feast With Redis: Machine Learning Feature Store Quickstart Tutorial,"December 15, 2021",Nava Levy,"This tutorial provides a step-by-step Feast for Redis quickstart that walks you through an end-to-end example of using Feast with Redis as its online feature store for machine learning. It’s based on the Feast Quickstart tutorial, but instead of using the default online store, it uses the Redis online store for delivering real-time predictions at scale. If you’re not familiar with Feast or Redis, then the fastest way to get started with Feast using Redis is through this tutorial. Please refer to this Feature Stores and Feast using Redis blog article for a high-level introduction. More detailed information on Redis and Feast, as well as additional resources, are available at the end of this tutorial.In this tutorial you will:You can run the tutorial on Google Colab or on your localhost following the guided steps below.Feast (Feature store) is an open source feature store and is part of the Linux Foundation AI & Data Foundation. It can serve feature data to models from a low-latency online store (for real-time serving) or an offline store (for model training or batch serving). It also provides a central registry so machine learning engineers and data scientists can discover the relevant features for ML use cases. Below is a high-level architecture of Feast:Feast is a Python library + optional CLI. You can install Feast using pip, as will be described soon in this tutorial.Feast with Redis solves several common issues in this flow:In this tutorial, we use feature stores to generate training data and power online model inference for a ride-sharing driver satisfaction prediction model. In the demo data scenario:To install Redis follow one of the alternatives below:Ubuntu:Docker:Mac (Homebrew):Additional information on alternative ways for installing Redis can be found here. Additional configuration information can be found in the Redis Quick Start guide.Install the Feast SDK and CLI using pip:A feature repository is a directory that contains the configuration of the feature store and individual features.The easiest way to create a new feature repository is to use the feast init command. This creates a scaffolding with initial demo data.Output:Let’s take a look at the resulting demo repo itself. It breaks down into:To configure Redis as the online store we need to set the type and connection_string values for online_store in feature_store.yaml as follows:The provider defines where the raw data exists (for generating training data and feature values for serving) in this demo, locally. The online_store defines where to materialize ( load) feature values in the online store database (for serving).Note that the above configuration is different from the default YAML file provided for the tutorial that instead uses the default online store :So by adding these two lines for online_store (type: redis, connection_string: localhost:6379) in the YAML file per the above, Feast is then able to read and write from Redis as its online store. Redis Online Store is part of the Feast core code, and as such, Feast knows how to use Redis out-of-the-box.Let’s take a look at the demo feature definitions at example.py (to view it in your terminal you can run cat example.py ).Example.pyFinally, let’s inspect the raw data. The raw data we have in this demo is stored in a local parquet file. The dataset captures the hourly stats of a driver in a ride-sharing app.Now we run feast apply to register the feature views and entities defined in example.py. The apply command scans Python files in the current directory for feature view/entity definitions, registers the objects, and deploys infrastructure. In this example, it reads example.py (shown above) and sets up the Redis online store.$ feast applyOutput:To train a model, we need features and labels. Often, this label data is stored separately (e.g. you have one table storing user survey results and another set of tables with feature values).The user can query that table of labels with timestamps and pass that into Feast as an entity dataframe for training data generation. In many cases, Feast will also intelligently join relevant tables to create the relevant feature vectors.Python(Copy the code below into a file gen_train_data.py and then run it):Output:We will now load or materialize feature data into your Redis online store so we can serve the latest features to models for online prediction. The materialize command allows users to materialize features over a specific historical time range into the online store. It will query the batch sources for all feature views over the provided time range, and load the latest feature values into the configured online store. materialize–incremental command will only ingest new data that has arrived in the offline store, since the last materialize call.$ CURRENT_TIME=$(date -u +""%Y-%m-%dT%H:%M:%S"")$ feast materialize-incremental $CURRENT_TIMEOutput:At inference time, we need to quickly read the latest feature values for different drivers (which otherwise might have existed only in batch sources) from Redis online feature store, using get_online_features(). These feature vectors can then be fed to the model.Python(Copy the code below into a file get_feature_vectors.py and then run it):OutputCongratulations! You have reached the end of the tutorial. To shut down the Redis server that is running in the background you can use the redis-cli shutdown command.In this tutorial you’ve deployed a local feature store with a Parquet file offline store and Redis online store. You then built a training dataset using time series features from Parquet files. Then, you materialized feature values from the offline store into the Redis online store. Finally, you read the latest features from the Redis online store for inference. With Redis as the online store you can read the latest feature very quickly for real-time ML use cases, with low latency and high throughput at scale.Join other Feast users and contributors in Slack and become part of the community!"
625,https://redis.com/blog/launchpad-building-zindagi-blood-donor-app/,How to Build an App that Connects Blood Donors with Patients by Using Redis,"April 18, 2022",Growth Team,"Giving blood is an easy and safe way to save a life. Yet, complications still exist in matching donors and patients with the right blood type. Time is a limited commodity when it comes to blood donations, making it absolutely fundamental to align donors with the right patients.The more efficient this process is, the more lives are saved. Taking on this challenge was Bhanu Korthiwada, who created a phenomenal application, Zindagi, that speeds up the entire blood donation process by matching blood donors with the ideal patients.At the heart of this application was a fundamental need for data to be transmitted with maximum efficiency to provide users with updates in real-time. A lag or a delay would hamper the user’s experience and not keep up to speed with the fast-paced demands for blood donations.Due to these demands, Redis was used as the application’s main database, which had a dramatic impact on performance. Data was transmitted with maximum efficiency. Users received real-time updates. And blood donations became seamless.Let’s take a look at how Bhanu created this application. But before we examine the ins and outs of this app, we’d like to point out that we have an exciting range of other apps for you to check out on the Redis Launchpad.So make sure to have a browse after this post!You’ll build an application that will match blood donors with patients who have the same blood type. This will promote a more seamless and efficient blood donation process that saves time to save lives.Below we’ll go through the A-Z of what’s required to bring this application to life, as well as highlight what components you’ll need. From start to finish, we’ll break everything down into bite-sized steps to make building this application as easy as possible.Ready to get started?Ok, let’s dive straight in.RedisJSON: Implements ECMA-404, the JSON Data Interchange Standard, as a native data type.RediSearch: Provides advanced querying, secondary indexing, and full-text search for Redis.Redis Pub/Sub: Used for event messaging and can provide messages to any number of subscribers on a channel.Telerik: Provides an array of software tools for web, mobile, desktop application, development, and much more.Blazor: Used as a free open source web framework that allows developers to build web apps using C# and HTML..NET Core Runtime – Provides basic services for internet-connected appsPrerequisiteUpdate .env file with Auth0 and SMTP detailsThe above Compose file defines two basic services:Redismod has in-built modules like RedisJSON and RediSearch that are used for this project. You’ll need to pass a number of auth0 environmental variables. SMTP remains optional. For persistence, Docker volumes mount has been added and the app is exposed to port 80.From terminal/command prompt run docker-compose up -dThe application can be accessed using localhost.To get the full benefits of the application, blood donors have to create an account. You can do this on the main dashboard by clicking on ‘Register Now.’Click on the blood donation button at the top of the navigation bar. You’ll then have a number of fields to fill in that will inform the database about your blood type and the quantity you want to donate.Once you’ve created an account, patients who are the right fit for your blood donation will be notified of your availability. They’ll then be able to send you a request for your blood donation. To access your full list of requests, click on the ‘Requests’ tab at the top of the navigation bar. Here you’ll have a complete overview of all the requests patients have sent for your donation.Fast access to blood is the difference between life and death for many patients. Matching donors with the right patients is often a time-consuming process, where every second jeopardizes the patient. Bhanu’s application helps to remove this obstacle through Redis’ ability to send data between components at lightning-fast speed.Having data transmitted with such efficiency allows Zindagi to quickly match blood donors with the right patients based on the given criteria. This accelerates the entire blood donation process, enabling donors and patients to interact with each other in real-time and arrange a possible blood donation.At the heart of this application is an ability to pull all parties together, providing the optimum direction for progress. This means more donations, less time wasted, and a totally seamless experience for everyone involved.To get more of a visual insight into how this application works, check out this YouTube video. If you enjoyed this post, make sure to check out the Redis Launchpad where you’ll have access to a wide variety of innovative applications that are having an impact on everyday life.We have applications that track buses in real-time on a map. We have applications that prevent supply shortages in hospitals in developing nations. And we have so much more for you to discover.So make sure to check them out!Bhanu KorthiwadaBhanu is an experienced software engineer who’s currently working as a senior consultant for ADP. If you want to keep up to date with all of his latest projects, make sure to follow him on GitHub."
626,https://redis.com/blog/introducing-redis-om-net/,Introducing Redis OM .NET,"December 8, 2021",Steve Lorello,"Redis is an awesome technology loved by millions of developers for two reasons: speed and simplicity. Redis provides an intuitive native interface logically organized into data structures that programmers already know. Moreover, those structures are simple to use and terrifically optimized. It’s this ultimate blend of speed and simplicity that we are angling at with the release of a new client library for .NET, Redis OM.Redis OM is object mapping and more for Redis. The motivation behind Redis OM is to answer the question “How can developers get amazing leverage out of Redis without learning all the Redis commands?” For this first  preview release, we’re looking to address the question, “How can we store and find our domain objects in Redis?” The preview release of Redis OM .NET is an object mapper, a secondary index builder, and a simplified and powerful query builder. All of this is geared towards helping you store and find your domain objects with Redis.The Module API has been a  game changer for Redis. The modules add a tremendous amount of flexibility to the platform while providing some essential features for Redis. RedisJSON, in particular, unlocks an enormous amount of functionality, particularly around secondary indexing and document modeling. While Redis OM works well with core Redis, Redis OM really hits its stride when you add RedisJSON, taking full advantage of RedisJSON to create a rich and powerful API for modeling objects and querying them in Redis.The preview version of Redis OM .NET has four primary capabilities:Before getting up and running with Redis OM, you need to install the package in your project. To do so, just run dotnet add package Redis.OM. The primary connection logic within the Redis OM library exists in the RedisConnectionProvider. This provider gives access to three different objects you can use to communicate with Redis:To connect Redis OM to Redis in ASP.NET Core, you should inject an instance of the RedisConnectionProvider instance as a singleton. To do this, you’ll use a Redis URI. If you’re using .NET 6, this means opening your program.cs file and adding:For .NET 5, which uses the Startup.cs file, you can add the following to Startup.ConfigureServices:RedisJSON lets you store JSON objects in Redis natively and query those objects. But to query your JSON, you need to first define your indexes. To make this process easier, we’ve introduced a declarative model in Redis OM .NET to allow you to define these indexes through a declarative interface. If you want to declare a class to store in Redis and index its properties, you decorate the class with a Document attribute and then dress the individual properties with the Indexed or the Searchable attributes. Indexed implies a standard index, whereas Searchable applies only to strings and means that the property will be queryable with full-text search. Next, you can create the index in Redis by passing your newly decorated type into the IRedisConnection.CreateIndex method. So, for example, if you wanted to declare a Customer class with an index, you would do something like the following:With an index created, you can now use RedisCollection<T> to query objects in Redis. So, if you had John stored in Redis as a Customer object, you can then query him:You can also easily do range queries. For example, let’s find all the Customers who have not reached retirement age:With the RedisCollection and RedisJSON, you can build rich and complex queries as you ordinarily would in LINQ, and Redis OM will manage the translation to Redis’ query language for you.In addition to querying, you can use Redis Aggregations to build aggregation pipelines that can do all sorts of things. For example, suppose you want to find the age of each customer in 3 years, you can do so with a straightforward arithmetic operation inside an Apply expression, which will translate to a properly formatted aggregation pipeline in Redis:You can also group records together and calculate summary statistics on them. So if you want to group customers by their last name, you can then calculate summary statistics. For example, here’s how you can calculate average age:This is just a brief overview of the capabilities of Redis OM for .NET. If you want to learn more, you can check out the tutorial and look through the API docs. We plan on adding more features to Redis.OM soon, and, to that end, we would love some community feedback. Come and try it out. If you find a problem or think of something you’d like to see added to the library, open an issue in GitHub. And naturally, we always love contributions from the community, so PRs are always welcome :)."
627,https://redis.com/blog/introducing-redis-om-for-node-js/,Introducing Redis OM for Node.js,"December 8, 2021",Guy Royse,"I wrote a thing—well, actually several of us wrote things—I just wrote the Node.js version. I think it’s a pretty cool thing that’s worthy of your attention, and I’m excited about it. The thing I’m talking about: Redis OM for Node.js.What’s Redis OM? Redis OM is a library that makes it easy to use Redis by mapping Redis data structures straight to your code. The OM bit stands for object mapping, but we plan to do more than just that in the future.Read on for more details.You probably already know that Redis is fast—faster than fast—and it stores the sorts of things your programs care about: hashes, lists, sets, things like that. These are the structures I think in as a programmer. When I’m explaining what Redis is to an uninitiated techie, I tell them that Redis is all those data structures you learned about in college with a wire-protocol plopped in front of them.Features make Redis even awesomer (yes, I know that’s not a word) by introducing new data structures for your programs to care about. Probabilistic adds probabilistic data structures like Bloom and Cuckoo filters.But there are two modules in particular that make Redis the powerful in-memory database every programmer wants it to be: JSON and Search and Query.JSON brings hierarchy to the table. Hashes are great, but what if you want to embed hashes in your hash? JSON has you covered by letting you store my hierarchical data in JSON documents.Meanwhile, Search and Query lets you find the data structures you care about. Sure, you can create manual indices using sets, but that approach is limited and manual. Yuck. Search and Query lets you write queries to go right to the data you want.Combined, Search and Query and JSON make Redis a pretty nifty document database. You get the hierarchy you want and the ability to find the things in it that you care about. Best of both worlds and all that.In fact, we think this is such a tasty combination, we’re rolling them together and just calling them JSON.The query language for Search and Query is powerful. Very powerful. It allows you to search Hashes and JSON documents within Redis in varied and sophisticated ways. And when I need all that power, it’s very cool. But sometimes I don’t need all that power. And I am a lazy developer—I want it to be as easy as possible (and no easier).Making things as easy as possible is what I was trying to do when I wrote Redis OM for Node.js. Redis OM makes it simple to add Redis to your Node.js application by mapping hashes and JSON documents to classes that you define. No complex commands, just pure code with a fluent interface. Take a look.Define an entity:Create a new entity and save it:Search for matching entities:Now I’m fairly biased since I wrote this library and all, but I think that’s pretty dadgum cool! Let’s take a closer look at this syntax and learn how it works.There are four classes you need to care about in Redis OM for Node.js. They are Entity, Schema, Client, and Repository.Entities are the classes that you work with. The things being created, read, updated, and deleted. The things being searched for. Any class that extends Entity is an entity. Usually, you’ll define an entity with a single line of code, but you can add custom logic in there as well:Schemas define the fields on your entity, their types, and how they are mapped internally to Redis. By default, entities map to Hashes in Redis, but you can also map them to JSON documents:When you create a Schema, it modifies the Entity you handed it, adding getters and setters for the properties you define. The type those getters and setters accept and return are defined with the type property above.Clients connect you to Redis. A Client has methods to open, close, and execute raw commands against Redis. You’ll mostly use open and close:A Schema and a Client are required to instantiate a Repository. Repositories provide the means to read, write, and remove entities. And the means to search for them:Once we have a repository, we can use it to create, read, update, and delete entities. Here I’m doing just that with a favorite album of mine by Mushroomhead:We can also use it to search for entities:And, for your copy-and-pasting convenience, here’s that sample as one big block of code:And that’s Redis OM for Node.js in, as they say, a nutshell.This has been a quick overview of what Redis OM for Node.js can do. If you’d like to learn more, there’s a tutorial that will guide you through building a simple service with Redis OM. And if you want to go deeper, and you totally should, check out the README and API docs on GitHub.Of course, this is brand new software that hasn’t been tested in the wild yet. You are that wild. Please try it out. Kick the tires. Try to break it. See what’s there and what could be there. And when you find that bug or think of that perfect feature, let me know by opening an issue or sending a pull request. Your help in making Redis OM better is sincerely appreciated. Thanks!"
628,https://redis.com/blog/how-to-create-a-healthy-lifestyle-app-using-spring-boot-and-redis/,How to Create a Healthy Lifestyle App Using Spring Boot and Redis,"March 21, 2022",Growth Team,"It’s not easy training your body out of bad habits. Whether that’s having a poor diet or slacking off at the gym, or ignoring it altogether, much of it comes down to mindset. Personal development is a trend that’s galvanized many of us to shake off bad habits and transform ourselves into new and better people.But invoking serious change begins with measuring your progress to determine whether the two steps you’ve made have taken you closer or further away from your goal. Without any forms of measurement, the journey can become rudderless.Helping to keep you on the right track is ABettaMe – a healthy lifestyle app that allows you to measure different variables to see how far you’ve progressed with your goals. Through this application, you can experiment with different diets and exercises to see what impact they have on your health.Redis was leveraged to streamline data transmission between components and to create an application that was highly responsive to the user’s commands. Let’s take a look at how this application was put together.But before we go any further, we’d like to point out that you can discover a variety of innovative applications just like ABettaMe on the Redis Launchpad.So make sure to check them out after this post!You’ll build an application that will help users make important lifestyle changes by allowing them to measure progress in granular detail. Below we’ll show you how to bring this application to life by walking you through each stage of the implementation process, along with the components you’ll need.Ready to get started?Ok, let’s dive straight in.This project is composed of five different components. In each section, we’ll highlight all of the different technologies and tools required to build the application.Google Protocol Buffers: used as Google’s language-neutral, platform-neutral extensible mechanism for serializing structured data.git clone: https://github.com/redis-developer/abetta-uxOpen the docker-compose file that specifies the various services required to build this application:Use the following command to start the docker:Next, open a browser and enter the following link: http://localhost:8080/Let’s deep dive into each of the modules  that build this application.UI interface to manage existing experiments, or record new ones.Technologies used: TypeScript, ReactThe below Dockerfile is used to build UX Docker image –Application gateway that routes traffic to appropriate downstream services.Technologies used: Java, Spring Cloud GatewayRepo link: abetta-gatewayFile: https://github.com/ABettaMe/abetta-gateway/blob/main/build-image.shThe build-image.sh script is used to build AbettaMe gateway.Recommendation service that analyses experiments & performs statistical significance testing.Technologies used: C#, .NET 5, RedisJSON, Redis Pub/SubRepo link: abetta-recFile: https://github.com/ABettaMe/abetta-rec/blob/main/heroku-release.shdocker build -t abetta-rec .heroku container:push -a abetta-rec webheroku container:release -a abetta-rec webRepository holding all contracts shared between different services.Technologies used: Google Protocol BuffersRepo link: abetta-contractFile: https://github.com/ABettaMe/abetta-contract/blob/main/build.shThis is one of the most important aspects of the application, where you’re able to test new things and see the impact of what different changes are having on your life. First, click on the ‘Create new experiment’ icon on the dashboard.You’ll then need to fill out some information about your experiment in the given fields, including:Treatment: Insert information on what tactics you’ll follow to execute your strategy.On the dashboard, you’ll see an overview of your experiment. Click on ‘Add metric’ to log your latest results. You’ll then be taken to a new page where you’ll have a number of fields to fill in, including date, metric, and value (see below).On the left-hand side of the screen, you’ll see the different experiments you have listed. Once you’ve clicked on the experiment you want to view, you’ll see a graph that will give you a visual representation of your progress.The application will also list any milestones achieved that are representative of how well you’re progressing towards your goals (see below).Measuring progress is a fundamental aspect of personal development. It allows you to discover whether you’ve made two steps forward, two steps back, or at a complete standstill. But for any measurement system to be effective, it needs to be fast, accurate, and easily accessible.Redis’ advanced data processing capabilities guaranteed a highly-responsive application where users were able to interact with it in real time without any lags, freezes, or delays. New metrics were saved on the RedisJSON store and were published through the RedisPubSub module with hyper-efficiency, providing users with a seamless application experience.To discover more about how this application was made then make sure to watch this YouTube video here. And if you’ve enjoyed this post, then head on over to the Redis Launchpad where you’ll have access to many more exciting applications that are having an impact on everyday life.We have real-time vehicle tracking systems, an app that accelerates the blood donation process, and much much more!Head on over. Check it out. And have fun with Redis.Antonis is an innovative software engineer who currently dedicates his expertise to Spotify. If you want to keep up to date with all of his projects then make sure to follow him on GitHub here."
629,https://redis.com/blog/introducing-redis-om-spring/,Introducing Redis OM Spring,"December 8, 2021",Brian Sam-Bodden,"Redis OM Spring is a new client library designed to help you model your domain and persist data to Redis in your Spring applications. The library works with open source Redis but provides many additional indexing and querying capabilities when used with RedisJSON.Redis, combined with its modules, makes for one of the fastest data platforms on the planet. But it hasn’t always been easy to take advantage of its speed. Redis OM Spring provides a compelling developer experience, and in this post I’m going to show you what you can build with it.It’s rare, as a developer, to stumble upon a technology that’s both powerful and simple. With OSS Redis alone, you have a host of data structures and associated commands to perform many of the data-oriented and messaging tasks needed by enterprise applications.Similarly, in the world of enterprise Java development, Spring has, over the last 18 years, tamed the complexity of building large, feature-rich applications. In this space, Redis has been supported by the amazing Spring Data Redis, which provides both low-level and high-level abstractions for interacting with OSS Redis.One of the challenges in using Spring Data Redis is that it doesn’t support the Redis modules. Here at Redis, we develop several game-changing Redis modules that enrich the Redis core data structures with search, JSON indexing and querying, graph data, time series data, and a complete framework for server-side computation (Redis Gears). We built Redis OM Spring to take advantage of these modules and provide an even broader set of capabilities.The Redis OM family of client libraries aims to provide high-level abstractions idiomatically implemented for your language/platform of choice. We currently cater to the Node.js, Python, .NET, and Spring communities. Redis OM Spring lets developers easily add the power of Redis to their Spring Boot applications.Specifically, Redis OM Spring provides a robust repository and custom object-mapping abstractions built on the amazing Spring Data Redis (SDR) framework.The current preview release provides all of the Spring Data Redis capabilities, plus:To map Java objects to JSON documents stored in Redis using RedisJSON, you can use the @Document annotation. Like any other Spring Data entity mapping annotation, you add it to the class declaration. For example, imagine you want to map objects of type Company. Simply add @Document as shown below:You might notice that the body of this class has several annotations. @Id comes from Spring Data, and it declares the id property as the Redis key that will store our JSON document.Arguably the most popular feature of the Spring Data family of libraries is declarative Data Repositories. The next two annotations let you use RediSearch to index JSON documents. This is exposed via the RedisDocumentRepository interface:The empty repository declaration is all you need to get basic CRUD functionality/pagination and sorting for your POJOs.Redis OM Spring uses the POJO fields annotated with @Indexed or @Searchable to build the index schema. In the case of the Company POJO, we have a name property annotated as “searchable”, which means we get full-text search capabilities over that field. This is reflected in the schema field definition $.name AS name TEXT.On the other hand, the field tags is annotated as “indexable,” which means we get an index field of type TAG, meaning that we can search for Companies by the exact value of the field. This is, again, reflected in the schema field definition: $.tags[*] AS tags TAGPerhaps the most compelling feature of Redis OM Spring is its ability to create repository implementations automatically, at runtime, from a repository interface. With the proper fields being indexed, you can now fulfill all of the queries below without having to write any additional code:The repository proxy has two ways to derive a store-specific query from the method name:Let’s examine a couple of the method declarations added to the repository interface.Redis OM Spring uses the method name and parameters to generate the appropriate query. It uses the method return type to determine how to package and return the result.findOneByName returns an Optional of Company. findOne also implies that only one result will be returned, even if there are multiple results. The library parses the method name to determine the number of expected parameters. For example, the ByName portion of the method tells us we expect a single parameter named name.Redis OM Spring supports the GeoJSON types to store geospatial data. By using the near keyword in our queries, we’re telling our code to expect a Point (org.springframework.data.geo.Point) and a Distance (org.springframework.data.geo.Distance) type as parameters.As with other Spring Data Repositories, you can inject a repository into another Spring component:This is just a preview release of Redis OM Spring. There’s still much more to build, but for now we need your help! You can check out the README for details on how to install the project in your Spring application. We welcome your feedback, PRs, and issues!"
630,https://redis.com/blog/introducing-redis-om-for-python/,Introducing Redis OM for Python,"December 8, 2021",Andrew Brookins,"I’m excited to introduce Redis OM for Python, a powerful new developer-centric library for Redis that gives you features like object mapping, data validation, and more.This preview release of Redis OM for Python lets you model data with declarative models that will feel right at home to users of object-relational mappers (ORMs) like SQLAlchemy, Peewee, and the Django ORM.But there’s more! Every Redis OM model is also a Pydantic model, so you can use Pydantic’s robust and extensible data validation features. Moreover, Redis OM models will work anywhere that a Python library expects Pydantic models. So, you can use a Redis OM model with FastAPI to automatically validate and generate API documentation for API endpoints.Another feature dear to my heart is that Redis OM supports fluent query expressions and secondary indexes. Redis OM for Python also supports both asynchronous (asyncio) and synchronous programming in the same library. And the list of awesome features goes on!Continue reading to learn about how I built this library and some behind-the-scenes details about a few key features. Or, if you’re ready to play with the code, check out the getting started tutorial.Developers usually access Redis through a client library to create Redis data structures (e.g., Hashes) and then commands against them.Many people enjoy this command-based interface because it’s simpler than writing SQL queries with a relational database. But when was the last time you wrote SQL? Developers using modern web frameworks tend to use ORMs instead, especially with declarative models.What I love about ORMs is that they remove a lot of complexity that isn’t related to the problem you’re trying to solve. We created Redis OM so you can finally have the same experience with Redis.Redis OM for Python includes two base model classes that you can use to build your models: HashModel and JsonModel.Open-source Redis users can use HashModel to store data in Redis as Hashes, while users who have the RedisJSON Redis module installed or are using Redis Enterprise Cloud or Software can use JsonModel to store data natively as JSON objects. I’ll talk more about the difference between these classes later, but for now, we’ll use HashModel.Take a look at this example Redis OM code that defines a Customer model and uses it to save data to Redis:This concise model definition immediately gives you methods like `get()` and `save()`. Behind the scenes, these methods manage data in Redis Hashes.Redis OM does much more though. It also generates globally unique and sortable primary keys. This part is super useful, so let me explain how it works.Redis OM automatically generates a globally unique primary key for every model instance. You can use this primary key to save and retrieve model data in Redis.These primary keys are guaranteed to be globally unique, but they’re also generated entirely in the client without any request to Redis. They’re also sortable and compact. All of this is possible thanks to the Universally Unique Lexicographically Sortable Identifiers (ULID) specification.Redis OM primary keys are ULIDs, courtesy of python-ulid. You can read more about the ULID specification here. It’s very cool!Aside from these persistence features, though, you also get data validation with Pydantic. Let’s dig into how validation works.One difference between Redis and relational databases is that Redis does not enforce a schema, so you can write a string in Redis and then overwrite it with a number later. This is more flexible than relational databases, but also means applications are responsible for data validation.We think you shouldn’t have to figure out the best way to handle validation in your applications, so every Redis OM model is also a Pydantic model. This means you get Pydantic validation based on the type hints in your model, and you can control validation through standard Pydantic hooks, including custom validators.Here’s some example code that shows how validation works:If Redis OM for Python only gave you persistence methods and data validation, I’d think it was pretty cool. But we wanted to handle even more complexity for you, and to do that we needed to help you write expressive queries just like you would with an ORM. Next, I’ll talk about how these queries work.ORMs don’t just give you declarative models. They also provide an API that lets you query for data based on attributes other than the primary key. Imagine finding all customers over a certain age, customers who signed up before a certain date, etc.Out of the box, Redis is great at looking up data with a primary key. It is, after all, a key-value store whose values are data structures. But Redis doesn’t include a querying and secondary indexing system, so if you want to index and query data, you have to manage indexes yourself in complex ways.Here again, we wanted to handle this complexity for you, so we built fluent query expressions on top of an essential Redis module: RediSearch. RediSearch is a source-available module that gives you the querying and indexing capabilities missing from Redis.Let’s see what happens if we mark a few fields on our Customer model as `index=True`. Now, we can use the model to query:This expression syntax may look familiar— it’s a blend of everything I like about Peewee, SQLAlchemy, and the Django ORM.As you model complex data with Redis, you inevitably want to store embedded data. If you were modeling customer data using Redis Hashes, you might want to store data like the customer’s shipping addresses within an individual customer’s Hash. Unfortunately, Redis Hashes can’t store nested containers like Lists, Sets, or other Hashes, so this doesn’t work.Here’s where storing data as a native JSON object can make a lot of sense. If you model the customer data as a JSON document, you can embed whatever you want inside the record for a single customer.However, Redis does not natively support JSON. This is exactly why we created the source-available RedisJSON module. RedisJSON makes it possible to use Redis as a document database, storing and querying complex JSON objects with ease.With Redis OM for Python, if your Redis instance has RedisJSON installed, you can use the JsonModel class. This model class lets you embed JsonModels within other JsonModels. Imagine customers having an array of orders, each of which has an array of items, and so on.Here’s what embedded JSON models look like with Redis OM for Python:Not only do you get the flexibility to store complex JSON objects, but Redis OM for Python is aware of these nested structures and lets you write query expressions against them. Awesome!I hope you can tell how excited I am about Redis OM for Python. I’ve worked to bring some of the best of the current Python ecosystem together to handle complexity for Redis developers that, in my opinion, no one should have to deal with.If I’ve piqued your interest, check out the getting started tutorial. Redis OM for Python is in a very early stage, which we call a “Preview.” So there are rough spots, you’ll run into bugs, and we’re still working to deliver complete documentation. But the vision is there, and I encourage you to check it out.Finally, I’ll say that we started with declarative data models, but we have so much more we want to build— both for data modeling and beyond. Stay tuned for more exciting Redis OM features!"
631,https://redis.com/blog/how-to-build-a-music-sharing-app-using-nodejs-and-redis/,How to Build a Music Sharing App using NodeJS and Redis,"March 15, 2022",Growth Team,"Music is magic. It has the power to create new memories, friends, and unforgettable sensations.All of us love music but for some, it’s a passion. Listening and sharing different tunes that tap into our emotions is a hobby that strengthens social bonds as well as pulls many friendships together.But given that we live in a more globalized age where many of our friends are dotted across the globe, trying to organize anything social with them can be difficult.Being both a music-lover and an innovator, Franco Chen took on this challenge by creating an application that allows users to listen, share and discover new music with their friends, irrespective of their location.To maximize the user’s experience, the application needed to operate with a database that’s capable of transmitting, processing, and retrieving data in real time. With Redis, Franco was able to create a low latency application where commands and interactions between users were hyper-responsive.Let’s take a look at how Franco brought this application to life. But before we go any further, we’d like to point out that we also have a range of exciting applications for you to check out on the Redis Launchpad.You’ll build a platform that allows you to listen to and share music with your friends online. The application works by creating private or public online rooms where you can invite people with different tastes to join in the experience.In its purest sense, the application is a gateway to venturing deeper into different genres, tastes, and preferences through interacting with people who have a shared appreciation of music. Below we’ll show you how to create this application from scratch by highlighting the different components as well as walking you through each stage of the implementation process.Ready to get started?Ok, let’s get started!Socket.IO: Used as a Javascript library for real-time web apps.Javascript: Used as the preferred programming language that allows you to make web pages interactive.RedisJSON: A JSON data type for Redis that allows storing, updating and fetching JSON values from Redis keysRediSearch: Used for querying, secondary indexing, and full text search for Redis.Chakra UI: Provides you with basic building blocks that can help you build the front end of your application.Redux: Used as an open source Javascript library for managing and centralizing application state.Axios: Used as a promise-based HTTP client for Node.js and the browser.Node.js: Used as an open source, cross-platform that executes JavaScript code outside of a web browser.Express: Used as a flexible Node.js web application framework that provides a robust set of features for web and mobile applications.Prerequisitesgit clone https://github.com/spatialdj/frontendUse the RedisMod docker image to set up the Redis modules. In the root directory of the frontend, type in the following code to install the frontend dependencies:npm installGo to the backend’s root directory and create a file called config.js with the following contents:Type in the following command to install backend dependencies:npm installRun the following command in the backend’s root directory:npm startYour app should be running at localhost:3000Roomgenres: Genres the room is geared towardsIndices (RediSearch):private: TAGCommands:Get JSON representation of this roomCreate a new room or update room (not all fields required)Check if a room exists (used when attempting to join room):Used for rooms page for searching:User queueSessionUserExample playlist:Example user:MessagesSocketOn the homepage, you can log in or create a new account by clicking on either one of these icons on the navigation bar on the right-hand corner of the screen.Rooms are online communities that host a range of music depending on the criteria outlined by the host. Here you’ll be able to connect and interact with different people to discover new music based on your preferred tastes and genres.Once you click on the ‘Create Room’ icon on the homepage, you’ll have to provide a description about the room you want to host. Here you can add a bit of character in the description to communicate the type of vibe and people you want to join in the room, along with the type of music that should be shared here (see below).Friends who join the room will appear with their character icons (see below).Once everyone has joined, you’ll need to create a playlist where you can share music. Click on the arrow at the bottom left-hand side of the screen to begin this process. Next, a search bar that’s connected to YouTube will appear.From here, you can search for the different songs you want to include and add them onto the playlist by clicking on the plus sign. RediSearch will identify and add songs to your playlist based on the phrases entered into the search engine.When each song is playing, the music video will appear in the center of the screen. You’ll also see a chat window on the right-hand side of the screen where everyone in the room can share their opinion on each song.At the bottom left-hand corner of the screen, you’ll be able to like or dislike a song. If over half of the people in the room dislike the song, then the application will skip to the next tune on the playlist.Transmitting reams of data between different components with hyper-efficiency was always going to be one of the biggest obstacles for Franco to overcome. In today’s digi-sphere, users expect commands and responses to be in real time and they expect nothing less.This was especially important for this application, given the amount of data moving between the server and the client. Leveraging Redis’ advanced data capabilities gave the application low-latency for data retrievals, creating a completely fluid and responsive app that allowed users to listen, share, and comment on each other’s music in real time.To get a more visual insight into the ins and outs of how this app was created, make sure to give Franco’s YouTube video a watch.If you’ve enjoyed this post, we have many more for you to dive into on the Redis Launchpad. Here you’ll have access to an exciting range of applications that are having an impact on everyday life around the world.These include real time vehicle tracking systems, an app that accelerate the blood donation process, split testing software, and much more.Check them out. Be inspired. And join in the Redis fun.Franco ChenFranco has over nine years worth of experience in software engineering and is currently studying at the University of Waterloo. If you want to keep up to date with all of his projects then make sure to follow him on GitHub here."
632,https://redis.com/blog/how-to-create-a-powerful-text-ranker-using-redis/,How to Create a Powerful Text Ranker Using Redis,"December 16, 2021",Redis Growth Team,"Living in a world where most people struggle to leave the house without their smartphone has meant Google is never too far away from us. In an instant, we can find an answer to almost any question through a search query on Google’s search engine.Search engines (and their access to infinite information) have been integrated into everyday life, making many of us incredibly reliant on them. And for one to be optimal, answers to queries need to be retrieved instantly due to the high standards set by Google and other tech giants.Any lags, delays, or drags will hamper the user’s experience, which is why this Launchpad App, Alexis, has used Redis as the main database to overcome this obstacle.The founder of this application, Bobby Donchev, has leveraged the power of RedisAI and RediSearch to retrieve information from a corpus in response to a query with maximum efficiency. Users are able to index PDFs and use a simple UI to extract information from their documents.Without Redis, the entire search process would be sluggish, hampering the functionality of Alexis. Let’s take a look at how Bobby put this application together.However, before moving on, we’d like to highlight that we also have an awesome range of applications for you to check out on the Redis Launchpad. So make sure to check it out!You’ll build an efficient text ranker capable of retrieving search queries at maximum speed. Users will be able to leverage this application to index important PDFs and extract answers from their documents with ease.We’ll go through each step in chronological order and highlight what components are required to build the application.Providing an answer to the searcher’s query happens in two steps:By using RediSearch in the first step, you’ll drastically reduce the search space. This will make the app’s overall experience faster. After this, you’ll need to use NodeJS with typescript in the backend and React with typescript in the frontend.Besides using RedisAI and RediSearch, you’ll be leveraging RedisJSON for your user model as well as an asynchronous worker implemented with Redis Streams.The webserver is exposed with the express framework that has the following endpoints:Once you register and log into the app, you’ll be able to start adding documents to the indexed library. When a PDF is uploaded, an event will be written into Redis Streams. Afterwards, somebody from the consumer group will pick up the event for async processing.You can then process the PDF, apply some cleaning and store the PDF in a Redis hash that’s indexed with RediSearch. You’ll now be able to send natural queries to the server and won’t be confined to basic keyword searches such as ‘kubernetes deployments,’ ‘DDD root aggregate’ etc.Instead, you’ll be able to query more relevant searches.FlowchartBelow is a general overview of how Alexis functions.Now let’s break down how the Upload PDFs & Index PDF Content and the Answer Query parts of the flowchart operate.Change the directory to alexis and run the below command:npm installThe below command will bootstrap server and client app and also it will initialize Redis server as well as RedisInsight GUI:npm run bootstrapnpm startOpen http://localhost:3000 to access the applicationRedisInsight is a visual tool that lets you do both GUI- and CLI-based interactions with your Redis database, and so much more when developing your Redis based application. It is a fully-featured pure Desktop GUI client that provides capabilities to design, develop and optimize your Redis application. Click Here to learn more about RedisInsightThe RedisInsight GUI is accessible  via the following link: http://localhost:8001Step 1: The user data is stored in a RedisJSONStep 2: A RediSearch index is created for each user with the code belowStep 3: Once a user uploads a PDF we update his pdfs array with RedisJSONStep 4: The file upload also triggers an event that’s being written to the ax:stream:pdf-processing stream. The payload of the stream is:Step 5: A consumer within a consumer group picks the event from the stream and processes the file, also writing the content in a hash.In this application, there’s a RediSearch index for each user that indexes the above hash. This provides lookup capabilities to match relevant content to a user’s query. Content is analyzed with the below code:The content that’s retrieved by RediSearch is then transmitted to RedisAI to be analyzed.Create an accountWhen you open the Alexis app, you’ll be directed to the portal for you to log in (see image below). If you haven’t already got an account, you’ll have the opportunity to create a new one from the hyperlink at the bottom.Once you’ve logged in, you’ll be taken to another page on the portal. If this is your first time on the portal, you won’t have any documents in the library. The next step will be to import them into the application.Import documents into your libraryTo begin this procedure, you can either drag and drop documents into the box in the centre of the screen or manually pull them up by clicking on the cloud icon.Once you’ve uploaded a document, you’ll receive confirmation of its storage through the display of its title on the left-hand side of the screen (see below).When this happens, the PDF is uploaded to the server and is also cleaned in the background. This process is efficient due to the advanced capabilities of Redis.Make your queryAs you can see from the image above, you simply have to type in the query you want the application to retrieve. In this example, the user has directly enquired what the content of the file is about by typing in ‘What is the journey about?’When you’ve submitted your query, you’ll instantly be provided with a few answers, each ranked hierarchically based on their relevancy (see example below).When it boils down to it, everyone expects search queries to be instant with no delays whatsoever. This is the digital age, after all, and any lags will only push users away and towards something that’s more optimal.By using Redis, Alexis was powered to operate at a premium standard. Gathering the most meaningful content across different locations was fast and efficient thanks to RediSearch. And the hyper-advanced capabilities of RedisAI were able to sift through this content and provide users with the most relevant and accurate answers to their query.If you want to discover more about how this application was made then you should check out Bobby’s YouTube video. We also have a wide variety of applications for you to get inspired on the Redis Launchpad.From creating real-time bus tracking systems in Helsinki to protecting crop insurers in developing nations, programmers from all around the world are tapping into the wonders of Redis to make a difference in everyday lives. And you can too!Bobby is a dynamic programming engineer who’s had over twelve years of experience designing and implementing systems for clients.Make sure to take a peek at his GitHub page to stay up to date with all of the projects he’s been involved in."
633,https://redis.com/blog/how-to-build-social-issues-app/,How to Build an App That Will Tackle Social Issues by Using Redis,"January 5, 2022",Redis Growth Team,"An injured animal. A tipped trash can. And a polluted hotspot. Most of us are likely to have come across one of these in our local community. But knowing how to react quickly to these scenarios can be a challenge. Who do I contact? What’s the process for reporting a health hazard? How much time will this take out of my day?Not knowing the answer to any of these questions could be the difference between action and inaction. What it boils down to is that community is everything, and if we’re able to report community issues with ease then we could promote a healthier, safer, and greener planet.Taking on this challenge is Bryon Rosas Salguero. Through his innovative application, Helplanet, individuals can instantly report local risks or accidents from just a few taps on their phone. But for this application to function, data had to be transmitted in real time to keep up with events that were happening in real life.Redis was crucial to achieving this and it enabled Byron’s innovations to come to life. Let’s take a look at how he was able to put this app together. But before we get down to the nitty gritty of it all, we’d like to let you know that we have a diverse range of interesting applications for you to check out on the Redis Launchpad.So don’t forget to check them out after this post!You’ll build an application that allows people to report social issues in their community. These can include:Below we’re going to walk you through each step of the application building process and highlight the components you’ll need along with their functionality.Ready to get started? Ok, let’s dive straight in.Clone this project or download as zipExec redis (https://github.com/RedisLabsModules/redismod)(Open three terminals)First start server (start services) with this commands:Second start ionic app (http://localhost:8100)Third start web angular app (http://localhost:4200)Below are the RediSearch CLI commands for each of the servicesRedis utilsAndMiddleware is also used for each of the routes:If somebody comes across a hazard or a social concern in their community, they can report it via  the Helplanet application. Users simply have to open the app and they’ll then be faced with a number of options (see below).The user will then select the option that best describes the issue or concern they’ve come across. Much like Facebook Messenger, a chat will appear where they’ll be able to provide more detail about the incident they’re reporting (see below).Note: Everything will be reported in real time thanks to Redis.The organization will receive notifications from all incidents that have been reported by members of the public. They’ll be able to gain a holistic view of all reported incidents on their dashboard (see below).From here, an organization can select an incident and view it in more detail. The information that will be provided about the incident will include:Organizations can filter everything based on location. This will allow them to get a quick insight as to where every instant is on the map (see below).An organization can also view a user’s personal details if they want to get in touch with them in a different form of communication e.g email (see below).Society thrives on individuals who bear the responsibility of reporting issues that pose a threat to their local community. But with so many different organizations and professionals to contact, the entire reporting process can be just as unclear as tedious.Byron’s application was able to eliminate these barriers by creating a powerful application that made the entire process easy and simple. Yet for this app to function effectively and promote a seamless experience, data needed to be transmitted in real time.Thanks to Redis, components in the application’s architecture became more interconnected where data transmission was both seamless and hyper-efficient.If you want to discover more about how Helplanet was created, then make sure to watch the YouTube video here.We also have a variety of other breakthrough applications that you can check out on the Redis Launchpad. Here you’ll get an insight into how Redis can be used to create exciting apps that are having an impact on everyday life around the world.So what can you build with Redis?Byron is a highly innovative software engineer who works independently as a freelancer. Make sure to keep up to date with all of his activity by visiting his GitHub page."
634,https://redis.com/blog/how-to-build-your-own-private-social-media-platform-using-redis/,How to Build Your Own Private Social Media Platform Using Redis,"November 18, 2021",Redis Growth Team,"To many, the social media experience has become a stressful experience. Newsfeeds have become pinboards for sponsored content. Statuses, comments, and any online interactions expose users to restless keyboard warriors. Some might say that social media has become anything but social.Now as the kingpins of social media continue to dither, this Launchpad App has tackled this issue head-on by creating its very own social media platform, Letus. The core value of this app is to provide users with a social media platform that promotes more natural and meaningful interactions between friends, family, and work colleagues.From start to finish, this application relies on the power, speed and flexibility of RedisGraph. Let’s take a look at how this app was created.But before we dive in, make sure to have a browse around the Redis Launchpad to discover a range of exciting applications for you to get involved in.You’ll build a social media platform that shields users from trolls and liberates them from sponsored content. This mobile application will only display posts that are made by your friends, family, or whoever you connect with on Letus.There won’t be any mysterious algorithm that exposes you to unwanted connections and advertisements. Instead, Letus will give you full control over the content you’ll see. Below we’ll go through A-Z of what actions you need to take to bring this application to life, along with the required components.RedisGraph: used to sparse matrices to represent the adjacency matrix in graphs and linear algebra to query the graph.Google Cloud Platform (GCP): used as a public cloud vendor that classifies text and analyzes sentiments in this project.Azure Functions: used as a serverless solution to write less code, maintain infrastructure, and save on costs.Google Identity: used as a customer identity and access management (CIAM) platform that authenticates users via web-based Oauth 2.0.Expo:  Used by Mobile developers to build appsFirebase: Backend as a ServiceNode modules with config plugins can be added to the project’s Expo config automatically by using the expo install commandFor local development, RedisGraph can be run using the redismod Docker containerYou can run just the Expo Go App locally to use, and develop, the react native app. By accessing existing (free) firebase auth and existing (free) Azure functions deployed for DEV only.Press i to launch an iOS simulator or a for Android. (note: Android simulator requires Android Studio and sdk setup)This project uses Azure Functions which can be accessed directly in your local development. If you wish to publish these functions to the cloud, you must create your own Azure account. Tip: There is a very useful VS Code Extension to help manage your azure projects and functions.Click https://redis.com/try-free/ and setup Redis Enterprise cloud database with RedisGraph as shown below:Note: single-direction :friended relationships determine pending friend requestsNLP CommandsUsing the free tier of GCP Natural Language, we apply both sentiment analysis and text classification to all posts made in the system.As social media continues to intertwine itself into everyday life, trolling remains as pervasive as ever. Facebook, Twitter, Instagram, and even Linkedin have become hunting grounds for restless keyboard warriors.Then you have the endless bombardment of sponsored content, whose interests are prioritized ahead of the user’s. But by using Redis, this marketplace app was able to create a social media platform for mobile users that removes both of these hazards.The power and speed of RedisGraph was crucial to building Letus since it allowed the application to query data with maximum efficiency. From what began as just a simple idea, Redis was able to bring this application to life… but that’s not all.Every day people are tapping into the wonders of Redis to create exciting applications that’s having an impact on everyday lives. How can you use Redis to help change society for the better?For more inspiration, you can visit the Redis Launchpad where you’ll find a whole range of innovative apps. Likewise, you can also discover more about how this app was made by clicking here.Matt PileggiMatt has over 20 years of experience in web development and is now the principal engineer for Games24x7.Make sure to keep to date with all of Matt’s projects on Github by checking out his page here."
635,https://redis.com/blog/how-to-create-a-real-time-online-multi-player-strategy-game-using-redis/,How to Create a Real-Time Online Multi-Player Strategy Game Using Redis,"December 23, 2021",Redis Growth Team,"Multiplayer gaming remains colossal in the gaming industry. And why wouldn’t it be? To settle old scores, solve disputes, or even satisfy that competitive itch , battling it out online against other users is just as cathartic as it is entertaining.This is why this Launchpad app has created its own real time strategy game, Pizza Tribes, that involves…wait for it… mice! The gameplay involves training a population of mice to bake and sell pizzas for coins, with the overarching objective being to generate more coins than any other player.For all its creativity, this application wouldn’t be able to provide users with real time gameplay without Redis’ ability to transmit data between components efficiently. Any delays would have made real time gameplay impossible.Let’s take a look at how this application was created. But before we go any further, we’d like to point out that we have an excellent range of applications that are having an impact on everyday life for you to check out on the Redis Launchpad.You’ll build a multiplayer browser-based real time strategy game using Redis. Below we’ll go through each step in chronological order and outline all of the components that you’ll need to create this application.Ready to get started? Ok, let’s dive straight in.The application has an unconventional approach when it comes to client-server communication. This is because it relies heavily on web sockets to carry out responsibilities that would normally be fulfilled by an HTTP request/response.Below is an example of a typical web-socket flow:Since this application is reliant on Web-Sockets, we’re going to spend a bit more time going over the role that they play. Firstly, web socket communication relies heavily on Redis to send and retrieve messages.The API does not do any game logic but simply validates client messages before pushing them to Redis. Meanwhile, the workers can be horizontally scaled whilst relying on Redis to push messages efficiently through the system.Since Web Socket bidirectional lightweight communication protocol over a single TCP connection, it is not easy to scale the Web API (holding the sockets).This solution attempts to minimize load on the Web API so that it can focus on shoveling data to the clients.Note: messages are not sent between the API and workers using Redis Pub/Sub. Instead, Redis lists (RPush and BLPop) are used. One of the advantages of this is that the workers or the API can be restarted without losing messages, whereas with Redis Pub/Sub everything will be forgotten.The worker will need to delay some tasks (e.g., finish construction of the building after 5 minutes). This is accomplished by carrying out updates to the Sorted set user_updates. The updater then pulls the top record of the sorted set which is determined by time. If the time has passed, it will remove the record from the set and will then update the game state of that user.Below is a simplified typical flow:i. Validates the commandii. Updates the user game state:iii. Discovers the next time the user game state needs to be updated (e.g. when the construction is completed)iv. Sets the next updated time of the user game state:3. An updater updates the user game state at the next update time by carrying out a number of commands:i) Runs the following command to fetch the next user that needs to be updated (and at what time)ii) If the score (timestamp) has been passeda) Remove the next update time:b) Perform game state updatec) Find next time the user game state needs to be updated againd) Set next update time:Protocol Buffers are used to define the messages that are sent between the client/server and the server/client.Below is the full definition.Below is the full definition.Prerequisite:There are two ways you can run the application locally. You can either run everything (Redis, services, web app) in a Docker container Or, you can pick and choose for faster development.Clone the repositorygit clone https://github.com/redis-developer/pizza-tribesThis is arguably the easiest way to run the project locally. To get started, you need to execute the docker-compose command as shown below:This command will achieve the following:When this command is carried out, the following is achieved:If you want to make changes then you’ll benefit from Hot Module Replacement (HMR) in the web app. This will allow you to build the Go apps more efficiently. To achieve this, run Redis using docker-compose, and then run the services and web app on your host OS:Once carried out, this command should give you:Note: The web app will proxy calls to /api to http://localhost:8080 (see webapp/vite.config.ts).The users are stored as a hash set in key user:{user_id} containing fields. These include:The user_id can be looked up using the username via username:{username}.Registration is achieved through the following commands:Authentication is done like so:Note: If the hashes match, create JWTUsing RedisJSON, the user game state is stored as a JSON value in the key user.It does this with the following structure:The game state is accessed in different ways depending on the use case. But for a complete retrieval, the following is used:In other cases, a path is used to retrieve only a subset of the data:The game state update is what makes the game tick and it’s one of the most important processes in the game. Its purpose is to:In addition, the game state update will also:The updater runs in a loop that queries a sorted set that’s called user_updates. It retrieves the top record in the sorted set by running the following command:By utilizing WITHSCORES we also retrieve the timestamp of when that user needs a game state. As such, the updater can check if timestamp < now. If so, then the user can carry out the following commands:2. Proceed to update the game stateNote: there is some level of risk involved because if the game state update fails, the user will no longer have a record in the user_updates sorted set. When this happens, no game state update will be scheduled.To avoid this, the game will ensure that the user is scheduled for game state updates when logging in.The update is executed with a check-and-set approach (WATCH, MULTI, EXEC). This is achieved through the following steps:For more details of a simple game state update, see the trace below:Note: Extrapolating resources, completing buildings, trainings and complete travels are all implemented using the flow described above.When the game state has been updated, you’ll have to schedule the next one. Here’s how to do it:The RedisTimeseries module is used to track the changes in user resources. The resources are tracked using the following keys:When every game state update is carried out, a new data point is inserted into each key. Below is an example:When the user wants to look at their resource history, the following command is used to retrieve the aggregated data points from the last 24 hours.The game state update will change the number of coins a user has which is why we need to update the leaderboard. The leaderboard is a sorted set with the key leaderboard. It’s updated by running the following command:When any user wants to have access to the leaderboard, the data is retrieved with the following command:From a performance perspective, achieving real-time gameplay is one of the most important objectives behind creating a successful online multiplayer game. Failing to achieve this will drastically hamper the user’s experience, irrespective of how advanced other qualities of the game are.Despite having a complicated architecture, Redis removed this obstacle through its ability to zip data between different components with ease. Having no lags, no delays, and no data setbacks whatsoever allowed this Launchpad App to create a complex yet engaging online multiplayer strategy game where users from around the world can battle it out against each other for the top spot.If you want to get more of an insight into how this application was made then you may want to check out this YouTube video.We should also let you know that we have an exciting range of game-changing applications (excuse the pun) on the Redis Launchpad. Here you’ll discover a collection of apps that are having an impact on everyday life by everyday programmers.So make sure to check it out!Matteus HemströmFrom Matteus is a highly innovative software engineer who currently plies his trade at Nuway.If you want to keep up to date with all of the projects he’s been involved with, then head over to his GitHub page here."
636,https://redis.com/blog/how-to-build-an-application-to-power-your-log-analysis-using-redis/,How to Build an Application to Power Your Log Analysis Using Redis,"December 2, 2021",Redis Growth Team,"As your IT infrastructure grows in size and becomes complex, you may be forced to spend more attention ensuring that everything is managed properly.Since the rise of the cloud, this has become especially more relevant, given that we now have an abundance of technical information spread across many different locations.But being able to carry out comprehensive log analysis can be tedious, time-consuming and painfully boring.A simple yet effective way to liberate yourself from this burden is to use software or applications that are designed to make monitoring logs easy… and that’s exactly what Alexis Gardin has done in his application, Logub.Unlike many other SaaS solutions, Logub can carry out these actions on-premises and open source. By following the steps below, you’ll be able to create an application that will collect, explore, and analyze application logs for you.At the heart of this application was a reliance on RediSearch’s ability to explore and analyze logs across different locations with unrivaled efficiency. Let’s look into how Alexis brought this application together.But before we dive in, we should let you know that we also have an exciting range of applications for you to check out on the Redis Launchpad.You’ll build a special app that’s powered by Redis to collect, analyse and explore log applications. In the steps below, we’ll go through A-Z of what’s required to build this application, along with the required components.Even if you’re new to Redis, we’ll show you exactly how to get to grips with this powerful database. Are you ready to get started?Ok, let’s dive straight in.In the demo, a DEMO app will publish logs in Logub. You’ll be able to interact with this DEMO app to generate your logs. You’ll then be able to request them in LogubFirstly, make sure that the given ports are open on your system: 8080, 8081, 3000 & 6379.git clone https://github.com/redis-developer/logubGo to localhost:3000 to explore logs.It may take around one minute for you to see the logs coming. You can view the logs on the page by filtering them with the sidebar on the right. Alternately, you search for them by filter or full-text query via the search bar at the top.When you click on a log, the details will be displayed and you’ll have the option to index business properties. These business properties can be used as filters afterwards.Next, go to localhost:3000/demo to access the playground and add your custom logs. This demo page will allow you to:If you return to the main page, you can try to search for the logs you’ve just generated.Note: There is a latency of around 1 minute between the production of a log in a container and its display in Logub. This latency is caused by the process of collecting, formatting, and ingesting the logs into the database.Data is stored using the Fluentd Redis Plugin. It stores each log with HSET. For example: HSET level DEBUG message “Hello World” thread mainThis is the object that you’ll use to manipulate logs and retrieve them from Redis. Carrying out this action will allow the Logub UI to display the logs. To make complex queries in your logs, you can use RediSearch.Since you’re able to change the RediSearch schema dynamically, you can use the ‘List’ data structure to keep track of which schema is indexed.The above command will allow you to create a plain text RediSearch query based on the user input. There are a bunch of small QueryBuilders built on the top of the RediSearch library. It’s the same command that will be sent by Logub UI to carry out a comprehensive and efficient search in your logs.You can carry out the search by tag or full-text search. Below are some examples you can use as models if you get stuck.For you to have a good testing experience with the app, we highly recommend that you create your own log with the playground, add business properties and then do some experimentation just to get a feel for it.For now, Logub can handle only one particular log format. In the future, this format will be extended and more customizable.Here’s the Logub formatPlease note, these fields are not mandatory.If you want to add your business properties, you’ll need to add a nested JSON object that has “mdc” as the key. For example:To explore logs in Loghub, your containers need to use the Docker Fluentd logging driver. Here’s a configuration example for customer integration.RediSearchLogub uses the functionality of RediSearch to process application logs. When logs are persisted in the Redis database, they’re accompanied by 3 types of fields:Here we have an example of a log that describes how our tool functions when Fluentd flattens and persists in the Redis database. The Logub API will allow the user or the company to index one or all fields of the mdc object.In this project, the Tag Datatype is widely used. Logs are often searched based on business properties when searching in logs (eg a customer id). Moreover, we also use the TextField Datatype for log messages. This allows the user to do a full-text search in this field.Here’s a simplified schema of the search process:RedisRedis is used to store logs by Fluentd like this in the HashSet type of Redis.To keep track of the indexed field by the user, you can also add a ‘schema’ object which uses the List type of Redis.A drawback to the explosion of digital innovations, such as the cloud, is that monitoring IT infrastructure can become complex. Applications are likely to be scattered across different locations, making it challenging to difficult to pull every log into one location.However, by using Redis, the monotony of this process is removed, allowing you to search, gather and store logs with ease. If you want to learn more about the ins and outs of how this application was made, then you can check out Alexis’ YouTube video here.We also have an exciting range of innovative applications that have been developed by talented programmers just like Alex on our Launchpad. Check it out, be inspired and see what innovations you can come up with using Redis.Who built this application?Alexis GardinAlexis is an innovative software engineer who currently works for Zendoc. Head over to his GitHub page to see what other projects he’s been involved in."
637,https://redis.com/blog/how-to-build-a-real-time-a-b-testing-tool-using-redis/,How to Build a Real Time A/B Testing Tool Using Redis,"November 24, 2021",Redis Growth Team,"A/B testing has become an indispensable asset to marketers and website owners competing in today’s digital economy. It allows users to test existing ideas, experiment with new ones and highlight what works and what doesn’t.Without this information, trying to optimize web pages can be a frustrating and time-consuming process. But unfortunately, A/B testing software can be expensive, preventing many from having access to its benefits.But with Redis, programmer Thiago Camargo was able to remove these barriers by creating an open A/B testing tool that can operate in real time. Redis’ powerful data processing features allow users to experiment with variables of their choosing and discover the most effective ways to optimize their web pages.Let’s take at how Thiago was able to put this application together. But before we go any further, we’d like to point out that we have a wide variety of innovative applications for you to discover on the Redis Launchpad.So make sure to have a browse after this post!You’ll build a powerful yet simple A/B testing tool that’s scalable and operates in real time. Below we’ll show you how to build this application from the bottom up, highlighting what components are required along with their functionality.Also known as split testing, A/B testing refers to a methodical experimentation process where two or more versions of a variable are shown to website visitors to determine which one has a greater impact on conversions.It’s designed to add clarity on how you can position your brand, products, and services more effectively in the marketplace.Let’s say for example you have a product on your website you’re trying to sell. One variable you could experiment with would be the call to action (CTA) button next to the product. There are a number of different ways you can A/B test the CTA, including:By split testing these variables, you’ll discover which version of your CTA is going to be most optimal to driving conversions.If you’re using Docker desktop, ensure that the file sharing option is enabled for the volume mount. Use the code below to set up Redis:Use the code below to start up the docker:Here we’re going to show you how to create scalable A/B testing experiments with trigger-based weighted enrollment.In this step, we’ll show you how to create an experiment that allows you to A/B test different variables. We’re going to put ourselves in the shoes of a marketer looking to split test the different screen colors to see what impact they have on sales.To get started, use the code below.It’s worth noting that we changed the ‘goalIds’ at the top to ‘purchase’ to measure the number of sales made from each screen color. Below that, we’ve made the ‘id’ of the experiment ‘subscription1.’To determine the trigger event, we inserted ‘user-plan-screen-view.’ This means that when an event is triggered, the user will automatically be enrolled in the experiment.Next, we identified the variants as ‘red’ and ‘blue’ since these are the two variables we’ll be A/B testing.Response.Here we’ll show you how to create a dynamic and scalable remote client configuration service (Firebase Replacement).ResponseThis step involves Timeseries Event Indexing.Here is a simple and flexible summary service that’s capable of keeping and maintaining summaries of multiple types of applications. These include: game scoreboards, product ratings, user ratings, incremental metrics and many more.Response:Flexible Property StorageResponse:Most of the commands used below are implemented using Lettuce Redis Command Annotation.A/B testing has become integral to any marketer competing in today’s digital playground. But obtaining this software can be expensive, forcing many to operate with a sub-optimal website. And to compound this even further, creating your split-testing software has its own programming difficulties.To create such an application requires a versatile and powerful database capable of transmitting data efficiently between components. Yet despite these obstacles, Redis removed all of these obstacles.From just by using Redis on your laptop, you can A/B test any variable on your website and bridge yourself closer to your target market. If you want to discover more about how this application was made, then make sure to watch this YouTube Video.Yet despite these demands, Redis’ advanced data processing capabilities made data transmission between components both hyper-efficient and reliable, creating a highly responsive application.This meant no lags, no delays, and no causes of friction between the user and the application. From just by using Redis on your laptop, you can A/B test any variable on your website and bridge yourself closer to your target market.If you want to discover more about how this application was made, then make sure to watch this YouTube Video.If you’ve enjoyed this post then we also have many more for you to discover on the Redis Launchpad. From creating real-time vehicle tracking systems to building powerful drone systems to protect crop insurers in developing countries, Redis has been leveraged by programmers all over the world to improve everyday lives.What can you do with Redis?Thiago CamargoIf you enjoyed this application then make sure to visit Thiago’s GitHub page to keep up to date with all of the projects he’s been involved in."
638,https://redis.com/blog/how-to-build-a-powerful-e-learning-platform-using-scala-and-redis/,How to Build a Powerful E-Learning Platform Using Scala and Redis,"November 5, 2021",Redis Growth Team,"Never before has online learning been so accessible. Whether you want to discover more about cryptocurrency, sharpen your programming skills or even just learn a new language, the digital age has gifted everyone access to a phenomenal amount of content.However, over time e-learning has been viewed as just another digital commodity, where users expect all online content to be instantaneous. Speed remains crucial to performance, where any lags or delays in page loading time kills the user’s experience.And so these high expectations require any competent e-learning platform to be powered by a database that’s capable of handling, processing and transmitting data with hyper-efficiency…which is exactly why this Launchpad App used Redis.From start to finish, this application was built to connect, educate and empower learners by connecting them with the most relevant courses based on their interests.Let’s investigate how this was achieved. But before we dive in, you may also want to check out all of the other amazing applications that we have on the Launchpad.You’ll build a powerful e-learning platform that will connect students and teachers with one another along with a diverse library of online courses. With speed being the linchpin to performance, you’ll deploy a number of different Redis components to achieve this objective.Below we’ll reveal what components are required to make this application come to fruition along with the functionality of each item.The data model is expressed through nodes and relations using RedisGraph. The model is very simple since it involves the Student, Course and Topic entities expressing the different kinds of relations between each other.X-Mentor follows an Event Driven Architecture approach in which the following Domain Events are considered:Wait until Keycloak and x-mentor-core are ready, then go to http://localhost:3000.You can access Keycloak via 8880 port as shown below:Use admin/admin to login into keycloak.This step starts the authentication process against Keycloak by:Use the following code to check and see whether the username already exists in users bloom filter:Signing up involves 4 steps:In this step we’re going to show you how to create courses to go on the e-learning platform. Each course is going to be stored as a JSON in RedisJSON.:Follow the commands below:Here we’ll uncover how you can enroll a student in a specific course.Below are the steps for you to follow:As part of any online resource, users generally are able to provide a review. To make this functionality happen, you need to carry out the following steps:The following diagram illustrates the interaction between Redis Graph and Redis Streams.To bring make this functionality happen, follow the below commands:AllThe following commands retrieves courses by query from redisJSON with rediSearchFT.SEARCH courses-idx ${query}*By IDBF.EXISTS courses ${course.id}JSON.GET course:${course.id}GRAPH.QUERY xmentor “MATCH (student)-[:studying]->(course) where student.username = ‘$student’ RETURN course”FT.SEARCH courses-idx ${course.title}Now we’ll show you how to allow students to filter preferred courses based on their interests. Here’s how to do it:The following diagram shows the interaction between RedisGraph and Redis Streams.Below are the commands for you to follow:Here we’re going to show you how to create a course recommendation system that connects users with courses that are most relevant to their interests. A lot of this comes down to the advanced capabilities of RedisGraph.Searching for relations between nodes in the graph database is the easiest way to implement the most effective recommendation strategies. Let’s have a look at how to do this.In order for you to create a special recommendation system that matches users’ personal interests with the most relevant coursesThis functionality will allow you to track the amount of time users spend watching courses on the platform. That information will then be used to implement the Leaderboard.Once x-mentor-core receives the request, it will then publish the Student Progress Registration Domain Event. This will end up as an element inside student-progress-registered stream (which is a Redis Stream) via the following command:All of the data sent to RedisGears will be pushed to the stream and will then sink this data into Redis TimeSeries using the following command:The Leaderboard functionality enables you to have a board that displays the rankings of the top students that use X-Mentor. Students are ranked based on the amount of time they spend watching content on the platform – the more you watch, the higher you rank.To accomplish this, you need to separate two functionalities:When the user request for the leaderboard data, first look at Redis for the time series keysFor each key, you need to use Redis TimeSeries to get the range of samples in a time window of three months performing sum aggregation. You can use the code below to make this happen:Below are additional requisites that need to be implemented for this to happen.Carrying out these commands will provide you with the accumulated screen time of each student. Once you receive these rankings, you can create a ranking system based on those who have the most screen time.Being this far into the digital age, a simple prerequisite of any application is for it to operate at maximum speed. This is especially true for e-learning platforms where users are meant to be engaged with its course content for long periods of time.A mere lag will create friction between users and the application, inhibiting its ability to connect teachers with students as well as providing value through its courses. Having Redis as the application’s main database removed this threat and helped to create a fully optimal application that catered to the user’s demands with ease.To get a more visual insight into how this application was created, then you can watch this YouTube video here. We also have a diverse range of applications for you to check out on the Redis Launchpad that are having an impact on everyday life around the world.So make sure to check them out!Sergio CanoSergio is a full-stack engineer and in his own words ‘loves solving problems and learning new stuff.’Being an enthusiastic learner, it’s not difficult to see where he got the inspiration to build this application.Make sure to check out his profile here and see what other projects he’s been involved in."
639,https://redis.com/blog/how-to-build-an-app-that-can-analyze-bike-traffic-patterns-in-nyc-using-redis/,How to Build an App That Can Analyze Bike Traffic Patterns in NYC Using Redis,"October 29, 2021",Redis Growth Team,"Citi Bike remains the nation’s favorite bike share program. With over 20,000 bikes scattered across Manhattan, Brooklyn, Queens, and the Bronx, getting from A to B has never been easier. It’s cheap, it’s convenient and it’s green.And most notably, it’s popular. Thanks to this Launchpad App, you’re now able to discover how many bikes are used at different times, throughout each day, in different neighborhoods across the city.Despite New York offering other modes of transport, those who are committed to a healthier and greener way of living can plan their trips in advance by knowing what times they’re most likely to have access to one of these bikes through the NYC Bike application.Once again, RedisGraph was the crux of this app by executing lightning-fast queries that enable the user to visualize the traffic patterns that occur around the network. Let’s explore how this was achieved.But before we dive into the nuts and bolts of this application, we’d like to point out that we have a diverse range of applications on the Launchpad for you to check out.So make sure to have a browse after this article.You’ll build an application that can provide users with a clear and detailed visualization of the different traffic patterns that exist in each area of New York. Using RedisGraph, users will be able to drag a geospatial index across different locations to uncover precise figures on the number of Citi Bike users across a period of time.Below we’ll walk you through each stage in methodical order, uncovering how each component should be deployed along with its functionality.In its purest sense, Citi Bike is New York’s most popular bike rental program. Users have access to over 20,000 bikes and 1,300 stations as a cheap and convenient way to get around the city. Getting from A to B on one of these bikes is simple.Using a ride code or a member’s key, you’ll be able to unlock a nearby bike and take as many short rides as you want while your membership is active. Spanning the Bronx, Brooklyn, Manhattan, and Queens, Citi Bike covers most of the densely populated areas in the city’s centre.Install the software below:Clone the repository.Create a new account by clicking https://www.mapbox.com/Access MapBox Access token by clicking https://account.mapbox.com/access-tokens/Run the command below to copy the access token to frontend/.env:Build the visual UI components, and run them using Docker Compose:The frontend should now be accessible at http://localhost:80/, but the map will be blank as Redis is empty.Results:Each reload of the UI at http://localhost:80/ should show these trips accumulating. On the live demo, there is a pre-built dump.rdb which is 674MB on disk.Drag the orange circle to some other location and you might notice that RedisGraph displays  trips in ms.Citi Bike publishes all of their trip data online, providing users with answers to some of the most commonly asked questions about this public bike system. These include:The graph has a source and a destination for each trip. This is highlighted by the two circles on the map, with the blue circle representing the area from where bikes depart and the red circle representing where the journey ends. The flow of traffic within each circle is depicted in the graph below.An amazing feature of this application is flexibility on the user interface. You can drag and drop both circles to any desired location on the map to discover that area’s flow of traffic. Equally, you can zoom in and out of each circle and decide on the size of its circumference based on your preference.As you drag each circle, RedisGraph processes the data at lightning speed, providing you with a detailed overview of each region’s flow of traffic within milliseconds. This is really handy if you want to make plans in advance and avoid the hassle of waiting around for a Citi Bike.The Go backend uses the redisgraph-go library to proxy graph queries from the front end. It’s important to highlight that the Go library doesn’t support the new point() type and so you can add the PR redisgraph-go#45 feature instead.To mark every station on the map (/stations API call), you can use a simple Cypher query to fetch all of the locations:To count all the edges in the graph (part of /vitals API call), you can use a different Cypher query:And finally, the main Cypher query to retrieve journeys (/journey_query API call) is of the form:This matches all of the:Stations within the $src and $dst circles, and all of the trip edges between these stations (in both directions). This is a fast query due to the geospatial index on the following:The returned egress that you’ll see is true if the trip started at $src, and it’ll be false if it started at $dst. And regarding the trip graph that’s presented on the UI, it’s built by aggregating properties on these :Trip edges, for both egress and ingress traffic.The frontend is built in React and around react-mapbox-gl and the custom drawing modes that have been implemented. And the aggregated trip graph is built using devexpress/dx-react-chart.Offline_importerThe offline importer downloads the public Citi Bike trip data, unzips each archive, and indexes all of the trips into the journeys graph. The graph contains every :Station as a node, an index on the station ID, and a geospatial index of the station’s locations:This Cypher query will either create a new hedge with one trip or will increment the appropriate counter on the edge to index the trip. The most efficient way for you to write all of the 56 million trips is to use pipelining and turn CLIENT REPLY OFF for each batch. The bulk import will take a couple of hours.Each reload of the UI at http://localhost:80/ will show how these trips accumulate. In the live demo, a prebuilt dump.rdb was used, which is 674MB on disk.Processing speed often determines the quality of an application. Anything that’s slow and jarring will hamper the user’s experience and even frustrate plans for a creative app to come to fruition.Redis’ ability to process queries at a phenomenal rate allowed the app to function to its full potential by displaying the different traffic patterns of each designated area. As a result you can toggle freely between different neighborhoods to discover the traffic flow and work out your optimal route.You can discover more about the ins and outs of this innovative application by visiting the Redis Launchpad. While you’re there, you might also want to browse around our exciting range of applications that we have available.Mitch WardMitch is currently a staff engineer at Datadog.Make sure to visit his GitHub page to see all of the latest projects he’s been involved in."
640,https://redis.com/blog/how-to-build-a-visual-project-management-app-using-redis/,How to Build a Visual Project Management App Using Redis,"October 27, 2021",Redis Growth Team,"Task management can be gruelling. For managers, monitoring tasks that are scattered and disconnected from one another remains just as much a norm as a bane in their profession. Being able to instantly understand the progress of projects without having to waste time digging for this information is an invaluable asset to any manager.Having a project management system that can provide a clear visualization of the progress of each assignment will save time, reduce errors and allow managers to plan their next move with more precision.Such an application requires a powerful database to effectively display graphs and resources to the user, which is why this Launchpad App used Redis to create its own project management application.RedisGraph was at the start of this application, due to its ability to simplify the traversal of highly connected data and deliver contextual insights.Let’s take a look at how they managed to bring this project to life. But before we dive in, make sure to check out our range of exciting apps on the Launchpad.You’ll build a powerful visual project management application that will help users visualize and categorize tasks whilst highlighting the different relationships between them using Redis. A task can be an idea, goal, epic feature, simple task or bug.From start to finish, the core objective of this application is to make monitoring clusters of tasks simple and easy. Below we’ll take you through each step,along with the required components and their functionality.Install the below software:First, ensure that you have a working Docker environment. Pull the images and start the containers:Database ‘codered_development’ and  ‘codered_test’ gets created.Once you’ve set this up, load the sample data into the PostgreSQL and Redis databases:The application should now be available at http://localhost:3000.First click on the plus sign on the top left hand side of the screen to create a new task. You’ll then be presented with the below image. At the top you can create a title for the task along with a description in the text box below.Sitting just below the title you’ll see a panel of drop down menus. On the far left you can decide on the nature of the task. In the middle you can set the due date for the task to be completed and on the far right you can assign the task to a specific individual.A core benefit of this app is being able to see the relationships between different tasks, as you’ll see from the following image.To achieve this, first click on a task to get access to its core menu. From here you’ll see a drop-down menu at the bottom called ‘Add relationship’ (see image below), where you can decide what relationship this task will have with others.On the right-hand side of this bar, you can choose which task you want the relationship to be shared with.As a user, you’ll want to view a task. A popup should display the title, description, deadline, status, type and the person who’s assigned the task.You can modify the title, description, deadline, status, type and assigned person of the task. Doing so is easy. Click on a task and you’ll have theTo delete a task, simply click on the trash icon at the bottom right hand side of the task menu.  A confirmation modal should be displayed before the task is deleted.As a user, you’ll want to create a relationship between two tasks. A relationship should have the following type:OrAt some point you’ll probably want to delete a relationship. No confirmation modal should be displayed before the task is deleted.Projects are stored relationally in PostgreSQL. A project has the following properties:id: User identifieruser: Owner of the projectname: Human readable name of the projectThe project is linked to a graph (using id as graph name) A graph has many tasks.Tasks are stored as nodes in Redis Graph. A task is a graph node and has the following properties:A task can be linked to many other tasks by relationships.Relationships are stored as edges in Redis Graph. A relationship is a directed graph edge and has the following properties:from: Source nodetype: One ofto: Destination nodeRelationships are stored as directed edges, but in the interface both directions are rendered. For example, if Task A is blocked by Task B, Task B will be shown as “blocks Task A”. It’s also possible to add relationships in both directions.A graph has many tasks, which are fetched using the following Redis Graph query:A task is created and updated with all its properties using the following query:A task is deleted using the following query:A task’s related nodes are always queried based on relationship type. The related tasks are fetched using the following query:Two tasks are linked to each other using the following query:Two tasks are unlinked from each other using the following query:A DSL was built to accommodate and simplify graph persistence. Its functionality is to provide a small but robust interface that will be familiar to developers who are used to Active Record’s API. The main class that’s implementing this construction can be found at app/graph/dsl.rb.Here’s an example of a query:Disconnected and hidden tasks are a threat to any organization looking to maximize efficiency and outgrow their competitors.Managers who can leverage an application capable of providing a clear visualization of assignments and their relationships can be more organized, think linearly and make quicker decisions in high-pressure situations.RedisGraph is a crucial component as it allows data to be transmitted efficiently whilst projecting a visualization of each task and their relationships.If you want to learn more about this app you can visit it on the Redis Launchpad. And whilst you’re there, make sure to explore all of the other innovative applications that we have available for you.Florian DejonckheereFlorian is an experienced Ruby on Rails developer who works as a software engineer at NephroFlow.You can check out his Github page here."
641,https://redis.com/blog/how-to-build-a-language-processing-pipeline-using-ai-with-redis/,How to build a language processing pipeline using AI with Redis,"October 20, 2021",Redis Growth Team,"Confirmation bias is a problem that all medical professionals have to wrestle with. Not being able to consider different ideas that challenge pre-existing views blights their ability to entertain a new diagnosis in the face of an established one.This is a problem rooted in medical literature, where professionals are more likely to lean towards articles that support their pre-existing views. Diversity of opinion is crucial to moulding a holistic perspective capable of delivering effective diagnoses.In response, a language processing machine learning pipeline was created by this Launchpad App to weed out confirmation bias in medical literature. Using Redis, the Launchpad App created a pipeline that was able to translate text into a knowledge graph through the efficient transmission of data.Let’s investigate how the team was able to achieve this. But before we dive in, make sure to check out all of the different and innovative apps we have on the Launchpad.Let’s explore how you can build a pipeline for Natural Language Processing (NLP) using Redis.We’ll reveal how Redis was used to bring this idea to life by highlighting each of the components used as well as unpacking their functionality.From start to finish, Redis is the data fabric of this pipeline. Its function is to turn text into a knowledge graph. Let’s have a quick overview of what a knowledge graph is and its function in this project.Systems today go beyond storing folders, files and web pages. Instead, they’re cobwebs of complexity that are composed of entities, such as objects, situations or concepts. A knowledge graph will highlight each of their properties along with the relationship between them. This information is usually stored in a graph database and visualized as a graph structure.A knowledge graph is made up of 2 main components:Below is an example of how nodes and edges are used to integrate data.In the first pipeline, you’ll discover how to create the knowledge graph for medical literature by using a medical dictionary. This processes information using RedisGears and stores it in RedisGraph.First, use the below code to unzip and parse metadata.zip, where names of files, titles and years are extracted into HASH:The following code works by reading JSON files – samples in the data/sample_folder: the-pattern-platform/RedisIntakeRedisClusterSample.pyIt also parses JSON into String:And the following code is the main pre-processing task that uses RedisGears: the-pattern-platform/gears_pipeline_sentence_register.pyIt also listens to updates on paragraphs: key:This uses RedisGears and HSET/SADD.How to turn sentences into edges (Sentence) and nodes(Concepts) using the Aho-Corasick algorithmThe first step is to use the following code:The next line of code will create a stream on each shard:Below is to increase the sentence score:How to populate RedisGraph from RedisGearsthe-pattern-platform/edges_to_graph_streamed.py works by creating nodes, edges in RedisGraph, or updating their ranking:the-pattern-api/graphsearch/graph_search.pyEdges with years node ids, limits and years:Nodes:Next use the following code to find the most scored articles:Using the most humorous code in the pipelineShow the below code to your security architect:This is necessary because RedisGears doesn’t support the submission of projects or modules.BERT stands for Bidirectional Encoder Representations from Transformers. It was created by researchers at Google AI and is a world leader in processing national language tasks, including Question Answering (SQuAD v1.1.), Natural Language Inference (MNLI)  and others.Below are some popular use cases of the BERT model:The most advanced code is in the-pattern-api/qasearch/qa_bert.py. This queries the RedisGears + RedisAI cluster, providing users with a question:This queries bertqa prefix on shard {5MP} where PMC140314.xml:{5M5}:44 (see below) is the key of pre-tokenized REDIS AI Tensor (potential answer) and “When air samples collected?” is the question from the user.RedisGears then captures the keymiss event the-pattern-api/qasearch/qa_redisai_gear_map_keymiss_np.py:RedisGears then runs the following:For the non-blocking main thread mode, models are preloaded on each shard using AI.modelset in the-pattern-api/qasearch/export_load_bert.py.Summarization works by running on sentence: prefix and running t5-base transformers tokenizer, saving results in RedisGraph using simple SET command and python.pickle module, adding summary key (derived from article_id) into:The following subscribes to queue running simple SET and SREM commands:This will then update hash in RedisGraph:Redis provides rgcluster Docker image as well as redis-cluster script. It’s important to make sure that the cluster is deployed in high availability configuration. Each master has to have at least one replica. This is because masters will have to be restarted when you deploy your machine learning models.If masters is restarted, and there isn’t a replica, then the cluster will become a failed state and you’ll have to manually recreate it.Other parameters include execution time and memory. Dirt models are 1.4 GB in memory and that’s where you’re required to include proto buffer memory. You also need to increase execution time for both cluster and gears chart because all tasks are computationally extensive, so you need to make sure that the time-outs are accommodated for.Make sure that you install virtualenv in your system, Docker and Docker compose.Wait for a bit and then check the following:RedisGraph has been populated:Whether if the API respondsNote: this will download and pre-load 1.4 GB BERT QA model on each shard. Because of its size, there’s a chance that it might crash on a laptop. Validate by running:Go to the repository on RedisGears cluster from “the-pattern” repo:This task may time out, but you can safely re-run everything.On the GPU or server, configure NVidia drivers:Configure access from instance to RedisGraph docker image (or use Redis Enterprise)While RedisGears allows you to deploy and run machine learning libraries like spacy and BERT transformers, the solution below adopts a simpler approach:Here’s a quick overview of the overall pipeline: The above 7 lines allow you to run logic either in a distributed cluster or on a single machine using all available CPUs. As a side note, no changes are required until you need to scale over more than 1000 nodes.Use KeysReader registered for namespace paragraphs for all strings or hashes. Your pipeline will need to run in async mode. If you’re a data scientist, we would recommend using gb.run to help ensure that RedisGears works and that it will run in batch mode. It will then run in batch mode. Afterwards, change it to register to capture new data.By default, functions will return to output, hence the need for count () – to prevent fetching the whole dataset back to the command issuing machine (90 GB for Cord19).Overall, pre-processing is a straightforward process. You can access the full code here.You can build Aho-Corasick automata directly from UMLS data. Aho-Corasick will allow you to match incoming sentences into pairs of nodes, and present sentences as edges in a graph. The Gears related code is simple:The Redis Knowledge Graph is designed to create knowledge graphs based on long and detailed queries.Preliminary step: Select ‘Get Started’ and choose either ‘Nurse’ or ‘Medical student.”Step 1: Type in your query into the search barStep 2: Select the query that’s most relevant to your searchStep 3: Browse between the different nodes and change the dates of your search with toggle bar at the bottomUsing Redis, the Launchpad App created a pipeline which helps medical professionals navigate through medical literature without suffering from confirmation bias. The tightly integrated Redis system promoted a seamless transmission of data between components, giving birth to a knowledge graph for medical professionals to utilize.You can discover more about the ins and outs of this innovative application by visiting the Redis Launchpad. When you’re there, you might also want to browse around our exciting range of applications that we have available.You can head over to the Launchpad to discover more about the application along with many others in our exciting collections of apps.Alexander MikhalevAlexander is a passionate researcher and developer who’s always ready to dive into new technologies and develop ‘new things.’Make sure to visit his GitHub page to see all of the latest projects he’s been involved in."
642,https://redis.com/blog/how-to-build-a-powerful-search-engine-using-redis-python/,How to Build a Powerful Search Engine Using Python & RediSearch,"October 22, 2021",Redis Growth Team,"While Google dominates the search engine landscape, it remains an awkward tool for programmers to have access to niche-specific resources on software development. This community feeds off content on how to deploy software through different programming languages, which can be found in different awesome lists.Google ranks websites through a strict optimization process that isn’t entirely compatible with content about programming. These in-demand resources are buried deeper in Google’s congested library, making it more challenging for programmers to have access to the most valuable programming content.This creates a need for a search engine that can cut through the clutter to identify in-depth tutorials, blogs and step-by-step guides that can be found on awesome lists… and this is where Awesome Search comes into play.Unlike Google, this tool is more specific than general in its purpose, catering to search terms that demand curated resources about programming that might not rank well on Google.Thanks to Redis, this tool was able to come to fruition. Data transmission between components occurred at lightning-speed and the search engine capabilities of RediSearch made indexing and querying hyper-efficient.Let’s investigate how this Launchpad app got this done. But before we go any further, make sure to check out all of the other exciting applications we have on the Launchpad.In this application you’ll build a search engine that specializes in identifying curated pieces of content on awesome lists. Below we’ll go through each stage in chronological order, highlighting what components are required to create this app as well as unpacking each one’s functionality.An awesome list is a curated list of coding projects within a specific niche, application or use case. Trying to find high-quality programming coding has always been a time-consuming process. Awesome lists removes this barrier by storing these curated lists of code projects in one area, making them more easily accessible for programmers.To get started, you’ll have to install the CLI along with Raycast. Let’s see how this is done.Pre-requisites:Go to https://redis.com/try-free/ and create a new Redis Enterprise Cloud subscription account. Afterwards, create a new database selecting RediSearch as a Module (shown below).Once your database is set up, make sure to save the Endpoint URL and credentials in a safe place.Under this section we’ll generate a token that can be used to access the GitHub API.The next step you should take is to request a personal access token for the Github API here.For detailed steps for deploying Django on the App Engine see the official documentation.In the searchapp/ root.Set your project ID:Then set the connection string/password in the deployment config.ini.Create a dist bundle.Before you push the python package, make sure to create an account with https://pypi.org/. Now let’s install twine Python module using the below command:Upload your python module:You can directly install awesome-search in your local system using the below command:You can directly install awesome-search in your local system using the below command:Usage:A search example would be to search ‘django redis’ projects. Make sure to sort results by stars.OptionsComma-delimited list of languages.Use comma-delimited list of terms to filter awesome lists results. For example, ‘redis,django’ for awesome-redis, awesome-django.Sort results by stars.Hits to return.Step 9. How to install RaycastRaycast is a piece of software that offers a more versatile way of controlling your tools and installing script commands. With just a few keystrokes, you can execute these commands from anywhere on your desktop.It’s an efficient way of speeding up everyday tasks such as converting data, opening bookmarks and triggering dev workflows.To add the script follow the instructions on the Raycast script commands page.If you already have a script directory for your Raycast scripts simply copy the raycast/awesome_search.py script to it.Install Script CommandsTo install Raycast, you first need to install Script Commands:Step 1: Choose your script from the community repo and save them into a new directory. Alternatively, you can use the _enabled-commands folder for this.Step 2:Open the Extensions tab in the Raycast preferencesStep 3: Click the plus buttonStep 4: Add Script DirectoryStep 5: Select directories containing your Script CommandsUseful tip: It’s recommended that you don’t directly load the community script directories into Raycast. This is to avoid potential restructuring and new script commands suddenly appearing in Raycast.All types of resources are prefixed with resource:. This will give you flexibility in extending new resource types such as blogs.To track which awesome lists appear on a repository list, you can simply use a set. The Redis SADD command is used to add members to a set stored at the key. SADD commands return the number of elements that were added to the set. This does not include all of the elements already present in the set.Once you’ve indexed the contents, the set is added as a documentary property for filtering search results by awesome list.Additionally, when you insert a new resource, make sure to maintain a list of unique awesome lists and languages to implement faceted search.Next you must define the index. You define an index by using the RediSearch library.All keys storing resource data are prefixed with resource:. This makes it a lot easier to define a RediSearch index with all the different resource types we want to search.As an option, if only specific resources such as Github Repos were to be indexed, more specific prefixes could be specified:However, before making any queries the index needs to be built.This specifies which fields should be indexed. Additionally the weight argument allows for increasing the effect of matches in certain fields such as “repo_name”.Once the index is created documents are indexed in real-time as they are added to Redis. To add new documents to the index simply create a hash for that document.Full text search across all the resources.Faceted search is a programming trick that involves improving conventional search techniques. It allows users to narrow down search results by applying multiple filters based on faceted classification of the items.RediSearch supports field modifiers in the query. Modifiers can be combined to implement filtering on multiple fields. You can use field modifiers to implement faceted search on specific sources, languages and awesome lists.Alternatively, instead of specifying the source (i.e tweet or GitHub) as a field modifier, separate indexes could be built for each source by providing a more specific key prefix.Having separate indexes will result in faster queries as well as introducing additional complexity for ranking/pagination if the user chooses to search across both sources.Google’s algorithms are not compatible with the resources on active lists, creating a barrier between users and this content that’s in demand. But thanks to Redis, this Launchpad app was able to create Awesome Search, providing programmers with access to invaluable coding content that would otherwise be buried deep in Google’s library.Although each component played a crucial role in its formulation, RediSearch’s ability to find and index content with great efficiency was the crux of creating a powerful search engine. Without these elements, building Awesome Search would not be possible.If you want to discover more about this app, then feel free to check it out on the Redis Launchpad, where we also have a diverse range of different applications for you to discover.Marko ArezinaMarko is a specialist in backend development and is currently sharpening his skills with Shopify.Make sure to check out his GitHub profile to see what other activities he’s been involved in."
643,https://redis.com/blog/redis-for-python-developers-course-redis-university/,Redis for Python Developers Course Is Now Live at Redis University,"August 11, 2020",Andrew Brookins,"Our latest course, Redis for Python Developers (RU102PY) is now live at Redis University! You can sign up for free today.I created this course, so in this post, I want to introduce myself, tell you more about what’s in the course, and give you an example of what you can learn from the full five-week session.I’m a 10+ year Python veteran and author of the book The Temple of Django Database Performance. I created the Redis for Python developers course because Redis is an essential part of the modern internet, and yet finding patterns for writing efficient, well-organized code that uses Redis is hard.There are plenty of established patterns in Python for working with relational databases, but what about Redis? I’ve seen many one-off calls to Redis scattered through Python projects, and encountered plenty of apps that could have benefitted from Redis’ geospatial capabilities or Redis Streams but failed to use them. I built this course to give you the definitive patterns for writing Python code using all the powerful features that Redis has to offer. In the course, you’ll integrate Redis deeply with a Flask application, gaining hands-on experience modeling data in Hashes, Sets, and Sorted sets, building geospatial indexes, and using Redis Streams.You’ll also learn how to squeeze every ounce of performance out of Redis with pipelines and Lua scripts. You’ll build rate limiters and leaderboards, learn how Redis 6’s new access control lists (ACLs) work, and more.Here’s an example of what you’ll learn in Redis for Python Developers. When you use relational databases from Python, you often do so with an object relational mapping (ORM) library like the Django ORM, SQLAlchemy, or Peewee. These libraries let you declare your data model, as shown in this example adapted from the Peewee documentation:In the example app for the Redis for Python Developers course, we use data classes to declare our Redis data models. Data classes are a feature introduced in Python 3.7 that automatically generate “special” methods such as __init__() and __repr__() based on type hints.Here’s an example model from the course:This SiteStats model represents stats for a hypothetical solar array site in the example app. Because we specified the fields the class should have, including last_reporting_time and meter_reading_count, the class’s automatically generated __init__() method takes those parameters and adds them as members on new SiteStats instances.When the app saves a SiteStats instance to Redis, or loads an instance from Redis, it does so using a “schema” class that handles validating, serializing, and deserializing the data.Notice that the model declaration doesn’t mention Redis. That’s by design, and it’s a major part of the example project’s architecture. Rather than using an ORM to validate and persist data to Redis, we break these functions into separate pieces.For validation—and serialization—we use a library called marshmallow. Marshmallow can work with data classes to automatically validate, serialize, and deserialize data based on the type hints stored in the data classes via “schema” classes. Check out this example, which creates a marshmallow schema class for SiteStats instances, with which we validate, serialize, and deserialize SiteStats data to and from a Python dictionary:Let’s try this out in an ipython terminal. First, let’s create a SiteStats instance.Next, let’s use our marshmallow schema class to serialize, or “dump,” the SiteStats instance to a Python dictionary.Now, let’s deserialize, or “load,” the data from a dictionary to a SiteStats instance, and verify that the new object is the same as the original SiteStats object:That’s handy, but what makes this setup even more useful is validation. What happens if we pass bad data into SiteStatsSchema.load()? Let’s find out in our ipython terminal:Pretty cool! It can tell we didn’t give it an integer for the “meter_reading_count” key, and that we’re missing the other required fields.But what about persisting these models to Redis? For that, we use the data access object, or DAO, pattern, which the course covers in depth.I hope this example piqued your interest in the Redis for Python Developers course!Did I mention the course is free? All five weeks of instructor-led learning, access to our community chat server, source code for the example app, and the certificate of completion that you can earn are all free!RU102PY: Redis for Python Developers has something to offer every Python developer, whether you’re building skills for your first Python job or designing a message broker for a microservices architecture. So come and learn with us! I’ll be around on our chat server to answer any questions, and I look forward to meeting you."
644,https://redis.com/blog/youre-probably-thinking-about-redis-streams-wrong/,You’re Probably Thinking About Redis Streams Wrong,"July 6, 2020",Redis,"Download the Tutorial: How to Build Apps using Redis Streams nowRedis Streams is a data type that provides a super fast in-memory abstraction of an append only log. The main advantages of Redis Streams are the highly efficient consumer groups, allowing groups of consumers to uniquely consume from different parts of the same stream of messages, and the blocking operations that allow a consumer to wait until a new data is added to the stream. With the release of version 5.0, Redis launched an innovative new way to manage streams while collecting high volumes of data — Redis Streams. Redis Streams is a data structure that, among other functions, can effectively manage data consumption, persist data when consumers are offline with a data fail-safe, and create a data channel between many producers and consumers. It allows users to scale the number of consumers using an app, enables asynchronous communications between producers and consumers and efficiently uses main memory. Ultimately, Redis Streams is designed to meet consumers’ diverse needs, from real-time data processing to historical data access, while remaining easy to manage.I am personally guilty of describing streams in the wrong way—I’ve defined it as a “series of hashmap-like elements, ordered by time, under a single key.” This is incorrect. The last bit regarding time and key are OK, but the first bit is all wrong.Let’s take a look at why streams are misunderstood and how they actually behave. We’ll evaluate the good and the bad of this misunderstanding and how it could affect your software. Finally, we’ll examine a few non-obvious patterns that take advantage of the little-known properties of Redis Streams.Data processing has been revolutionized in recent years, and these changes present tremendous possibilities. For example, if we consider a variety of use cases — from the IoT and Artificial Intelligence to user activity monitoring, fraud detection and FinTech — what do all of these cases have in common? They all collect and process high volumes of data, which arrive at high velocities. After processing this data, these technologies then deliver them to all the appropriate consumers of data.Redis Streams offers several possibilities for users, including the ability to integrate this new data structure into various apps. In order to make it easier for users to start using Redis Streams, we have written up a few tutorials to help get you started:First, let’s look at the XADD command, as this is where the misunderstanding starts. The command signature as it stands in the official redis.io documentation looks like this:XADD key ID field value [field value ...]key is self-explanatory. id is the timestamp/sequence combo for the new entry, but in reality, it is almost always * to indicate auto generation. The root of this confusion starts with field and value. If you look at the command signature for HSET, you’ll see a pretty similar pattern:HSET key field value [field value ...]The signature for the two commands are only a single argument off and that argument in XADD is almost always a single *.  Looks pretty similar, uses the same terms, must be the same, right?OK. To continue the problem, let’s set aside Redis and look at how programming languages deal with field-value pairs. For the most part, no matter the language, the most representational way of articulating field-value is with a set (no repeats) of fields that correlate to values—some languages will retain order of fields and some will not. Let’s look deeper with a small cross-language comparison:This maps nicely to Redis hashes—all of these can articulate the properties of a hash, which is unordered and has no repeats. PHP arrays, Python dicts, and JavaScript maps can define field order, but if you’re working with hashes in Redis, then it doesn’t matter, you just have to know that you can’t depend on this order at the application level.For most people, the natural conclusion has been that since the command signatures of HSET and XADD have a correlation, then they probably have a similar correlation in return. Indeed, at the protocol level in RESP2, these both are returned as interleaved RESP Arrays. This is continued in early versions of RESP3 where the response for HGETALL and XREAD were both maps (more on that later).Normally, I code in JavaScript and occasionally Python. As someone who communicates about Redis, I need to reach as many people as possible and these two languages are pretty well understood, so a demo or sample code in either will be understood by a high percentage of the developer world. Recently, I had the opportunity to talk at a PHP conference and needed to convert some existing Python code to PHP. I have used PHP on and off for close to 20 years, but it’s never been a passion of mine. The particular demo didn’t fit well into a mod_php style execution, so I used swoole and its co-routine execution (on a side note, being comfortable in the JavaScript world, swoole makes PHP very, very familiar for me). The library was a tad out-of-date and required sending raw Redis commands without any real client library assistance in decoding the returns in a high level way. Generally, sending raw Redis commands and decoding the results provides a little more control and isn’t onerous.So, when sending commands to XADD, I was building the field-value portion and had an off-by-one error (chalk this up to diving back into PHP after years of absence). This resulted in me unintentionally sending something along the lines of:XADD myKey * field0 value1 field0 value2 field2 value3Instead of sending correlated fields and values (field0 to value0 and so on).Later in the code, I was putting the results of XREAD into an existing PHP array (which is associative) in a loop, assigning each field as a key with each value and skipping anything already set. So, I started with an array like this:And ended up with an array like this:I could not fathom how that was possible. I was quickly able to track down the bug on why value1 got assigned to field0 (the above mentioned off-by-one error in my XADD), but why wasn’t field0 set as value2? In HSET, this behavior for adding fields is basically upsert—update a field if it exists, otherwise add the field and set the value.Examining the MONITOR logs and replaying the values, I ran XREAD as follows:Repeats are present and recorded, not upserted; additionally the order is preserved. This is nothing like a hash!Thinking about this using JSON as an approximation, I thought stream entries looked like this:But they actually look like this:Good newsIf you have code that currently works with Streams and you’re assuming entries are like hash maps, you’re probably fine. You just need to watch for potential bugs regarding putting in duplicate fields, as they may not behave the way you expect in a given application. This may not apply in every circumstance or client library, but a good practice would be to supply a data structure that doesn’t allow repeats (see above) and serialize this into arguments when supplying them to XADD. Unique fields in and unique fields out.Bad newsNot all client libraries and tools get it right. Relatively low-level client libraries (non-exhaustive: node_redis, hiredis) don’t do much as far as changing the output from Redis into language constructs. Other higher-level libraries do abstract the actual return of Redis into language constructs—you should check to see if your library of choice is doing this and put in an issue if it does. Some higher-level libraries got it right from the start (stackexchange.redis), so kudos are due there.The other part that is a bit bad: if you were a very early adopter of RESP3, you might have experienced XREAD / XREADGROUP returning the RESP3 map type. Until early April, the under-development version of Redis 6 was confusingly returning maps with repeats when reading Streams. Thankfully, this was resolved and the GA version of Redis 6—the first time you should have been really using RESP3 in production—shipped with the proper return for XREAD / XREADGROUP.The fun partSince I’ve gone over how you’re probably wrong about Streams, let’s think for a bit about how you can leverage this heretofore-misunderstood structure.Apply semantic meaning to order in Stream entriesSo, you actually have three vectors to play with in this pattern. Imagine storing a path for vector graphics. Each Stream entry would be a unique polygon or path and the fields and values would be the coordinates. For example, take this fragment of SVG:This could be articulated as:> XADD mySVG * 50 150 50 200 200 200 200 100Each additional shape would be another entry on the same key. If you attempted to do something like this with a Redis Hash, you’d have only two coordinates and no order guarantee. Admittedly, you could do this with things like bitfields, but you’d lose a ton of flexibility with regards to length and coordinate size. With Streams, you could probably even do something neat with the timestamp to represent a series of shapes that appear over time.Create a time-order set of sequenced itemsThis one requires a tiny hack, but could provide a lot of functionality. Imagine you are keeping a sequence of arrays-like data. Effectively, an array of arrays—in JSON you could think if it something like:You could articulate this as a series of Stream entries with one small nuance: you need to make sure the number of (pseudo) elements in your inner lists are not odd. If they are odd, as above, you’ll need to record that somehow—here is how I’m doing it with an empty string:You gain a lot in this pattern at the (minor) expense of having to filter out any 0-length strings.Streams as a pagination cacheA tricky thing that you see pretty often is a listing of items on a site (e-commerce, message boards, etc.). This is commonly cached but I’ve seen people have fits trying to find the best method to cache this type of data—do you cache the entire result set into something like a sorted set and paginate outward with ZRANGE, or do you store full pages at string keys? Both ways have merits and downsides.As it turns out, Streams actually work for this. Take, for example, the e-commerce listing. You have a series of items, each with an ID. Those items are listed in a finite series of sorts that usually have a reversal (A-Z, Z-A, low to high, high to low, highest-to-lowest rating, lowest-to-highest rating, etc).To model this type of data in a Stream, you would determine a particular “chunk” size and make that an entry. Why chunks and not full result pages at an entry? This allows for you to have different-size pages in your pagination (e.g. 10 items per page could be made of 2 chunks of 5 each, while 25 per page would be 5 chunks of 5 each). Each entry would contain fields that map to product IDs and the values would be the product data. Take a look at this simplified example with an artificially low chunk size:When you want to retrieve the cached values, you would run XRANGE with the COUNT argument set to the number of chunks that make up a result page for your interface. So, if you want to get the first page of four items you would run:To get the second page of 4 items, you’ll need to provide a lower bound Stream ID incremented by 1, in our case, the lower bound would be 0-2.This does provide a computational complexity advantage over Sorted Sets or Lists as XRANGE is effectively O(1) in this use, but there are a few things to keep in mind:Like any key, you can use expiry to manage how long the Stream stays around. An example of how this can be done is in stream-row-cache.I hope this post gives you some additional context on how Streams really work and how you can leverage these largely unknown properties of Streams in your applications."
645,https://redis.com/blog/enabling-secure-connections-to-redis-enterprise/,Enabling Secure Connections to Redis Enterprise,"June 19, 2018",Tague Griffith,"Building software that utilizes secure connections to a server should be a skill every developer possesses. Even if you choose not to enable SSL in a specific production environment, you should know how to secure connections to every server that you work with. This post is the promised follow up to Enabling Secure Connections to Redis Enterprise Cloud in Python for our Java fans.This post walks through an easy process to turn on, test and configure encrypted connections between Redis Enterprise Cloud and a Java client program using SSL. One of the confusing parts of enabling SSL for Java programs is converting the certificates and keys into a format Java understands.  This is a Java-only procedure we have to sneak in between the test and configuration steps of our process.ToolsetGrab these tools, so you can close that ticket for enabling SSL on Redis:You will also need to have the Bash shell installed so you can run a small script that we provide.  If your operating system doesn’t include Bash as part of the standard install, you will need to install it or translate our script into your preferred shell.To use SSL, your Redis Enterprise Cloud subscription must have the SSL feature enabled. If your account isn’t already enabled for SSL, you will need to contact the Redis support team to enable it. You’ll find a link to contact the support team in the main menu of your account dashboard. Documentation for setting up SSL can be found in the Redis Enterprise Cloud Operations and Administration Guide.SSL vs. TLSJust a reminder: as in our Python post, we are going to follow the informal convention of using the acronym “SSL” to refer to either “SSL” or “TLS”-secured connections. Even though Redis Enterprise Cloud currently (June 2018) uses version 1.2 of the TLS protocol to secure connections, both Redis Enterprise Cloud and Jedis use “SSL,” so we are going to follow suit.Steps One and TwoThe first two steps are identical to those used to set up SSL for a Python client. Instead of repeating it all here, we’re just going to refer you back to our Python post. There is lots of useful information in that post for working with SSL in any language. I recommend that you read everything up to Step 3 before you continue.The process starts to diverge for Java after Step 2, so we’re going to insert a Step 2.5 here to transform the credential files you downloaded from Redis Enterprise Cloud into a format that works well with JavaSE.Step 2.5: Transforming the CredentialsAs a standard part of JavaSE, Oracle provides a variety of security services, including the Java Cryptography Architecture (JCA). The JCA defines a set of APIs for Java applications to invoke security services that are implemented by plug-in providers. Applications are free to use any available provider (who often provide additional services beyond the standard API), but most applications use the default providers shipped with the JDK.The challenge of using the default providers is that they don’t support the PEM format that is commonly used with Linux SSL libraries to store certificates and keys. In the JCA, credential material is stored in password-protected keystore repositories. In Java applications, it is common to use two repositories: one is referred to in documentation as a keystore and the other is referred to as a truststore.  The keystore is used to store the certificates and private keys for the client software and the truststore is used to store trusted certificates from certificate authorities. The PEM format is used by Redis Enterprise Cloud, so we’ve created a script using OpenSSL and the Java Keytool to execute all of the commands necessary to convert your PEM files into a truststore and keystore usable for your Java client programs.The following script, transmogrify.sh, should be run from the directory where you unpacked your redis_credentials.zip. The script uses the Redis Certificate Authority certificate (redis_ca.pem) to create a truststore (redis_truststore.p12) and uses the client certificate (redis_user.crt) and corresponding private key (redis_user_private.key) to create a keystore (redisclient_keystore.p12). While the client keystore is password protected, you should still take steps to ensure the security of your keystore. Both the keystore and the truststore will need to be distributed to every client that connects to a particular Redis database instance, so again you will want to integrate these files into your credential management system.Java versions prior to JDK9 favored the proprietary Java KeyStore (JKS) format, but starting with JDK9, Java defaults to the industry standard PKCS12 format. This script creates PKCS12 format stores, but if you can modify the script, use the JKS format by changing the store type option and the file extension.Step 3: Configure SSL in your client codeThe final step to enable SSL in your Java client is to modify the client code to establish an SSL connection. Our sample code will establish a secure connection to our Redis Enterprise Cloud instance, then send the Redis PING command. Our modified code looks like:You’ll notice, nowhere in the code do we reference the keystore or the truststore that we created for our Java program in Step 2.5. The default behavior of the JCA providers authenticates the server; we just have to provide our key and certificate information. This can be done with system properties read by the JCA providers. Add the following parameters to your IDE application configuration or the java command line:modifying them to use the appropriate locations and passwords from your system.The keystore properties:specify the type (aka format), location and password for the keystore created by the transmogrify script. Similarly, the trustStore properties:specify the type, location and password of the trustStore. The final property javax.net.debug=ssl:handshake is an optional property used to enable debugging information.Configuring Jedis to use SSL is not particularly difficult, but it can be off-putting at first if you’re unfamiliar with the steps necessary to convert PEM credentials into Java key and truststores.Hopefully, this post provided our Java fans with the same gentle introduction to setting up a secure connection to Redis Enterprise Cloud that we gave to the Python developers. I do want to leave you with one final reminder about security: most organizations have specific policies and procedures for managing passwords and private keys. Please make sure to check with your operations and security teams to ensure you’re following their guidelines."
646,https://redis.com/blog/redisearch-1-4-phonetics-spell-check/,RediSearch 1.4: Phonetics & Spell Check,"August 24, 2018",Redis,"It’s always exciting when a new version of RediSearch comes out—we just released version 1.4 (yes, we skipped 1.3 to align with a new versioning methodology). This new version has two key features which add quite a bit of smarts to querying:Let’s first take a look at spell check. Everyone knows what spell check is from a broad perspective, but let’s examine how it works in a search engine context. It’s best to think of it as a primitive that would power a “did-you-mean” feature.Take, for example, this particular query:As a human, you probably could know that this means “Hockey stick” but without spell check, the returned results would not be great. Here is how RediSearch 1.4 can help you. First, you run the query through the FT.SPELLCHECK command; it will return nothing (empty list or set) if everything is spelled correctly. If the the query passed into FT.SPELLCHECK has words that seem misspelled, RediSearch will return which words are questionable and some suggestions.Let’s run that example from above on a populated data set:Easy enough! Each misspelled word will return “TERM” and the misspelled word, and then any alternatives along with their respective confidence scores, sorted by highest score. The example only has one alternate but you certainly can have more.Let’s say you have a security guard incident report index created with this command:The feature assumes that any word that exists in the index is spelled correctly (e.g. there is no built-in dictionary of valid words). However, you may have words that you want to assume are spelled correctly that are not yet indexed. This index will contain reports from people involved in security incidents and might have many slang words.(empty list or set) means that the spell check thinks that it is misspelled, but it doesn’t have a correction. To remedy this situation, we’ll add some slang words to a dictionary:Now, we can run a spell check on a query and EXCLUDE these terms. The terminology here is a bit confusing, but think of it as excluding words from being spell checked, e.g. assuming they are spelled correctly.The response of (empty list or set) means that all the words in the passed string are not spelled incorrectly. This only gets us so far though. What happens if one of these new words are misspelled? Let’s see.So, the spell check has identified the word is spelled wrong but it doesn’t know how to correct it. To do this, you’ll need to INCLUDE the dictionary as well as EXCLUDE it.Now you can see that it’s correcting the spelling.It’s important to revisit that you don’t need to bother with the custom dictionaries if you already have documents with these terms. Let’s say you have a document like this:Since this report contains all these slang words, they are automatically populated into the spell check without custom dictionaries, both inclusively and exclusively:So it’s best to use custom dictionaries for domain-specific terms that haven’t been mentioned in your existing documents yet.Phonetic Matching solves the canonical problem of searching for someone named “Jon” but typing it as “John”—sounds the same, but they’re spelled differently. If you want to go down a rabbit hole, try to figure out why both spellings exist as modern English names. I digress.This is a tricky search problem because even with the tricks that search engines use (like stemming) this doesn’t help. To combat this, search engines can use algorithms that break text down into language-specific codes based on the linguistic pronunciation rules. RediSearch does this using an algorithm called Double Metaphone, which has a fascinating history, look it up sometime.First you need to define the fields you want to index phonetically (only TEXT fields obviously). Let’s create a small index with two phonetic fields:So, now we have phonetics enabled as we add documents on both the name and almamater fields. Let’s add some documents:When RediSearch adds the documents to the index, it isn’t just recording “jon” or “john,” it’s recording both with their metaphone codes. In this case, both “jon” and “john” translate into JN.  To search for these you’ll just need to search on a specific field that is denoted as PHONETIC.See how this is matching both “john” and “jon”?  This is because they have the same Double Metaphone translation. At this point, you may be dancing at your desk at the wonder that is phonetic matching in a search engine. All of your problems are solved!Not so fast—phonetic matching should be used carefully. It’s a very sharp tool, but it can cut you. Let’s take for example the second field in our micro-example:“Trent” and “Toronto” look and sound nothing alike! This not a bug, but rather a weakness in the Double Metaphone algorithm that takes away some information and emphasizes others. Metaphone should be used carefully on fields that are likely to not contain sound-alikes. You can also use an attribute to turn off phonetic searching:RediSearch 1.4 has some exciting features that add gobs of flexibility to search. These features get at the heart of what a good search engine does: it accommodates human error without losing sight of the user’s true intention."
647,https://redis.com/blog/connection-pools-for-serverless-functions-and-backend-services/,Connection Pools for Serverless Functions and Backend Services,"October 10, 2019",Redis,"When writing a server application that connects to a database, you often have to deal with connection pools, with problematic repercussions if you ignore the issue for too long. So, let’s dive into the problem, and explore how connection pools address it.Server applications share a common requirement: they must respond to independent requests originating from multiple clients. A naively written server application that uses Redis (or any other database) will open a new connection to Redis for each request. Unfortunately, this approach doesn’t scale, because you can open only a limited number of connections at the same time before everything blows up. Opening and closing connections is also not cheap: not for the application and not for the database.The good news is that this problem is entirely solvable by changing the code of your application. The bad news is that it’s not always trivial to do so by yourself, and this is where patterns such as connection pools can help.Serverless functions are a relatively recent addition to cloud offerings, but in many ways they resemble old CGI scripts: a small snippet of code that gets invoked per-request. If each invocation is independent from all others, does that mean it’s impossible to share connections? Not exactly.Normally, when an application calls a serverless function, the process remains active for a while before being shut down, in case more requests come in. As long as the process remains active, it can maintain a long-running connection with Redis, but it’s up to you to implement that correctly.In general, it’s easy to create a persistent connection, you just have to instantiate it outside of the main function’s body. Here’s an example with AWS Lambda using JavaScript:Note that you will need to upload this script in a Zip archive that includes node_redis, a Redis client for Node.js (the AWS documentation explains this in more detail).The same concept can be applied to other languages and Function-as-a-Service (FaaS) cloud offerings (Google Cloud Functions, Microsoft Azure Functions, and so on).In the case of JavaScript, the client doesn’t offer a connection pool because of the single-threaded nature of Node.js. In the example above, we were trying to reuse the same connection across more than one request, but if you were to use a function in Go, or in another language that has multi-threaded concurrency, the client would need also a connection-locking scheme, such as a connection pool.The basic principle is simple: a client that implements a connection pool opens n connections to the database and then has a mechanism to mark connections as “available” or “in use,” and use only the free ones. Many connection pools work as in-place replacements for single connections, so calling .connect() will pluck a connection from the pool (i.e. mark it as “in use” and return the real connection to the caller), while .close() will simply put it back (without actually closing it).If you’re using a multi-threaded language like Go, make sure to choose a client that supports connection pooling. Go-redis is a good choice from this perspective, as you can read in the documentation.Some clients also let you send commands directly without first plucking a connection from the pool. While handy, there are some things to keep in mind when using a pool this way (more on this below).With serverless functions, the whole application architecture is extremely simple: it’s just a function. When dealing with “serverful” services, though, sharing a connection becomes more burdensome when concurrency is involved.A simple socket connection can’t be directly used by more than one thread, as some degree of coordination is required to avoid sending bits and pieces of multiple requests at the same time, which would result in a mix incomprehensible to the receiver.In such cases, connection pools are a good way to make each sub-component seem like it is the only one using a connection, but even connection pools can’t completely abstract away every detail of connection management.When a connection is plucked from the pool, your code must ensure the connection eventually gets put back. Connection pools implement an upper limit on how many connections can be open at any time (remember, limiting the total amount of connections is part of the goal), so leaking connections will eventually deadlock your service when the last .connect() hangs forever, refusing to open a new connection and waiting in vain for an existing one to return to the pool.Occasionally, code that was not designed to be long-running gets incorporated in a bigger project and starts leaking connections. To prevent leaks, you just have to make sure to .close() the connection once you don’t need it anymore, but it’s not always easy to implement that in practice, especially in big, messy projects.Let’s see a good way to use a connection pool and ensure proper cleanup in Python.To show you some sample code, I’ll use aio-redis, a Redis client for Python that supports asyncio. With aio-redis is possible to use a connection pool directly without plucking a connection first, as shown here:As mentioned earlier, this works fine for simple usage, but explicitly plucking a connection from the pool is preferable in some situations, particularly when an operation takes a long time to complete, such as in blocking operations on Streams, Lists, Sorted Sets, or WAIT.While Redis commands tend to be very fast, some commands are designed to be blocking, meaning that they will not return an answer until certain conditions are met. For example, blocking reads on Streams (XREAD) will wait for new entries to get into the stream when used with the BLOCK option (without it, XREAD would immediately return with an empty result-set). Keep in mind that these operations block the client, not the server. Redis will still be able to respond to commands sent through other connections.Those types of commands are a problem for the usage pattern that we previously showed, because aio-redis doesn’t know for how long a given command will run and could decide to enqueue a new command to a connection that’s busy with a blocking command. This means that in the previous example, if there was another async function using the pool to do a blocking XREAD, our SET and INCRBY commands might have taken a surprisingly long time to complete, or might even timeout.In those cases, you need to pluck a connection from the pool explicitly and also make sure to return it once you’re done. Python helps with the last part with a language feature called context managers, which you can access using a with block. The context manager block is created around a resource that must always be cleaned up (connections, file descriptors). At the end of the block, regardless of whether we are exiting successfully or by throwing an exception, the context manager triggers the appropriate cleanup procedure, which in our case consists of returning the connection to the pool, as shown here:(If you’re familiar with context managers and asyncio, you might notice that the with await pool … part is a bit odd, as this is usually expressed as async with pool … . This is a small quirk of the implementation of aio-redis, so everything still works as expected. You can find more information on this issue here.)Here’s another special case: MULTI/EXEC transactions. Make no mistake, transactions aren’t client-blocking operations, but they do make special use of the connection.When you send MULTI the connection changes state and Redis starts enqueueing all commands, instead of executing them immediately. When a command is successfully enqueued (i.e., it doesn’t contain blatant formatting errors), Redis replies with OK. This means that the connection is not literally blocked, but it can’t really be used to multiplex commands from multiple sub-parts of the program that are unaware of the fact that there is a transaction taking place.Once you call EXEC or DISCARD, the whole transaction will respectively succeed or fail, and the connection will be returned to a normal state.For this reason many clients have dedicated objects that represent a transaction. Normally transaction objects don’t even send the commands until you conclude the transaction. This improves performance without changing the semantics, since, as mentioned earlier, the commands will be enqueued by Redis only until the transaction is finalized:Connection management is an important part of any server-side application because it’s often a sensitive path, given the one-to-many relationship between servers and clients. The promise of infinite scalability in serverless functions can cause problems when you’re not properly managing connections, but fortunately the solution is easy to implement.For more complex architectures, connection pools allow you to think about connection management only at the local (sub-component) level, but you can’t completely forego connection management, especially when doing operations that make special use of the connection, such as transactions or blocking operations."
648,https://redis.com/blog/getting-started-with-redis-streams-and-java/,Getting Started with Redis Streams and Java,"January 7, 2020",Tugdual Grall,"As a new Enterprise Technical Account Manager at Redis, one of my first tasks was to learn more about Redis. So I started digging in, and quickly discovered Redis Streams. As a big fan of streaming-based applications, I am thrilled to share what I’ve learned about how to use Redis Streams and Java.Redis Streams is a Redis data type that represents a log, so you can add new information and message in an append-only mode (Note: This is not 100% accurate, since you can remove messages from the log, but it’s close enough.)  Redis Streams lets you build “Kafka-like” applications, which can:In addition, Redis Streams has the concept of consumer groups. Redis Streams consumer groups, like the similar concept in Apache Kafka, allows client applications to consume messages in a distributed fashion (multiple clients), making it easy to scale and create highly available systems.So, while it may be tempting to compare Redis Streams and Redis Pub/Sub and decide that one is better than the other, these two features aim to accomplish different things. If you’re evaluating Pub/Sub and Redis Streams and it’s not immediately clear, you might want to think either more about your problem to be solved or re-read the documentation on both.(Enroll in the Redis University: Redis Streams course to learn more.)The best way to learn how to use Redis Streams and Java, is to build a sample application. The redis-streams-101-java GitHub repository contains sample code that shows how to post messages to a Stream and consume messages using a consumer group. To get started, you’ll need Redis 5.x, Java 8 or later, Apache Maven 3.5.x, and Git.Redis has many Java clients developed by the community, as you can see on Redis.io. My current favorite for working with Redis Streams is Lettuce, so what I use in this sample app. Let’s walk through the steps involved in creating the sample project:Add the dependency below to your project file:Import the following classes:Then connect with:When your application is done with the connection, disconnect using the following code:Once you have a connection, you can send a message. In this example, I let Redis generate the message ID, which is time-based, and build the body using a map representing Internet of Things weather data capturing wind speed and direction in real-time:Here’s what’s happening in the code:(The complete producer code is available here.)Redis Streams offers several ways to consume and read messages using the commands: XRANGE, XREVRANGE, XREAD, XREADGROUP. To focus on how to build an application with Apache Kafka, let’s use the XREADGROUP command from Lettuce.The consumer group allows developers to create a group of clients that will cooperate to consume messages from the streams (for scale and high availability). It is also a way to associate the client to specific applications roles; for example:Each of these consumer groups will act independently, and each of this group could have multiple “consumers” (clients).Here’s how it works in Java:This code is a subset of the main() method. I removed the connection management part to make it more readable. Let’s take a look at the code.The complete consumer code is available here.Now that you have a better understanding of the code, let’s run the producer and consumer. You can run this from your IDE, or using Maven, but here’s how it works in the Maven CLI. Start by opening two terminals, one to produce messages and one to consume them, then follow these steps:Step 1: Clone and build the project:Step 2: Post a new message:Step 3: Consume messagesOpen a new terminal and run this command:The consumer will start and consume the message you just posted, and wait for any new messages.Step 4: In the first terminal, post 100 new messages:The consumer will receive and print all the messages.Step 5: Kill the consumer and post more messagesLet’s do another test: Stop the consumer using a simple Ctrl+C and then post five new messages:The messages are not yet consumed by any application, but are still stored in Redis Streams. So when you start the consumer, it consumes these new messages:This is one of the differences between Redis Streams and Redis Pub/Sub. The producer application has published many messages while the consumer application was not running. Since the consumer is run with StreamOffset.lastConsumed(), when the consumer is starting, it looks to the last consumed ID, and starts to read the streams from there. This method generates a XGROUPREAD command with the group.This small project was designed to show you how to use Lettuce, a Java client for Redis, to publish messages to a stream, create a consumer group, and consume messages using the consumer group.This is a very basic example, and in upcoming posts I plan to dive into how to work with multiple consumers and how to configure the consumer group and consumers to control which messages you want to read."
649,https://redis.com/blog/beyond-the-cache-with-python/,Beyond the Cache with Python,"July 7, 2020",Guy Royse,"If you’re a Python developer—and since you’re reading this, you probably are—you’ve almost certainly used Redis and thought it was a great cache. (That was my first impression, too.) And Redis does make a great cache. But it turns out that Redis can solve a lot more problems than just caching.We’re going to explore some of those other uses for Redis and Redis Enterprise. For fun, I’m using the Bigfoot data I used in my blog post on using geospatial data in Redis. And, since we’re Python developers, all of the examples will, of course, be in Python!For the code samples below, I’ve chosen to use the aioredis client library as it has great support for async/await. If you’re not familiar with async/await, we have a great blog post demonstrating how it helps improve performance.Redis has numerous data structures for you to take advantage of: strings, hashes, sets, and lists to name a few. They’re all great for storing data, but a list can also make a great queue.To use a list as a queue, you simply push new items to the end of the list using RPUSH and then pop them off the front of the list using LPOP or BLPOP. Since Redis makes all changes in a single thread, these operations are guaranteed to be atomic.Take a look at this code that adds a few Bigfoot sightings to a queue:It’s pretty straightforward. We just call redis.rpush on line 18 and it pushes the item onto the queue. Here’s the code that reads from the other end of the queue. It’s equally simple:Lines 11 and 12 endlessly loop as they await and print Bigfoot sightings pushed onto the queue. I chose to use redis.blpop instead of redis.lpop because it blocks the client and waits until there is something in the list to return. There’s no point in making Redis, our Python code, and the network between them churn through endless polling if we don’t have to. Much more performant to wait for something to process!There are other cool commands in Redis to make lists work as queues, or even stacks. My favorite is BRPOPLPUSH, which blocks and pops something from the right side of a list and pushes that popped value onto the left side of a different list. You can use it to have queues feed into other queues. Neat stuff!Redis has a few bits that aren’t really data structures. Pub/Sub is one of those. It’s just what it sounds like, a publish and subscribe mechanism built right into Redis. With just a handful of commands you can add robust Pub/Sub to your Python applications.We’ll start with subscribing to events, since it’s easier to see an event if you subscribe to it! Here’s the code:I made a choice here to subscribe to a glob-style pattern using redis.psubscribe on line 10, as I want to receive all the Bigfoot messages. By using bigfoot:broadcast:channel:* as my pattern, I’ll receive all events published that start with bigfoot:broadcast:channel:.The redis.psubscribe (and the less-patterned redis.subscribe) functions are both variadic, so they return Python lists—one entry for each argument. I destructure (or unpack in Python parlance) that list to get the one channel I asked for. Once I have that channel, I make a blocking call to .get to await the next message.Publishing events is very simple. Here’s the code:The key line to understand is line 18, where I publish the message to the desired channel using the well-named redis.publish function.It’s worth noting that Pub/Sub is a fire-and-forget mechanism. If your code publishes an event and nobody is listening, it’s lost forever. If you want your events to stick around, consider using the aforementioned queue or check out our next topic.Redis can be used to publish and read events to a stream. Redis Streams are a big topic even though there are only a few commands to master. But from Python, it’s reasonably straightforward and I’ll show you how to do it.The following code adds three Bigfoot sightings to a stream:The important code here is in lines 17 and 18, where we use the redis.xadd function to add the fields of a sighting to the stream.Each added event has a unique identifier containing the timestamp in milliseconds since the start of 1970 and a sequence number joined with a dash. For example, as I write this, 1,593,120,357,193 milliseconds (1.59 gigaseconds?) have expired since midnight on January 1, 1970 (the Unix epoch). So, if I had run the code and that command had executed in Redis at that exact moment the event id would be 1593120357193-0.When you add an event, you can specify ‘*’ instead of one of these identifiers and Redis will use the current time to generate one. And, since the redis.xadd function defaults it to that value for you, you don’t need to worry about it too much.Until you go to read from the stream that is. When you read from a stream, you need to specify a starting identifier. You can see that on line 10 where we have it set the variable last_id as 0-0 which represents the first record for the first moment in time:On line 12, using the redis.xread function, we ask for (at most) five events from the stream that are after 0-0. This returns a list of lists which we loop over and destructure to get the fields and the identifier of the event. The identifier of the event is stored for future calls to redis.xread so we can get new events and reread the old ones.Redis can be extended to add new commands and capabilities. There are scads of modules available for things as diverse as time-series data, and, for this example, search.Search is a powerful search engine that ingests data ridiculously fast. Some folks like to use it for ephemeral search but you can use it to search in lots of ways. Here’s how to use it:On lines 12 and 13 we use FT.CREATE to create an index. An index requires the schema describing the fields in each document we will be adding. In my example, I’m adding Bigfoot sightings and we have a title and a classification—both of them text fields.Once we have an index, we can start adding documents to it. This happens on lines 27 and 28 with the FT.ADD command. Each document requires a unique ID, a rank between 0.0 and 1.0, and the fields that make it up.With the index loaded with documents, we can now search it using the FT.SEARCH command and a query. You can see this on line 31. The particular query (on line 20) is “chase|east” which instructs Search to find documents containing either of these terms. In our case, this will return two documents.Redis can also be used as a database. A wicked fast, in-memory database. Just add the data you want to Redis and go read it later. This example uses hashes to do this, which is a good data structure for modeling the types of records you might want to store, and includes the primary key to the data as part of the key name:I know what you’re thinking: “What if I turn the computer off? What if it crashes? Then I lose everything!” Nope. You can modify your redis.conf file to persist your data in a couple of different ways. And, if you’re using Redis Enterprise, we have solutions that manage that for you so you can just use it and not worry about it.RedisInsight isn’t Python specific but it’s something any developer would find useful. What is RedisInsight? It’s a free, capable GUI for looking at and managing the data you have in Redis. With it you can look at your queues and streams, execute searches with Search, and browse all the data in your database. All the stuff I’ve just shown you!It supports Search and Query, of course, including JSON, and Time Series. I’ve been using RedisInsight since it was first released and have found the visuals to be particularly beautiful and genuinely useful. Go check it out!If you want to try some of these examples yourself, all of my code is up on GitHub. You can clone it and get started. If you’re a Docker user, there is a shell script named start-redis.sh that will pull down an image and start a version of Redis that works with all these examples.And once you’re done playing and want to build some software, sign up and try Redis Enterprise Cloud. It’s the same Redis you know and love, but managed for you in the cloud so you can focus on your software."
650,https://redis.com/blog/how-to-use-redis-in-infrastructure-microservices/,How to Use Redis in Infrastructure Microservices,"November 24, 2020",Martin Forstner,"Back in 2019, I wrote about how to create an event store in Redis. I explained that Redis Streams are a good fit for an event store, because they let you store events in an immutable append-only mechanism like a transaction log. Now, with an update of the sample OrderShop application introduced in that blog, I’m going to demonstrate how to use Redis as a message queue, further demonstrating Redis Enterprise’s many use cases beyond caching.Redis is a great solution for creating infrastructure services like message queues and event stores, but there are a few things you need to take into account when using a microservices architecture to create a distributed system. Relational databases were often good for monolithic applications, but only NoSQL databases like Redis can provide the scalability and availability requirements that are needed for a microservices architecture.Distributed systems imply a distributed state. According to the CAP theorem, a software implementation can deliver only two out of these three attributes: consistency, availability, and partition tolerance (hence CAP). So, in order to make your implementation fault tolerant, you must choose between availability and consistency. If you choose availability, you’ll end up having eventual consistency, which means that the data will be consistent but only after a period of time has passed. Choosing consistency impacts performance because of the need to synchronize and isolate write operations throughout the distributed system.Event sourcing, which persists the state of a business entity such as an order, or a customer, as a sequence of state-changing events, goes for availability instead of consistency. It allows write operations to be trivial, but read operations are more costly because, in case they span multiple services, they may require an additional mechanism such as a read model.Communication in a distributed system can be brokered or brokerless. Brokerless styles are well known, with HTTP as its most famous instance. The brokered approach has, as the name implies, a broker between the sender and the receiver of a message. It decouples the sender and receiver, enabling synchronous and asynchronous communication. This results in more resilient behavior as the message consumer does not have to be available at the moment when the message is sent. Brokered communication also allows independent scaling of sender and receiver.(For more information, see our post on What to Choose for Your Synchronous and Asynchronous Communication Needs—Redis Streams, Redis Pub/Sub, Kafka, etc.)The “Hello World” of a microservice architecture is the OrderShop, a simple implementation of an e-commerce system using an event-based approach. This sample application uses a simple domain model, but it fulfils the application’s purpose.OrderShop is orchestrated using Docker Compose. All network communication is done over gRPC. The central components are the event store and the message queue: each and every service is connected to and only to them over gRPC. OrderShop is a sample implementation in Python. You can see the OrderShop source code on GitHub.(Note: This code is not production-ready and is for demo purposes only!)In this case, the server architecture consists of multiple services. The state is distributed over several domain services but stored in a single event store. The Read model component concentrates the logic for reading and caching the state, as shown here:Commands and queries are communicated via the Message queue component, whereas events are communicated via the Event store component, which also acts as an event bus.In OrderShop v2, all unicast communication happens over the Message queue component. For this, I’ll be using Redis Lists, and in particular, two lists combined into a so-called “reliable queue”. It processes simple commands (e.g. single entity operations) synchronously, but long-running ones (e.g. batches, mails) asynchronously and supports responses to synchronous messages out of the box.The Event store is based on Redis Streams. Domain services (which are just dummies to demonstrate OrderShop’s functionality) are subscribed to event streams named after the event topic (i.e the entity name) and publish events onto these streams. Each event is a stream-entry with the event timestamp acting as the ID. The sum of the published events in the streams results in the state of the overall system.The Read model caches deduced entities from the Event store in Redis using the domain model. Disregarding the cache, it’s stateless.The API gateway is stateless as well, and serves the REST-API on port 5000. It terminates HTTP connections and routes them either to the read model for reading state (queries) or to dedicated domain service for writing state (commands). This conceptual separation between read and write operations is a pattern called Command Query Responsibility Segregation (CQRS).The domain services receive write operations over the Message queue from the API gateway. After successful execution, they publish an event for each of them to the Event store. In contrast, all read operations are handled by the Read model which gets its state from the Event store.The CRM service (Customer Relation Management service) is stateless—it’s subscribed to domain events from the event store and sends emails to customers using the Mail service.The central domain entity is the order. It has a field called ‘status’ which transitions are performed using a state machine, as shown in the diagram below.These transitions are done in several event handlers, which are subscribed to domain events (SAGA pattern), for example:Clients are simulated using the Unit testing framework from Python. There are currently 10 unit tests implemented. Take a look at tests/unit.py for further details.A simple UI is served on port 5000 to watch events and browse state (using WebSockets).A RedisInsight container is also available to inspect the Redis instance. Open the web browser to http://localhost:8001/ and use redis:6379 to connect to the test database.Redis is not only a powerful tool in the domain layer (e.g. a catalog search) and application layer (e.g. a HTTP session store) but also in the infrastructure layer (e.g. an event store or message queue). Using Redis throughout these layers reduces operational overhead and lets developers reuse technologies they already know.Take a peek at the code and try your hand at implementing it. I hope this helps demonstrate Redis’ versatility and flexibility in domain and infrastructure services and proves how it can be used beyond caching.Let me know how it goes on Twitter: @martinez099."
651,https://redis.com/blog/redisearch-update-aggregation-new-features/,RediSearch Update: Aggregation & New Features,"May 30, 2018",Redis,"One of the key announcements at RedisConf was the aggregation engine for RediSearch (version 1.1.0). Aggregations are an incredibly powerful feature for a real-time search engine like RediSearch. They allow for data to not only be queried, but summarized mathematically to gain analytical insight. RediSearch aggregations come with the standard toolkit of reducers:Conceptually, aggregations are comprised of a pipeline of operations. Each operation can be used in any logical order and can be repeated. The basic operations are:For example, take shipments in an e-commerce scenario wherein you have a timestamp and total box size (box_area) for millions of shipments over the past ten years. Let’s say you want to find the top three years with the most shipments of boxes that have an area greater than 300. We don’t care about the exact number; we just want rough figures and it should be nicely formatted. Our aggregation query would look like this:This probably looks unlike any Redis command you’ve ever seen, but when you break it down it’s not that complicated.This outputs the following results:By arranging these operations together you can rapidly analyze your data in ways previously not possible. It’s a very deep and expansive feature, larger than can be easily summed in this blog post, so it’s best to see it in action. Watch this video to see Dvir demoing aggregations:Soon after RedisConf, we released RediSearch 1.2.0, which included a raft of new features:Make a sub-query modify the clauses of a query. This allows for:The above command results in any document with “ice”, “cream” and “sandwich” to have a weight of 0.5. You can also modify the slop ($slop) and “in order” requirements ($inorder) based on a sub-query.Match any items with a single character distance. For example, “%redis%” would match not only ‘redis’ but also ‘jedis’ and ‘predis.’Update a document only if a condition is met, such as:This code would update the document ‘myDoc’ when the timestamp is below 12313134523. It would only update the title.Use a backslash as an escape so that control characters are processed as normal text. This query would look for both “hello-world” and “world” in the indexed documents:In a situation where you have complete equivalents with different spellings, matching can be tricky. With FT.SYNADD and FT.SYNUPDATE you can add terms that are equivalents, and subsequently added documents will be matched. Given the following:Any new documents with any of those terms would be matched on a query like this:Lastly, version 1.2.0 was the last version of RediSearch being led by Dvir. We’re sad to see him go but we wish him well on his new adventures at a new organization (building something, we understand is very cool and very different from RediSearch). Don’t fret though, RediSearch is being developed by a full team of people here at Redis that will continue to innovate and push RediSearch forward."
652,https://redis.com/blog/mastering-redisearch-part-iii/,Mastering RediSearch / Part III,"January 22, 2018",Redis,"Today we’re going to dive quite a bit deeper and make something useful with Node.js, RediSearch and the client library we started in Part II.While RediSearch is a great full-text search engine, it’s much more than that and has extensive power as a secondary index for Redis. Considering this, let’s get a dataset that contains some more field-based data. I’ll be using the TMDB (the movie database) dataset. You can download this dataset from the Kaggle TMDB page. The data is formatted in CSV over two files and has a few features we’re not going to use yet, so we’ll need to build an ingestion script.To follow along, it would be best to get the chapter-3 branch of the GitHub repo.The first file is tmdb_5000_credits.csv, which contains the cast and crew information. Though cast and crew rows are modeled a bit differently, they do share some features. Initially, it’s not a very usable CSV file since two columns (cast, crew) contain JSON.movie_id (column) This correlates with a movie row in the other file.title (column) The title of the movie identified with the movie_idcast (column with JSON)crew (column with JSON)Another problem with the CSV is that it’s quite huge for a file of its type: 40mb (for comparison, the total works of Shakespeare is just 5.5mb). While Node.js can handle files of this size, it’s certainly not all that efficient. To ingest this data optimally, we’ll be using csv-parse, a streaming parser. This means that as the CSV file is read in, it is also parsed and events are emitted. Streamed parsing is a fitting addition to the already high-performing RediSearch.Each row in the CSV represents a single movie, with about 4,800 movies in the file. Each movie, as you might imagine, has a dozens to hundreds of cast and crew members. All in all, you can expect to index about 235,838 cast and crew members — each represented by nine fields.The other file in the TMDB dataset is the movie data. This is quite a bit more straightforward. Each movie is a row with a few columns that contain JSON data. We can ignore those JSON data columns for this part in the series. Here is how the fields are represented:movie_id (column) A unique ID for each moviebudget (column) The total film budgetoriginal_language (column) ISO 639–1 version of the original languageoriginal_title (column) The original title of the filmoverview (column) A few sentences about the filmpopularity (column) The film’s popularity rankingrelease_date (column) The film’s release date (in YYYY-MM-DD format)revenue (column) The amount of money it earned (USD)runtime (column) The film’s runtime (in minutes)status (column) The film’s release status (“released” or “unreleased”)Ignored columns: genres, production_companies, keywords, production_countries, spoken_languagesImporting Data from TMDBNow that we’ve explored both the fields and how to create an index, let’s move forward with creating our actual indexes and putting data into them.Thankfully, most of the data in these files is fairly clean, but that doesn’t mean we don’t need to make adjustments. In the cast/crew file, we have the challenge of cast and crew entries having slightly different sets of data. In the data, each row represents a movie and the cast column has all the cast members while the crew column has all the crew members. So, when we’re representing this in the schema, we’re effectively creating a union of the fields (since there is overlap). cast and crew are numeric fields that are set to “1” for each kind of credit—think of it like a flag.For the movie database, we’re going to convert release_date to a number. We’ll simply parse the date into a Javascript timestamp and store it in a numeric field. Finally, we’ll ignore a number of fields — we’ll just compare columns’ keys to an array of columns in order to skip (ignoreFields).From a project structure, we may end up doing more with our fieldDefinitions later on, so we’ll store both schema in a Node.js module. This is purely optional but is a clean pattern that reduces the likelihood of having to duplicate your code later on.For importing both movies and crew, we’ll be using an Async queue and the above-mentioned streaming CSV parser. These two modules work similarly and well together, but have some different terminology in their syntax. First, the CSV parser will read a chunk of data ( parser.on(‘readable’,…) ) and then continue to read a full-row in at a time ( while(record = parser.read()){ … } ). Each row is manipulated and readied for RediSearch (csvRecord(record)). In this function, a few rows are formatted while some are ignored, and finally the item is pushed into the queue ( q.push(…) ).Async is a very useful Javascript library that provides a huge number of metaphors for handling asynchronous behavior. The queue implementation is pretty fun — items are pushed into a queue and are processed at a given concurrency by a single worker function defined at instantiation. The worker function has two arguments : the item to be processed and a callback. Once the callback has ran, the next is available for processing (up to the given concurrency). There is a great animation that explains it:The other feature of queue that we’ll be using is the drain function. This function executes when there are no items left in the queue, e.g. the queue was in a working state but is no longer processing anything. It’s important to understand that a queue is never “finished,” it just becomes idle. Given that we’ll be using a streaming parser, it’s possible that RediSearch is ingesting faster than the CSV parser, resulting in an empty Async queue (triggering drain). To address this potential problem, each record is added to the queue and when it is successfully indexed, two individual counters are incremented (total and processed, respectively) and the CSV parser sets a parsed variable from “false” to “true.” So when drain is called, we check to see if parsed is true and if the value of processed matches total. If both of these conditions are true, we know that all the values have been parsed from the CSV and that everything has been added to our RediSearch index. After you’ve successfully added an item to the index, you invoke the callback for the worker function and the queue manages the rest.As mentioned earlier, the credits CSV is more complex, with each row in the table representing multiple cast/crew members. To manage this complexity, I’ll be using a batch.  We’ll be using the same overall structure with the CSV parser and Async queue, but each row will contain multiple calls to RediSearch via the batch (one for each cast or crew member). Instead of pushing a plain object into the queue, we’ll actually push a RediSearch batch. In the worker function we’ll call exec on it and then the callback. While in the movies CSV we’ll have a single Redis (RediSearch) command per movie, in the credits CSV we’ll have a single batch (made up of dozens of individual features) for each movie.The two imports are different enough to warrant separate import scripts. To import the movies, you’ll run the script like this:And the credits will be imported like this:A killer feature of RediSearch is that as soon as your data is indexed, it’s available for query. True real-time stuff!Now that we’ve ingested all this data, let’s write some scripts to get it back out of RediSearch. Despite the data being quite different between the cast/crew and the movie datasets, we can write a single, short script to do the searching.This little script will allow you to pass in a search string from the command line (–search=”your search string”) and also designate the database you’re searching through (–searchtype movies or –searchtype castcrew). The other command line arguments are the connection JSON file (–connection path-to-your-connection-file.json) and optional arguments to set the offset and number of results ( –offset and –resultsize, respectively).After we instantiate the module by passing in the argv.searchtype, we’ll just need to use the search method to send the search query and options as an object to RediSearch. Our library from the last section takes care of building the arguments that will be passed through to the FT.SEARCH command.In the callback, we get a very standard looking error-first callback. The second argument has the results of our search, pre-parsed and formatted in a useful way — each document has it’s own object with two properties: doc and docId. The docId contains the unique identifier and document as an object.All we need to do is JSON.stringify the results (so console.log won’t display [Object object]) and then quit the client.You can try it out by running the following command:This should return an entry about the movie Lone Star (anyone seen it? No, I didn’t think so). Now, let’s look in the cast/crew index for anything with the same move_id:This will give you the first ten items of the cast and crew for Lone Star. Looks straight forward except for the search string — why do you have to repeat 26748 twice? In this case it’s because the movie_id field in the database is numeric and numerics can only be limited by a range.Getting a document from RediSearch is even easier. Basically, we instantiate everything the same way as we did with the search script, but we don’t need to supply any options and instead of a search string we’re getting a docId.We just need to pass the docId to the getDoc method (abstracting the FT.GET command) along with a callback and we’re in business! The show function is the same as in the search.This will work equally well for both cast/crew or movie documents:If you try to import a CSV file twice you’ll get an error similar to this:This is because you can’t just create an index over a pre-existing one. There is a script included to quickly drop one of your indexes. You can run it like this:In this installment, we’ve covered how to parse large CSV files and import them into RediSearch efficiently with a couple of scripts tailored to their different structures. Then we built a script to run search queries over this data, grab individual documents and drop an index. We’ve now learned most of the steps to managing the lifecycle of a dataset.In our next installment, we’ll build out a few more features in our library to better abstract searching and add in a few more options. Then we’ll start building a web UI to search through our data. Stay tuned to the Redis blog!"
653,https://redis.com/blog/mastering-redisearch-part/,Mastering RediSearch / Part I,"September 22, 2017",Redis,"I’ve been working with the RediSearch module quite a bit lately — it’s one of the more fascinating developments in the Redis ecosystem and it deserves it’s own series.If you’ve built an application with Redis as a primary data store, you’ve likely experienced both the elation and confusion of the native data types. When you understand the data types, you realize that much of your data fits neatly into one of them. However, many common application patterns require both indexing (“what key has x value?”) and search (“what key contains some text string?”). While these questions can be answered by leveraging the native datatypes in creative ways, the code can be complex and has speed and/or space efficiency trade offs. The RediSearch module fills in these blanks with few trade offs. In this first installment we’re going to be exploring the very basics of the module as a gentle introduction.Modules are add-ons for your Redis server. At their most basic level, they implement new commands, but they can also implement new data types. Modules are written in systems programming languages; C/C++, Rust and Golang have been used but other languages are possible. Since they’re written in compiled languages, extreme high performance is possible.Modules are distinct from Redis scripting (Lua) in that they are first-class commands in the system and can interface storage directly, enabling the creation of their own datatypes. The only thing that sets them apart from in-built commands is that module commands are namespaced by a prefix, often two letters, and a dot (ex: XX.SOMECOMMAND).Modules can be loaded either on the fly with MODULE LOAD, in the redis.conf file with loadmodule, or through the command line argument “loadmodule”. My personal preference is to load them via the conf file as it ensures that it’s always available and the configuration is portable.I’ve asked myself the question what isn’t RediSearch — but I’ll attempt to answer it without inverting. RediSearch is a module that provides three main features:RediSearch utilizes both it’s own datatype and the in-built Redis data types. In this way, it’s more of a solution that uses Redis and also resides with Redis. That may seem confusing now, but stay with me.Let’s evaluate each of the features from above. First, consider full-text searching. With RediSearch you can index text that hasn’t already been processed. Let’s say that you have a list of one million client comments and you want to find all that mention “rendering.” Before RediSearch, you could certainly store those comments in Redis (in, say, a hash), but finding a specific word inside those comments was a struggle at best. Even if you managed to build your own index of words to comments (which involves splitting each comment into words at the app level), matching would need to be exact — “render,” “rendering,” and “rendered” would not match one another. Instead, by storing the data with RediSearch you could find all the comments without having to do anything special at your application level and it would match “rendered” to “rendering” automatically since it smartly processes both the index and the query.Obviously, if it’s possible to do the above, it’s also possible to do it without the language processing smarts — as you start to think of this, you start to realize that RediSearch can be used as a general purpose secondary index. But it’s also possible to go beyond text matches — RediSearch can do numeric and geo indexes on a single item (termed “document”). It is possible to have multiple fields on each document — each with individual attributes.Finally, somewhat separately, RediSearch provides a suggestion engine that can drive auto-complete-like services. This allows you to take known valid values and provide users “hints.” It’s based on a prefix model, so if a user starts to type “Hamb” the suggestion engine would provide, say, “Hamburger,” “Hambone,” and “Hamburg.” It’s important to note these suggestions aren’t integrated with the search results directly, so it’s up to your application to add or delete them from this suggestion store.As a hands on exercise, let’s install the module:(or install it in your redis.conf file and restart redis-server)After your module is loaded, go ahead and run this command in redis-cli to verify the module is running:In the results to this command you should see an entry for each module you have installed (likely just one). The name field of one of the entries should read “ft” (meaning full text). That’s how RediSearch is identified and the command prefix. Your version number will likely be different from mine, progress on this module is moving fast.Now that the module is up and running it’s best to start with a clean database for these exercises (flushdb or a clean database/instance). To start let’s create an index and add an item:This might look a tad complicated, especially if you’re used to commands with 1 or 2 arguments. Let’s break it down:FT.CREATE shakespeareThis is just the command and the “key” (more on that later)SCHEMAThis indicates that the following arguments will be about the fields in the search index.line TEXT SORTABLEHere we are creating a field named line that holds text values and will be sortable later on.play TEXT NOSTEMThis is the field “play” that is for text values but it won’t be stemmed (e.g. rendering will not match render)speech NUMERIC SORTABLEWe’re creating a field named “speech” that is numeric and sortable.speaker TEXT NOSTEMJust like the play field the speaker field will hold text that will only do exact, word-for-word matches.entry TEXTThis field (entry) holds text values that are processed for exact or stemmed match.location GEOThe location field holds a geographic coordinate.See — it’s just a lot in one line, but not really complicated.Now, let’s add a document to our index:Comparing the two commands, you might notice that the FT.CREATE and FT.ADD commands are following a similar pattern. Let’s look at the command in more depth:FT.ADD shakespeare 57956 1We’re adding a document with an ID of 57956 to the index (shakespeare). Note that in this command the document ID is a number (just a feature of the dataset I’m using), but it can be any valid Redis key. The final argument in this section is the weight — we’ll get into this in a later part of the series, but, for now, you just need to know that it can be between 0 and 1 and 1 is a good default value.FIELDS …“FIELDS” indicates that we’re going to specifying the fields of the document in a [fieldname] [value] repeating pattern. Note that when the value is single word or number, you don’t need quotes, but if you’re using spaces or other odd characters, enclose your value in quotes. The other special one is the location field that includes a set of coordinates (longitude,latitude)Recall that we created an index with the key “shakespeare” (via the FT.CREATE command). Let’s do a quick experiment:Strange, right? This is where we start departing from normal Redis behaviour and you’ll start seeing where RediSearch is a solution that is both using and integrated with Redis.If you’re running this on a non-production database, let’s do KEYS * for debugging purposes:Running two commands had yielded 9 keys. I want to highlight a few of these keys just to fill out the understanding of what is actually going on here:Here we can see that RediSearch has created a key with it’s own datatype (ft_index0). We can’t really do much with this key directly, but it’s important to know that it exists and how it was created.Now, let’s look at key 57956A hash! We can work with this — let’s look at this key directly:This should look familiar as it’s your data from the FT.ADD command and the key is just your document ID. While it’s important to know how this is being stored, don’t manipulate this key directly with HASH commands.Interesting — the field speech in our dataset is a numeric index and the type is a “numericdx.” Again, since this is a RediSearch native datatype, we can’t manipulate this with any ‘normal’ Redis commands.The key here gives you a hint — while the TYPE command returns that it’s a ZSET, Redis geohash sets are stored as ZSETs and will report as them when the type is queried. That being said, let’s look at a couple of GEO commands:Brilliant! RediSearch has stored the coordinates in a bog-standard GEO set. But, like the hash above, don’t modify these values directly with ZSET or GEO commands.Finally, let’s take a look at one more key:Sharp readers might notice that the term “lady” was only indexed in a full-text field (speaker). Data stored ft_invidx keys are textual indexes.Now that we know a little about how RediSearch is storing our data, we can start to load more substantial information into database and explore querying but that will have to wait to Part II of Mastering RediSearch coming in a few weeks."
654,https://redis.com/blog/mastering-redisearch-part-ii/,Mastering RediSearch / Part II,"December 7, 2017",Redis,"In our last installment, we started looking at RediSearch, the Redis search engine built as a module. We explored the curious nature of the keys and indexed a single document. In this segment, we’ll lay the groundwork necessary to make working with RediSearch more productive and useful in Node.js.Now, we could certainly bring in all this data using the RediSearch commands directly or with the bindings, but with a large amount of data using direct syntax becomes difficult to manage. Let’s take some time to develop a small Node.js module that will make our lives easier.I’m a big fan of the so-called “fluent” Javascript syntax, wherein you chain methods together so that functions are separated by dots when operating over a single object. If you’ve used jQuery then you’ve seen this style.This approach will present some challenges. Firstly,  we need to make sure that we can interoperate with “normal” Redis commands and still be able to use pipelining/batching (we’ll address the use of MULTI in a later installment).  Also, RediSearch commands have a highly variadic syntax (e.g. commands can have a small or large number of arguments). Translating this directly into Javascript wouldn’t gain us much over the simple bindings. We can, however, leverage a handful of arguments and then supply optional arguments in the guise of function-level options objects. What I’m aiming to design looks a little like this:Overall, this is a much more idiomatic way of doing things in Javascript and that’s important when trying to get a team up to speed, or even just to improve the development experience.Another goal of this module is to make the results more usable. In Redis, results are returned in what is known as a “nested multi bulk” reply. Unfortunately, this can get quite complex with RediSearch. Let’s take a look at some results returned from redis-cli:So, when using node_redis you would get nested arrays at two levels, but positions are associative (except for the first one which is the number of results). Without writing an abstraction, it’ll be a mess to use. We can abstract the results into more meaningful nested objects with an array to represent the actual results. The same query would return this type of result:So, let’s get started on writing a client library to abstract RediSearch.Let’s first examine the entire “stack” of components that let you access RediSearch at a higher level.This is a bit confusing due to the terminology and duplication, but each layer has its own job.node_redis-redisearch just provides the commands to node_redis, without any parsing or abstraction. node_redis just opens up the world of Redis to Javascript. Got it? Good.Since RediSearch isn’t a default part of Redis, we need to check that it is installed. We’re going to make the assumption that RediSearch is installed on the underlying Redis server. If it isn’t installed then you’ll simply get a Redis error similar to this:Not having the bindings is a more subtle error (complaining about an undefined function), so we’ll build in a simple check for the ft_create command on the instance of the Redis client.To be able to manage multiple different indexes and potentially different clients in a way that isn’t syntactically ugly and inefficient, we’ll use a factory pattern to pass in both the client and the index key. You won’t need to pass these again. The last two arguments are optional: an options object and/or a callback.It looks like this:The callback here doesn’t actually provide an error in its arguments;  it is just issued when the node_redis client is ready. It is entirely optional and provided primarily for benchmarking so you don’t start counting down the time until the connection is fully established.Another useful feature of this function is that the first argument can optionally be the node_redis module. We’ll also automatically add in the RediSearch bindings in this case. You can designate this library to manage the creation of your client and specify other connection preferences in the options object located at clientOptions. Many scripts have specialized connection management routines so it is completely optional to pass either a client or the node_redis module.We’ll be using similar signatures for most functions  and  the final two arguments are optional: an options object and a callback. Consistency is good.Creating an index in RediSearch is a one-time affair. You set up your schema prior to indexing data and then you can’t alter the schema without re-indexing the data.As previously discussed, there are three basic types of indexes in RediSearch:(Note: there is a fourth type of index, the tag index, but we’ll cover that in a later installment)Each field can have a number of options—this can be a lot to manage! So let’s abstract this by returning a fieldDefinition object that has three functions: numeric, text, and geo. Seems familiar, eh?All three methods have two required options and text fields have an optional options object. They are in this order:These methods return arrays of strings that can be used to build a RediSearch index. Let’s take a look at a few examples:So, what do we do with these little functions? Of course, we use them to specify a schema.This makes a clear and expressive statement on the fields in the schema. One note here: while we use an array to contain the fields, RediSearch has no concept of order in fields, so it doesn’t really matter in which order you specify fields in the array.Adding the item to a RediSearch index is pretty simple. To add an item, we supply two required arguments and consider two optional arguments. The required arguments are (in order):The two optional arguments follow our common signature: options and a callback. As per common Node.js patterns, the first argument of the callback is an error object (unset if no errors) and the second argument of the callback is the actual data.Batch, or “pipeline” as it’s called in the non-Node.js Redis world, is a useful structure in Redis, it allows for multiple commands to be sent at a time without waiting for a reply for each command.The batch function works pretty similarly to any batch you’d find in node_redis — you can chain them together with an exec() at the end. This does cause a conflict, though. Since ‘normal’ node_redis allows you to batch together commands, you need to distinguish between RediSearch and non-RediSearch commands. First, you need to start a RediSearch batch using one of two methods:After you have created the batch, you can add normal node_redis commands to it or you can use RediSearch commands.Take note of the HGETALL stuck in the middle of this chain; this is to illustrate that you can intermix abstracted RediSearch commands with ‘normal’ Redis commands. Cool, right?As mentioned earlier, the output of RediSearch (and many Redis commands) is likely in a form that you wouldn’t use directly. FT.GET and FT.SEARCH produce interleaved field / value results that get represented as an array, for example. The idiomatic way of dealing with data like this in Javascript is through plain objects. So we need to do some simple parsing of the interleaved data. There are many ways to accomplish this, but the simplest way is to use a lodash chain to first chunk the array into 2-length individual arrays then use the fromPairs function to convert the 2-length arrays into field/values in a single object. We’ll be using this quite a bit so we’ll contain it in the non-public function deinterleave in order to reduce repetition.If we didn’t need to contend with pipelines, adding these parsing functions would be a somewhat simple process of monkey patching the client. But with batches in node_redis, the results are provided both in a function-level callback and at the end of the batch, with many scripts omitting function-level callbacks and just dealing with all the results at the end. Given this, we need to make sure that the commands are only parsing these values when needed—but always at the end.Additionally, this opens up a can-of-worms when writing our abstraction. Normal client objects and pipeline objects both need RediSearch-specific commands injected. To prevent writing two different repetitious functions, we need to have one function that can be dynamically injected. To accomplish this, the factory pattern is employed :  the outer function is passed in a client or pipeline object (let’s call it cObj) and then it returns a function with the normal arguments. cObj can represent either a pipeline or just a node_redis client.Thankfully, node_redis is consistent in how it handles pipelined and non-pipelined commands, so the only thing that changes is the object being chained. There are only two exceptions:These two exceptions only need to be applied when pipelined, thus we need to be able to detect pipelining. To do this, we have to look at the name of the constructor. It’s been abstracted into the function chainer.In the RediSearch module, search is executed with the FT.SEARCH command, which has a ton of options. We’ll abstract this into our search method. At this point we’re going to provide only the bare minimum of searching abilities — we’ll pass in a search string (where you can use RediSearch’s extensive query language), then an optional Options argument and finally, a callback. Technically the callback is optional, but it would be silly not to include it.In our initial implementation, we’ll just make a couple of options available:These options map directly to the RediSearch LIMIT argument (very similar to the LIMIT argument found throughout SQL implementations).The search also implements a result parser to make things a little more useable. The output object ends up looking like this:The property results is an ordered array of the results (with the most relevant results at the top). Notice that each result has both the ID of the document (docId) and the fields in the document (doc).  totalResults is the number of items index that match the query (irrespective of any limiting). requestedResultSize is the maximum number of results to be returned.  resultSize is the number of results returned.In the previous section you may have noticed the docId property. RediSearch stores each document by a unique ID that you need to specify at the time of indexing. Documents can be retrieved by searching or by directly fetching the docId using the RediSearch command FT.GET. In our abstraction, we’ll call this method getDoc (get has a specific meaning in Javascript, so it should be avoided as a method name). getDoc, like most other commands in our module, has a familiar argument signature:Like the search method, getDoc does some parsing to turn the document from an interleaved array into a plain Javascript object.One more important thing to cover before we have a minimal set of functionalities — the dropIndex, which is just a simple wrapper for the command FT.DROP is a little different as all it takes is a callback for when the index is dropped.Neither dropIndex nor createIndex allow for chaining as the nature of these commands prevent them from having further chained functions.In this piece we’ve discussed the creation of a limited abstraction library for RediSearch in Node.js, as well as its syntax. Reaching back to our previous piece, let’s look at the same small example to see the complete index lifecycle.As you can see, this example covers all the bases, though it probably isn’t very useful in a real-world scenario. In our next installment we’ll dig into the TMDB dataset and start playing with real data and further expanding our client library for RediSearch.In the meantime, I suggest you take a look at the GitHub repo to see how it’s all structured."
655,https://redis.com/blog/geobike-building-location-aware-application-with-redis/,GeoBike: Building Location Aware Application with Redis,"December 19, 2017",Tague Griffith,"I travel a lot on business as a Developer Advocate for Redis!  I’m not much of a car guy, so when I have some free time, I prefer to walk or bike around a city.  Many of the cities I’ve visited on business have bike share systems which let you borrow a bike for a few hours.  Most of these systems have an app for renting bikes, but they only share the details of their system.  This got me thinking – using publicly available bike share information to build an “app” showing you global information would be a fun way to demonstrate the geospatial features of Redis.  With that GeoBike, the Redis bike share application was born.GeoBike incorporates data from many different sharing systems, including the CitiBike Bikeshare in New York City.  We are going to take advantage of the General Bikeshare Feed provided by Citi Bike system and use their data to demonstrate some of the features we can build using Redis to index geospatial data.  The CitiBike data is provided under the NYCBS Data Use Policy.General Bikeshare Feed SpecificationThe General Bikeshare Feed Specification (GBFS) is an open data specification developed by the North American Bike Share Association to make it easier for map and transportation applications to add bike share systems into their platforms.  The specification is currently in use by over 60 different sharing systems in the world.The feed consists of several simple JSON data files containing information about the state of the system.  The feed starts with a top level JSON file referencing the URLs of the subfeed data:The first thing we’re going to focus on is loading information about the bike sharing stations into Redis.  For this part of our application, we’re going to need data from the system_information and station_information feeds.The system_information feed will provide us with the system ID, which is a short code that we will use to create namespaces for our Redis keys.  The GBFS spec doesn’t specify the format of the system ID, but does guarantee that it is globally unique.   Many of the Bikeshare feeds use short names like coast_bike_share, boise_greenbike, or topeka_metro_bikes for system IDs.  Others use familiar geographic abbreviations such as NYC or BA, and one uses a UUID.  Our code uses the identifier as a prefix to construct unique keys for the given system.The station_information feed provides static information about the sharing stations that comprise the system.  Stations are represented by JSON objects with several fields.  There are several mandatory fields in the station object that provide the ID, name and location of the physical bike station.  There are also several optional fields that provide helpful information such as “cross-street” or “accepted payment methods.”  This is the primary source of information for this part of the bike sharing application.Building Our DatabaseI’ve written a sample application, load_station_data.py, that mimics what would happen in a backend process for loading data from external sources.Finding the Bike Share StationsLoading the bike share data starts with the systems.csv file from the GBFS repository on Github.The systems.csv file provides the discovery URL for registered Bike Share Systems with an available GBFS feed.  The discovery URL is the starting point for processing bike share information.The load_station_data application takes each discovery URL found in the systems file and uses it to find the URL for two sub-feeds: system information and station information.  The system information feed provides us with a key piece of information: the unique ID of the system.  Note, the system ID is also provided in the systems.csv file, but some of the identifiers in that file do not match the identifiers in the feeds, so we will always fetch the identifier from the feed.  Details on the system, like rental URL, phone numbers and emails could be useful in future versions of our application, so we’ll store the data in a Redis hash using the key ${system_id}:system_info.Loading the Station DataThe station information provides us with data about every station in the system, including the location of the system.  The load_station_data application iterates over every station in the station feed and stores the data about the station into a Redis hash using a key of the form ${system_id}:station:${station_id}.  The location of each station is added to a geospatial index for the bike share using the GEOADD command.Updating DataOn subsequent runs, we don’t want our code to remove all of the feed data from Redis and reload it into an empty Redis database, so we need to think about how we handle in-place updates of the data.Our code starts by loading the set which contains all of the bike sharing stations information for the system currently being processed into memory.  When information is loaded for a station, the station (by key) is removed from the in-memory set of stations.  Once all of the station data is loaded, we’re left with a set containing all of the station data that must be removed for this system.Our application iterates over this set of stations and creates a transaction to delete the station information, remove the station key from the geospatial indexes and remove the station from the list of stations for the system.Notes on the CodeThere are a few interesting things we should point out in the sample code.  First you’ll notice that we added items to the geospatial indexes using the GEOADD command, but removed them using the ZREM command.  The underlying implementation of the geospatial type uses sorted sets, so items are removed using ZREM.  A word of caution: for simplicity, the sample code demonstrates working with a single Redis node; the transaction blocks will need to be restructured to run in a cluster environment.If you are using Redis 4.0 (or later), you have some alternatives to the DELETE and HMSET commands used in this code.  Redis 4.0 provides the UNLINK command as an asynchronous alternative to the DELETE command.  UNLINK will remove the key from the keyspace but reclaims the memory in a separate thread.  The HMSET command is deprecated in Redis 4.0 and the HSET command is now variadic.Notifying ClientsAt the end of the process, we send out a notification to those clients relying on our data.  Using the Redis Pub/Sub mechanism, we send out a notification on the geobike:station_changed channel with the ID of the system.Data ModelWhen structuring your data in Redis, the most important thing to think about is how you are going to query the information.  For our bike share application, the two main queries we need to support are:Redis provides two main data types that will be useful for storing our data, Hashes and Sorted Sets.  The hash type maps well to the JSON objects that represent stations and since Redis hashes don’t enforce a schema, so we can use them to store our variable station information.Of course, to find stations geographically, we will want to build a geospatial index to search for stations relative to some coordinates.  Redis provides several commands to build up a geospatial index using the Sorted Set data structure.We’ll construct keys using the format ${system_id}:station:${station_id} for the hashes containing information about the stations and keys using the format  ${system_id}:stations:location for the geospatial index used to find stations.Mapping the ResultsLet’s check the results of our data load by generating a map of the data loaded into Redis.  We can create a map for the data by constructing a KML (Keyhole Markup Language) file and loading it into Google Maps.  I’ve provided the generate_station_kml.py script to generate a KML file of the station locations for a station ID.  Google maps limits KML files to 10 layers and 5000 features, so the KML generator application will only generates a file for a single system.The application uses a redis-py scan_iter to iterate over the station keys (keys matching the pattern ${system_id}:station:*) and uses the Python minidom package to construct the output XML.I ran the generate_station_kml.py script against my Redis instance after running load_station_data.py and generated the following map of the New York City CitiBike system.Map example:If we look for the 6th and Canal Bike Station on our custom map, we see the coordinates 40.72242, -74.00566 and a blue pin on top of the CitiBike Station on the base layer of our map.  Of course this is not a complete QA cycle, but a good way to eyeball the data to build confidence in our code.In the next GeoBike post, we’ll look at how a developer can query the data in the database to add interesting features to an application.  In the meantime, you can get the associated code from this post on Github.  If you have any questions about this post, please connect with me (@tague) on Twitter."
656,https://redis.com/blog/how-to-power-crop-insurance-with-drones-using-redis/,How to create a powerful drone system using Redis to protect crop insurers from false claims,"October 14, 2021",Redis Growth Team,"In the face of climate change, crop insurers are faced with a whole new range of problems. A lack of data to assess cultivable land and yield has always been an issue, but accurately estimating crop damage has also proven to be almost an impossible task.To compound this even further, the remote location of crops can prevent insurers from finding specialists to carry out inspections and risk assessments. These all have implications on cost and time to insurers.But the capabilities of drones are fast being recognized as the solution to these problems.A Launchpad App demonstrated how this could be achieved by creating an interconnected drone system that uses Redis to transmit data between components. Redis pulled each component closer together, enabling reams of data to be transmitted in real time.With this asset, drones were able to fly over fields, take images of crops, then send them back to an online portal for assessment. A demo was created to illustrate how this was achieved.Let’s investigate how the team managed to achieve this. But before we go any further, we also have a great variety of different apps for you to explore, so make sure to check them out on the Launchpad.You’ll build a drone system which uses Redis and cloud technologies that captures accurate data of crops in rural areas at speed. Crop insurers can leverage this asset to create more secure insurance policies whilst maximizing transparency during the claims process.We’ll explore how they managed to tie in all of these different components to work harmoniously with one another.DevelopmentCloud and servicesNow let’s have a look at the architecture. To simplify things we’ve broken it down into 4 sections.Thanks to this product, the insurer can carry out crop inspections in real time and create an insurance policy from his/her office.Step 1. Clone the project repository:If you look at the source code repository, the whole project is divided into 3 major sections:Step 2. Examine the Airsim SimulatorRedis modules, RedisGears, RediStreams, and RedisAI were each deployed in this project. They were all used to analyze images of the land that were captured by the drone in real-time as well as calculating the percentage of different categories in those images using Tensorflow, Microsoft Custom Vision, and RedisAI.To connect to the AirSim simulator, simply use the Python-based code below. This also allows you to set the coordinates that will determine the path your drone will follow during its flight.Step 3. Installing the required softwareBefore executing the Python script, you should install the prerequisite software discussed below.You can install Docker in your environment by using this link. Also, make sure that you have Docker compose installed in your system.Step 4. Setting up Unreal EngineUnreal Engine is the world’s most open and advanced real-time 3D creation tool. At this point, it’s required that you set up the Unreal Engine on your local machine to simulate the flying of drones on virtual fields.Here are the hardware requirements to set up the Unreal Engine:Once the Unreal Engine is set up on your local environment, you need to download the folder from Google drive. It includes the maps of the different landscapes that the drones will fly over. We’ve placed the folder inside Google Drive due to its large size.Once the folder is downloaded, you’ll need to double click the ‘FinalProjDroneSquad’ file as shown below:This will launch the landscape on Unreal Editor as shown below:Click on the ‘Play’ button as highlighted below to start level 1.To change the level of the game, first navigate to the Content -> Maps folder. Double click on the Level 2/Level 3 files as shown below and click on the play button.Thereafter you can proceed to the installation section to set up other prerequisites.From the root folder of this project, run the below docker command:docker-compose upAs you can see from the docker-compose file above, this project combines several Redis modules, such as RedisGears, RediStreams, and RedisAI. These are all used to analyze the images of the land captured by the drone in real-time as well as calculating the percentage of different categories in those images using Tensorflow, Microsoft Custom Vision and RedisAI.Essentially, this will create two containers that are used in this project on your machine:Finally, we need to create the blob storage account to store the analyzed images that are generated using RedisAI. To achieve this, create the container inside the blob storage with the name ‘droneimages.’cd redisedge\secretsCopy the connection string from the azure blob storage account and paste it on the ‘azureblobsecret’ file inside the folder.Open your favorite terminal and run the following commands.On the first tab run the command below and change the level arguments as 1, 2 and 3 to be able to initiate different levels of the game.python flyDrone.py –level=1This will initialize the drone and place it into ‘waiting to take off’ mode.On the second tab run the command below. This will listen to the inspection carried out by Redis Streams. Data is entered from the front end whenever the inspection is triggered.python captureImagesFromDrone.py –level=1Once the data arrives on Redis Streams, the drone will take off and start to capture images. These are then processed and analyzed by RedisAI using Tensorflow.Note: After changing the level of the game on the Unreal Editor as mentioned, run the above two scripts on a different terminal once again and close the existing ones.Below are the output images generated by RedisAI.6.  How to set up the BackendThe backend system is built using microservices that are divided into different entities such as :These microservices are built using the Java Spring Boot framework, which, in-turn, uses Redis, RedisJSON, and RediSearch. The backend API will perform all the inspection of data. Based on this data, sum assured and premium gets calculated and passed to the frontend app.7. How to set up the FrontendLet’s examine the code which will help you create a public app that can be accessed by a crop insurer.Note: Need Node Version 14+git clone https://github.com/redis-developer/CropInsurernpm i -g yarnyarn installyarn startUsername: admin@gmail.comPassword: adminBelow are screenshots of the application.Conclusion: Removing barriers with real-time dataThrough the advanced capabilities of Redis, this Launchpad App created a powerful drone system which enables crop insurers to scan, monitor, and assess the quality of farm yields from their office. Yet the real asset that brought the project’s ambitions to life was Redis’ ability to provide real-time data.Transmitting data from A to B at such speed enables the components to work seamlessly with each other in a system that has a complex architecture. Crop insurers can now carry out accurate crop-quality assessments whilst ensuring a more transparent claims process.To discover more about this innovative project, check out the full app on the Launchpad.Also, make sure to have a browse around the exciting range of applications that we have there.Piyush JainPiyush has over 16 years worth of experience in software development and currently works as a solution architect at Publicis Sapient. Make sure to check out his GitHub profile to see all of his exciting work."
657,https://redis.com/blog/how-to-build-a-real-time-geo-distributed-multiplayer-top-down-arcade-shooting-game-using-redis/,How to build a Real-Time Geo-distributed Multiplayer Top-down arcade shooting game using Redis,"October 1, 2021",Redis Growth Team,"As the gaming industry continues to grow in size, the need to create a unique and dynamic user experience has become even more mandatory. Because of its fandom, businesses have to maximize the multiplayer gaming experience to drive customer acquisition and retention. However, companies are faced with a number of obstacles when trying to scale multiplayer games, all of which can be solved through Redis.Personalized interactions and high-speed reactions are at the heart of creating a unique user experience. Redis provides game publishers with a powerful database that can support low-latency gaming use cases. Recently a Launchpad App built a unique application that could have only been deployed by Redis due to its unmatched speed in transmitting data.This is crucial because participants play from all around the world. Interactions between players must be executed in real-time to support gameplay, requiring the latency to be less than one millisecond.Let’s take a deep dive into how this was done. But before we do so, make sure to have a browse through the exciting range of different applications that we have on the Launchpad.You’ll build a real-time Geo-distributed multiplayer top-down arcade shooting game using Redis. The backbone of the application is Redis, for it acts as an efficient real-time database that enables you to store and distribute data.As we progress through each stage chronologically, we’ll unpack important terminology as well as each Redis function.Let’s identify the different components you’ll need to create this game. The application consists of 3 main components:The main idea of this multiplayer game is to keep it real-time and distribute it geographically. That means that all the instances in your cluster should be updated so that you don’t run out of synchronization. Now let’s take a look at the architecture.Now let’s take a look at the flow of the architecture.Under the root of the repository, you will find a Docker compose YAML file:Under this YAML file, there are two major services that are defined – redis and backend.Below is how the Dockerfile for Redis looks like:Below is the Dockerfile for NodeJS backend:Bringing up the servicesRuin the following commands from the online_game directoryYou access the online WebServer via http://127.0.0.1:8080There are three RedisGears functions that each have their own set of sub-functions:Once a user starts the game, the first thing the user will do is search for a game using RediSearch. If the game is present then that person will try and join the game. If not then RedisGears is triggered to create a new game. See the function belowOnce the user has created a new game, then other players will join and everyone will be able to play.If no game is present, then RedisGears is triggered to create a new one.Once the user has created a new game, then other players will join and everyone will be able to play.This function adds a new player to the game and it has the same approach as Create_new_game function. Again, RedisGears is triggered which then creates a new userWhen a user joins the game, RedisGears is triggered to enable the Join_game function. This also increments the player count of the game_instance (HINCRBY).When a user is eliminated from the game or chooses to leave, RedisGears is triggered to facilitate the process. This also automatically reduces the player count and automatically creates a notification to confirm that this action has been completed.During gameplay, players will fire missiles to eliminate other competitors. When a player fires a missile, the below sub-function is then triggered:If a missile hits another player, then that user will be eliminated from the game. The below code determines whether a player has been hit by a missile.RediSearch indexes are registered on container startup in the redis/start_redis.shCreated Redis Search indexes:As with any Redis database, one of the most adored assets is its ability to transmit data between components with unrivalled efficiency. Yet in this application, without the exceptional latency speed that Active-Active provides, the game would simply not be able to function.Gameplay is purely interactive, where users from all around the world react and fire missiles at other players. From start to finish, RedisGears deploys a sequence of functions based on the chronological order of the application set-up.This is done with ease due to the efficiency of RedisGears which enables an active-active geo-distributed top-down arcade shooter application to be deployed.If you want to find out more about this exciting application you can see the full app on the Redis Launchpad. Also make sure to check out all of the other exciting applications we have available for you.Jānis VilksJānis is a Big Data engineer who works at Shipping Technology.If you want to discover more about his work and the projects he’s been involved in, then make sure to visit his GitHub profile here."
658,https://redis.com/blog/how-to-build-a-squad-health-check-application-with-redis/,How to Build a Squad Health Check Application with Redis,"October 7, 2021",Redis Growth Team,"Squad health checks are crucial for businesses to assess the team chemistry of their workforce. They pave the way for a better working environment by enabling managers to measure cohesion and learn about areas of concern from employees. But to fully reap their benefits, they need to be primed with a database that’s capable of providing instant feedback.These apps require responses to be instantaneous, which is why this Launchpad App used Redis to create their very own squad health check application, Feature Creep. With Redis, the efficient transmission of data was seamless, creating a more interconnected system of components that allowed feedback to become immediate.The acceleration of efficient data facilitated the development of a squad health check system that could be deployed by workforces of all sizes.Let’s take a look at how they achieved this. We also have a wide variety of breakthrough apps in our Launchpad for you to get started, regardless of your preferred frameworks and languages.Let’s take a deep dive into how you can build a squad health check system using Redis and sending the report to Discord. We’ll reveal how each component of Redis was deployed by going through each step in chronological order.Now let’s have a look at the different components you’ll need, their functionality and the steps to take for its implementation:Let’s look at the overall architecture and components used for this project.1.A user registers and logs in to access Feature Creep Dashboard UI. RedisJSON is used in the background to store session details.2.The user then creates a ‘squad’ for their team on the dashboard. Details are stored in the form of a JSON document using RedisJSON. They can invite others to join their squad via a link that can be shared through social media or other applications e.g Microsoft Teams, Slack etc.Once other members accept, they then have access to the dashboard. In a session, every squad member is going to get asked some questions and they can answer either positive, negative or neutral to those. Users begin with default questions, but can also add their own using RediSearch.3. After each session, RedisGears begins the background processing of data.4. Redis Streams is then used to handle the incoming data.5. After background processing, a report is sent to Discord and data is saved to RedisTimeSeries.If you don’t require a production-grade set up, we recommend using the Docker compose installation method in development mode. This will allow you to skip deploying the AWS authentication stack. The easiest way to start up the application is to use the following Docker compose file:In development mode, any calls to the API are automatically authenticated. This means that only 1 user can exist in the database at any time.  Run the below command to boot up all of these containers:Doing so will start up Redis along with a container for the server and one for the client.For the production environment, authentication will happen through JWTs provided by AWS Cognito. If you want more details on how to set this up, visit the infra/auth folder under the project GITHUB repository.Run the below command to bring up the server-side components:The ‘npm ci’ command performs a clean install of all the dependencies of your app. The ‘npm run dev’ command is used to view or run the application worked on while in development mode to see active changes.Run the below commands to initiate the client-side components:Open up https://localhost:3000 to view the feature-creep dashboardTypescript was used extensively. GraphQL APIs are strongly typed which enables you to take advantage of this in the frontend. These generated types are committed to the repo. Only run this if changes happened in the API.Storybook helps developing components in isolation.Once the server is started, you can find the GraphQL playground at http://localhost:4000/. You do not need to provide any authentication when you’re in development mode.Contains the GraphQL API, created with Apollo. When the application is running, you can visit the same URL to view the API playground.Authentication stackIf you require a production-grade set up, then you’ll need to set up an authentication stack which is an AWS CDK deployment of a cognito user pool. An AWS CDK app is an application which is written in TypeScript, JavaScript, Python, Java, or C#. Bear in mind that each of them uses the AWS CDK to define the AWS infrastructure.Cognito should be used because it’ll provide you with instant authentication, allowing you to move wrt creating features more efficiently. What’s more is that Cognito handles a range of tasks that include email verification, forgotten password reminders and much more.Please note that deploying this requires a valid AWS account. Follow the below steps to configure AWSIn production mode you must first obtain a valid JWT. You can visit the Cognito login page to grab the tokens from the redirect URL. At the playground page at the bottom, you’ll see a tab “HTTP Headers.” Make sure to use the access token here, not the id token.You can find the backend code inside the folder server/src.RedisGears contains all RedisGears functions. The file gears.ts includes a client to interact with RedisGears. It supports running functions directly and registering background functions.The results are pushed to Redis Streams to be picked up byContains a very rudimentary ORM. This is where most of the business logic lives.Contains a client and the logic for searching existing questions. When a user creates a session and adds their own question(s), they are stored and indexed by RedisSearch. Users who later want to search for questions will see these as recommendations. These can easily be added to a new session.Contains a client and logic for your Redis TimeSeries integration. Once a session ends, background processing happens with Redis Gears. The results from that are handled by this module. It uses Redis Streams to handle the incoming data.Inside the integration folder, you’ll find scripts that simulate how a client might use the API. There are also tests for individual files.  These are next to the files they’re testing.After creating your squad, you’ll land on the config page. At the bottom of the page, you’ll have a Discord URL input. Copy and paste your Discord URL into the box to set the new Discord token. Afterwards, you’ll then receive a test message in one of your Discord channels.Once users have completed a session, a message will be sent to Discord. Below is an example of a report being sent to Discord.Fast, instant and efficient, this launchpad app leveraged Redis’ ability to transmit data at great speeds to create a powerful squad health check system. Users had the freedom to customize questions which were tailored to their requirements, enabling them to devise a personalized solution.From start to finish, the Redis components made the transmission of data smooth and efficient which generated instantaneous feedback. You can see the full app on the Launchpad along with many other exciting applications that we have.Niek CandaeleNiek is a passionate software engineer who works at RightCrowd. To see more of his work and his activity on GitHub, you can view his profile here."
659,https://redis.com/blog/create-a-real-time-vehicle-tracking-system-with-redis/,Create a Real-time Vehicle Tracking System with Redis,"September 29, 2021",Redis Growth Team,"As you’d expect, real-time vehicle tracking is no child’s play. From start to finish, there’s a whole range of variables that you have to consider before the benefits of real-time vehicle tracking come to fruition. Arguably, the most important factor is speed. Any communication lag will substantially produce stale data, and the staler it gets, the less valuable it becomes.It’s a complex operation, yet this Redis Launchpad app made it work, combining Redis and Golang for their ability to extract data with hyper-efficiency and accuracy. This ensured that users got the most value out of vehicle tracking.Let’s take a look at how it was done. (We’d also like to point out the wide variety of exciting apps in our Redis Launchpad for you to get started with, regardless of your preferred frameworks and languages.)Let’s look at how to create a reliable and powerful real-time vehicle tracking system with Redis. We’ll uncover how Redis was used to create a real-time tracking system in Helsinki to publish real-time locations of buses on a web UI.Given that HSL publishes on the order of ~50 million updates per day, Redis was the preferred tool due to the robustness of the RedisTimeSeries module. It was capable of quickly aggregating tens of thousands of data points.Using this application, users were able to gather real-time information about each bus, including its location, historical positions, and current speed.Let’s analyze each component, unpacking its functionality along with the steps required for its implementation. Below is an overview of the system components that you’ll need:To tie everything together and give you a clear visualization of how each feature was implemented, here’s a quick summary of how everything flows chronologically.Before we look into the nuts and bolts of the architecture, it’s important to highlight that all components were hosted on single AWS  t3.medium with a GP3 EBS volume. Although the t3.medium was appealing because of burstable CPU, a smaller instance could handle the application in its current state.The GoLang broker was used to process incoming messages from Helsinki into Redis. Once processed by GoLang, the messages are sent to a number of different locations:Event data sent to Redis Streams is then processed with a RedisGears function and written to persistent storage (PostgreSQL).Event data is published by GoLang and sent to Redis PubSub which is then sent to each connected client via Web socket. This provides live updates of positions in the browser on the live-location layer.The current speed and location of each bus was recorded via RedisTimeSeries. Time Series data was divided into different series for position (GeoHash) and speed for each scheduled trip. After every 15 seconds, these recordings were standardised using a compaction rule to avoid storing different intervals of data for any given trip.Prerequisites:Clone the repository:Building the application:A functional version of the system can be spun up locally with docker-compose.The above command will spin up (almost) all services required to run a local demo in their own isolated environments.Open the browser and access the application using http://localhost:8080/.The following command can be run if you’re interested in receiving periodic updates to the traffic speeds/neighborhoods layer. This is not absolutely necessary as it can take several hours to gather enough data to get a reasonable amount of data (and you’d still need to wait for the tilegen job to come around to repopulate layers).The Redis PubSub channel is used for live locations. It’s relatively simple: the broker publishes an event to the Redis PubSub channel, then the live location API subscribes to the same message. When a client connects, they receive the same data passed through the web socket. The MQTT broker uses Golang as a Redis client and also utilizes the code below:The incoming event is pushed to Redis Streams. It’s then cleared and processed by a code that runs through RedisGears. This is written using the Redis Go Client below:The current speed and location of each bus trip were recorded in a RedisTimeSeries. A unique identifier was created for each “trip” (a JourneyHash), hashing certain attributes from the event. Just to clarify, the broker creates a time series for both speed and location for each JourneyHash.What’s important to note is that the position and speed series have a short retention and are compacted to a secondary time series. This compacted series has a much longer retention time (~2hr) and is used by the API to show users the trip history layer. By being able to aggregate individual events efficiently, this pattern allows us to mitigate memory usage.CommandsThe commands are executed using Golang. First, check whether a JourneyHash has yet to be seen. You can do this by checking its inclusion in a set (journeyID). If the following returns 1,continue with creating series and rules, or else just TS.ADD the data.The first series is created with the following command. To simplify things, I will refer to these as Time Series A.The aggregation series are fed by the “main” timeseries and are created with the command below. Again, to simplify things, I’ll refer to these as Time Series B.For the rule that governs Time Series A -> Time Series B, you can use the following command:If you want to add data to TimeSeries A, use the following:The redislabs/redismod is a Docker image that contains all of the essential Redis Modules. This was used as the base image for this project. From a stream, this function writes data to PostgreSQL/PostGIS every 5s/10,000 events. Despite Gears running off the main thread, this component is designed to do minimal data processing.Its main purpose is to dump MQTT data into PostGIS and allows the PostGIS and TileGen processes to transform these events to MBtiles.You should also know that the RedisGears function is written in Python and doesn’t call any Redis commands.The PostGIS and TileGen containers are fundamental to serving GTFS and current traffic layers. Just to highlight, PostGIS is a PostgreSQL extension that enables geospatial operations.TileGen is an Alpine container that contains two common utilities used in geospatial processing, GDAL and tippecanoe (and psql, the PostgreSQL client). This container is required for:The TilesAPI is a simple Golang API which is used to fetch those tiles from disk and send them to the frontend.There are two endpoints in the Locations API: /locations/ and /histlocations/.CommandsThe /locations/ endpoint subscribes/reads data from the PUB/SUB channel defined in the MQTT broker section. While written in Go, the redis-cli command for this would be:The /histlocations/ endpoint needs to gather data from multiple time series to create a combined response for the client. This means making a TS.MRANGE call. Because each Timeseries B is labelled with its JourneyHash, the TS.MRANGE gathers the position and speed stats with a single call, filtering on JourneyHash.Frontend (OpenLayers)The frontend uses a JS library called OpenLayers. This is used to create a map and display the layers that were created by previous described services. In production, this is served using Nginx rather than Parcel’s development mode.The frontend also makes calls to a publicly available API for basemap imagery.Technical AppendixData ThroughputIt’s important to underline that this system was not specifically created to handle huge amounts of data. Despite this, it performed competently given this (relatively small scale) task.Based on anecdotes, the system processed ~15GB of messages per day when subscribed to the MQTT topic corresponding to all bus position updates. Below, we have some charts to illustrate the rise in event throughput that occurred on a Sunday morning into afternoon and evening.If you look closely, you’ll notice that towards the middle of the day the events/second hit the highest level at 500/s. This is after growing gradually from <10 events early in the morning.​​However, on a weekday morning at 8am we can see the system handling ~1600+ events/s with ease. Below are some stats from a five minute window on the morning of 5/14/2021.Memory, CPU, and Disk UsageIn local testing, it was originally suspected the CPU would be the most stressed part of the system. As things turned out, it was actually the disk. Below is a capture for the docker stats from 8am on 5/14/2021.By upgrading from AWS standard gp2 EBS to gp3,we were able to get 3000 IOPs and 125MB/s throughput for free, which made hosting the PostgreSQL instance in a container viable. Despite not having a robust disk, the site was still functional but this came at a cost—tile generation was quite slow and could lag 10+ minutes.Since the team liked to expand this component to allow for 30 minute, 1 hour, 2 hour and 6 hour traffic layers, being able to get access to historical positions from the disk efficiently was critical.Before the upgrade, the system load was very high due to the write-behind from gears (writing to disk) and tile generation (from disk). Even during rush hour and tile regeneration, %iowait stays low and system load stays <1.Below are the results from sar during a tile regeneration event:At the heart of the app was a need to have a database capable of transmitting data with precision and speed. Each Redis component was a vital organ that worked harmoniously with one another to produce a highly-effective real-time tracking-system.But by merging Redis with Golang, you have the ability to extract data with hyper-efficiency and accuracy. You can see the full app on the Redis Launchpad and be sure to check out the other great apps we have available.Dustin WilsonDustin is a backend engineer who currently works for Numina. To discover more about his work and his activity on GitHub, you can check out his profile here."
660,https://redis.com/blog/index-and-query-json-docs-with-redis/,"Indexing, Querying, and Full-Text Search of JSON Documents with Redis","July 7, 2021",Emmanuel Keller and Pieter Cailliau,"Related Resource: Click to download RedisJSON module.RedisJSON and RediSearch are by far the most popular Redis modules in our cloud. (See Fig. 1) The docker images of RedisJSON and RediSearch (bundled with Redis) are pulled more than 2000 times every single day. This is why we think of Itamar Haber, technology evangelist at Redis, as a visionary when he wrote the first version 4 years ago. In April, we made several announcements at RedisConf related to JSON, indexing and full-text search capabilities. Today, we’re happy to announce the private preview of these capabilities.In this blog, we’ll give you an overview of the current RedisJSON capabilities. After that we’ll dive into the new capabilities section of this private preview. The ability to index, query, and use full-text search on JSON documents using RediSearch is the coolest new feature of this release. Finally, we’ll show you how to quickly get started.When you don’t have RedisJSON, you model nested documents in Redis by using the String data structure.But, what if we need to update a subpart of the document?To preserve the atomicity of the operation, we will need to:We may need to retry all these steps if another client updated the document during this process.However, with RedisJSON, we can do this update with a single atomic transaction:Let’s look at another example, one where you have a large JSON, but only require a subpart of that document in your application.Without RedisJSON:You have to:With RedisJSON, you can retrieve only the data you require with a single command, minimising CPU cycles, network overhead, and, most importantly, latency.As you can see, RedisJSON simplifies JSON document manipulations. The current GA version of RedisJSON (v1.0) is the version the community is already widely using and solves exactly the shortcomings of modeling nested structures with a String data structure. Here’s an overview of some of its key capabilities.Store (or update) a JSON document associated with a key in RedisReplace a subpart (eg. the string value of a key)Add an item to a collection or a mapExtract the whole documentExtract part of it using a subset of JSONPathWe announced this version at RedisConf 2021, and today we’re happy to announce that it’s available as a private preview for a select group of our Redis Enterprise customers—and as a release candidate to our community. This version has three major features, namely, full support of JSONPath expression, support for Active-Active (with Redis Enterprise), and the ability to index, query, and use full-text search on JSON documents with RediSearch. But there’s more! Let’s dive into the new goodies.System programming languages is a family of languages oriented to efficiency. Programs written in these languages are usually lightweight and provide the best performances. This is the reason why Redis has been historically written in C. It also explains why Redis is able to achieve extremely low latencies and high throughputs. Most of the Redis modules are written in C, C++, or Rust, which are languages of the same family.JSON is especially well served by the Rust community including very fast and efficient JSON serialisation and JSONPath implementation. Giving the benefit of those implementations to Redis users was obvious and just required a mapping between the Redis module API and Rust.And here is the benefit of this RUST rewriting. This new version includes a comprehensive support of JSONPath. It is now possible to use all the expressiveness of JSONPath expressions.Given a JSON documentWildcards (was previously limited to the first item)Extract slicesA more advanced example with filter expressionsActive-Active is a feature provided by Redis Enterprise. Active-Active allows you to replicate your database into several geographically-distributed Redis Enterprise clusters. The users can connect to the closest cluster with local read and write latencies.The implementation is based on Conflict-free Replicated Data-Type (CRDT) technology. While implementing it for most of the core data structures supported by Redis, Redis developed a strong knowledge and experience confirmed by this new implementation made for JSON.Application developers can now rely on this to build geo-distributed applications using JSON documents. Here is an example of a succession of operations in an active-active environment with two clusters:Let see the detail of each operations:We’ll elaborate more on all the synchronization flows when this capability is in Public Preview, but if you’re interested in this capability, don’t hesitate to get in touch now at support@redis.com.This blog also announces the availability of a private preview for RediSearch 2.2 (as a private preview for a select group of our Redis Enterprise customers and as a release candidate to our community).In this section we’re going to describe the new features provided by this new release of RediSearch. But first, here is a reason why we are releasing those two popular modules together:This particular new feature will bring Redis’ JSON capabilities to a whole new level. Going beyond being a Key-Value store, until now, RediSearch has been providing indexing and search capabilities on hashes. Under the hood, RedisJSON 2.0 exposes an internal public API. Internal, because this API is exposed to other modules running inside a Redis node. Public, because any module can consume this API. So does RediSearch 2.2 !By exposing its capabilities to other modules, RedisJSON gives RediSearch the ability to index JSON documents so users can now find documents by indexing and querying the content. These combined modules give you a powerful, low latency, JSON-oriented document database!Let’s have a look at what this would look like.We should first populate the database with a JSON document using the JSON.SET command.To create a new index, we use the FT.CREATE command. The schema of the index now accepts JSONPath expressions. The result of the expression is indexed and associated with an attribute (here: title).We can now do a search query and find our JSON document using FT.SEARCH:Aggregation is a powerful feature of RediSearch that can be used to create analytic reports or perform faceted search style queries. Now that RediSearch can access JSON documents, it’s possible to load any value from a JSON document using JSONPath expression and use it in a pipeline whether the value is indexed or not.Let’s create an index:Add a JSON document to the database:And do a simple computation using two numeric value extracted from the JSON document:With the new version of RediSearch, it’s now possible to index the same value (field on hashes, or JSON Values from a JSON document) with different parameters. Here is a typical use case, solved by this new feature:Let’s have a database containing documents that belong to categories.Using the TAG type you can then easily filter your search results on any category:But what if you also want to be able to do a full-text search on categories?Until now, with hashes, you had to duplicate the value into two fields, which would consume twice the memory.This is where FT.CREATE…AS has become more than handy. Let’s get back to our nice and simple document:…and use the new AS feature:…and…Bingo! We can now filter by a tag, and do a full text search in the same field, without having to duplicate the data.Time complexity of most of the Redis commands is well documented. As an example, HMGET comes with a complexity of O(N), “where N is the number of fields being requested.” With RediSearch, it’s possible to write advanced queries. The complexity of the FT.SEARCH and the FT.AGGREGATE commands, however, depend on the complexity of the query.We wanted to give you the tools to understand what’s happening under the hood when a query is executed, to figure out where time is consumed, and how the query can be optimized. The new FT.PROFILE command returns a tree showing the main steps used by RediSearch to execute the query. For each step, a time information is given.So what happens inside RediSearch when we are doing a query with a fuzzy search ?Let see an example:We are ready to profile our query. Let’s run the profiling and decompose the profiling result.redis.cloud:6379> FT.PROFILE idx SEARCH LIMITED QUERY ""%hello%""First we get the result. Useful to check that the profiling query returns what is expected.Here is the total time, called “profile time”, because it includes the time spent in collecting the profile information.The time spent in parsing the query and building the execution plan:Here is the time spent in finding the fuzzy matches in the dictionary:And finally, have you ever wondered what it means to build a search result? We need to compute the full-text score for each document, sort them by score, and finally load the fields. With this information you can identify bottlenecks, make queries faster, and improve performance of the server.We believe that these new capabilities are game changers for application developers and the Redis community. Here’s how you get started.To get started you can pull the following docker image with the :preview tag:Alternatively, you can compile from the RC1 release tags (v2.2.0 for RediSearch, v2.0.0 for RedisJSON) on both repositories and load them to Redis.Once you’re up and running, you can try out all the above commands or with this quickstart guide. We will also be launching a series of blogs about RedisMart, an online retail application that we showcased during the keynote of RedisConf 2021. RedisMart leverages RediSearch and RedisJSON deployed in a geo-distributed manner to deliver the best online retail experience. In this series, we’ll walk you step by step through how we build this application.The following list of clients are currently being upgraded so you’re able to use the new features with a good developer experience. Check the latest releases and/or the pull requests (at this moment most of them are supporting the preview version on the master branches).We welcome any feedback, bug reports, feature requests while we work towards General Availability. Leave feedback on the documentation websites or in the github repositories of RediSearch (on Github) or RedisJSON (on Github), or get in touch with us on Discord."
661,https://redis.com/blog/geobike-part-two-querying-the-data/,GeoBike Part Two: Querying The Data,"February 1, 2018",Tague Griffith,"Indexing The DataIn a previous post, we built the back end for a location-aware application using Redis and  talked about how to use a Python program to load data from a General Bikeshare Feed Specification (GBFS) data feed and store information about bike-sharing systems operating throughout the world. Our program parsed the JSON structured data from the feed to store metadata about each station in Redis, and then index the location of each station using Redis’ geospatial data structure.Our indexing and parsing program loaded the metadata for sharing stations into Redis using structured keys in the form ${system_id}:station:${station_id} with the system_id and the station_id being unique identifiers specified in the GBFS data. The geospatial location index of all the stations in a particular sharing system is stored using the key ${system_id}:stations:location.Now that we have built a back-end database of sharing stations and indexed it with location information, we can use that data to find sharing stations located near a user’s current location.Getting the User’s LocationThe next step in building out our application is to determine the user’s current location. Most applications accomplish this through built-in services provided by the operating system. The OS can provide applications with a location based on GPS hardware built into the device or approximated from the device’s available WiFi networks.On Apple devices, you can find the user’s current location using CoreLocation, which is available in all Apple device SDKs.  If you are experimenting with our code and want to try this out on your Mac OS X laptop, you can download and build the whereami tool which uses CoreLocation to estimate your computer’s current longitude and latitude.Finding StationsAfter we have found the user’s current location, we want to locate any bike sharing stations that are nearby. Using the geospatial functions of Redis, we can look up stations within a given distance of our current coordinates. Let’s walk through an example of this using the Redis CLI.Imagine I’m at the Apple Store on Fifth Avenue and I want to head downtown to Mood on West 37th to catch up with my buddy Swatch. I could take a taxi or the subway, but I’d rather bike.  Are there any nearby sharing stations where I could borrow a bike for my trip?The Apple store is located at 40.76384, -73.97297. We can draw a 500 ft radius around the store (in blue) on the map overlay we created as part of the first post and see that two Bike Share Stations—Grand Army Plaza & Central Park South and E 58th St & Madison—fall within the radius.Using the Redis GEORADIUS command, I can query the index we loaded for the NYC system for stations within a 500-foot radius by issuing the following command to Redis:Redis returns the two elements found within that radius. The elements in our geospatial index are the keys for the metadata about a particular station, so we’ll look up the names for the two stations:We find those keys correspond to the stations we identified from the map above.  We can add additional flags to the GEORADIUS command to get a list of elements, their coordinates and their distance from our current point:After looking up the names associated with those keys, we can present an ordered list of stations to the user so that they can choose ah location. Redis doesn’t provide any directions or routing capability, so you will need to use the routing features of the device’s OS to plot a course for the user from their current location to the selected bike station.The GEORADIUS function can be easily implemented inside an API in your favorite development framework to add location functionality to an app.Other Query CommandsIn addition to the GEORADIUS command, Redis provides three other commands for querying data from the index: GEOPOS, GEODIST, and GEORADIUSBYMEMBER.The GEOPOS command can be used to get the coordinates for a given element from the geohash. For example, if I know that there is a bike sharing station at W 38th and 8th and Its ID is 523, then the element name for that station is NYC:station:523. Using Redis, we can find the longitude and latitude of that particular station:The GEODIST command provides the distance between two elements of the index. If I wanted to find the distance between the station at Grand Army Plaza & Central Park South and the station at E 58th St & Madison, I would issue the following command:Finally, the GEORADIUSBYMEMBER command is similar to the GEORADIUS command, but instead of taking a set of coordinates, the command takes the name of another member of the index and returns all of the members within a given radius centered on that member. To find all of the stations within 1000 feet of the Grand Army Plaza & Central Park South, enter the following:The geospatial indexing functions of Redis make it easy for developers to add location-aware features to their applications. If you have any questions regarding this post, please connect with me (@tague) on Twitter."
662,https://redis.com/blog/bits-and-bats/,Bits and Bats,"May 21, 2018",Sandro Pasquali,"In this article you’re going to learn how to perform bitwise operations on Redis keys, as well as setting, getting and comparing binary values. To begin, let’s think about how “flipping bits” can be a useful way of storing information in general, and then how we can execute binary operation with Redis.Take the following example: there are 30 Major League Baseball teams, and each team plays 162 games per year. There are 2430 total games played each year. These games are played over roughly six months. We know a month never has more than 31 days, so we can store the playing schedule of a single team over one month using 31 bits of a 32-bit number.As you already know one (1) byte is 8 bits, and bits are tracked “right-to-left” in binary (base 2) numbers. The zero (0) bit is rightmost and smallest. The largest number you can represent with 8 bits is 256:In the examples below, we’ll be working with 32-bit numbers (4 bytes), with the 31st bit position (the largest possible in any given month) addressed at 2^30.Let’s say The New York Yankees play on the 1st, the 3rd to the 10th, the 13th-28th and the 31st. In bits, beginning rightmost, where the zero bit represents the first day, this looks like:This shows that we’ll need 10 bytes to store the monthly schedule for an MLB team. Which means we can store the schedules for every team in 10 * 30 bytes (bytes * number of teams), which is not many bytes, and on top of that we can prove that this is the maximum space we’ll ever need, unless the rules governing baseball change.If you wanted to know if the team was playing on the 13th, you would need to get a bitmask for the 13th bit (using the left-shift operator <<):1 << 12 // 4096Using a language like JavaScript we can demonstrate the left-shift and implied 0 padding on a 31 bit number more clearly:Or visualized as the comparison of two strings of zeros and ones:Similarly, we can set the 13th bit using the OR(|) operator:1342174205 | 4096Or more explicitly:Neat! Now let’s use Redis to store much larger amounts of binary data.Redis is a data structure store built on the key/value paradigm, and the most basic operation is to store a string in a key:The Redis BITOP-family of commands allow you to perform bit operations on string keys. Using the well named SETBIT and GETBIT commands you can…  set and get bits on a key. You can set comparison keys using the bitwise operators AND, OR, and XOR, as we did with the MLB schedule above. For example, let’s turn on the 3rd bit key test:That was easy. It should be clear how to accomplish the test above by setting bits on certain days of a team schedule, or creating masks.Let’s use bitmaps to solve the problem of storing the win/loss record of the Yankees. We know they will play 162 games in a year. If 1 is a win and 0 is a loss, we can create a Redis key ‘yankees_record’ and set all the wins like this:By applying bitmasks mapping a range of bits to other binary values, you can make very rapid and memory-efficient analytical comparisons. In the next section we will learn some typical examples of how to use this technique.Any key in a Redis database can store (2^32 – 1) bits, or just under 512 MiB (for now). This means that there are approximately 4.29 billion columns, or offsets, that can be set per key. This is a large number of bits referenced in a single key. We can set bits along these ranges to describe the characteristics of an item we would like to track, such as the number of users who have viewed a given article.Assume that we are serving many different articles and each article is assigned a unique identifier. Also assume that we have 100,000 active members on our website, and that each user also has a unique identifier—a number between 1 and 100,000. We can use bit operations to track view activity by creating a key unique to that article and setting bits corresponding to the ID of the user viewing the given article. The following example shows that article 808 has been viewed by users 4, 6, 9-11, and so on:article:808:01-03-2018 : 00010100111010001001111...This key represents article 808 on a specific date, efficiently storing the unique user IDs of viewers on that day by flipping a bit at the offset corresponding to the user’s assigned ID. Whenever a user views an article, we use the SETBIT command to set a bit at the offset provided by that user’s ID:Let’s create data for three articles:Here, we simply created three Redis keys, article (1-3):today, and randomly set 100,000 bits on each key—either 0 or 1. Using the technique of storing user activity based on user ID offsets, we now have sample data for a hypothetical day of traffic against three articles.To count the number of users who have viewed an article, we can use BITCOUNT:This method is straightforward: the number of users who saw the article equals the number of bits set on the key. Now, let’s count the total number of article views:Once MULTI returns an array of results corresponding to the results SETBIT returned from Redis for each operation (a count of bits), we reduce the count to a sum representing the total number of views of all our articles.If we are interested, instead, in how many articles user 123 has viewed today, we can use GETBIT, which simply returns the value (either 0 or 1) at a given offset. The result will be in the range 0–3:These are very useful and direct ways to glean information from bit representations. Let’s go a little further and learn about filtering bits using bitmasks and the AND, OR, and XOR operators.What if we want to check whether user 123 has read both articles? Using the BITOP AND, this is easy to accomplish:First, we create a mask that isolates a specific user stored at the key user123, containing a single positive bit at offset 123 (again, representing the user’s ID). The results of an AND operation on two or more bit representations is not returned as a value by Redis but rather written to a specified key, which is given in the preceding example as “123:sawboth.” This key contains the bit representation that answers the question of whether or not both the article keys contain bit representations that also have a positive bit at the same offset as the user123 key.The OR operator works well when trying to find the total number of users who have seen at least one article:Here, the atleastonearticle key flags bits at all offsets that were set in any one of the three articles. We can use these techniques to create a simple recommendation engine.For example, if we are able to determine via other means that two articles are similar (based on tags, keywords, and so on), we can find users who have read one and recommended the other. To do this we use XOR to find all users that have read the first article or the second article, but not both. We then break that set into two lists: those who have read the first article and those who have read the second article and compare these lists to offer recommendations:While it is not necessary, we also fetch a count of each list and delete the result keys when we are done.To calculate the total number of bytes occupied by a binary value in Redis, divide the largest offset by 8. Storing access data for 1,000,000 users on one article requires a maximum of ~125 kB—not a very large amount of memory or storage to spend in return for such a rich set of analytics data. Because we can accurately measure the space needed, this also gives us some confidence when planning storage costs, scaling and so forth.Sandro Pasquali formed a technology company named Simple in 1997, that sold the world’s first JavaScript-based application development framework and was awarded several patents for deployment and advertising technologies that anticipated the future of Internet-based software. He has written three books on NodeJS. He builds enterprise-class software for private clients, often deploying the incredibly versatile Redis to help meet complex information management requirements for large clients."
663,https://redis.com/blog/redisearch-in-action/,RediSearch in Action,"March 30, 2021",Abhishek Gupta,"Redis has a versatile set of data structures ranging from simple Strings all the way to powerful abstractions such as Redis Streams. The native data types can take you a long way, but there are certain use cases that may require a workaround. One example is the requirement to use secondary indexes in Redis in order to go beyond the key-based search/lookup for richer query capabilities. Though you can use Sorted Sets, Lists, and so on to get the job done, you’ll need to factor in some trade-offs.Enter RediSearch! Available as a Redis module, RediSearch provides flexible search capabilities, thanks to a first-class secondary indexing engine. It offers powerful features such as full-text Search, auto completion, geographical indexing, and many more.To demonstrate the power of RediSearch, this blog post offers a practical example of how to use RediSearch with Azure Cache for Redis with the help of a Go service built using the RediSearch Go client. It’s designed to give you a set of applications that let you ingest tweets in real-time and query them flexibly using RediSearch.Specifically, you will learn how to:As mentioned, the example service lets you consume tweets in real-time and makes them available for querying via RediSearch.It has two components:At this point, I am going to dive into how to get the solution up and running so that you can see it in action. However, if you’re interested in understanding how the individual components work, please refer to the Code walk through section below, and the GitHub repo for this blog: https://github.com/abhirockzz/redisearch-tweet-analysis.PrerequisitesStart off by using this quick-start tutorial to set up a Redis Enterprise tier cache on Azure. Once you finish the set up, ensure that you have the the Redis host name and access key handy:Both the components of our service are available as Docker containers: the Tweet indexing service and the Search API service. (If you need to build your own Docker images, please use the respective Dockerfile available on the GitHub repo.)You will now see how convenient it is to deploy these to Azure Container Instances, which allows you to run Docker containers on-demand in a managed, serverless Azure environment.A docker-compose.yml file defines the individual components (tweets-search and tweets-indexer). All you need to do is update it to replace the values for your Azure Redis instance as well as your Twitter developer account credentials. Here is the file in its entirety:Create an Azure context:Clone the GitHub repo:Deploy both the service components as part of a container group:(Note that Docker Compose commands currently available in an ACI context start with docker compose. That is NOT  the same as docker-compose with a hyphen. )You will see an output similar to this:Wait for services to start, you can also check the Azure portal. Once both the services are up and running, you can check their respective logs:If all goes well, the tweet-consumer service should have kicked off. It will read a stream of tweets and persist them to Redis.It’s time to query the tweet data. To do so, you can access the REST API in Azure Container Instances with an IP address and a fully qualified domain name (FQDN) (read more in Container Access). To find the IP, run docker ps and check the PORTS section in the output (as shown below):You can now run all sorts of queries! Before diving in, here is a quick idea of the indexed attributes that you can use in your search queries:(Note, I use curl in the examples below, but would highly recommend the “REST Client” for VS Code)Set the base URL for the search service API:Start simple and query all the documents (using * ):You will see an output similar to this:Notice the headers Page-Size and  Search-Hits: these are custom headers being passed from the application, mainly to demonstrate pagination and limits. In response to our “get me all the documents” query, we found 12 results in Redis, but the JSON body returned 10 entries. This is because of the default behavior of the RediSearch Go API, which you can change using different query parameter, such as:Or, for example, search for tweets sent from an iPhone:You may not always want all the attributes in the query result. For example, this is how to just get back the user (Twitter screen name) and the tweet text:How about a query on the user name (e.g. starting with jo):You can also use a combination of attributes in the query:How about we look for tweets with specific hashtags? It is possible to use multiple hashtags (separated by |)?Want to find out how many tweets with the biden hashtag were created recently? Use a range query:If you were lucky to grab some coordinates info on the tweets, you can try extracting them and then query on coordinates attribute:These are just a few examples. Feel free to experiment further and try out other queries. This section in the RediSearch documentation might come in handy!Important: After you finish, don’t forget to stop the services and the respective containers in Azure Container Instances:Use the Azure Portal to delete the Azure Redis instance that you had created.This section provides a high-level overview of the code for the individual components. This should make it easier to navigate the source code in the GitHub repo.Tweets consumer/indexer:go-twitter library has been used to interact with Twitter.It authenticates to the Twitter Streaming API:And listens to a stream of tweets in a separate goroutine:Notice the go index.AddData(tweetToMap(tweet))—this is where the indexing component is invoked. It connects to Azure Cache for Redis:It then drops the index (and the existing documents as well) before re-creating it:The index and its associated documents are dropped to allow you to start with a clean state, which makes it easier to experiment/demo. You can choose to comment out this part if you wish.Information for each tweet is stored in a HASH (named tweet:<tweet ID>) using the HSET operation:Tweets search exposes a REST API to query RediSearch. All the options (including query, etc.) are passed in the form of query parameters. For example, http://localhost:8080/search?q=@source:iphone.  It extracts the required query parameters:The q  parameter is mandatory. However, you can also use the following parameters for search:For example:Finally, the results are iterated over and passed back as JSON (array of documents):That’s all for this section!Redis Enterprise is available as a native service on Azure in the form of two new tiers for Azure Cache for Redis which are operated and supported by Microsoft and Redis. This service gives developers access to a rich set of Redis Enterprise features, including modules like RediSearch. For more information, see these resources:This end-to-end application demonstrates how to work with indexes, ingest real-time data to create documents (tweet information) which are indexed by RediSearch engine and then use the versatile query syntax to extract insights on those tweets.Want to understand what happens behind the scenes when you search for a topic on the Redis documentation? Check out this blog post to learn how Redis site incorporated full-text search with RediSearch! Or, perhaps you’re interested in exploring how to use RediSearch in a serverless application?If you’re still getting started, visit the RediSearch Quick Start page.If you want to learn more about the enterprise capabilities in Azure Cache for Redis, you can check out the following resources:"
664,https://redis.com/blog/how-to-build-a-competitive-online-racing-game-using-unity-and-redis/,How to Build a Competitive Online Racing Game Using Unity and Redis,"November 11, 2021",Redis Growth Team,"If you’re into gaming and love creating things, then you may take inspiration from this Launchpad App. By using Redis, Graham Pinsent created his very own online racing game where friends, family, and colleagues can settle old scores and compete against each other for the finish line.Just like any racing game, the ultimate objective is to get from A to B in the fastest possible time. Players are given a car, times are contested, and the winner is decided based on whoever can zoom through each lap the fastest.But for this application to function optimally, data must be transmitted with hyper-efficiency to ensure player commands are carried out in real time. Failing to achieve this would create a disconnect between the player’s commands and their in-game vehicle, leading to a frustrating gaming experience.To prevent this from happening, Graham leveraged Redis’ advanced capabilities to create an application where components are able to seamlessly transmit data between one another with maximum efficiency.Let’s investigate how this was done. We also have a variety of exciting applications created by the Redis community for you to check out. Make sure to catch all of the action on the Redis Launchpad.You’ll build an online racing game that allows players to compete against each other using Redis. Below, we’ll go through each stage chronologically and highlight what components you need to bring this application to life.Ready to get started? Ok, let’s dive straight in.You can also use Redis Enterprise Cloud with RedisJSON module enabled as shown below:Under the Node JS folder, create the .env file with details of the database.Change directory to nodejs and execute the below CLI:Finally, you can either build the game using Unity, or just press play inside the project.Open the web browser https://IP: 3000 to access the game.Once a player selects their name and is connected to the server, their position on the map is sent to the Redis database. As the player navigates through the track, their position is being updated 10x/second. This is what the player data might look like:Every time a player makes a POST to the server with their new location, the response from the server includes all players current positions and data. This is then used to place everyone else’s car on the track for you to see as you are driving. Linear interpolation is used to smoothen the movement, making it look faster than 10 updates per second.Once a player crosses the finish line, their time and name are sent to the server. Below is an example of track time data:This data is used to create a live leaderboard that everyone can see as they race against each other for the fastest time possible. When any car crosses the finish line, the client will request for the new leaderboard data from the server.When a player first starts the game, they’ll be prompted with an option to choose their name. Once they do this and press start, the game client sends a POST request to the server with their name. The server will then carry out checks on the name to validate it, including verifying whether an existing name is already taken.This is done by obtaining every player’s JSON from the Redis database and pushing it into an array to loop over. When this happens, the server will be checking their name against the data.And finally, after the validation, the server will send a success response back to the client. If the name was invalid then the server will send a failure response.Once the player has spawned into the game, a POST request is sent to the server every tenth of a second. The request includes their name, position, and rotation. As soon as the server receives the request, it will add the current time into the JSON then register it in the Redis database.When the player’s new position has been set, the server will then obtain every player’s JSON from the Redis database and send it as a response back to the game.As soon as the game client receives the data, it can update the position of every connected player.When a player finishes a lap, a POST request will be sent to the server. The information in this request will include the player’s name and the time it took for them to finish the lap. At this point, the server will then automatically try to retrieve their previous lap time(s) from the Redis database.The server then checks whether they had an old-time or if this is the player’s first lap time that they’re submitting. If the player has an old-time, it will compare it with the new time to see if it’s faster. If it is, then it will register the new time into the leaderboard JSON in the database.If it’s their first time, their name and lap-time will be registered into the leaderboard.And finally, the server will obtain all of the leaderboard’s data and send the response back to the game client.The game client can then form the leaderboard with the data.After every 30 seconds, the server will obtain every player’s JSON data from the Redis database.When a player closes their game, the game client will stop sending update position requests. The server will loop through all players to check if their last update was more than thirty seconds ago. If it was, then they are deleted from both the player’s JSON and the leaderboard JSON.The same check happens to the game client. If their last update was received more than 30 seconds ago, then their car object will be destroyed.When you first start the game, you’ll have the opportunity to choose your player’s name as seen below. Once you enter your name and press start, the game client will then send this information as a POST request to the server.The server will then perform various checks on the name to validate it. For example, it will check whether or not the name is already taken. This validation process happens by getting every player’s JSON from the Redis database to then push it into an array to loop over and check their name against the data.Once the validation has been completed, the server will send a success response back to the client. If the name was invalid then it will send a failure response back.Once a player has spawned into the game, a POST request will be sent to the server every 0.1 seconds. The request will include their name, position, and rotation. After the server receives the request, it will add the current time into the JSON and set the JSON in the Redis database.As soon as the client receives the data, it will update the position of every player that’s connected to the game.Whenever a player finishes a lap, a POST request is sent to the server that includes important information, such as their name and the time it took to finish the lap. Prior to this, the server will access and include a player’s previous lap times from the Redis database.At this point, the server checks whether or not this is the player’s first lap time that they’re submitting. As you’d expect, if the player already has a previous lap-time recorded then the server will compare it with the new time to see if it’s faster. If it is, then it will set the new time into the leaderboard JSON in the database.However, if it’s their first time, then their name and time will be recorded and inserted into the leaderboard.And finally, the server will access the entire leaderboard and send the response back to the game client.At this point, you’ll be able to create the leaderboard with the data.In this application, the server will receive every player’s JSON data from Redis after every 30 seconds.When a player closes their game, the game client will stop sending updated position requests. This is significant because the server carries out its own checks to see whether if each player is still connected to the game.The server achieves this by looping through all players to see whether if their last update was more than 30 seconds ago. If it was, then these players will be deleted from both the players JSON and the leaderboard JSON.The same check happens on the game client. If their last update was received more than 30 seconds ago, then their car object will be destroyed.Racing games remain a hot favorite in the gaming industry. First we had Mario Kart, then we had Need for Speed, and now we have something that you can create on your own laptop. With Redis, this application is able to operate in real-time and maximize the gaming experience by allowing users to battle it out without any lags.But that’s not all…Innovative programmers from all around the world are tapping into the wonders of Redis to create phenomenal applications that are having their own impact on society.Whether you just want to have a bit of fun or are looking to build something that’ll improve everyday life, Redis can help bring your project ambitions come to life. Make sure to check out the Redis Launchpad for more inspiration.Or if you want to learn more about how this application was made, then you can watch the full YouTube video by clicking here.Graeme is a DevOps engineer who operates in the gambling and sports betting industry. Make sure to head over to his GitHub page to see what other projects he’s been involved in."
665,https://redis.com/blog/how-to-create-notification-services-with-redis-websockets-and-vue-js/,"How to Create Notification Services with Redis, Websockets, and Vue.js","August 19, 2020",Tugdual Grall,"It is very common to get real-time notifications when navigating in a web application. Notifications could come from a chat bot, an alerting system, or be triggered by an event that the app pushes to one or more users. Whatever the source of the notifications, developers are increasingly using Redis to create notification services.In modern applications powered by a microservices architecture, Redis is often used as a simple cache and as a primary database. But it is also used as a communication layer between services using a persistent messaging layer powered by Redis Streams, a lightweight eventing system using its well-known Pub/Sub (Publish/Subscribe) commands.In this blog post, we’ll show you how easy it is to create a small notification service using Redis Pub/Sub to send messages to a web application, developed with Vue.js, Node.js, and WebSockets.PrerequisitesThis demo service uses:If you do not have already a Redis instance running, you can start it using Docker; in a terminal, run this command:> docker run -it --rm --name redis-server -p 6379:6379 redisRedis should now be up and running and ready to accept connections.To configure the project with the proper structure, open a terminal and enter the following commands:Create a new Node.js project using npm (the -y parameter will set all values to the default one):The final command above adds the WebSocket and Redis dependencies to your project. You are now ready to write some code!Writing the WebSocket serverOpen your favorite code editor for Node.js (I use Visual Studio Code) and simply enter the code “code .” to open the current directory. In your editor, create a new file called server.js.This simple Node.js program is limited to the demonstration and focuses on:Lines 5 and 6 are simply used to configure the Redis server location and the port to use for the Web Socket server. As you can see it is pretty simple.Running the WebSocket serverIf you have not yet installed nodemon, install it now. Then start the WebSocket server using the following command:Let’s now create the frontend that will receive the notifications and print them to the user.Open a new terminal and run the following command from the notifications directory:If you have not already installed the Vue CLI tool already, do so now using the command npm install -g @vue/cli.This command creates a new Vue project that is ready to be executed and extended.One last package to install for this demonstration is BootstrapVue, which makes it easy to use the CSS library and components from the popular Bootstrap framework.Open the web-client directory in your favorite code editor, then start the newly created Vue application:The last command starts the Vue development server that will serve the pages and also automatically reloads the pages when you change them.Open your browser, and go to http://localhost:8080; where you should see the default Vue welcome page:Adding WebSocket to the frontendThe Vue framework is quite simple, and for this post we will keep the code as simple as possible. So let’s quickly look at the directory structure:The files at the root level (babel.config.js, package.json, package-lock.json, node_modules) are used to configure the project. The most interesting part, at least for now, is located in the src directory:The public/index.html is the static entry point from where the DOM will be loaded. If you look at it you will see a <div id=”app”>, which is used to load the Vue application.This demonstration is quite simple, and you will have to modify only two files: the App.vue and main.js files. In a real-life application, you would probably create a Vue.js component that would be reused in various places.Updating the App.vue file to show WebSocket messagesOpen the App.vue file in your editor and add the information listed below. At the bottom of the page, just before the </div> tag, add the following HTML block:Using {{message}} notation, you are indicating to Vue to print the content of the message variable, which you will define in the next block.In the <script>, replace the content with:These few lines of code are used to:If you look carefully at what has been changed, you can see that you have added:Sending messages from Redis to your Vue applicationThe WebSocket server and the Vue frontend should now be running and connected thanks to the few lines of JavaScript you added. It’s time to test it!Using the Redis CLI or RedisInsight, publish some messages to the app:notifications channel. For example, if you started Redis using Docker, you can connect to it using the following command and start publishing messages:You should see the message appear at the bottom of the application in your browser:As you can see, it is pretty easy to push content to your web frontend in real time using WebSocket. So now lets improve the design and add a more user-friendly interface using Bootstrap.Creating an alert block with BootstrapIn this section, we’ll show you how to use a Bootstrap alert component, which appears when a new message is received and disappears automatically after a few seconds, using a simple countdown.Main.js fileOpen the main.js file and add the following lines after the last import:These four lines import and register the Bootstrap components in your Vue application.App.js fileIn the App.vue file, replace the code you added earlier (everything between the two <hr/> tags and the tags themselves) with the following:This component uses several attributes:Let’s add a few lines of JavaScript to define the variables and methods used by the alert component:In this section you have:Let’s try it!Go back to redis-cli or Redis Insight and post new messages to the app:notifications channel.As you can see, it is easy to use Redis to create a powerful notification service for your application. This sample is pretty basic, using a single channel and server and broadcasting to all the clients.The goal was really to provide an easy way to start with WebSocket and Redis Pub/Sub to push messages from Redis to a web application. There are many options to deliver messages to specific clients using various channels, and to scale and secure the application.You can also use the WebSocket server in the other direction, to consume messages as well as to push messages to clients. But that’s a big topic for another blog post. In fact, stay tuned for more blog posts on how you can use Redis Gears to easily capture events directly in the Redis database and push some events to various clients.For more information, see these resources:"
666,https://redis.com/blog/how-to-manage-real-time-iot-sensor-data-in-redis/,How to Manage Real-Time IoT Sensor Data in Redis,"February 2, 2021",Ajeet Raina,"Imagine you’re an air-conditioner manufacturing company that sells millions of smart AC units to consumers. You are building a centralized, smart climate control system that collects sensor data about a house’s temperature, pressure, and humidity and sends it to a central location for an efficiency analysis to help end users trim their electricity bills.This blog post will show a simplified version of such a use case to demonstrate how it all works—so you can understand how to manage a wide variety of real-time IoT sensor data in Redis.Here’s what we used:Hardware requirements:Software requirements:Follow this 10-step process to see how it all fits together:There’s a huge variety of sensors on the market, but this demonstration uses a Pimoroni BME680 breakout board. BME680 is an integrated environmental sensor developed for mobile applications and wearables, where size and low power consumption are key requirements. It can measure temperature, pressure, humidity, and indoor air quality, and is Raspberry Pi and Arduino-compatible.For this demonstration, we’re using an NVIDIA Jetson Nano board, a small, powerful computer for developers to learn, explore, and build AI applications for edge devices. Priced at $59, it’s basically a developer kit that includes a Jetson Nano module with 2GB memory and delivers 472 GFLOPS of compute power. This demonstration should also work with other popular IoT devices, such as the Raspberry Pi, Arduino, Banana Pi, etc.The BME680 plugs directly into a Jetson Nano board without any connecting wires.After wiring the sensors, we recommend running I2C detection with i2cdetect to verify that you see the device: in our case it shows 76. Please note that the sensor communicates with a microcontroller using I2C or SPI communication protocols.You will need a Redis server up and running, either on your local laptop or in the cloud, and the Redis server must be compiled with the RedisTimeSeries module. In this demonstration, we’re using Redis Enterprise Cloud, a fully managed cloud database service that comes with the RedisTimeSeries module already built in and integrated.If you are completely new to RedisTimeSeries, check out our RedisTimeSeries Quick Start tutorial. It explains how to get started with Redis Enterprise Cloud and how to enable RedisTimeSeries. You will need a few details for this implementation:Reading the sensor values from the BME680 is fairly straightforward, but requires you to set a few configuration values. You can also run the sensor in two different “modes”—with or without gas readings. Just taking temperature, pressure, and humidity readings lets you sample data much faster.Let’s look first at the library import and the configuration settings. Open a terminal window, create a file, and then type the following:The first module, bme680, allows you to easily write Python code that reads the humidity, temperature, and pressure from the sensor. Similarly, there are other Python modules, such as time, to handle time-related tasks, redis to import Redis Python modules, and so on. We’re using the time library to introduce a small delay between each reading of the sensor to help ensure consistent results.The sensor = bme680.BME680() command creates an instance of the sensor that we’ll use to configure the settings and get the sensor’s readings. The _oversample settings we established for the humidity, pressure, and temperature measurements are designed to strike a balance between accurate readings and minimizing noise. The higher the oversampling, the greater the noise reduction, albeit accompanied by a reduction in accuracy.The _filter protects sensor readings against transient changes in conditions, e.g. a door slamming that could cause the pressure to change momentarily, and the IIR filter removes these transient spiky values.Shown in the code below, the gas measurement has a few settings that can be tweaked. It can be enabled or disabled with set_gas_status. Disabling it allows the other readings to be taken more rapidly, as mentioned above. The temperature of the hot plate and how long it’s held at that temperature can also be altered, although we recommend not changing these settings if your gas resistance readings look sensible.Next, we define the Redis connector, where we specify the Redis instance host, port, and password. As shown below, the code below defines the various RedisTimeSeries keys, such as a temperature key (TS:TEMPERATURE), pressure key (TS:PRESSURE), and humidity key (TS:HUMIDITY).The sensor.get_sensor_data() instruction gets the data from the sensor and populates the three variables with temperature, humidity, and pressure.Next, a “transactional pipeline” is constructed by calling the .pipeline() method on a Redis connection without arguments. Under the covers, the pipeline collects all the commands that are passed until the .execute() method is called. As you can see, we used RedisTimeSeries’ TS.ADD command to populate the sensor data structure. You can access the complete code via this GitHub Repository.Before you execute the script, you will need to import the bme680 and smbus Python modules, as shown here:Make sure you supply the right Redis Enterprise Cloud database endpoints, username, and password:You can run the monitor command to verify that sensor data is being populated, as shown here:It’s exciting to see the sensor data plotted in Grafana. To implement this, run the command below:Be sure that you have Docker Engine running in your system, either on your desktop system or in the cloud. For this demonstration, I have tested it on Docker Desktop for Mac.Point your browser to https://<IP_ADDRESS>:3000. Use “admin” as username and password to log in to the Grafana dashboard.Click the Data Sources option on the left side of the Grafana dashboard to add a data source.Under the Add data source option, search for Redis and the Redis data source will appear as shown below:Supply the name, Redis Enterprise Cloud database endpoint, and password, then click Save & Test.Click Dashboards to import Redis and Redis Streaming. Click Import for both these options.Click on Redis to see a fancy Grafana dashboard that shows the Redis database information:Finally, let’s create a sensor dashboard that shows temperature, pressure, and humidity. To start with temperature,  first click on + on the left navigation window. Under Create option, Select Dashboard and click on the Add new panel button.A new window will open showing the Query section. Select SensorT from the drop-down menu, choose RedisTimeSeries as type, TS.GET as command and ts”temperature as key.Choose TS.GET as a command.Type ts”temperature as the key.Click Run followed by Save, as shown below:Now you can save the dashboard by your preferred name:Click Save. This will open up a sensor dashboard. You can click on Panel Title and select Edit.Type Temperature and choose Gauge under Visualization.Click Apply and you should be able to see the temperature dashboard as shown here:Follow the same process for pressure (ts:pressure) and humidity (ts:humidity), and add them to the dashboard. You should be able to see the complete dashboard readings for temperature, humidity, and pressure. Looks amazing. Isn’t it?This demo shows how RedisTimeSeries combines the benefits of Redis and a purpose-built time-series database. The combination allows you to easily track environmental factors by effectively storing and managing RedisTimeSeries data. Finally, by integrating Grafana with RedisTimeSeries, you can create a useful, informative dashboard that lets you zoom in and out on the charts in real time.This sample application is just one example of the many cool things you can do with RedisTimeSeries. For more ideas, check out these interesting use cases:Build Your Financial Application on RedisTimeSeries3 Real-Life Apps Built with Redis Data Source for GrafanaReal-Time Observability with Redis and Grafana"
667,https://redis.com/blog/schemaless-databases/,Schemaless Databases: Pros and Cons,"June 17, 2022",Paula Dallabetta,"A schemaless database manages information without the need for a blueprint. The onset of building a schemaless database doesn’t rely on conforming to certain fields, tables, or data model structures. There is no Relational Database Management System (RDBMS) to enforce any specific kind of structure. In other words, it’s a non-relational database that can handle any database type, whether that be a key-value store, document store, in-memory, column-oriented, or graph data model. NoSQL databases’ flexibility is responsible for the rising popularity of a schemaless approach and is often considered more user-friendly than scaling a schema or SQL database.With a schemaless database, you don’t need to have a fully-realized vision of what your data structure will be. Because it doesn’t adhere to a schema, all data saved in a schemaless database is kept completely intact. A relational database, on the other hand, picks and chooses what data it keeps, either changing the data to fit the schema, or eliminating it altogether. Going schemaless allows every bit of detail from the data to remain unaltered and be completely accessible at any time. For businesses whose operations change according to real-time data, it’s important to have that untouched data as any of those points can prove to be integral to how the database is later updated. Without a fixed data structure, schemaless databases can include or remove data types, tables, and fields without major repercussions, like complex schema migrations and outages. Because it can withstand sudden changes and parse any data type, schemaless databases are popular in industries that are run on real-time data, like financial services, gaming, and social media.How much information do you know about your new database setup? Can you see its structure well ahead of time and know for certain it will never change? If so, you may be dealing with a situation that best suits a schema database. Its strictness is the basis of its appeal. Let’s get granular and weigh the pros and cons of going one way or the other."
668,https://redis.com/blog/redis-days-india-bangalore-sessions/,RedisDays India: Live sessions from Bangalore now available,"February 9, 2023",Alex Patino,"Couldn’t make it to RedisDays India in Bangalore last November? Catch up with these tech sessions by watching them online!RedisDays is always a welcome occasion for people who use Redis. It’s a chance to learn how other companies are using the software, to learn practical techniques, and to talk techie with colleagues.The RedisDays held in Bangalore was a day full of networking, with hundreds of guests in attendance. Primarily, it was an event packed with actionable insights from enthusiastic Redis customers based in India, such as AngelOne, Meesho, and RazorPay.Want to catch up on what you missed? We include six of the sessions below.Udi Gotlieb, Redis’s VP of Product Marketing, kicked off the RedisDays India sessions with a keynote that put the spotlight on India as a world leader in Redis adoption. “Over 10,000 students from India went through Redis University courses,” he said. “And this is the number one [region] in terms of attendance in the world. That represents a 40% growth in the number of students year over year.”In his keynote, Udi emphasized the considerable growth in Redis Cloud accounts coming out of India. “India is the global leader, with a growth of 10x between three years ago till today,” he said, “And that is what makes India our number one international market outside of the U.S.”Watch this first of two RedisDays India keynotes to get the full picture of how Redis is helping leaders shape digital experiences.Just how popular is Redis? In the second keynote of the day, Redis CTO Yiftach Shoolman discussed Redis’ adoption and popularity. He shared some notable stats, such as Redis’ 2.33 million daily launches on Docker.In this presentation, Yiftach illuminated Redis’s hyper-versatility and how it’s used for inventory management, cyber security, and making instant decisions on real-time data. Redis is a “real-time data platform,” he explained, with examples from its use as a caching system and a front-end database. “Use it for everything you want, and we will allow you to run it at speed, anywhere.”How are decision-makers in the financial sector using Redis, and what pain points does a real-time data platform need to resolve in 2023? When billion-dollar decisions have to be made in milliseconds, what are the non-negotiables every financial services team needs? Udi Gotlieb invited Jyotiswarup Raiturkar, CTO of stock brokerage firm AngelOne, to join him on stage at RedisDays India. They discussed evolutions in risk management, fraud detection, and cloud adoption.Watch this customer chat and see how AngelOne is using Redis to stay ahead.Meesho is an Indian resale retailer and, as its Chief Data Scientist Debdoot Mukherjee attested, the company is on a mission to make digital commerce a reality for the 700 million internet users in the country.In this customer chat, Debdoot showed Redis in action at Meesho. He described how Redis helps power the company’s sustained growth with the use of microservices and product aggregators. Debdoot also gave viewers a peek into the development of its artificial intelligence (AI) and machine learning (ML) stack.Pranjal Yadav, the current Staff Data Scientist at Razorpay, is an AI industry thought leader. Yadav has worked with well-known brands, including Amazon, American Express, and Expedia.In this session, Pranjal explained, “how [a] machine learning setup or a simple use of data can empower a thing as simple as a payment gateway.” He also described how Redis has helped Razorpay scale all its digital operations.Learn more about how machine learning is scaling digital payments with this final customer session.AngelOne CTO Jyotiswarup Raiturkar wrapped up RedisDays India with a session about what he called “the most important feature you need in a product,” namely, reliability. As he mentions early in the presentation, “It doesn’t matter how fast it works if it’s not reliable.”With that, Jyotiswarup dives into a discussion about AngelOne, and how the fintech company (and the largest listed retail broker in India)  deals with everything from mutual funds to stocks to gold peer-to-peer lending. To provide millions of Indian citizens with the most reliable information, AngelOne uses Redis not just as a cache but also as a primary database.On Redis’s dedicated YouTube channel, you can find more informational sessions from past RedisDays events in San Francisco, London, New York, and many other stops from years past. Look through the RedisDays playlist now to get started."
669,https://redis.com/blog/cve-2015-4335dsa-3279-redis-lua-sandbox-escape/,CVE-2015-4335/DSA-3279 – Redis Lua Sandbox Escape,"June 9, 2015",Itamar Haber,"What happened? Last week, in a post titled “Redis EVAL Lua Sandbox Escape”, security researcher Ben Murphy unveiled the details of a known Lua exploit that can be used for breaking out of Redis’ sandbox. Mr. Murphy was also kind enough to provide a patch that addresses said issue, which in turn was immediately included in new releases for Redis v2.8.21 and v3.0.2 from antirez, a.k.a. Salvatore Sanfilippo (as reported in last week’s Redis Watch newsletter).How serious is this? Any security vulnerability is serious business, but while a malicious person can indeed exploit vulnerability to wreck havoc, it does require a non-trivial amount of skill do so. For example, refer to this sample code that’s designed to attack Lua 5.1 on a Windows 32-bit platform: https://gist.github.com/corsix/6575486. Furthermore, the exploit itself is but one step in a process that would involve attacking additional components of the compromised system. The bottom line is that while you should definitely take steps to protect your Redis servers against this exploit, keep in mind that the risk isn’t considered high.How can you protect yourself? There are several steps you can take to protect your Redis from would-be-hackers’ pwnage. Here they are in order of importance:Upgrade your Redis. The new versions of open source Redis include the fix so you should be all good once you upgrade (if you’re running your own). RLEC users only need to install the newest software update in order apply the patch to their databases. We’ve also backported the patch to our cloud service, so new databases are immune to the vulnerability and we’re applying it online to existing instances. If you have any concerns or questions please contact our help desk at: support@redis.comHot patch the exploit using itself. Mr. Ben Murphy outdid himself today (June 15th, 2015) and had published a hot patch that uses the vulnerability he himself had exposed to patch Redis and be done with it. Ideation by antirez, full details at: http://benmmurphy.github.io/blog/2015/06/09/redis-hot-patch/Quick n’ dirty n’ awesome tip: here’s a little nugget from geocar – rename QUIT to POST to deal with that SSRF threat <- lovely!Redis Trivia: AFAIK, this is the first Redis vulnerability that was registered in the MITRE Common Vulnerabilities and Exposures Directory (CVE-2015-4335 which is no longer up for some reason, did it escape too?) and at the Debian Security Advisory (DSA-3279) so double yay! Also note how, by sheer conspicuous coincidence, the DSA’s ID for the vulnerability is given by the following formulae:I’m still working on cracking the logic behind the CVE ID thoughâ€¦ Any Suggestions? Questions? Feedback? Email or tweet at me – I’m highly available 🙂"
670,https://redis.com/blog/scaling-microservices/,Intelligently Scaling Microservices,"July 27, 2023",Redis,"Microservices offer a lot of technical advantages, but getting the balance requires some experience. You need to dynamically adjust resources based on where they are needed, just as you do when optimizing application performance. Here we delve into the decision-making process behind when and how to scale microservices effectively.Imagine a bustling city on a typical weekday. The streamlined transportation infrastructure accommodates the flow of commuters efficiently. Traffic lights are timed based on the number of the cars on the road, the buses and trains are scheduled to match commute times, and technology is employed to create alternate routes during sports matches and weather events. The result – in our idealized city – is that the traffic system is scaled appropriately to ensure smooth travel, minimize traffic congestion, and create a happy citizenry that has one less item to complain about.In other words: scaling matters. But it needs to be done thoughtfully.Microservices architecture brought about a radical departure from the monolithic status quo. Instead of treating applications as a single, monolithic entity, developers embraced a modular approach that makes it easier to achieve flexibility, scalability, and resilience.  Developers have the freedom to develop, test, and deploy services independently, responding rapidly to business needs and leading to faster time-to-market.The benefits? Adaptability in the face of changing requirements, less-fragile codebases (because you can update one service without affecting other services), and you can scale individual services independently based on demand.In monolithic architecture, scaling is rather an all-or-nothing affair. When the demand for a particular function increases, the entire monolithic application must be scaled, whether or not other components experience high traffic.Imagine a scenario where only a single feature within an application experiences a surge in usage, while the rest of the system remains relatively idle. In a monolithic setup, additional resources are allocated to handle the increased load, even though most components do not actually need the added capacity. The lack of scaling granularity leads to resource allocation inefficiencies.Going back to our traffic scenario, a city might have one highway that regularly gets backed up during rush hour.  The monolithic solution would be akin to adding additional lanes to both the overloaded highway (sensible) and to all other roads in the city (not so much). The end result is a reduction in traffic congestion, but the time, cost, and materials involved are overkill.Microservices architecture offers a more precise and targeted scaling approach. Each microservice operates as an independent entity with its own boundaries and responsibilities. This autonomy allows individual services to scale independently. When a particular microservice faces increased demand, only that service needs to be scaled up, leaving the rest of the system untouched.Each microservice can have its own dedicated database. By contrast, monoliths often rely on a single shared database that serves all system components. Sure, you could use a relational database for microservices. But using other types of databases, such as NoSQL databases or specialized databases, allows each microservice to have its own data storage that best suits its requirements. This is in line with the domain-driven design principle of microservices.Designing relational databases for scalability requires careful consideration. NoSQL databases were designed to scale. However, balancing data consistency, transactional integrity, and performance is crucial when choosing database strategies.Simply scaling every microservice leads to inefficient resource allocation and unnecessary costs – rather like paying to add more roads in a city when those roads aren’t necessary. You need to understand your application’s behavior and to identify the services that require scaling.Here are some key considerations to determine if scaling is needed:Once you decide that your microservices need to scale, there are several approaches to consider, including horizontal scaling, vertical scaling, scaling data stores and databases. You can also look at leveraging caching and content delivery networks (CDNs) to enhance performance.Horizontal scaling involves adding more instances of a service to distribute the load and to increase capacity. Load balancing and service discovery mechanisms play a crucial role in distributing traffic across multiple instances. Container orchestration platforms like Kubernetes provide convenient ways to manage and scale microservices horizontally.Think of a city growing its transportation infrastructure by adding more roads and highway lanes to accommodate growing traffic.When to use: This approach is beneficial when a service experiences increased traffic or high demand.Vertical scaling focuses on increasing the resource capacity of a single instance. This may involve upgrading hardware or adjusting resource allocation such as CPU and memory. Vertical scaling is suitable for services with specific resource requirements or limited options for horizontal scaling.Vertical scaling, in our transportation scenario, is like upgrading the public transport system. The city turns its low capacity buses into much larger buses that can carry more people, that use less energy, or that run more often.When to use: This approach is suitable when a service has specific resource requirements or limited options for horizontal scaling.Implementing caching mechanisms helps improve performance and scalability. Caching frequently accessed data or computed results reduces the need for repeated computations or database queries. Distributed caching and CDNs can further enhance performance by bringing data closer to the end-users, reducing latency and network traffic.Compare caching and CDNs to implementing an express lane on the highway to improve traffic flow and efficiency.When to use: These approaches are particularly useful when dealing with querying data (reads) from traditional databases that can’t be scaled easily and static content or large media files, resulting in faster delivery and improved user experience.Once you’re contemplating the importance of scaling microservices intelligently, consider whether Kubernetes, a powerful container orchestration platform, can help you achieve this goal. Kubernetes provides robust capabilities that address many of the challenges inherent in deploying and scaling containerized microservices and underlying data layer.Kubernetes’s horizontal pod autoscaling feature automatically adjusts the number of instances based on resource utilization and predefined rules. By defining resource limits and requests, Kubernetes optimizes resource allocation and ensures efficient scaling. You can implement custom scaling rules and metrics to adapt to specific application requirements.Kubernetes simplifies service discovery and load balancing for microservices. Services can be registered and discovered dynamically using Kubernetes service discovery mechanisms. Load balancing strategies, such as round-robin and session affinity, ensure even distribution of traffic across instances. Ingress controllers provide external traffic management capabilities.Scaling microservices requires the right tools and technologies, and Redis Enterprise proves to be an invaluable asset in achieving high performance and scalability. By harnessing the power of distributed caching, Pub/Sub messaging, advanced data structures, and Redis Enterprise Operator for Kubernetes, Redis Enterprise enhances microservices architectures, enabling seamless scaling and efficient event-driven communication.Redis Enterprise empowers organizations to build resilient, scalable, and high-performing microservices applications. Embrace Redis Enterprise as a key component of your microservices infrastructure and unlock the full potential of your scalable and distributed applications.Accelerate your microservices now! Read the Cache and Message Broker for Microservices solution brief  to harness the power of Redis Enterprise for fast, simplified, and resilient microservices applications."
671,https://redis.com/blog/velocity-based-data-architectures/,An Introduction to Velocity-Based Data Architectures,"August 7, 2023",Amine El Kouhen,"There are several ways to classify and understand data architectures, each with its own pros and cons. They can help you make an informed decision about the best design for your needs. Here, I explain velocity-based data architectures, and where they fit in the grand scheme of things.The two most popular velocity-based architectures are Lambda and Kappa. Data architectures are also classified based on their operational mode or topology, including data fabric, data hub, and data mesh–but I leave that explanation to a later blog post.Data architecture is an element in enterprise architecture, inheriting its main properties: processes, strategy, change management, and evaluating trade-offs. According to the Open Group Architecture Framework, data architecture is “a description of the structure and interaction of the enterprise’s major types and sources of data, logical data assets, physical data assets, and data management resources.”Per the Data Management Body of Knowledge, data architecture is the process of “identifying the data needs of the enterprise (regardless of structure) and designing and maintaining the master blueprints to meet those needs.” It uses master blueprints to guide data integration, control data assets, and align data investments with business strategy.Not every process is a good one. Bad data architecture is tightly coupled, rigid, and overly centralized. It uses the wrong tools for the job, which hampers development and change management.Data velocity refers to how quickly data is generated, how quickly that data moves, and how soon it can be processed into usable insights.Depending on the velocity of data they process, data architectures often are classified into two categories: Lambda and Kappa.Lambda data architectures were developed in 2011 by Nathan Marz, the creator of Apache Storm, to solve the challenges of large-scale real-time data processing.The term Lambda, derived from lambda calculus (λ), describes a function that runs in distributed computing on multiple nodes in parallel. Lambda data architecture provides a scalable, fault-tolerant, and flexible system for processing large amounts of data. It allows access to batch-processing and stream-processing methods in a hybrid way.The Lambda architecture is ideal when you have a variety of workloads and velocities. Because it can handle large volumes of data and provide low-latency query results, it is suitable for real-time analytics applications such as dashboards and reporting. The architecture is useful for batch processing (cleansing, transforming, data aggregation), for stream processing tasks ( event handling, developing machine learning models, anomaly detection,  fraud prevention), and for building centralized repositories (known as “data lakes”) to store structured and unstructured information.Lambda architecture’s critical distinction is that it uses two separate processing systems to handle different types of data processing workloads. The first is a batch processing system that stores the results in a centralized data store (such as a data warehouse or a data lake). The second system in a Lambda architecture is a stream processing system, which processes data in real-time as it arrives and stores the results in a distributed data store.Lambda architecture solves the problem of computing arbitrary functions, whereby the system has to evaluate the data processing function for any given input (whether in slow motion or in real-time). Furthermore, it provides fault tolerance by ensuring that the results from either system can be used as input into the other if one fails or becomes unavailable. The efficiency of this architecture becomes evident in high throughput, low latency, and near-real-time applications.Lambda architecture consists of an ingestion layer, a batch layer, a speed layer (or stream layer), and a serving layer.Lambda architectures offer many advantages, such as scalability, fault tolerance, and the flexibility to handle a wide range of data processing workloads (batches and streams). But it also has drawbacks:In 2014, when he was still working at LinkedIn, Jay Kreps pointed out some Lambda architecture drawbacks. The discussion led the Big Data community to an alternative that uses fewer code resources.The principal idea behind Kappa (named after the Greek letter ϰ, used in mathematics to represent a loop or cycle) is that a single technology stack can be used for both real-time and batch data processing. The name reflects the architecture’s emphasis on continuous data processing or reprocessing in contrast to a batch-based approach.At its core, Kappa relies on streaming architecture. Incoming data is first stored in an event streaming log. It then is processed continuously by a stream processing engine ( Kafka, for instance) either in real-time or ingested into another analytics database or business application. Doing so uses various communication paradigms such as real-time, near real-time, batch, micro-batch, and request response.Kappa architecture is designed to provide a scalable, fault-tolerant, and flexible system for processing large amounts of data in real-time. The Kappa architecture is considered a simpler alternative to the Lambda architecture; it uses a single technology stack to handle both real-time and historical workloads, and it treats everything as streams. The primary motivation for the Kappa architecture was to avoid maintaining two separate code bases (pipelines) for the batch and speed layers. This allows it to provide a more streamlined and simplified data processing pipeline while still providing fast and reliable access to query results.Data reprocessing is a key requirement of Kappa, making visible the effects of any changes in the source side on the outcomes. Consequently, the Kappa architecture is composed of only two layers: the stream layer and the serving one.In Kappa architecture, there is only one processing layer: the stream processing layer. This layer is responsible for collecting, processing, and storing live-streaming data. This approach eliminates the need for batch-processing systems. Instead, it uses an advanced stream processing engine (such as Apache Flink, Apache Storm, Apache Kafka, or Apache Kinesis) to handle high volumes of data streams and to provide fast, reliable access to query results.The stream processing layer has two components:For almost every use case, real-time data beats slow data. Nevertheless, Kappa architecture should not be taken as a substitute for Lambda architecture. On the contrary, you should consider Kappa architecture in circumstances where the batch layer’s active performance is not necessary to meet the standard quality of service.Kappa architectures promise scalability, fault tolerance, and streamlined management. However, it also has disadvantages. For instance, Kappa architecture is theoretically simpler than Lambda but it can still be technically complex for businesses unfamiliar with stream processing frameworks.The major drawback of Kappa, from my point of view, is the cost of infrastructure while scaling the event streaming platform. Storing high volumes of data in an event streaming platform can be costly and raise other scalability issues, especially when the data volume is measured in terabytes or petabytes.Moreover, the lag between event time and processing time inevitably generates late-arriving data as a side effect. Kappa architecture will need, thus, a set of mechanisms, such as watermarking, state management, reprocessing, or backfilling, to overcome this issue.Lambda and Kappa were attempts to overcome the shortcomings of the Hadoop ecosystem in the 2010s by trying to integrate complex tools that were not inherently compatible. Both approaches struggle to resolve the fundamental challenge of reconciling batch and streaming data. Yet, Lambda and Kappa have provided inspiration and a foundation for further advancements.Unifying multiple code paths is a significant challenge in managing batch and stream processing. Even with the Kappa architecture’s unified queuing and storage layer, developers need to use different tools to collect real-time statistics and run batch aggregation jobs. Today, they are working to address this challenge. For instance, Google has made significant progress by developing the dataflow model and its implementation, the Apache Beam framework.The fundamental premise of the dataflow model is to treat all data as events and perform aggregations over different types of windows. Real-time event streams are unbounded data, while data batches are bounded event streams that have natural windows.Data engineers can choose from different windows, such as sliding or tumbling, for real-time aggregation. The dataflow model enables real-time and batch processing to occur within the same system, using almost identical code.The idea of “batch as a special case of streaming” has become increasingly widespread, with frameworks like Flink and Spark adopting similar approaches.There’s another twist on the data architecture discussion in regard to velocity models: the suitable design choices for working with the Internet of Things (IoT). But I’ll leave that for a separate discussion.It’s clear that the debate over how best to structure our approach to handling data is far from over. We’re just getting started! Consult our white paper, Best Practices for a Modern Data Layer in Financial Services, on the best ways to modernize a rigid and slow IT legacy system and turn it into a modern data architecture."
672,https://redis.com/blog/choose-microservice-monitoring-tool/,How to Choose a Microservices Monitoring Tool,"August 8, 2023",Esther Schindler,"Microservices allow developers to break down their applications into smaller, loosely coupled services that are developed, deployed, and scaled independently. But you need a monitoring tool to track whether the software works correctly–and that means you need useful criteria for choosing such a tool.You’re used to tracking application performance to confirm that it functions correctly, but microservices adds a new twist. Monitoring is a critical aspect of managing any microservices architecture. By definition, there are a lot of independent parts.But how do you choose the best microservice monitoring tool for your business? We’re not here to play favorites or to endorse any tool in particular. What we do have, however, is a whole lot of experience in this field, which we are happy to share.Before you choose a tool, contemplate your motivations for acquiring one. Your team should discuss, “What problems are we trying to solve or prevent?” That leads to, “What data do we need to examine to determine if we are solving or preventing those problems?” The answers help you identify what to monitor–and what you can ignore (or pay less attention to).Be intentional about what you monitor. Have a reason. Don’t adopt the attitude, “Monitor all the things just in case it might be useful.” Most teams have limited resources, which means that it isn’t possible to monitor all the things anyway; at best, you end up with alert fatigue.That said: Your expectations may not match reality. It’s hard to tell ahead of time what’s going to be useful when an unexpected thing breaks. It is not clear what needs to be monitored until everything is on fire and you try to figure out what’s going on. You need a mix of “think carefully” and “adjust given experience.”Any type of application monitoring tool has a host of features. You may not need all of them. It’s a good idea to start with the top criteria, as identified by our Redis experts and by experienced practitioners (the people who have the scars).It should scale. As your microservices architecture grows, so do your monitoring needs. The last thing you want is a tool that can’t keep up with the load. Make sure your monitoring system can go down without bringing down your microservice!It needs to collect the right data and analyze it. Look carefully at the data the tool collects and how it presents that information.A robust monitoring tool collects and analyzes data from every nook and cranny of a distributed system–but it shouldn’t overwhelm you with noisy, irrelevant information. It should provide you with comprehensive insights that deserve to be called “insights,” including performance metrics, logs, and traces.For microservices architectures, prioritize distributed tracing. Debugging issues that span multiple microservices can be a nightmare. Distributed tracing helps you track the flow of requests across services, which assists in identifying performance bottlenecks and understanding complex interactions. For example, ensure every log message/record/line has an attributable traceid attached to it, and use a system that lets you aggregate views.It should integrate with other tools you use without fussy setups or custom code. Perhaps more than any other application, a monitoring tool should play well with others.Similarly, look at the process of migration to the new monitoring tool from your existing provider, including data structuring requirements. Research what it would take to switch to another tool if this one doesn’t work out.  Learn what the API is like, because you’re bound to need it at some point. Consider future standards support, such as OpenTelemetry.Please, let it be easy to learn and easy to use (which are not the same things). Who wants to struggle to learn yet another tool? Navigating through distributed systems is complicated enough; your monitoring tool should simplify things, not add to the system’s complexity. Configuration should not be a pain. Peer closely at its dashboards and visualizations to decide if they are as intuitive as the vendor promises.It should set sensible alerts and notifications. When a storm is brewing, you need to know immediately! Your monitoring tool should offer robust alerting and notification features, so you can take action before minor issues turn into big problems.It has to fit your budget. While you want the best tool for your distributed system, you don’t want uncomfortable conversations with the CFO. That’s true for any IT expenditure, but especially so here, because the cost and pricing models vary widely. Unexpected usage has been known to create incidents of, shall we say, accidental overspending. Pay-per-user models sometimes create awkward decisions about who gets access.A microservice monitoring tool should offer visibility across the entire microservices ecosystem, including performance metrics, resource utilization, service mesh data, custom metrics, and error rates.The ideal tool should excel at collecting, storing, and analyzing data from distributed systems, providing actionable insights into the health and performance of each microservice. It should seamlessly integrate with other tools and systems, such as logging systems, alerting tools, and incident management platforms.Practically speaking, if you use Redis – for microservices or other uses – this is a good starter set for what to monitor. Your dashboard might include these items, configured to alert you when any metric spikes significantly.You have a lot of choices. Each of these microservice monitoring tools offers unique features. Perhaps these links can give you a head start on your shopping list – though there are many more options than we include here.Which is right for you? That’s your call.As with any other software choice, ultimately the only thing that matters is that it works for you.There’s no definitive right or wrong choice. The key question is, “Is this the right tool for my specific project?” The crucial factor is finding a tool that aligns with your project’s current and future needs and making an informed choice. Ideally, the tool you choose helps you maintain a healthy and efficient microservices environment, ultimately delivering reliable, high-performance applications. Redis works with all of them.And, we believe, you can take your microservices applications to the next level with Redis Enterprise. Read the Cache and Message Broker for Microservices solution brief to learn how to use caching with Redis Enterprise, explore top caching patterns, and use Redis Streams as a lightweight message broker for inter-services communication."
673,https://redis.com/blog/jedis-vs-lettuce-an-exploration/,Jedis vs. Lettuce: An Exploration,"September 12, 2022",Redis,"I’m an explorer by heart, so when I have to make a technical decision—like, say, choosing a Redis client—I go a-spelunking. Herein is the account of my exploration of the rhyming duo of Java clients: Jedis versus Lettuce.Let’s start with the basics, and examine each client, to understand the differences between the two.Lettuce is a Redis Java client that is fully non-blocking. It supports both synchronous and asynchronous communication. Its complex abstractions allow you to scale products easily.Consider Lettuce as a more advanced client that supports Cluster, Sentinel, Pipelining, and codecs.Jedis is a client library inside Redis that’s designed for performance and ease of use. Jedis is a lightweight offering compared to other Redis Java clients; it offers fewer features but can still handle large amounts of memory.Due to its simpler functionality, Jedis is easier to use, but it only works with clusters synchronously. If you choose Jedis, you may find it less challenging to focus on the application and data rather than the data storage mechanism.The gnomic goal of profit, like underpants, is always there. But the part that you can benefit from is the selection criteria. It will allow us to decide when Jedis is the right call and when Lettuce is the way to go. This is super important because we all know the answer to any question when selecting tools is, “it depends.”Jedis is a client library inside Redis that’s designed for performance and ease of use. Jedis is a lightweight offering compared to other Redis Java clients; it offers fewer features but can still handle large amounts of memory.Due to its simpler functionality, Jedis is easier to use, but it only works with clusters synchronously. If you choose Jedis, you may find it less challenging to focus on the application and data rather than the data storage mechanism.My comparison plan was simple:The gnomic goal of profit, like underpants, is always there. But the part that you can benefit from is the selection criteria. It allows us to decide when the Java client, Jedis, is the right call and when Lettuce software is the way to go. This is super important because we all know the answer to any tool selection question is, “It depends.”To get to the bottom of the Jedis versus Lettuce debate, let’s compare code for the simplest of all exercises: setting and getting a value from a single instance of Redis.First, we do this with Jedis:Looking at the code, this is pretty simple. Create a connection. Use it. Close it.Next, we do it with Lettuce:This looks a bit more involved. There’s a Java client, a connection, and a command object. And their names and templated nature suggest that there might be multiple varieties of them. Maybe in addition to a  StatefulRedisConnection<String, String> type, we have a stateless variety that takes a byte[]]? (Spoiler: there are multiple connection types for clustering and master/replica configurations, but not stateless ones.)Once you’re beyond the setup and teardown, however, it’s the same basic code in either client: Create a connection. Use it. Close it.Right now, for something as simple as this, Jedis looks easier. That makes sense since it has less code. But I am certain that the Lettuce software has all this stuff for a reason—probably to handle more advanced scenarios.Jedis can handle multi-threaded applications just fine, but a Jedis connection is not thread-safe. So don’t share them. If you share a Jedis connection across threads, Redis blurts out all sorts of protocol errors, like:expected '$' but got ' 'To solve these sorts of problems, use JedisPool—a thread-safe object that dispenses thread-unsafe Jedis objects. Using it is simple, like the rest of Jedis. Just ask for a thread and return it to the pool via .close() when you’re done. Here it is in action:Each of these Jedis objects encapsulates a single connection to Redis, so (depending on how large your pool is) there may be blocking or idle connections. Furthermore, these connections are synchronous, so there’s always a degree of idleness.I feel like I should talk about clustering, but there’s not much to say—at least in terms of comparisons. There’s plenty of capability to discuss, but both libraries support clustering. Unsurprisingly, Jedis is easier to use, but it works with clusters only synchronously. The Lettuce software is more difficult to use but is capable of synchronous, asynchronous, and reactive interaction with the cluster.This is the recurring theme. And that shouldn’t be surprising. By its own admission, “Jedis was conceived to be EASY to use.” And Lettuce states, “Lettuce is a scalable Redis client for building non-blocking Reactive applications” right on its homepage.Of course, if you’re using Redis Enterprise, you don’t have to worry about clustering as it’s handled server-side. Just use the non-clustered APIs of Jedis or Lettuce, manage your keys so they’re slotted to the correct shards, and you’re good to go."
674,https://redis.com/blog/introducing-triggers-and-functions/,Triggers and Functions: Bring Code Closer to Your Data,"August 15, 2023",Thomas Caudron and Pieter Cailliau,"Developers can use Redis to build and maintain real-time applications. You can create JavaScript functions that automatically execute code on data changes directly in the Redis database, and thus ensure a lower latency.In general, applications handle business logic operation, sending code for execution to the database. This is a slow process, because code flows from a client into the server each time a function is executed. The developer is responsible for maintaining code consistency across all applications that access the same database, whether the code is simple requests or complex data operations, and many times code is repeated across applications.Following the Redis manifesto guideline, We are against complexity, we had to take action and to find a solution to those challenges.Four years ago, we introduced RedisGears, our first programmability model within the platform.  Developers wrote and executed scripts where data lived. However, the scripts were ephemeral, provided by each client, and that could lead to inconsistencies.Following that direction, in Redis 7.0 we introduced the initial implementation of the scripting method with functions. Functions boosted usability and durability because they are part of the database, inheriting the level of replication and persistence of the data.And now, we are proud to present the next step in programmability. With Redis 7.2, we introduce triggers and functions. These enhance Redis’ programmability, expand server-side capabilities, improve how and when functions are executed in the database, and facilitate the execution of complex business logic directly where the data lives.Triggers and functions is a new generation of programmability available through Redis Stack. It allows developers to program, store, and automatically execute JavaScript code on data changes directly in a Redis database.This capability lets developers define events (called triggers) to execute functions closer to the data. That is, developers define business logic that executes in response to database events or commands. That speeds up the code and related interactions, because there is no  wait to bring code from clients into the database.It also speeds up reaction time to other events in Redis, such as keyspace notifications that are not handled in real time with other means such as Publish and Subscribe (Pub/Sub) events.Triggers and functions handle the distribution within a clustered database, installing the libraries on each shard and executing the functions based on where the key lives.We are also introducing remote functions. Remote functions allow you to perform read actions that can access data from any slot, even in a clustered database, so all your data is accessible from each function.Redis uses Lua for scripting and functions. There are many benefits to Lua, such as code reusability, but it is not a commonly used language among professional developers. According to the 2022 StackOverflow developer survey, only 3.2% of developers use Lua in a professional capacity.In contrast, two thirds of developers use JavaScript. Using a well-known language lowers the adoption barrier for new Redis developers. It’s one less thing to learn.Another benefit of triggers and functions is that it reduces the complexity of managing business logic across multiple applications.When multiple applications access the same database, developers have to coordinate how applications process data in a consistent manner. It is common to duplicate code in each application to validate data, to enrich search results, or to update the database when another application makes changes.With triggers and functions, there is no longer a need to duplicate code on multiple applications. Code always executes in the same way, on-demand or initiated by an event in the database.Until now, reacting to database events within Redis required developers to rely on the Pub/Sub mechanism. While Pub/Sub has a lot of advantages, it isn’t always the right choice. In particular, Pub/Sub is not real-time. A client has to actively listen to the events; if the client is not listening, the events are lost.Now developers can register keyspace triggers that execute based on a key prefix and event type. The trigger can be executed in an atomic way so that no other Redis event is processed between the event and the business logic.It’s always easier to understand things with a practical example. Here, we introduce registering a function and a trigger. A function is executed when it is called via the TFCALL command; a trigger is executed based on the events in Redis.The prologue defines that we use the js engine, the library name is lib, and the minimum required Triggers and Functions API is version 1.0.Next, we create a function that returns the result of a Redis command. The client gives access to execute Redis commands within our function. The data event contains the keys and arguments that can be provided when running the function.The Redis global variable allows you to register triggers and functions, and to log to the log file. We register the function with a name with which we call it when it executes.The full JavaScript file looks like this.Save this as lib.js.Then we register our function in triggers and functions using the TFUNCTION LOAD command. The TFUNCTION LOAD command also distributes the library in a clustered database.Now we can execute the function using the TFCALL command. The command gets the library name and the function name separated by a period.By doing so, you successfully created, registered, and triggered a function in the Redis database.We can extend this example with a keyspace trigger. We add a new registration that reacts to keys that are prefixed with 'fellowship:'. Add this code at the end of the lib.js file.Use the TFUNCTION LOAD command with the REPLACE argument to update an existing library. The TFUNCTION LOAD REPLACE command immediately updates all the clients using the Redis database, and they start using the new business logic.To test the new keyspace trigger, create a new key starting with fellowship: and check the fields using RedisInsight. The keyspace trigger is executed with the command, so the last_updated field is already added when the key is created.Join the public preview of triggers and functions with Redis Stack 7.2. Get started with Redis Stack in the cloud by creating a database on Redis Enterprise Cloud in the fixed tier within the Google Cloud/Asia Pacific (Tokyo) or AWS/Asia Pacific (Singapore) region, or deploy a self-managed instance from our download center.The General Availability for triggers and functions is planned for Redis 8. This will include feedback received from the preview users, as well as additional features, such as timed triggers and more debugging options.Comments and feedback are welcome on the Redis mailing list."
675,https://redis.com/blog/implementing-designing-microservices/,The Principles of Designing Microservices,"May 1, 2023",Redis,"So, you’ve evaluated your application’s state of affairs and have concluded that the adoption of microservices will improve overall performance and scalability. Great. What comes next? In this article, we outline the baseline considerations for microservices design and implementation.Microservices is a software architecture strategy that breaks down applications into a collection of decoupled, autonomous services. These independent application services communicate with one another through APIs. Each service is managed by its own team of domain experts so that every software development team can control its own development cycles, test and deploy on its own schedule, use its own enterprise tools and resources, and accelerate time to market.Our Microservice Architecture Key Concepts drill into the foundations of this concept and offers some practical advice for getting started, including evaluating the wisdom of microservice architecture adoption for your shop. Now it’s time to get into the weeds.The first step in designing a microservices architecture is surveying the lay of the land, so to speak. One of Developer.com’s top ten microservices design principles is the single responsibility principle, which dictates that each service needs to be responsible for and infuse all of its resources into the successful development of one function and one function only within the microservices based application.Software architects should consider conducting a domain analysis to map out how to compartmentalize each service and what elements need to be factored into the application stack. This domain analysis is known as domain-driven design (DDD). It applies patterns, such as the entity pattern and the aggregate pattern, to a single bounded context in order to identify a single domain’s boundaries with more calculated precision.As author and Agile Manifesto signatory Martin Fowler explains, DDD is “an approach to software development that centers the development on programming a domain model that has a rich understanding of the processes and rules of a domain.” In other words, you should build each microservice around a specific business function.Once you identify the domains and outline their boundaries, it’s time to define the variables that best suit the application stack.Creating a microservices tech stack is a bit ad hoc. You often have to use a host of tools, frameworks, and programming languages to implement it all into a cohesive yet loosely coupled system.Factor in these variables as you choose your tools:Narrowing down the best programming language to use for microservices comes down to your familiarity with the language, the libraries available for the features you need, and the suite of features each language provides. Obviously, it saves time and energy to choose languages that are already in your development team’s repertoire.According to a 2021 JetBrains survey on microservices, “the three most popular languages for microservices development are Java (41%), JavaScript (37%), and Python (25%).” Each of these popular programming languages has substantial developer online support, many examples of successful application development, running environments, such as Node.JS, and vast client libraries.Make sure the language is suited for the business problem at hand; for instance, Python is popular in data analytics, while JavaScript is a solid option for full-stack development.Understand more about intuitive object mapping and high-level client libraries in Introducing the Redis OM Client Libraries.When you choose a suitable database to use with the applications you build for a microservices architecture, keep scalability, availability, and security at the top of your mind. Choose a database that best supports the data model which you plan to use in your microservice. Your tech stack should scale to handle any application load, ensure availability with failover protocols, and secure the application from any malicious attacks.For more information on landing a high-performance database for microservice based applications, read Microservices and the Data Layer.Your business function may require your microservices to use synchronous interservice communication methods for certain operations and asynchronous communication for others. Several communication formats and protocols can be used to assist microservices communication, including HTTP/REST, gRPC, and AMQP.For asynchronous communications, an event-driven message broker leveraging Streams with consumer groups can help fortify scalability and reliability so applications can grow and no service is ever out of reach.For more on choosing the right communication tools and patterns, see What to Choose for Your Synchronous and Asynchronous Communication Needs.Each microservice team is responsible for monitoring application performance, which usually employs logging and observability tools to keep a pulse on operations. That lets developers and operations staff track the entire system, from application performance to message broker streams to database resource utilization.When using a message broker, consider using a logging stream where each microservice can publish messages. This way, you can connect your preferred logging and observability tools to the stream and monitor your application asynchronously without slowing things down.Learn how the right monitoring resources can combat system complexity in our 5 Microservices Misconceptions blog post.Want your microservice architecture to truly thrive? Here are five microservice application design principles to reference as you design an architecture built to ensure top performance.Loose coupling and strong cohesion can be explained both by the single responsibility principle mentioned earlier; giving each domain team one single responsibility helps fortify the cohesion within that one domain, making, ironically enough, a monolith of all the functions within that service to the point where they’re essentially inextricably linked. Each service has its own domain experts and tools, but can still communicate with each other via APIs and other protocols. It’s much like the way coworkers from different departments interact: You share information with one another when it helps get the work done, without being too chatty about irrelevant-to-others details.Business applications are rarely stagnant.  Software changes as new business needs arise, industry assumptions change, and technology capabilities offer more functionality. Microservices should be evolvable and adapt to new requirements when needed.Responding to change is one of the foundations of the Agile Manifesto and for good reason. The world changes. People change. So should your software.One reason to implement microservices is their ability to automate processes that improve overall scalability. With container orchestration systems like Kubernetes, you can deploy a microservice’s entire database from a single image alongside the microservice. With the assistance of a Kubernetes controller, these portability benefits can help DevOps teams to manage, schedule, and orchestrate an automatic container deployment.Implementing microservices requires that the services in any given application maintain their own decentralized data. Service boundaries should isolate all the logic and data pertaining to any single service from other services within an application.This is the same logic that permits containerized microservices to have independent deployments. According to Red Hat, this principle has its own set of naysayers who believe that the principle leads to a proliferation of data redundancy. But one of the greatest upsides to establishing these discrete boundaries is that “When a microservice carries its own data, any strange behavior is confined within the microservice.” Who needs the guesswork?Disruptions occur. Application services go down without warning. Fiber-optic-seeking backhoes take down network operations. People forget to renew domains. Systems are interrupted by data connection issues resulting from a firewall failure.Plan for all the ways that things can go pear-shaped. Do your best to account for potential failures at the implementation level. Design for resiliency, such as with the Circuit Breaker Pattern, to keep services from dropping when a microservice isn’t capable of performing a given operation.Infrastructure automation may be a substantial plus in favor of microservices, but operational disruptions are still a very real possibility. In Organizing the Chaos of Data, Redis’ Allen Terleto is joined by Mike Leone of Enterprise Strategy Group (ESG) and Jim Roomer of Google Cloud to pull back the curtain on database disasters. They reveal the path toward efficient data handling, featuring Redis on Google Cloud customer stories.Watch the webinar."
676,https://redis.com/blog/redis-enterprise-7-2-brings-exciting-innovations-that-operators-will-love/,Redis Enterprise 7.2 Brings Exciting Innovations That Operators Will Love,"August 17, 2023",Adi Shtatfeld,"Among the highlights for operators are better access control, troubleshooting features, and maintenance capabilities.The Redis 7.2 release has a lot of new features that matter to developers, and we go into many of the details elsewhere. But the new version has many new capabilities that make life easier for operators and system administrators–from better access control to troubleshooting in multi-tenant environments–and we expect these features to make you smile.To begin with, the new Redis Enterprise Cluster Manager makes your life easier.Imagine this scenario: You are on-call. You receive a phone call notifying you of a problem with an application or database. Your priority is to identify the source of the issue. That’s rarely fun.When browsing to the Cluster Manager user interface (UI), the default page shows the database list. You see all your databases. Databases whose status is different from “active” are listed at the top, followed by databases with alerts, sorted by the number of alerts.To help you further investigate the problem, the new Redis Enterprise Cluster Manager has an expanded view for each row in the database view. That gives you quick access to crucial information in a user-friendly manner.The main databases’ view when one of the databases is an expanded view looks like this:The expanded view provides key matrices, updated status and alerts, along with configuration. The expanded view lets you retain the context without disruptions. And you can compare metrics and data across multiple databases.The new Redis Enterprise Cluster Manager makes it easier to perform maintenance operations.The new release prominently displays shards consumption (out of the total entitled by the license) on the main cluster configuration screen. This helps you stay on top of your license usage. When your license nears expiration, the new UI makes it easier to replace it.We added an indication for the primary node, so each node is displayed along with its role in the cluster. That makes it easier to plan maintenance or cluster upgrades.New in version 7.2, the Redis Enterprise Cluster Manager helps you manage certificates by displaying essential information (such as expiration dates) and providing a convenient way to upload new certificates directly from the UI.When you create a database in the new Redis Enterprise Cluster Manager UI, you now can easily access and highlight the option to add additional capabilities, also known as modules.Redis Enterprise 7.2 adds more functionality. To simplify maintenance, the Cluster Manager UI specifies the minimum Redis database version required for each module. The modules management screen provides visibility to the databases that use a given functionality so you can highlight any version dependencies.Redis Enterprise 7.2 offers a host of exciting new features and improvements, including auto tiering, support for triggers and functions, expanded JavaScript and client support, and a lot of other enhancements. You can experience all the benefits by downloading the 7.2 release and starting a free 30-day trial today."
677,https://redis.com/blog/introducing-auto-tiering/,Auto Tiering Offers Twice the Throughput at Half the Latency for Large Datasets,"August 15, 2023",Alon Magrafta,"More and more applications rely on huge data collections – and those applications have to respond quickly. Redis Enterprise 7.2 makes it possible to create super-fast applications with no extra work on a developer’s part. What’s not to like?Organizations have always depended on their collected data, but those datasets are growing – particularly in analysis-heavy markets such as e-commerce, finance, location-based computing, and high-end gaming. In medical image analysis research, for example, the median dataset size grew by three to ten times between 2011 and 2018.One reason for Redis’s popularity is that it provides incredibly fast data access. It does so by storing data in memory, so applications can retrieve and manipulate data at the fastest speeds. The more data an application needs to process, the more memory it requires to store the dataset. Still, those applications have to respond at near-instant speeds, even when the data stores from which they draw are huge.When the amount of data an application accesses is measured in terabytes, developers have to cope with the limitations of in-memory processing. As a result, they turn to disk-based solutions to support Redis behind the scenes. Doing so forces the developers to build an entire data management system in their applications, which means they spend their time on extraneous tasks instead of their original goal of providing a performant application.There has to be a better option. And there is.Using Redis Enterprise’s auto tiering, developers can extend large volume databases beyond the limits of the existing DRAM in the cluster by using solid state disks (SSD) as part of available memory. Taking advantage of some clever programming on our part, Redis Enterprise identifies what data should be in-memory and what data should stay on SSDs at any given moment, doubling the throughput and cutting latencies in half what it did with previous solutions.Everything happens automatically. The developer doesn’t need to write extra code or learn another new technology. By combining dynamic RAM with fast external storage, Redis Enterprise makes it easy to use system resources efficiently while still providing fast access to frequently accessed data.Auto tiering automatically manages data. It promotes data that becomes hot into DRAM and intelligently demotes unused data to SSDs. This opens new possibilities for applications that rely on large data collections.Fast data on large datasets is not the only benefit. Saving money is another advantage – and a reason that the finance department understands. In-memory storage can be expensive. By offloading less frequently accessed data to SSD, developers can optimize memory usage and reduce the costs associated with high-capacity memory requirements.Practically speaking, that makes data-heavy applications run faster without extra effort on the developer’s part. It also saves up to 70% in infrastructure costs, compared to deployments only using DRAM. And because auto tiering efficiently and automatically manages data access patterns, you don’t have to spend cycles (computing or human-brain-wise) identifying hot data versus warm data.To boost this feature, Redis forged a strategic partnership with Speedb, an innovative key-value storage engine. We integrate its technology as the default auto tiering engine.With the integration of Speedb, Redis Enterprise achieves a remarkable enhancement in performance, doubling the throughput and cutting latencies in half while using the same resources. This significantly widens the range of use cases that can leverage auto tiering’s benefits. Following this improvement, Redis Enterprise sizing for databases using Auto Tiering got increased to 10k ops/sec per core.Sure, we doubled the throughput, and we cut latencies in half, but numbers tell only part of the story. Examples matter.The following graphic shows a sample of the performance evolution of auto tiering in real workload scenarios. The blue bars represent Redis Enterprise 6.4 with the previous storage engine (RocksDB), and the red bars represent Redis Enterprise 7.2 with Speedb. For infrastructure, we used I4i.8xlarge AWS instances to host a 1TB database on 10 shards, replicated for high availability for a total of 20 shards, serving 1,024 clients.To simulate the most standard Redis’ use case, we defined two different payloads, 1KiB and 10KiB, over a configuration with 20% DRAM and 80% SSD with three possible use patterns, balanced read/write (1:1), heavy read (1:4), and heavy write (4:1). In both scenarios we measured the throughput in operations per second, and the corresponding latency. The following charts show the results.Compared to RS 6.4 (RocksDB), RS 7.2 (Speedb) improves:Compared to RS 6.4 (RocksDB), RS 7.2 (Speedb) improves:In all cases, Redis Enterprise 7.2 with Speedb has a better throughput, which means faster applications and less infrastructure needed to sustain this level of performance.Auto tiering is particularly applicable in scenarios that involve segregating data into hot data and warm data. One example would be banking applications that need to access up-to-the-minute data and historical data.Let us take a closer look at a mobile banking application example.Nowadays, everyone has a banking application on their mobile device. Users log into the application, get their balance, check the last transaction, and obtain other relatively small and focused information. Everyone expects this process to be smooth, simple, and instantaneous. That data is our hot data, which resides on DRAM in the Redis Enterprise database.Less frequently, users want additional information, such as a record of old transactions–perhaps a tax document from two years ago. It needs to be accessible but data access speed is less critical. This dataset is our warm data and can be kept in SSDs.Speed matters in other industries, too. For instance, gaming applications have strict latency requirements. Plus, by their nature, games are trendy. Over time, a gaming company accumulates user data, which is stored in a profiles database. But not all users are active users. With auto tiering, the active users’ profile information can reside on DRAM, while information about the rest of the users resides on the SSD.Our auto tiering product page goes into more technical details. Or if you’re ready to use it today, you can try Redis Enterprise 7.2 with auto tiering by creating a database on Redis Enterprise Cloud in the free or flexible plans or deploying a self-managed instance from our download center and following the auto tiering configuration guide."
678,https://redis.com/blog/introducing-redis-data-integration/,Introducing Redis Data Integration,"August 15, 2023",Yaron Parasol,"Redis is announcing the public preview release of Redis Data Integration (RDI). RDI lets developers offload a database to Redis Enterprise, mirror application data, and operate at in-memory speeds. And you don’t need to invest in coding or integration efforts.The underlying problem: Your existing database is too slow.You have a lot of applications, a growing number of users, increasing technical demands, and an unrelenting demand for real-time response. Redis Enterprise provides real-time access to data and it scales horizontally, but how do you keep your Redis cache in line with your database so that all queries can be executed from the cache?Some organizations decide to take it on themselves – only to discover how hard it is to build a cache prefetch (or refresh ahead, as it is sometimes called). To create one, you need to build a reliable streaming pipeline. That starts with capturing all data changes in the source database as they occur, and then translating the data to Redis data types to allow an application to fetch it. This process typically involves data transformations and denormalizations.We saw users struggling to build these streaming pipelines on their own. It required integration of several components (Change Data Capture (CDC), streaming, and Redis connectors), coding transformations, error handling, and many other enterprise essential requirements. That tool-building time could be spent on more productive endeavors that the business is waiting for.We decided to take on the challenge ourselves.Redis Data Integration (RDI) is a tool that runs inside Redis Enterprise. It helps you synchronize data from your existing relational database into Redis in near real-time so that application read queries are completely offloaded from the relational database to Redis.RDI pipelines have two stages:The data transformation processDebezium, an open-source CDC platform, captures changes to data in the source database and streams it into RDI. Within Redis, the data may be further filtered, transformed, and mapped to one or more Redis keys. RDI supports several Redis data types (Hash, JSON, Set, and Stream). RDI writes the data to the destination Redis database.It does the heavy lifting, so developers can focus on application code instead of on integration chores and data transformation code.RDI can connect with other CDC tools and streaming data. We already started building our ecosystem, and are happy to share that we have a technology partnership with Arcion.With this integrated solution, developers have a simple way to stream changes from a variety of databases to Redis Enterprise and other data platforms, using RDI as the backbone.Capturing changes from a source database and getting the data from one place to another is difficult enough. However, there is yet another challenge in moving data: the transformation part, which means filtering data and mapping the data to Redis data models.RDI provides an option to specify all the filtering and transformation steps required per source table. This is called a job, in RDI terms; every job is a YAML file.Filtering is important. CDC products provide complex filtering, but you have to write custom code. RDI does the same without coding. Instead, a declarative filter using SQL expressions or Jmespath functions is applied. RDI comes with additional custom Jmespath functions for the convenience of the job creator.RDI has several levels of data transformation:RDI includes a Trace tool that helps you create and troubleshoot complex data pipelines without writing custom code. That speeds up the process and reduces the required efforts and skill set.After troubleshooting, amending the pipeline is done by a simple deploy command with no downtime.Additional features in the public preview:Learn more in our Redis Data Integration documentation.RDI is ideally suited for applications that meet the following criteria:This is a public preview. We are seeing RDI Ingest flow to general availability, we are working on features to integrate Redis in the opposite direction: to apply changes to Redis data to downstream databases.RDI is currently available only for self-managed Redis Enterprise clusters.If you are an existing customer of Redis Enterprise, download the RDI CLI package and follow the steps in the quick start guide. The installation guide walks you through installing and configuring the Debezium server. After you run a handful of RDI CLI commands, your pipeline will begin moving data from your source database to Redis.If you are not an existing customer of Redis Enterprise, you need to first install Redis Enterprise Software for Kubernetes. Then download the RDI CLI package and follow the steps in the quick start guide."
679,https://redis.com/blog/five-official-redis-clients/,Five New Official Redis Clients,"August 15, 2023",Cody Henshaw,"Redis is committed to making using our software a delight to use. Adding these five new clients makes it even easier to do so.Our starting point in encouraging Redis use is making it easy for any developer to adopt. If you can get underway without having changing from familiar languages, tools, and application development platforms, you’re more likely to explore the powerful features we’re so proud of. Redis Stack supports a lot of functionality, with the latest supported protocols, embedded security, and performance that makes you gasp, “Wow, that’s fast!”Redis now officially supports five open-source client libraries:More are on the way.These client libraries seamlessly integrate into your applications. That means you can put your attention on creating awesome applications, instead of on debugging and patching third-party clients.We are dedicated to keeping the client libraries updated with the latest functionality and optimizations that Redis Stack has to offer. Whichever Redis version you rely on, you can count on seamless migration and upgrades. We will provide a standard interface across all Redis flavors, whether you use Redis Open Source (Redis OSS), Redis Enterprise Cloud, Redis Enterprise Software, or Redis on Kubernetes.By choosing our official Redis clients, you get:As part of our commitment to support developers, the officially supported libraries offer:Choosing one of our official Redis clients means prioritizing your development experience. You and your teams can work on the fun stuff–building incredible applications–while we handle the complexities of client libraries.To get underway, consult one of our Quick Start guides:"
680,https://redis.com/blog/understanding-redis-for-cloud-and-multicloud-in-90-seconds/,Understanding Redis for Cloud and Multicloud in 90 Seconds,"December 23, 2021",Will Johnston,"Welcome back to our ongoing Redis in 90 seconds series. In this post, we’ll demonstrate how to use Redis with any major cloud providers, or in a hybrid cloud.Most Redis providers simply host open source Redis and provide Redis as a cache. They don’t support Redis as a database. Not only that, they tend to lock your app into their cloud.If you need to move to a different cloud provider for any reason, you’re often restricted. It’s because the rest of your data is stored in cloud-specific databases and services such as DynamoDB, Kinesis, etc. If you use multiple clouds at the same time in place of a multicloud, often because of constraints on the region’s availability, or specific business needs, you can’t easily do that. Lastly, if you need a hybrid cloud capability to store some private data on-prem while still using the cloud for the rest, you’re in for a lot of trouble. When you use other Redis providers you lose all the flexibility and you’re “cloud locked-in”.With Redis Enterprise, since all or most of your data is stored in a single system, you can easily move from one cloud to another. Redis Enterprise is available on all major cloud providers, such as Amazon AWS, Google Cloud, Microsoft Azure, and even Heroku, making multicloud deployments a breeze. Redis Enterprise is available as downloadable software. Keep your private data in your private data center and keep the rest in the cloud, using Redis Enterprise’s hybrid deployment.Watch the video below to see what we mean:"
681,https://redis.com/blog/lowering-costs-with-redis-enterprise-in-90-seconds/,Lowering Costs with Redis Enterprise  in 90 Seconds,"January 4, 2022",Will Johnston,"We have reached the final post in our “Redis in 90 seconds” series. The purpose of this series is not to cover every aspect of Redis in-depth. Instead, we want to briefly highlight a few topics that you might not already know about Redis. In this post, we’ll walk you through what you can do to start lowering costs with Redis Enterprise.Have you ever compared Redis with other Redis providers and been surprised at the cost? It is a common question we are asked all the time. These providers allow you to use Redis as a cache and get the performance gains with sub-millisecond queries. The problem is, other Redis providers don’t optimize Redis to minimize your costs.Even database solutions such as DynamoDB can be expensive and slower than Redis, with queries upwards of 10 milliseconds or more.Redis Enterprise, even with all the added features, starts at just $7/mo and stays as low as $0 for your first six months when you use credits. This is great for small and medium-sized businesses (SMBs), and unlike other providers, Redis Enterprise is optimized for performance and cost savings at scale.With Redis Enterprise, take advantage of Redis on Flash to store terabytes of data while achieving sub-millisecond latencies and keeping costs at a minimum. This results in up to 80% in savings versus Redis from other cloud providers. So, Redis Enterprise isn’t only feature-rich and blazing fast, it’s also cost-effective for both SMBs and large enterprises.Watch the video below to see how you too can start lowering costs with Redis Enterprise"
682,https://redis.com/blog/scale-redis-across-the-globe/,Learn How to Scale Redis  Across the Globe in 90 Seconds,"December 28, 2021",Will Johnston,"We love technical articles that go deep. However, sometimes you’re on the go and need a quick summary to get the gist of something. This is why we’re continuing our “Redis in 90 Seconds” series with a short post about how to scale Redis.In today’s world, enterprise workloads can be very demanding. Enterprise customers demand low latency and fast failover with no data loss. Enterprise applications need to be distributed globally while minimizing latency.You might think throwing Redis on top of your existing database might solve this problem. Unfortunately, it only solves part of the problem. This is because most Redis providers simply host Redis OSS. They provide Redis as a cache, but not as an enterprise-grade real-time database.Redis Enterprise provides 99.999% uptime, sub-millisecond latency, single-digit-seconds failover, Active-Active Geo-Replication, and no data loss. It’s the only Redis provider that can meet the demands of enterprise customers.For example, with Redis Enterprise you can have multiple primaries spread across the globe and provide local, sub-millisecond latencies for both reads and writes. In order to avoid any write conflicts, Redis Enterprise uses the cutting edge Active-Active Geo-Replication feature that’s based on conflict-free replicated data types (CRDTs). Similarly, Redis Enterprise provides enterprise clustering, Redis on Flash, and many other native functions that address the enterprise solutions you won’t find anywhere else.Watch the video below to see Redis Enterprise in action:"
683,https://redis.com/blog/redis-cache-vs-redis-primary-database-in-90-seconds/,Redis as a Cache vs Redis as a Primary Database in 90 Seconds,"August 8, 2022",Will Johnston,"We received a lot of good feedback on our post titled, “Learn How Redis Simplifies Your Architecture in 90 Seconds,” so we decided to do a follow-up about Redis as a cache versus Redis as both a cache and a primary database.Get Started With Redis Cloud: Try FreeRedis began as a caching database, but it has since evolved into a primary database. Many applications built today use Redis as a primary database.  However, most Redis service providers support Redis as a cache but not as a primary database. This means you need a separate database like DynamoDB in addition to using Redis. This adds complexity, compromises latency, and prevents you from realizing the full potential of Redis.StackOverflow voted Redis the Most Loved Database three years in a row, and more than 2 billion Redis Docker containers have been launched. Knowing this, it shouldn’t be hard to find Redis expertise. And when Redis developers get stuck, there are literally thousands of resource books, tutorials, blog posts, and more to help resolve the issues.There are hundreds of Redis client libraries covering every major programming language and even some obscure ones. In many languages, developers can choose from various libraries to get just the right style and abstraction level. Redis is a database for a range of data sizes, from a few megabytes to hundreds of terabytes.With Redis Enterprise, you can use Redis as both an in-memory cache and a primary database in a single system, thus eliminating the complexity and latency of two separate systems. Not only that, you can use it as a multi-model primary database, enabling you to build modern applications, as well as low-latency microservice-based architectures, all on top of Redis.Instead of relying on separate databases and caches, utilize the native features of Redis Enterprise, such asLeverage all of the above with auto-scaling, enterprise clustering, and Active-Active Geo-Distribution.Watch the video below to see what we mean:Scaling both a cache and a database is often complicated; each data layer scales differently, reaching infrastructure and optimization opportunities at different times. Additionally, reducing the number of moving parts reduces latency; even though any given piece of the architecture may be fast, each item adds some sort of latency, either through the database itself or the connections made between items. Going to a single data store eliminates several internal network traversals. Finally, developing applications with a single data store requires only a single programmatic interface. Hence, developers need only understand the intricacies of a single database rather than a database and a cache. That reduces the mental cost of context-switching during development.Redis Enterprise’s Active-Active deployments are integral to achieving that 99.999% reliability and global scalability. This means a single dataset can be replicated to many clusters spread across wide geographic areas, with each cluster remaining fully able to accept reads and writes.Redis Enterprise uses Conflict-Free Replicated Data Types (CRDTs) to automatically resolve any conflicts at the database level and without data loss. Spreading clusters widely keeps data available at a geo-local latency and adds resiliency to cope with even catastrophic infrastructure failures.Many applications have relatively simple data needs, which can easily be supported via Redis’ built-in data structures. Other applications may need a bit more. For them, Redis provides an extensible engine that allows modules to add just the capabilities needed and no more. This approach extends to durability – Redis gives you the option of being entirely ephemeral, achieving durability through periodic snapshotting, or going all the way up to on-write durability with Append-only File (AOF). Redis can make the optimal trade-off between performance and durability depending on your use case.BSD-licensed and relatively compact, Redis is often cited as an example of a clean, well-organized C codebase. If something doesn’t make sense, it’s easy to find out and understand the absolute truth of what the database is doing. Nothing Redis does is magic – it’s just using long-established, efficient patterns to implement fundamental data structures.Performance expectations are a mainstay and are only getting more stringent with time. You’ll never hear a business leader say, “I wish our database was slower.” Thinking about building modern applications involves making them real-time, easy to develop, operationally elegant, scalable, and future-proof.Sure, Redis makes a great database cache but expanding Redis’ role as a primary database gives developers a head start on building the applications of tomorrow."
684,https://redis.com/blog/redis-enterprise-free-trial-aws-marketplace/,Introducing Redis Enterprise Cloud Free Trial on AWS Marketplace,"August 21, 2023",Sowmya Narayanan,"Redis is offering a new 14-day free trial on AWS Marketplace.For two full weeks, you can explore the full power of Redis Enterprise Cloud with no upfront costs or commitments. That gives you an opportunity to try Redis for a variety of use cases. Among them: caching, real-time analytics, publish/subscribe messaging, geo-location support, vector database analysis, and a whole lot more.Redis Enterprise Cloud is the only option on AWS that delivers sub-millisecond speeds, provides 99.999% uptime, and saves up to 80% on infrastructure costs (compared to other Redis-compatible managed services). Our partnership helps existing AWS users access Redis Enterprise Cloud’s simplified and automated database provisioning, billed directly from their AWS accounts.With Redis Enterprise Cloud, you get:Redis Enterprise Cloud is the most performant, reliable, and easy to use version of Redis, and getting started on AWS is easy and efficient. No matter where you are in your cloud journey, there’s an option to meet your needs. Hybrid? Multi-cloud? No problem.The free trial lasts for a 14-day period, OR when you use up to $500 in database subscriptions, whichever comes first. During the free trial, you can access Redis Enterprise Cloud with full features, including product support access. Consult the step-by-step instructions to learn what’s involved, and sign up through the AWS Marketplace.At the end of the trial period, you will automatically be subscribed as a Redis Enterprise Cloud pay-as-you-go customer, unless you cancel your subscription first.If you have any questions during the trial or need sign-up assistance, email aws@redis.com to connect with a Redis expert.New Redis Enterprise Cloud customers qualify for the 14-day free trial.If you are currently a Redis Enterprise Cloud customer, you may qualify for a Marketplace credit or proof of concept program. Fill out the form to get assistance from a Redis expert to learn about your options.Learn more about the free trial. Or just start your free trial today."
685,https://redis.com/blog/building-llm-applications-with-redis-on-googles-vertex-ai-platform/,Building LLM Applications with Redis on Google’s Vertex AI Platform,"August 24, 2023",Tyler Hutcherson,"Google’s Vertex AI platform recently integrated generative AI capabilities, including the PaLM 2 chat model and an in-console generative AI studio. Here, you learn about a novel reference architecture and how to get the most from these tools with your existing Redis investment.Generative AI, a fast-growing AI subset, has captured significant attention for its potential to transform entire industries. Google’s Google Cloud Platform (GCP) has been making strides to democratize access to generative AI to make adoption easier, and the company is backing it with robust security, data governance, and scalability.Recently, Google announced generative AI support on Vertex AI with four new foundation models:An application’s ability to produce, comprehend, and engage with human language is becoming essential. The need for this functionality spans numerous domains, from customer service chatbots and virtual assistants to content generation. And it is achievable thanks to foundation models, such as Google’s PaLM 2, that were meticulously trained to generate human-like text.Amidst this dynamic environment, two fundamental components consistently stand out as critical for the creation of efficient, scalable language model applications: foundation models and a high-performance data layer.Foundation models, of which large language models (LLMs) are a subset, are the cornerstone of generative AI applications. LLMs are trained on vast collections of text, which enables them to create contextually relevant, human-like text for an array of tasks. Improvements in these models have made them more sophisticated, leading to more refined and effective responses to user inputs. The chosen language model significantly impacts an application’s performance, cost, and overall quality.Yet, for all their capabilities, models such as PaLM 2 have limitations. When they lack domain-specific data, the models’ relevance can wane, and they might lag in reflecting fresh or correct information.There are hard limits to the context length (i.e number of tokens) that LLMs can handle in prompts. Plus, LLM training or fine-tuning requires substantial computational resources that add considerable cost.Balancing these limitations with the potential benefits requires careful strategy and a robust infrastructure.An efficient LLM application is underpinned by a scalable, high-performance data layer. This component ensures high-speed transactions with low latency, which is crucial for maintaining fluid user interactions. It plays a vital role in caching pre-computed responses or embeddings, storing a history of past interactions (persistence), and conducting semantic searches to retrieve relevant context or knowledge.Vector databases have emerged as one popular solution for the data layer.  Redis invested in vector search well before the current wave, and the technology reflects our experience–particularly with performance considerations. That experience is reflected in the just-announced Redis 7.2 release, which includes a preview of scalable search features that improves queries per second by 16X, compared to the previous version.Foundation models and vector databases have sparked substantial interest (and hype) in the industry, given their pivotal role in molding LLM applications across different sectors. For example, some newer standalone vector database solutions, such as Pinecone, announced strong funding rounds, and are investing a lot of effort into gaining developer attention. Yet, with all of the new tools emerging each week, it’s hard to know where to put your trust to be enterprise-ready.What sets GCP apart is its unified offering. It marries the availability of powerful foundation models with scalable infrastructure and a suite of tools for tuning, deploying, and maintaining these models. Google Cloud places paramount importance on reliability, responsibility, and robust data governance, ensuring the highest level of data security and privacy. Guided by its publicly declared AI principles, GCP champions beneficial use, user safety, and ethical data management, providing a dependable and trustworthy application foundation.However, to truly tap into these advancements, a complementary high-performing and scalable data layer is indispensable.Naturally, this is where Redis steps in. In the subsequent sections, we dissect these core components and explore their interaction through a reference architecture.The reference architecture illustrated here is for general-purpose LLM use cases. It uses a combination of Vertex AI (PaLM 2 foundation model), BigQuery, and Redis Enterprise.GCP and Redis Enterprise reference architecture for LLM applicationsYou can follow along with the setup of this LLM architecture step-by-step using the Colab notebook in an open-source GitHub repository.Once you complete the necessary setup steps, this architecture is primed to facilitate a multitude of LLM applications, such as chatbots and virtual shopping assistants.Even experienced software developers and application architects can get lost in this new knowledge domain. This short summary should bring you up to speed.Semantic search extracts semantically similar content from an expansive knowledge corpus. Doing so depends on the power of Natural Language Processing (NLP), foundation models like PaLM 2, and a vector database. In this process, knowledge is transformed into numerical embeddings that can be compared to find the most contextually relevant information to a user’s query.Redis, in its role as a high-performing vector database, excels at indexing unstructured data, which enables efficient and scalable semantic search. Redis can enhance an application’s capacity to swiftly comprehend and react to user queries. Its robust search index facilitates accurate, responsive user interactions.The Retrieval-Augmented Generation (RAG) approach uses methods like semantic search to dynamically infuse factual knowledge into a prompt before it’s sent to a LLM. The technique minimizes the need to fine-tune an LLM on proprietary or frequently changing data. Instead, RAG permits contextual augmentation of the LLM, equipping it to better tackle the task at hand, whether that’s answering a specific question, summarizing retrieved content, or generating fresh content. RAG is typically implemented within the scope of an agent.Agents involve an LLM making decisions about which actions to take, taking the stated action, making observations, and iterating until complete. LangChain provides a common interface for agent development.Redis, as a vector database and full text search engine, facilitates the smooth functioning of RAG workflows. Owing to its low-latency data retrieval capabilities, Redis is often a go-to tool for the job. It ensures that a language model receives the necessary context swiftly and accurately, promoting efficient AI agent task execution.Caching serves as a potent technique to enhance LLM responsiveness and computational efficiency.Standard caching provides a mechanism to store and quickly retrieve pregenerated responses for recurrent queries, thereby reducing computational load and response time. However, in a dynamic conversational context with human language, identically matching queries are rare. This is where semantic caching comes into play.Semantic caching understands and leverages queries’ underlying semantics. Semantic caching identifies and retrieves cached responses that are semantically similar enough to the input query. This ability dramatically increases the chances of cache hits, which further improves response times and resource utilization.For instance, in a customer service scenario, multiple users might ask similar frequently-asked-questions but use different phrasing. Semantic caching allows LLMs to respond swiftly and accurately to such queries without redundant computations.Redis is highly suited for facilitating caching in LLMs. Its robust feature set includes support for Time-To-Live (TTL) and eviction policies for managing ephemeral data. Coupled with its vector database capabilities for semantic searches, Redis enables efficient and rapid retrieval of cached responses, resulting in a noticeable boost in LLM response speed and overall system performance, even under heavy loads.It is important to retain past interactions and session metadata to ensure contextually coherent and personalized dialogues. However, LLMs do not possess adaptive memory. That makes it crucial to rely on a dependable system for swift conversational data storage.Redis offers a robust solution for managing LLM memory. It efficiently accesses chat history and session metadata, even under substantial demand. Using its data structure store, Redis handles traditional memory management, while its vector database features facilitate the extraction of semantically related interactions.What does it matter? Who needs these features? These three scenarios demonstrate the practical application of this LLM architecture.Some businesses need to handle vast volumes of documents, an LLM empowered application can serve as a potent tool for document discovery and retrieval. Semantic search aids in pinpointing pertinent information from an extensive corpus of knowledge.An LLM can serve as the backbone for a sophisticated ecommerce virtual shopping assistant. Using contextual understanding and semantic search, it can comprehend customer inquiries, offer personalized product recommendations, and even simulate conversational interactions, all in real time.Deploying an LLM as a customer service agent can revolutionize customer interactions. Beyond answering frequent queries, the system can engage in complex dialogue, providing bespoke assistance while learning from past interactions.A whirlwind of new products and buzzwords can easily lead to a shiny object syndrome. Amidst this cacophony, the combination of GCP and Redis shines through, providing not just an innovative solution but a reliable and time-tested foundation.Grounded in factuality: GCP and Redis empower LLM applications to be more than just advanced text generators. By swiftly injecting domain-specific facts from your own sources at runtime, they ensure your applications deliver factual, accurate, and valuable interactions that are specifically tailored to your organization’s knowledge base.Architectural simplification: Redis is not just a key-value database; it’s a real-time data Swiss army knife. By eliminating the need for managing multiple services for different use cases, it vastly simplifies your architecture. As a tool that many organizations already trust for caching and other needs, Redis’ integration in LLM applications feels like a seamless extension rather than a new adoption.Optimized for performance: Redis is synonymous with low-latency and high-throughput data structures. When coupled with GCP’s unrivaled computing prowess, you have an LLM application that not only talks smart but also responds swiftly, even under heavy demand scenarios.Enterprise readiness: Redis isn’t the new kid on the block. It’s a battle-tested, open-source database core, reliably serving Fortune 100 companies worldwide. With achievable five nines (99.999%) uptime in its enterprise offering and bolstered by GCP’s robust infrastructure, you can trust in a solution that’s ready to meet enterprise demand.Accelerate time to market: With Redis Enterprise available at your fingertips via the GCP Marketplace, you can put more focus on crafting your LLM applications and less on wrangling with setups. This ease of integration speeds time to market, giving your organization a competitive edge.While new vector databases and generative AI products might create a lot of noise in the market, the harmony of GCP and Redis sings a different tune–one of trust, reliability, and steadfast performance. These time-tested solutions aren’t going anywhere soon, and they’re ready to power your LLM applications today and for years to come.Redis will attend Google Next ‘23 between August 29-31 in San Francisco. We invite you to visit the Redis booth, #206, where we’re happy to talk about the details.Ready to get your hands dirty now? You can set up Redis Enterprise through the GCP Marketplace and work through the getting started tutorial."
686,https://redis.com/blog/cache-eviction-strategies/,Cache Eviction Strategies Every Redis Developer Should Know,"August 30, 2023",John Noonan,"Here’s a real-world scenario. You set up a Redis database, and it’s doing wonders at speeding up your application. But as the data flows in and the volume increases, you notice a potential issue: the cache is filling up. What will happen when it’s full? You may have heard about cache eviction but, perhaps, you’re fuzzy on the details.You’re not alone. Whether you’re a developer in a budding company or a system administrator in a large corporation, it’s important to understand cache eviction and know when and how to implement it. In this guide, we explore why.Cache eviction policies are a critical aspect of cache management when you use Redis (or any system that relies on caching… but we speak here from our own expertise). It addresses the challenges of cache size and memory usage. As the cache reaches its limits, it must make a crucial decision: should new data be rejected, or should space be created by discarding old data?This is where cache eviction comes into play. However you resolve it, cache eviction involves determining which cache entries to retain and which to discard when a cache fills up. It’s necessary to achieve or maintain optimal application performance and consistency.Eviction, in the context of caching, does not involve any landlords or overdue rents. Rather, it refers to the process of removing specific data from a cache. Because when a cache reaches its maximum storage capacity, some data must be removed to make space for new data–just like a bookshelf where you cannot force another book into the space available.Cache eviction strategies are protocols that dictate how a system responds when the cache is full. The decision about which data to evict (that is, remove) is made programmatically based on one of several strategies. Common strategies include:The effectiveness of these strategies depends on the specific use case.While Redis does have a default eviction policy (volatile-LRU), relying solely on it without understanding its implications can be risky. Applications serve diverse user needs, which means that data patterns and eviction requirements can vary significantly. Setting the right eviction policy can prevent potential headaches.Before even thinking of eviction, it’s essential to know when to act. This is where monitoring tools come into play.Redis provides tools like the INFO command for monitoring cache performance, while third-party monitoring tools like New Relic and Datadog offer more detailed analysis.Tuning cache performance involves adjusting cache settings and eviction policies based on the information you discover from monitoring performance. In distributed caching scenarios, monitoring and tuning become even more critical to ensure consistent and efficient cache management across multiple nodes. (We have additional advice for what to look for in a third-party monitoring tool.)In Redis, the cache entry data structure is managed by the maxmemory configuration directive, which sets the memory limit. The maxmemory-policy configuration directive guides Redis in making its eviction decisions based on the chosen cache eviction policy. Both the maxmemory-policy and the eviction policy are among the configuration settings stored in the redis.conf configuration file.There are quite a few Redis eviction policies, but you probably care most about these.Removes the least recently used cache entries, whether or not they have an expiration time set.Removes the least recently used cache entries with an expiration time set. This is suitable for scenarios where data needs to be refreshed periodically.When Redis needs to make room for new data, this policy removes the least frequently used keys.Similar to allkeys-lfu, this policy applies only to keys with an expiration time set.This policy removes keys with the shortest TTL first.Instead of evicting any keys, this policy returns an error when the memory limit is reached and a write command is received. (Don’t throw out anything in the closet. Send an alarm!)Each policy has its strengths and weaknesses. The best one for you depends on your specific needs.It’s important to have a well-structured cache, combined with the right cache eviction policy in order to achieve performance goals when you have vast amounts of data. Redis, with its versatile capabilities, serves as an excellent caching solution and a powerful asset for applications handling large datasets. Effective cache management not only expedites data retrieval through cache hits but also mitigates the impact of cache misses, making Redis a reliable and efficient caching solution for diverse use cases.Discover the intricacies of scaling cache with our comprehensive guide: The Definitive Guide to Caching at Scale with Redis. Learn the basics of caching to advanced enterprise application techniques in this one-stop resource."
687,https://redis.com/blog/introducing-redis-7-2/,Redis 7.2 Sets New Experience Standards Across Redis Products,"August 15, 2023",Rowan Trollope,"You have trusted Redis for over a decade because we make it easy to create powerful, fast applications that perform at scale––and we try hard to deserve that reputation. Redis is continuing that spirit with all the innovation we put into Redis 7.2.Here’s what we are doing to increase the number of reasons you love us.There’s an irreplaceable sort of learning that can only be gained by using a tool you’re leading the development of. Before I took up the mantle of Redis CEO, I dove headfirst into using the product as a developer. Its unparalleled performance, its scalability, and its design approach (as outlined in the Redis manifesto) inspired me, just as it has inspired millions of other developers. I saw why Redis is one of the most successful open-source databases in the world.Among the reasons for Redis’ growth are increasing demands for faster applications, real-time inferencing, and the emerging broad adoption of generative AI and vector databases. As Redis has expanded far beyond its beginnings as a data structure server (often used for caching), new waves of developers want to enjoy Redis’ benefits. We realized that we need to expand our responsibility for the full experience for all Redis practitioners: developers, architects and operators.As a developer, I appreciated how elegant and easy it is to use the Redis core API. So much so, in fact, that I wanted that delightful experience to be reflected in other parts of the product portfolio, such as in clients, integrations, tools, and documentation. We need to address this, I decided––and to take a more active guiding role.I joined Redis in February. Since then, we’ve established a strategy to expand our role as the stewards of the Redis project. In that role, we embrace Redis’s community-driven beauty, while making all distributions of Redis easier to navigate, with clear directions and well-marked trails. By acting as community stewards, we help both newcomers and long-time Redis practitioners enjoy Redis’ uncompromising performance, legendary reliability, and simplicity.Redis has always been a formidable contender in the tech industry, but the journey to becoming the best version of oneself is never-ending. We’ve been hard at work to make Redis even more rewarding for developers, architects, and operators.Today, I’m excited to introduce Redis 7.2, a step forward in our continuing journey to refine and improve your experience.So, let’s dive in and explore the many ways we are making technology easier for Redis users. Because at Redis, we’re committed to helping everyone build better software, faster, and with more confidence.Redis 7.2 is our most far-reaching release. It encompasses a broad set of new features, and a significant investment in the functionality that supports AI initiatives. In each of these enhancements, you’ll notice a strong theme of making it easier for developers to use Redis, making it run even faster, and making it easier to achieve innovative results.And we are committing to deliver all of these capabilities through every distribution channel all at once, with an approach we call the Unified Redis Release.The effort to leverage large language models (LLM) and generative AI is transforming computer software at a blistering pace, and we’ve been hard at work delivering capabilities in our platform to make that effort an easy one. We’ve been powering some of the largest customers in the world (including OpenAI) and have years of investment in making machine learning (ML)–and now vector databases–seamless and easily accessible.We also grok the concerns of the businesses that are doing their best to innovate based on them. For instance, one of my first customer visits was to a large financial services customer. The company has multiple ML workloads running exclusively on Redis, with hundreds of terabytes running at 5 9’s of availability. Enterprises are looking for a vector database that is a proven, enterprise-grade database with Active-Active geo-distribution, multi-tenancy, tag-based hybrid search, role-based access, embedded objects (i.e. JSON), text-search features, and index aliasing. We have all those built-in and battle-tested on Redis Enterprise.Redis supports generative AI workloads in its database service through several strategies that aim to improve efficiency, reduce costs, and enhance scalability and performance. Redis’ vector database supports two vector index types: FLAT (brute force search) and HNSW (approximate search), as well as three popular distance metrics: Cosine, Inner Product, and Euclidean distance. Other features include range queries, hybrid search (combining filters and semantic search), JSON objects support, and more.But, when people ask us how Redis can help with building and deploying LLM-powered apps? What can we really do?In the past 12 months, we’ve integrated Redis with the most popular application development frameworks for creating LLM-powered chatbots, agents, and chains. Among these are LlamaIndex, Langchain, RelevanceAI, DocArray, MantiumAI, and the ChatGPT retrieval plugin.  In addition, we’ve worked closely with NVIDIA on some of its leading AI projects: NVIDIA’s AI Workflows (Merlin and Morpheus), Tools (Triton and RAPIDS), and then in the works is RAPIDS RAFT–state of the art indexing provided by NVIDIA to deliver higher queries per second (QPS).These use cases require a new level of high-performance search. With Redis Enterprise 7.2, we’re introducing a preview of scalable search capability. It allows running high-QPS, low-latency workloads with optimal distributed processing across clusters. It can improve query throughput by up to 16X compared to what was previously possible with Redis Enterprise’s search and query engine.You don’t become the most admired NoSQL database (according to the Stack Overflow 2023 survey) without some serious developer focus.Redis 7.2 addresses one area that frustrated me at the beginning of my Redis developer journey, which was figuring out which of the hundred-plus community-developed client libraries fit my needs. Which one supports the latest Redis functionality? The right level of security and performance?With Redis 7.2, we bring a new level of guidance and support to Redis clients. We are working directly with the community maintainers of five client libraries––Jedis (Java), node-redis (NodeJS), redis-py (Python), NRedisStack (.Net), and Go-Redis (Go)––to establish consistency in such things as documentation, user interface, governance, and security. We also support the RESP3 protocol in Redis Stack and Redis Enterprise (cloud and software).With this release, we also bring a new level of programmability for real-time data. The public preview of Triggers and Functions brings a server-side event-driven engine to execute Typescript/JavaScript code within the database. This feature lets developers perform complex data manipulations directly on Redis, ensuring consistency of execution across any client application.Triggers and functions enable cross-shard read operations at the cluster level. This functionality was not available in previous generations of the Redis programmability engine, such as Lua and functions.Geospatial features are improved, too. We improved polygon search in Redis Stack to facilitate the search of geospatial data to find information within a geographic area.For example, in an application to locate taco restaurants, the geospatial information is the indexed location data of all the restaurants in San Francisco. The polygon that a user draws on a digital map is the geographic area of the search. Redis retrieves only the keys associated with the restaurants within the boundaries of the drawn polygon.We also made significant performance improvements in the Redis data type of sorted sets, commonly used to create gaming leaderboards among its other uses. Our enhancements generate gains between 30% and 100% compared to Redis Enterprise Cloud 6.2.You can now use Redis Data Integration (RDI), a tool that runs directly on Redis Enterprise and effortlessly transforms any dataset to Redis. We captured the most common use cases and made them available through an interface with configuration, not code.RDI can take data from a variety of sources (such as Oracle, Postgres, or Cassandra) and functionally turn it into real-time data. Similarly, when the data is no longer “real-time,” RDI can bring downstream changes from Redis Enterprise into the system of record without having to add more code or perform arcane integrations.RDI (currently in public preview) streams changes from source databases directly into Redis, where they are further filtered, transformed, and mapped into formats such as JSON and Hash.Your applications don’t always require top speed for every use case. Sometimes you don’t need to store all the data in memory. It makes more sense–and saves money–to leverage lower cost storage like SSD.With Redis Enterprise 7.2, we introduce Auto Tiering (formerly called Redis on Flash) with a new default storage engine, Speedb. Auto tiering allows operators to extend the size of Redis databases beyond the limits of physical DRAM using solid-state drives (SSDs). This makes sense for applications with large datasets where heavily used data stays in memory and, maintaining less frequently used data in SSD. Redis Enterprise automatically manages memory based on usage.Auto tiering delivers significant performance improvements in terms of throughput and latency, doubling throughput at half the latency of the previous generation storage engine (RocksDB), and reducing infrastructure costs by up to 70%.There are two other innovations in this release that we want to highlight. Operators will find the updated cluster manager (CM) user interface of great help to speed up everyday administrative tasks. Its intuitive interface has features comparable to those of the Redis Enterprise Cloud that reduces learning curves and minimizes errors. For example, with the new CM, you can deploy a new Redis Enterprise database with only two mouse clicks.The other piece of good news that surely will please operators of containerized applications is the general availability of the Redis Enterprise Operator for Kubernetes with support for Active-Active database deployments. With a few declarative lines in a YAML file, you can simplify several tasks that take time and effort, such as creating geo-distributed databases and removing, adding, and updating participating clusters from an Active-Active database.Many developers prefer to start building on a Redis service in the cloud rather than download it to their machines. We’ve also heard from many customers about the challenges of dealing with separate delivery dates of new Redis releases for their mix of Redis OSS, Redis Stack, and Redis Enterprise instances.Redis 7.2 is our first Unified Redis Release, and it is generally available today. We are making it easier for developers to build and port code between different Redis distributions. This is a significant boon for operators who want to streamline control over their Redis footprint. Architects will appreciate the freedom to integrate other data stores with Redis.Our commitment to the Redis community and customers is to release all Redis products and distributions at the same time. This includes Redis OSS, Redis Stack, Redis Enterprise Cloud, Redis Enterprise Software, and Redis Enterprise on Kubernetes.Redis 7.2 signals our unwavering commitment to make it easier for all of you, Redis lovers, to get started building with Redis, to more easily and cost-effectively run Redis at large scales, and to bring slow data into Redis and make it actionable in real time.The AI revolution is here, and I cannot overstate its far-reaching effects on our work and lives in general. Our focus on our vector database and vector similarity search, make it a perfect match in helping you harness your Redis skills to jumpstart your AI projects.We invite you to read more details on the new features in Redis Enterprise 7.2 with the specifics of Auto Tiering, Triggers and Functions, and Redis Data Integration.Are you ready to try it for yourself? The easiest way is to use Redis Enterprise Cloud, create a free account to try the latest features using Redis Stack, or download the software for a self-managed or Kubernetes deployment.For the features in public preview, such as triggers and functions, deploy a database on Redis Enterprise Cloud in the fixed tier within the Google Cloud/Asia Pacific (Tokyo) or AWS/Asia Pacific (Singapore) region. For a self-managed experience, visit our download center."
688,https://redis.com/blog/redis-observability-with-uptrace/,Enhancing Redis Observability with Uptrace,"July 19, 2023",Talon Miller,"Everyone needs a way to monitor server behavior, if only to confirm that the system is running to spec. Several Application Performance Monitoring (APM) tools work with Redis, but perhaps you haven’t heard of Uptrace. Let me tell you about it.Uptrace is an OpenTelemetry-based observability platform that helps developers and Ops users monitor and optimize complex distributed systems. It’s an open-source APM tool that supports distributed tracing, metrics, and logs. Automatic alerts for critical infrastructure can be sent via email, Slack, Telegram, and other notification channels.Uptrace is a great platform for monitoring Redis that you should know about. Let’s dive in.Uptrace is an open source APM available both as an open-source version and an enterprise cloud version; the latter has more features and support options. The open-source version is quite robust as it has Prometheus remote write, AWS CloudWatch, vector logs, percentiles, and alerting, to name just a few examples. To evaluate its use with Redis, we set up open-source deployments of Uptrace and also tested the enterprise edition, called Uptrace Cloud.Its UI looks amazing, and it genuinely is easy to navigate.You can connect to multiple databases but, for our purposes, we drill down to just see the Redis database. It presents us with traces of how our GET, SET, and DEL commands are doing. That provides a quick health check on our cache performance.The Uptrace spans tab helps us visualize command responsiveness down to the millisecond using a heatmap. This gives a quick, easy look at whether a Redis database is performing to expectations. If the heat moves a bit higher (longer millisecond response time), then a bottleneck is starting or inefficiencies are developing.In the image below, this Redis database is completing the majority of GET, SET, and DELETE commands at around 1 millisecond, which is a good target for our cache performance.You can build your own custom metrics that appear on the Uptrace dashboard. For example, memory usage is an important metric for Redis. This screen capture shows how to accomplish that.Uptrace has plenty of other appealing features. One that we found super useful is its optimized storage, which includes performance optimization for common queries, efficient sharding for databases, and improved support for cold storage like S3. Specifically for Redis, Uptrace also supports improved storage policies that give more flexibility to move data between SSD and cold storage; doing so reduces costs, because you don’t have to store as much on SSD.So, what are you waiting for? Try Uptrace as your next OpenTelemetry backend or check out the cloud demo of Uptrace.We offer huge thanks to Vladimir Mihailenco for his many contributions to the DevOps ecosystem, which includes not only Uptrace but also the now-officially-supported Redis Client library for Golang, go-redis. To learn more about the fastest and easiest way to get started with the Go Programming Language and Redis, read Go-Redis Is Now an Official Redis Client or visit the GitHub repo. Happy developing!"
689,https://redis.com/blog/go-redis-official-redis-client/,Go-Redis Is Now an Official Redis Client,"February 15, 2023",Igor Malinovskyi,"The Go-Redis client joins the family of officially supported clients under the Redis umbrella.Do you program in the Go language and want to use Redis? We have good news for you! The open source Go-Redis, which provides a type-safe API for Redis commands, now is easier for you to access and use.Go-Redis is a community-driven project started by Vladimir Mihailenco, whose Uptrace monitoring application creates automatic alerts for complex distributed systems. Community contributors including Dimitrij Denissenko and monkey92t helped make the Go-Redis client a top choice for developers working with Redis.Uptrace is an OpenTelemetry APM with an intuitive query builder, rich dashboards, alerting rules, and integrations for most languages and frameworks. It can process billions of spans and metrics on a single server, allowing you to monitor your applications at 10x lower cost.Beginning with version 9, Go-Redis is hosted under the official Redis organization on GitHub. This change encourages even more collaboration and contributions from the community and ensures that the library stays up-to-date with the latest Redis and Redis Stack features. It also aligns the Go client with other officially supported Redis clients, such as redis-py for Python, nredisstack for .NET, jedis for Java, and node-redis for Node.js.The end result: It’s easier for developers to find and use the appropriate Redis client for their preferred programming language.Existing Go-Redis users should update their imports and dependencies to begin using version 9:If you’re new to Go-Redis, please do explore it! Version 9 adds support for the RESP3 protocol, introduces a new hooks API, improves pipeline retries, and allows performance monitoring via OpenTelemetry.Thank you again to everyone who has been a part of this project thus far. We can’t wait to see what the future holds for the Go client for Redis and the wider Redis community!"
690,https://redis.com/blog/understanding-topology-based-data-architectures/,Understanding Topology-Based Data Architectures,"August 28, 2023",Amine El Kouhen,"Data architectures can be classified based on their operational mode or topology, including data fabric, data hub, and data mesh. What’s the distinction between those? Buckle in.Previously, when I explained velocity-based data architectures, I went into detail about Lambda and Kappa architectures. But that’s just the beginning of the story.While data architectures can be grouped in relation to data velocity, as we’ve seen with Lambda and Kappa, another way to classify them is technology-agnostic operational models. As you surely remember if you have been following along with this Data Basics series, three types of operating models can exist in any organization: centralized, decentralized, and hybrid.Here, I describe three topology-based data architectures in more detail: data fabric, data mesh, and data hub.A data hub is an architecture for managing data in a centralized way. Think of it as a data exchange with frictionless data flow at its core.A data hub acts as a central repository of information with connections to other systems and customers, allowing data sharing between them. Endpoints interact with the data hub by providing data into it or receiving data from it. The hub provides a mediation and management point, making visible how data flows across the enterprise.A data hub architecture facilitates this exchange by connecting producers and consumers of data. The seminal work was a Gartner research paper, Implementing the Data Hub: Architecture and Technology Choices, published in 2017. Gartner suggested a technology-neutral architecture for connecting data producers and consumers, which was more advantageous than point-to-point alternatives. Subsequent research further developed this concept, resulting in the current definition of a data hub’s attributes.The hub is structured and consumed according to the models defined by its users. Governance policies are established by data managers to ensure data privacy, access control, security, retention, and secure information disposal. Developers can use integration strategies such as APIs or Extract-Transform-Load (ETL) processes to  work with the data stored within the hub. The persistence attribute defines which type of datastore should be used for storing this data (such as data lakes, data warehouses, or data lakehouses) and the different parameters of storage, such as the raw support, storage system, and storage layers.Implementing a data hub architecture facilitates:Gartner proposed that businesses could use specialized, purpose-built data hubs. For instance, analytics data hubs might collect and share information for downstream analytics processes. Or application data hubs could be used as domain context for specific applications or suites.Data centralization implemented in data hubs ensures that data is managed from a central source–with a unified vision–and it keeps the data accessible from many different points. The beneift is that it minimizes data silos, fosters collaboration, and provides visibility into emerging trends and impacts across the enterprise.However, there are challenges with data centralization. Processes can be slow without putting accelerators or some sort of self-service strategy in place. As a result, requests take longer and longer to get done. The business can’t move forward fast enough, and opportunities for better customer experiences and improved revenue are lost simply because they can’t be achieved quickly.That’s among the reasons Gartner recently updated its data hub concept to allow organizations to run multiple hubs in an interconnected way. This way, the data hub can take advantage of data centralization and leverage decentralization by giving the lines of business more responsibility and power.The most common data hub usage is data warehouses. A data warehouse is a central data hub used for reporting and analysis. Typically, data in a data warehouse is highly formatted and structured for analytics use cases. As a result, it’s among the oldest and most well-established data architectures.In 1989, Bill Inmon originated the notion of the data warehouse, which he described as “a subject-oriented, integrated, nonvolatile, and time-variant collection of data in support of management’s decisions.” Though the technical aspects of the data warehouse have evolved significantly, the original definition still holds its weight.Traditionally, a data warehouse pulls data from application systems by using ETL. The extraction phase pulls data from source systems. The transformation phase cleans and standardizes the data, organizing and imposing business logic in a highly modeled form.One ETL variation is Extract-Load-Transform (ELT). With the ELT mode in data warehouse architectures, data is moved more or less directly from production systems into a staging area in the data warehouse. In this context, staging indicates that the data is in a raw form. Rather than using an external system, transformations are handled directly in the data warehouse. Data is processed in batches, and transformed output is written into tables and views for analytics.When the Big Data era began, the data lake emerged as another centralized architecture. The idea was (and is) to create a central repository where all types of structured and unstructured data are stored without any strict structural constraints. The data lake was intended to empower businesses by providing unlimited data supply.The initial version of the data lake started with distributed systems like Hadoop (HDFS). As the cloud grew in popularity, these data lakes moved to cloud-based object storage, which could depend on extremely cheap storage costs and virtually limitless storage capacity. Instead of relying on a monolithic data warehouse where storage and compute are tightly coupled, the data lake stores an immense amount of data of any size and type.It got a lot of hype. But the first generation of data lakes–data lake 1.0–had significant drawbacks. The data lakes essentially turned into a dumping ground, creating terms such as “data swamp” and “dark data,” as many data projects failed to live up to their initial promise. Management became increasingly difficult as the data volume grew exponentially, and schema management, data cataloging, and discovery tools were lacking. In addition, the original data lake concept was essentially write-only, creating huge headaches with the arrival of regulations such as GDPR that required targeted deletion of user records. Processing data was also a major challenge, with relatively basic data transformations, such as joins, requiring the implementation of complex MapReduce jobs.Various industry players have sought to enhance the data lake concept to fully realize its promise and to respond to the first-generation data lake limitations. For example, Databricks introduced the notion of a data lakehouse, which suggests a convergence between data lakes and data warehouses. The lakehouse incorporates the controls, data management, and data structures found in a data warehouse while still housing data in object storage and supporting a variety of query and transformation engines. In particular, the data lakehouse supports atomicity, consistency, isolation, and durability (ACID) transactions. It is a significant disruption from the original data lake, where you simply pour in data and never update or delete it.Data decentralization is a data management approach that eliminates the need for a central repository by distributing data storage, cleaning, optimization, output, and consumption across organizational departments. This helps reduce complexity when dealing with large amounts of data and issues such as changing schema, downtime, upgrades, and backward compatibility.Data fabric was created first as a distributed data environment that enables the ingestion, transformation, management, storage, and access of data from various repositories for use cases such as business intelligence (BI) tools and operational applications. It provides an interconnected web-like layer to integrate data-related processes by leveraging continuous analytics over current and inferred metadata assets. To maximize efficiency, it uses techniques like active metadata management, semantic knowledge graphs, and embedded machine learning/AutoML capabilities.This concept was coined in 2016 by Noel Yuhanna of Forrester Research in Forrester Wave: Big Data Fabric, and has been updated since then. Yuhanna’s paper described a technology-oriented approach combining disparate data sources into one unified platform using Hadoop and Apache Spark for processing. The goal is to increase agility by creating an automated semantic layer. This later accelerates the delivery of data value while minimizing pipelines complexity.Over the years, Yuhanna developed his Big Data Fabric concept further. His current vision for data fabrics is for them solve a certain class of business needs, such as creating an all-encompassing view of customers, customer intelligence, and analytics related to the Internet of Things. The data fabric’s components include AI/ML, data catalog, data transformation, data preparation, data modeling, and data discovery. It also provides governance and modeling capabilities.Gartner also adopted the term “data fabric” and defined it similarly. The analyst firm describes it as “an emerging data management and integration design that enables flexible, reusable, and enhanced data integration pipelines, services, and semantics to support various operational or analytics use cases across multiple deployment platforms”.Data fabrics combine different data integration techniques while using active metadata, knowledge graphs, semantics, and machine learning (ML) to improve their design process. They organize them into five inner attributes.In a fabric, active metadata contains catalogs of passive data elements such as schemas, field types, data values, and knowledge graph relationships. The knowledge graph stores and visualizes the complex relationships between multiple data entities. It maintains data ontologies to help non-technical users interpret data.With the help of AI and ML features, a data fabric may assist and enhance data management activities. It also offers integration capabilities to dynamically ingest disparate data into the fabric to be stored, analyzed, and accessed. Automated data orchestration allows users to apply DataOps principles throughout the process for agile, reliable, and repeatable data pipelines.Data fabric is a technology-agnostic architecture. Its implementation enables you to scale Big Data operations for both batch processes and real-time streaming, providing consistent capabilities across cloud, hybrid multi-cloud, on-premises, and edge devices. It simplifies the flow of information between different environments so that a complete set of up-to-date data is available for analytics applications or business processes. And it reduces time and cost by offering pre-configured components and connectors, so nobody has to manually code each connection.Data mesh was introduced in 2019 by Zhamak Dehghani, who argued that a decentralized architecture was necessary due to shortcomings in centralized data warehouses and data lakes.A data mesh is a framework that enables business domains to own and operate their domain-specific data without the need for a centralized intermediary. It draws from distributed computing principles, where software components are shared among multiple computers running together as a system. In this way, data ownership is spread across business domains, each of which is responsible for creating its own products. A data mesh–in theory, anyway–allows easier contextualization of the collected information to generate deeper insights while simultaneously facilitating collaboration between domain owners to create tailored solutions according to specific needs.Dehghani later revised her position to propose four principles that form this new paradigm: domain-oriented, data-as-product, self-service, and federated governance.The data mesh concept is based on decentralizing and distributing responsibility for analytical data, its metadata, and the computation necessary to serve it to people closest to the data. This allows for continuous change and scalability in an organization’s data ecosystem.To do this, data meshes decompose components along organizational units or business domains that localize changes or evolution within that bounded context. By doing so, ownership of these components can be distributed across stakeholders close to the data.One issue with existing analytical data architectures is that it can be difficult and expensive to discover, understand, trust, and use quality data. The problem only gets worse as more teams provide data (domains) in a decentralized manner, which would violate the first principle.To address these challenges related to data quality and silos, a data mesh must treat analytical data provided by domains as a product and treat the consumers of that product as customers. The product becomes the new unit of architecture that should be built, deployed, and maintained as a single quantum. It ensures that data consumers can easily discover, understand, and securely use high-quality data across many domains.The infrastructure platform allows domain teams to autonomously create and consume data products without worrying about the underlying complexity of the building, executing, and maintaining secure and interoperable solutions. A self-service infrastructure should provide a streamlined experience that enables data domain owners to focus on core objectives instead of worrying about technical details.The self-serve platform capabilities fall into multiple categories or planes:Data mesh implementation requires a governance model that supports decentralization and domain self-sovereignty, interoperability through a dynamic topology, and automated execution of decisions by the platform. Integrating global governance and interoperability standards into the mesh ecosystem allows data consumers to gain value from combining and comparing different data products within the same system.The data mesh combines these principles into a unified, decentralized, and distributed system. The premise is that data product owners have a self-service, shared infrastructure that supports data-sharing pipelines that work in an open yet governed manner. This allows the developers to be productive without sacrificing governance or control over their domain’s data assets.A data mesh differs from traditional approaches, where  pipelines and data are managed as separate entities with shared storage infrastructure. Instead, it views all components (i.e., pipelines, data, and storage infrastructure) at the granularity of a bounded context within a given domain to create an integrated product. This allows for greater flexibility in terms of scalability and customization while providing better visibility into how different parts interact.How do we put all that together? Data types are increasing in number, usage patterns have grown significantly, and a renewed emphasis has been placed on building pipelines with Lambda and Kappa architectures in the form of data hubs or fabrics. Whether grouped by velocity or the kind of topology they provide, data architectures are not orthogonal. The data architectures and paradigms I described in these blog posts, so far, can be used as appropriate for a given need. And, of course, they can be mixed in architectures like data mesh, in which each data product is a standalone artifact. We can imagine scenarios where a Lambda architecture is implemented in some data products and Kappa architectures are employed in others.Want more detail? Our white paper, Best Practices for a Modern Data Layer in Financial Services, discusses the steps to modernize a rigid and slow IT legacy system."
691,https://redis.com/blog/elements-of-a-data-strategy/,The Elements of a Data Strategy,"July 20, 2023",Amine El Kouhen,"Organizations cannot unlock the potential of their data–to help them make better decisions–unless they have a clear data strategy. Start with these fundamentals.A data strategy is a plan of action that outlines how an organization collects, stores, manages, and uses data. It includes goals for the data’s short-term and long-term usage as well as rules to ensure compliance with relevant laws and regulations.Data strategies encompass a lot of things, from how the data is acquired, analyzed, protected, and delivered. A successful data strategy includes analysis plans so the organization can gain insights about customer behavior or other business trends – which is the whole point of the exercise.Practically speaking, a data strategy determines how a company combines people, processes, and technology into one cohesive plan that is genuinely achievable.If you don’t know where you’re headed, you cannot judge progress. A data strategy allows organizations to be innovative, empowers business users to work effectively, and keeps the business competitive.Without a proper data strategy in place, organizations encounter these common business challenges:Many organizations claim to be data-driven or data-centric, and they use the two terms interchangeably. But the two are not the same. Each has particular applications.Data-centric organizations focus on the data itself, collecting and analyzing it to gain insights. They use this information to inform decisions about how they operate their business. As a result, data is perceived as a valuable asset to use for competitive advantage.Data-driven organizations take an even more proactive approach. They use data to make informed decisions, as above. They also actively drive change in their operations and strategies based on what the data tells them. This type of organization puts greater emphasis on leveraging technology. For example, predictive analytics and machine learning (ML) algorithms allow them to identify patterns within large datasets so they can act upon those findings quickly.There are several facets to an effective data strategy.A successful data strategy reinforces and addresses the business needs to generate accurate and palpable value.A data strategy that is misaligned with a business strategy might prioritize the wrong projects, generate useless insights (good answers to the wrong questions), and waste time and money by allocating scarce resources to unprofitable activities. Following such a path likely causes an organization to lose interest and confidence in any data initiative.A data strategy will not succeed without executive support and business champions, which means those executives need to understand how the plan supports the business goals. Be ready to demonstrate how data can support their initiatives.To that end, establish clear goals and measurable objectives that serve your larger business strategy.Set both short-term and long-term quantifiable objectives. A data initiative leader is  tasked with the challenge of achieving success in three key areas: revenue growth, operational efficiency, and security/privacy risk management. Prioritize these based on what matters most at this moment for your organization.Each department or team should have its own local data-centric goal, with an action plan outlining how data utilization can help them reach their desired outcome. This usually requires negotiation among the stakeholders.Every goal set should be accompanied by an actionable, specific plan to accomplish it. Plans should include information like who owns the goal, which technology and process to use, the cost of reaching the objective, the time needed, and the intended outcome. Plans should also remain flexible to account for unforeseen circumstances.The roadmap that comes out of all of this makes it possible to put your plans into action. With a road map, you know where you are and where you want to be.Prioritize activities that are relatively easy to implement and provide quick wins for the business while assessing the feasibility and expected value of each recommendation designed to bridge the current state to the future.Include the following constraints in your data strategy roadmap:Once you secure support from executives and identify the end goals, it is time to evaluate the current state of your ecosystem. Consider which aspects are functioning well and which need improvement to create a data-driven experience. Identify the obstacles preventing you from executing your roadmap.You can rely on existing data initiatives as a starting point. These may inspire those attainable quick wins, identify blocks you hadn’t considered, and take realistic, incremental steps to become more data-driven.Don’t be starry-eyed about what you can accomplish with the resources available. Do your best to recognize the existing shortcomings, existing pain points, and weaknesses, whether they involve technology, processes, or people.Data architecture includes data identification, ingestion, storage, transformation, and analysis. A well-documented and implemented data architecture is essential for an effective, reliable data strategy because it simplifies scaling your data operations when the need arises.Review existing data infrastructures (which I sometimes call data platforms) and analyze how business users already take advantage of them. This helps identify potential gaps that need resolution.The next step involves making technology-centric decisions based on specific requirements such as data quality, data compliance, and of course, the main characteristics of data. Any data strategy requires the right tools and technologies to work as planned.Data architecture consists of the tools and processes that allow you to design the data value chain (the data journey). It provides a blueprint for how and where data is collected, stored, and the flows between different data sources and applications. These elements may include various kinds of on-premises and cloud-based hardware and software. The goal is to make data as accessible, shareable, and actionable as possible for the stakeholders who need it, with the right security controls in place.The first step in constructing your data architecture is to identify the datasets present within different business units of the company. A data catalog can be beneficial for this purpose, if you have one. Storing your data in a single repository, such as a data warehouse or data lake, is necessary to analyze and work on it effectively.Obviously, this is a project on its own, and the specifics depend on your own shop. You may need to ingest raw data from disparate sources and replicate it to a destination for storage and analysis. You may need to integrate or transform the stored information into an alternate format to make analysis easier. This is how data pipelines come into the overall picture.The right people in the right roles are essential to ensure that technology and processes are adopted and business objectives are met. The data strategy team typically includes representatives from the executive board, business analytics, and IT.Create a data strategy team to ensure that everyone’s interests are represented. Think about everyone who uses your organization’s data, even those whose job responsibilities don’t suggest that data is central to their work. When an organization has multiple data sets, you should specify which data set belongs to which “owner,” meaning who is responsible for storing, processing, and interpreting data sets.An operating model dictates the data strategy team structure. An organization can subscribe to three types of operating models: decentralized, centralized, and hybrid.No model is inherently better than the others. Deciding which model to use comes down to your organization’s size and resources as well as its data needs.Evaluate and understand the data team’s skillsets. What level of data literacy does everyone have? Do you need to hire additional people with specific skill sets? What kind of training does your staff need?While every organization is different, these roles are common personas that come together to create and implement a data strategy:Data governance sets out processes and responsibilities to guarantee the strict alignment of data strategy with the business strategy. As your business evolves, so must its governance rules.You’ve probably seen stories about businesses with great data strategies that failed. Their data strategies were sound, but they failed to create an internal organization that could both deliver value from their data assets to internal customers and generate adequate levels of profit from it.While you can find lots of advice on how to devise better strategies, there needs to be more guidance on how to execute those strategies.This is why data governance is a key element of any successful data strategy.  It establishes control over data assets and is directly accountable for the data strategy execution.Data governance encourages team members to view data as a valuable asset rather than just an output of business operations. The practice also ensures that everyone follows established policies when dealing with data.Whatever your data strategy looks like, it is bound to involve a lot of changes for teams and potentially new responsibilities or expectations. If you do not take the culture shift into account, you may not realize the potential benefits from your data strategy plan.Motivate the people who work on the project, and put even more attention on selling the benefits of “the new way” to the people whose existing processes you’ll be up-ending. Demonstrate how implementing this strategy can help meet business objectives beyond “hours saved.” Lean on your champions and stakeholders to support and vouch for your ROI messaging.Turn again to your data team. Equip them with the knowledge and tools they need to succeed. This should include orientation and training on data literacy, technical enablement, and business goals awareness.Finally, put a communication plan into place detailing such things as who receives information about the process or technical changes or how metrics are captured. Executive backing makes a big difference here.You have your team. You have the elements of a data strategy. What’s the next step? You have the building blocks to begin thinking about designing a data architecture that meets your needs.Consult our e-book, 8 Data Modeling Patterns in Redis, to get a peek of where we’re headed."
692,https://redis.com/blog/data-101-data-science-fundamentals/,Data 101: The Fundamentals of Data Science,"July 10, 2023",Amine El Kouhen,"Let’s start with data science fundamentals, and consider the concept of data, its typology of data, its main characteristics, and its impact on our way of life.Data is any organization’s most valuable asset. Beyond the obvious reasons to collect data – customer records, accounting information, project information, and the other elements by which a knowledge-based organization makes money – the data also provides insights into customer behavior, market trends, and product performance that help inform decisions about allocating resources. As a result, companies often invest in collecting or purchasing data from third-party sources to gain a competitive advantage over their competitors.Data is any information you collect that is organized and structured to make it worthwhile for analysis. Data is collected every time you make a purchase, navigate a website, travel, make a phone call, or post on a social media site. Data can come from many sources, including sensors, surveys, experiments, observations, or existing records (historical data) such as financial transactions. Never before has so much data about many different things been collected and stored every second of every day.The theory of information pushed the concept of data way further. The Theory of Information is a field of study that seeks to understand the nature and origin of information. According to this study, everything can be considered data. This includes physical objects as well as abstract concepts such as ideas or emotions. Furthermore, data is defined as any set of symbols that conveys meaning when interpreted by a receiver. Therefore anything that has some form of symbolic representation (such as DNA sequences, words, or numbers) could be classified as data in this context.Data is classified depending on a given perspective, such as by value, velocity, structure, sensitivity, or any number of other characteristics.From a purely statistical point of view, data can be part of two major categories according to their value.Quantitative (numerical) data is any information that can be expressed, measured, and compared using numerical values, such as integers or real numbers.Examples of quantitative data include height, weight, length, temperature readings, population size, or countable items such as the number of students in a classroom. This type of data can be further divided into discrete values (whole numbers) or continuous values (decimals).Qualitative (categorical) data is non-numerical information such as opinions, feelings, perceptions, and attitudes. This data can answer questions such as, “How did it occur?” or “Why did this occur?” Examples of qualitative data include gender, rankings, and enumerations.This kind of data can be divided into nominal or ordinal.Statistically, qualitative variables must be transformed into dummy variables before any analysis. For example, we can artificially assign numbers to categories. For instance, if your categories are colors, we could assign the number 1 to red and 2 to blue, but these do not have meaning in any mathematical sense. We would not conclude that blue is twice as much as red is!Data… information… knowledge. What is the difference? The DIKW model, which describes the relationship between data, information, knowledge, and wisdom, answers this question and the underlying one: What is the finality of data?In the DIKW model, data is considered raw material for wise decision-making because it provides an objective basis for drawing conclusions. By analyzing large amounts of data in various ways, such as through statistical analysis or machine learning algorithms, we can uncover patterns within the data that may not previously have been obvious. This information is then processed into meaningful insights, forming the basis of decision-making processes. Finally, wisdom comes in when these insights are applied with experience and judgment so that someone can make an informed choice about what action should occur next, influencing future strategies.Thus, data adds value by providing insights and information that can be used to make informed decisions. Data helps organizations identify trends, measure performance, optimize processes, improve customer experience, and drive innovation. It also enables businesses to gain a competitive edge in the market through better decision-making capabilities based on data analysis.I deeply appreciate the power and the impact that the data has. Coming to the table with concrete evidence in data, in comparable titles, really allows any team to feel holistically comfortable with how we forecast the business, based on signals, measures, and facts.In the early years of this century, data was only studied in terms of three characteristics, known as the three V’s of Data: Volume, Velocity, and Variety. Over time, two more V’s – value and veracity – were added to help data scientists and managers more effectively articulate and communicate the essential characteristics of the data they work with.The five main and innate characteristics of data are:Marketing organizations added two additional data characteristics that can significantly affect the insights it generates. These two characteristics are:In an organization, the meaning can constantly change, significantly impacting data homogenization. This concept differs from variety. A coffee shop can offer six different blends of coffee – representing  Variety –  but you get the same blend every day. But Variability captures the variations of taste that can occur even within a single coffee blend and the factors that cause them, such as supply chain conditions, store temperature, water quality, and the brewing equipment.Ultimately, data scientists today pay attention to 7 Vs of data – and as I continue in this series, you will too.Want to apply data modeling concepts to your current projects? We wrote a comprehensive e-book, Data Modeling Patterns in Redis, that goes through eight scenarios and shows how to model them in Redis, accompanied bycode snippets."
693,https://redis.com/blog/introducing-google-cloud-private-serivce-connect-support-for-redis/,Introducing Google Cloud Private Service Connect Support for Redis Cloud,"September 11, 2023",Redis,"Redis Cloud enhances security and reliability with its new support for Google Cloud Private Service Connect.At its annual Google Cloud Next conference, the company shared several exciting innovations underway. Networking and infrastructure optimization earned particular attention.We were excited that Redis was featured as an ecosystem partner in Google’s cloud networking session. As part of this partnership, we’re announcing our support for Google Cloud Private Service Connect on Redis Cloud.Google Cloud Private Service Connect allows organizations to connect applications securely to specific cloud services. Doing so bypasses the public internet and limits network exposure. This provides a safer and more reliable way for applications to communicate with cloud services without exposing entire networks over the web.There are a few reasons why you might want to use Private Service Connect:Redis Cloud can now connect to all of your Google Cloud applications and services through Private Service Connect as a hosted SaaS service.Set up and configure Private Service Connect through the Redis Cloud admin console. Or, if you are new to Redis Cloud, start today through the Google Cloud Marketplace.We thank our hardworking development team for making this integration a reality and the Google Cloud team for its support throughout the process."
694,https://redis.com/blog/amazon-bedrock-integration-with-redis-enterprise/,Redis Enterprise Cloud Integration With Amazon Bedrock Now Available,"September 13, 2023",Antony Prasad Thevaraj and Tyler Hutcherson and Srinivas Pendyala,"Following the recent launch of Redis 7.2, which underscores our dedication to advanced developer tools and AI innovation, Redis is thrilled to announce the integration of Redis Enterprise Cloud with Amazon Bedrock as a knowledge base for building RAG applications.AWS designed Amazon Bedrock with customer needs in mind: seamless access to powerful foundation models (FMs) without complex infrastructure management, accompanied by the ability to customize these FMs securely. Today’s announcement ensures that developers can use a unified API to access FMs from Cohere, AI21 Labs, Anthropic, Stability AI, and Amazon, backed by a high-performance data layer (Read the AWS announcement on their blog here).The traction Redis Enterprise Cloud has garnered as a highly-performant, stable vector database doesn’t surprise many people, given the company’s investments in enterprise-grade capabilities such as high availability, persistence, scalability, and geo-replication for business continuity.In the past, merging private, domain-specific data with the capabilities of a large language model (LLM) was labor-intensive for Redis developers. The routine was multifaceted: extract source documents, morph them into embeddings, feed them into Redis, and then, on demand, execute complex vector search queries.The newly established integration between Redis and Amazon Bedrock marks a pivotal step in streamlining the generative AI development process. By seamlessly connecting Redis to Agents for Amazon Bedrock as an external knowledge base, it bridges the gap between your domain-specific data and the vast capabilities of LLMs. This synergy automates the import of business documents from Amazon S3 (PDF, doc, txt, etc.) into Redis Enterprise Cloud, freeing developers from the complex chore of data preparation and integration. With this foundation, the intelligent agents crafted using Amazon Bedrock can enrich model prompts, extracting relevant context from Redis via Retrieval Augmented Generation (RAG).On their own, LLMs can be clever and even astonishing, but they certainly are not infallible. Without domain-specific data, an LLM’s relevance and utility diminishes. Simply put, while LLMs have a wealth of general knowledge, the magic happens when they adapt to your specific data.Training and fine-tuning LLMs on custom data to fit your needs may require significant computational firepower, however, and that introduces new costs. RAG aims to solve this problem: to bridge the gap between the LLM’s general knowledge and your own valuable data.This reference architecture highlights:Contextual relevance: Redis Enterprise stores and indexes domain-specific data as vector embeddings. At runtime it fetches the most contextually relevant data chunks using vector similarity search based on a user’s query. By incorporating this retrieved context into the LLM prompt (input), it ensures that outputs are tailored to your domain.Scalability: As your knowledge base expands, Redis Enterprise scales horizontally by sharding data and search indices across a distributed cluster.Security: You don’t want sensitive data or knowledge leaked by mistake. Safeguard critical data from being incorporated into the central LLM by housing it externally in a vector database–like Redis. This ensures adherence to the highest security standards, including encryption at rest and in transit, along with robust role-based access controls.Cost effectiveness: The integration reduces the need for continuous LLM training or fine-tuning. The result? You save both in computational requirements (faster chatbot performance) and costs.The union of Redis Enterprise Cloud and Agents for Amazon Bedrock redefines generative application potential on the technology side of things, and reduces time to market, which makes the business happy.Let’s consider practical examples.As businesses scale, addressing customer inquiries promptly and efficiently becomes increasingly challenging. Traditional chatbots may fall short in providing accurate or contextually-aware responses.By connecting a company’s extensive knowledge base or FAQ database with a vector database, a RAG system can empower chatbots to pull real-time, context-specific information. For instance, a user might ask about a specific product feature or troubleshooting step; the RAG-backed bot would then retrieve the most relevant details from the database and generate a coherent, helpful response, improving customer satisfaction and reducing manual intervention.Content marketers often need to write articles, blog posts, or promotional materials that resonate with specific audience segments, but it can be time-consuming to find relevant past content for reference or inspiration.By converting previous marketing materials, product documentation, customer feedback, and market research into vector embeddings, a RAG system can assist content creators in generating new content. For instance, when crafting a new article about a product feature, the system can pull relevant customer testimonials, past marketing angles, and current market trends, enabling the writer to create a more compelling and informed piece.To begin using both Amazon Bedrock for generative AI and Redis Enterprise Cloud as your vector database, you create a database through the AWS Marketplace, set up an AWS secret using Amazon Secrets Manager, and then create an index for vector similarity search within Redis Enterprise Cloud.Whether you link from the Amazon Bedrock knowledge base configuration screen or use this customized link to the AWS Marketplace offering, you will create a flexible, pay-as-you-go subscription for Redis Cloud with TLS enabled. If you are new to Redis, we give you a 14-day free trial of $500 to get started.Follow this detailed guide to create your Redis Enterprise Cloud database.When you create the database, you eventually land on a step that requires inputting a “Memory limit (GB).”Amazon Bedrock takes raw documents from Amazon S3, creates smaller chunks of text, generates embeddings and metadata, and upserts them in Redis as hashes. For RAG to work well, text chunks should capture just enough isolated semantic context from the source documents to be useful. Thus, Bedrock creates text chunks that are about 200 words long on average.Let’s work through an example where you have a dataset in Amazon S3 that is 10,000KB in size. (Read more about figuring out the size of your bucket.)Pulling all of this together, we can estimate a final Redis database size as the sum of the raw text content, vector embedding, and vector index, multiplied by the total number of expected chunks:(1KB + 12.5KB) * 10,000 expected chunks = 135,000KBWe created the table below to help you pick a reasonable starting point. While it is optional, we strongly recommend enabling high-availability (HA) with replication for any production deployment to ensure peak performance. You can always monitor and adjust this sizing in the Redis Cloud console as needed.After you create the database, use Amazon Secrets Manager to create a secret with the following fields:Once you store this secret, you can view and copy the Amazon Resource Name (ARN) of your secret on the secret details page. If you need more help, follow this step-by-step guide to create the required connection secret.After you set up your database within Redis Enterprise Cloud, you need to create an index with a vector field as your knowledgebase for Amazon Bedrock.With Redis you can build secondary indices on hash or JSON fields including text, tags, geo, numeric, and vectors. For vector similarity search, you must choose between a FLAT (brute-force) or HNSW (approximate, faster) vector index type, and one of three supported distance metrics (Inner Product, Cosine Distance, L2 Euclidean). Picking the right index type should be based on the size of your document dataset and the required levels of search accuracy and throughput.For starting with Amazon Bedrock, we recommend a simple index configuration with the following settings:Assuming your documents dataset is not massive and you require the utmost retrieval accuracy for RAG, this index configuration is a solid starting point.Out of the box, Redis provides a few options to create an index using RedisInsight,  RedisVL, or the Redis command-line interface (CLI).RedisInsight is the desktop graphic user interface (GUI) to visually browse and interact with data in Redis. Follow the steps in our dedicated guide to create your vector index with RedisInsight.RedisVL is a new, dedicated Python client library that helps AI and ML practitioners leverage Redis as a vector database.First, install RedisVL in your Python (>=3.8) environment using pip:pip install redisvl==0.0.4Copy this YAML file that contains a basic Redis vector index spec for Amazon Bedrock and paste it into a local file named bedrock-idx.yaml:You can now use the rvl command in a terminal to connect to the database and create a new index from the YAML schema definition.Set the REDIS_URL environment variable that incorporates the username, password, host, port, and full paths to all three required TLS cert files:Now, run the command to create the index. Watch for a success message:rvl index create -s bedrock-idx.yamlValidate that the index was created successfully using the listall or info commands:rvl index info -s bedrock-idx.yamlTo use the Redis CLI, download Redis on your local machine. Connect to your database with the CLI including TLS certs:Execute the snippet below to create an index named bedrock-idx:At this point, everything should be ready. It’s time to integrate your Redis deployment with Amazon Bedrock as a knowledgebase for RAG.✅ Redis database deployed✅ AWS secret created✅ Vector index createdOn the Amazon Bedrock knowledgebase configuration screen select your source Amazon S3 bucket. Make sure Redis Enterprise Cloud is selected as your vector database. Plug in your database Endpoint URL, Credentials secret ARN, and Vector index name.Finally, plug in user-defined name for your Index field mapping and Amazon Bedrock-created Metadata field mapping. These are created automatically by Amazon Bedrock when processing your source data from Amazon S3.Ready to begin your generative AI journey with Redis Enterprise Cloud and Amazon Bedrock? Start by visiting the Amazon Bedrock page, in the AWS console, and read the AWS announcement here.Have a use case or example you want to highlight? Kick the tires and open a PR with a community example in our github repo.Want to go deeper after trying out this integration or need additional assistance? We’re here to help. Contact a Redis expert today."
695,https://redis.com/blog/migrate-from-elasticache-to-redis-enterprise/,3 Ways to Migrate From ElastiCache to Redis Enterprise Cloud,"September 18, 2023",James Tessier,"There are lots of choices when you migrate data from one platform to another. Fortunately, the issues are so well-understood that they are suited for a simple flow chart.Data migration is an important, meaningful project, even if it isn’t one that brings practitioners a lot of glory. That is all the more reason to do it efficiently.You have a multitude of ways to migrate data. Let’s offer some guidance.There are three main migration paths for moving from ElasticCache into Redis. This flowchart helps you match your current environment to the appropriate process, with cautions to protect your data and make the migration a success.The primary decision point is whether you can tolerate a flush of all your Redis data. If you can, there’s no need to migrate the data at all. “Rip and Replace” is easy, but a flush results in slow performance while data is rehydrated.ProsConsIf you use Redis as more than a cache–for instance, you employ Redis as a session store–you can’t stomach the lost data and poor performance of a full data flush. We don’t blame you. All data is important. That’s why we offer a durable persistence option.The next question is whether you can tolerate downtime. How much downtime are we talking about? It could be as short as a few minutes. It depends on how long it takes to export and import your Redis data.If a short amount of downtime seems tolerable, It might be worthwhile to do a few experiments in your test environment to get a more accurate estimate of downtime. Be careful of memory usage as it is a common issue with ElastiCache deployments.If the estimated downtime is acceptable, we recommend offline data migration. This option is easy to accomplish but it results in downtime as data is exported from ElastiCache and imported into Redis Enterprise Cloud.ProsConsThere’s one more option. Live data migration can be executed without any downtime or lost data. However, it is more operationally complicated (a polite way of saying, “You may do a lot of cursing”), and you should think through the tradeoffs.This option relies on an external tool developed by our in-house Redis experts called RIOT. Its documentation details operational guidance in migrating from ElastiCache to Redis Enterprise. If you’re choosing this option, you may be grateful for the opportunity to contact one of our experts for guidance.ProsConsHopefully this has helped you decide how to migrate to Redis Enterprise Cloud. Now all you have to do is do it.And now that you’re using Redis Enterprise Cloud, perhaps you should experiment with the new-to-you features that you now have available? Learn about triggers and functions  or how auto-tiering can help you manage large datasets."
696,https://redis.com/blog/how-slow-queries-hurt-your-business/,How Slow Queries Hurt Your Business (and How to Fix Them),"September 18, 2023",Chris Fallon,"Every second of delay tests a user’s patience. A seemingly minor inconvenience reflects deeper database management issues. A small hiccup can have outsized repercussions. Slow queries aren’t just technical obstacles. They represent potential fissures in customer loyalty and confidence.Imagine you’re at a bustling coffee shop during the morning rush. The barista, overwhelmed by an influx of complex drink orders, struggles to keep up. Customers get impatient, the line extends out the door, and the entire operation slows down. If this happens frequently, those previously-loyal customers are motivated to buy their Frappuccino elsewhere.With databases, queries can become the equivalent of complex drink orders. They bog down the system and affect business operations. At a global financial institution that processes millions of transactions daily, a slow query could delay updates on account balances, leading to discrepancies and eroding customer trust.Understanding the reasons behind the coffee shop delay might lead to solutions, such as an extra barista or a better ordering system. Similarly, pinpointing the causes of slow queries can help businesses streamline their operations.The speed and efficiency of websites and applications play a pivotal role in shaping the customer experience. According to a report by Think with Google, a mere one tenth of a second’s delay in mobile page load can impact conversion rates by up to 10%. Just as a barista’s efficiency at a coffee shop can make or break the morning rush, site or application speed can determine its success in the digital marketplace. Slow-loading sites and applications, often a result of slow queries, can lead to user frustration, decreased engagement, and lost business opportunities.A slow query is a database query that takes an unusually long time to execute. This extended “long query time” can be attributed to any number of factors, ranging from the intricacies of SQL statements to underlying server complications. The technical repercussions of slow queries are manifold, leading to delayed data retrieval, prolonged page load times, and sluggishness in system operations.Just as a barista might grapple with a sudden influx of complex drink orders, databases too can be bogged down by intricate queries.Complex SQL statements: SQL queries are powerful, but their efficiency varies. An unoptimized SQL statement often increases execution times. For example, using the DIST keyword without proper indexing harms query performance.Server configuration nuances: Configuration files make a big difference in database performance. The settings you choose in the MySQL my.cnf file directly affect database speed, especially if you misconfigure them or don’t adjust settings to the current user load or data center specifications.Underlying server performance: The base server, whether a MySQL server, Apache Cassandra, or SQL Server, needs adequate resources to handle database queries efficiently. Resource constraints, whether memory or CPU-related, lead to slow-running queries.Network latency: Network speed and reliability are essential for distributed databases or systems that use cloud storage. The typical effect of high network latency is delayed query execution.Concurrency issues: When many users or applications access the database simultaneously, resource contention slows down query performance.Suboptimal query plans: The database engine creates a plan to execute a query. Sometimes, it selects a less efficient plan. End result: slower execution times.Database maintenance failures: Optimize performance by performing regular maintenance tasks including updating statistics, defragmenting indexes, and clearing old log files. Ignore these tasks at your peril.Lack of proper caching: Implement caching mechanisms to dramatically speed up frequently accessed data. Without caching, the database might process the same time-consuming queries repeatedly.There are no magic wands, alas. Dealing with slow queries requires a combination of tactics, tools, and expertise. If you don’t want to spend your time reacting to performance problems, plan ahead.Optimize SQL statementsReview configuration filesPerform an In-depth log analysis of slow queriesExplore alternative database systemsJust as a doctor monitors vital signs to gauge a patient's health, developers and operations monitor database queries to ensure the company’s digital heartbeat remains strong and steady. Slow queries sometimes are subtle and insidious, and they gradually erode a system's efficiency and a business's credibility.Keep a vigilant eye on database performance to preempt potential pitfalls. Among the tools and techniques that make this vigilance possible:As technology continues to evolve, so do the challenges associated with database management. Big Data, IoT, and AI-driven applications all generate a huge amount of data, putting more of a burden on your existing systems and possibly straining them. As datasets grow, those databases will be subjected to even more intensive workloads, which means it’s even more critical to address slow queries. One possibility to look forward to is that AI and machine learning might play a role in automatically optimizing database queries. Imagine a world where your database system learns from past queries, identifies patterns, and automatically optimizes new queries."
697,https://redis.com/blog/amazon-bedrock-integration-with-redis-enterprise/,Redis Enterprise Cloud Integration With Amazon Bedrock Now Available,"September 13, 2023",Antony Prasad Thevaraj and Tyler Hutcherson and Srinivas Pendyala,"Following the recent launch of Redis 7.2, which underscores our dedication to advanced developer tools and AI innovation, Redis is thrilled to announce the integration of Redis Enterprise Cloud with Amazon Bedrock as a knowledge base for building RAG applications.AWS designed Amazon Bedrock with customer needs in mind: seamless access to powerful foundation models (FMs) without complex infrastructure management, accompanied by the ability to customize these FMs securely. Today’s announcement ensures that developers can use a unified API to access FMs from Cohere, AI21 Labs, Anthropic, Stability AI, and Amazon, backed by a high-performance data layer (Read the AWS announcement on their blog here).The traction Redis Enterprise Cloud has garnered as a highly-performant, stable vector database doesn’t surprise many people, given the company’s investments in enterprise-grade capabilities such as high availability, persistence, scalability, and geo-replication for business continuity.In the past, merging private, domain-specific data with the capabilities of a large language model (LLM) was labor-intensive for Redis developers. The routine was multifaceted: extract source documents, morph them into embeddings, feed them into Redis, and then, on demand, execute complex vector search queries.The newly established integration between Redis and Amazon Bedrock marks a pivotal step in streamlining the generative AI development process. By seamlessly connecting Redis to Agents for Amazon Bedrock as an external knowledge base, it bridges the gap between your domain-specific data and the vast capabilities of LLMs. This synergy automates the import of business documents from Amazon S3 (PDF, doc, txt, etc.) into Redis Enterprise Cloud, freeing developers from the complex chore of data preparation and integration. With this foundation, the intelligent agents crafted using Amazon Bedrock can enrich model prompts, extracting relevant context from Redis via Retrieval Augmented Generation (RAG).On their own, LLMs can be clever and even astonishing, but they certainly are not infallible. Without domain-specific data, an LLM’s relevance and utility diminishes. Simply put, while LLMs have a wealth of general knowledge, the magic happens when they adapt to your specific data.Training and fine-tuning LLMs on custom data to fit your needs may require significant computational firepower, however, and that introduces new costs. RAG aims to solve this problem: to bridge the gap between the LLM’s general knowledge and your own valuable data.This reference architecture highlights:Contextual relevance: Redis Enterprise stores and indexes domain-specific data as vector embeddings. At runtime it fetches the most contextually relevant data chunks using vector similarity search based on a user’s query. By incorporating this retrieved context into the LLM prompt (input), it ensures that outputs are tailored to your domain.Scalability: As your knowledge base expands, Redis Enterprise scales horizontally by sharding data and search indices across a distributed cluster.Security: You don’t want sensitive data or knowledge leaked by mistake. Safeguard critical data from being incorporated into the central LLM by housing it externally in a vector database–like Redis. This ensures adherence to the highest security standards, including encryption at rest and in transit, along with robust role-based access controls.Cost effectiveness: The integration reduces the need for continuous LLM training or fine-tuning. The result? You save both in computational requirements (faster chatbot performance) and costs.The union of Redis Enterprise Cloud and Agents for Amazon Bedrock redefines generative application potential on the technology side of things, and reduces time to market, which makes the business happy.Let’s consider practical examples.As businesses scale, addressing customer inquiries promptly and efficiently becomes increasingly challenging. Traditional chatbots may fall short in providing accurate or contextually-aware responses.By connecting a company’s extensive knowledge base or FAQ database with a vector database, a RAG system can empower chatbots to pull real-time, context-specific information. For instance, a user might ask about a specific product feature or troubleshooting step; the RAG-backed bot would then retrieve the most relevant details from the database and generate a coherent, helpful response, improving customer satisfaction and reducing manual intervention.Content marketers often need to write articles, blog posts, or promotional materials that resonate with specific audience segments, but it can be time-consuming to find relevant past content for reference or inspiration.By converting previous marketing materials, product documentation, customer feedback, and market research into vector embeddings, a RAG system can assist content creators in generating new content. For instance, when crafting a new article about a product feature, the system can pull relevant customer testimonials, past marketing angles, and current market trends, enabling the writer to create a more compelling and informed piece.To begin using both Amazon Bedrock for generative AI and Redis Enterprise Cloud as your vector database, you create a database through the AWS Marketplace, set up an AWS secret using Amazon Secrets Manager, and then create an index for vector similarity search within Redis Enterprise Cloud.Whether you link from the Amazon Bedrock knowledge base configuration screen or use this customized link to the AWS Marketplace offering, you will create a flexible, pay-as-you-go subscription for Redis Cloud with TLS enabled. If you are new to Redis, we give you a 14-day free trial of $500 to get started.Follow this detailed guide to create your Redis Enterprise Cloud database.When you create the database, you eventually land on a step that requires inputting a “Memory limit (GB).”Amazon Bedrock takes raw documents from Amazon S3, creates smaller chunks of text, generates embeddings and metadata, and upserts them in Redis as hashes. For RAG to work well, text chunks should capture just enough isolated semantic context from the source documents to be useful. Thus, Bedrock creates text chunks that are about 200 words long on average.Let’s work through an example where you have a dataset in Amazon S3 that is 10,000KB in size. (Read more about figuring out the size of your bucket.)Pulling all of this together, we can estimate a final Redis database size as the sum of the raw text content, vector embedding, and vector index, multiplied by the total number of expected chunks:(1KB + 12.5KB) * 10,000 expected chunks = 135,000KBWe created the table below to help you pick a reasonable starting point. While it is optional, we strongly recommend enabling high-availability (HA) with replication for any production deployment to ensure peak performance. You can always monitor and adjust this sizing in the Redis Cloud console as needed.After you create the database, use Amazon Secrets Manager to create a secret with the following fields:Once you store this secret, you can view and copy the Amazon Resource Name (ARN) of your secret on the secret details page. If you need more help, follow this step-by-step guide to create the required connection secret.After you set up your database within Redis Enterprise Cloud, you need to create an index with a vector field as your knowledgebase for Amazon Bedrock.With Redis you can build secondary indices on hash or JSON fields including text, tags, geo, numeric, and vectors. For vector similarity search, you must choose between a FLAT (brute-force) or HNSW (approximate, faster) vector index type, and one of three supported distance metrics (Inner Product, Cosine Distance, L2 Euclidean). Picking the right index type should be based on the size of your document dataset and the required levels of search accuracy and throughput.For starting with Amazon Bedrock, we recommend a simple index configuration with the following settings:Assuming your documents dataset is not massive and you require the utmost retrieval accuracy for RAG, this index configuration is a solid starting point.Out of the box, Redis provides a few options to create an index using RedisInsight,  RedisVL, or the Redis command-line interface (CLI).RedisInsight is the desktop graphic user interface (GUI) to visually browse and interact with data in Redis. Follow the steps in our dedicated guide to create your vector index with RedisInsight.RedisVL is a new, dedicated Python client library that helps AI and ML practitioners leverage Redis as a vector database.First, install RedisVL in your Python (>=3.8) environment using pip:pip install redisvl==0.0.4Copy this YAML file that contains a basic Redis vector index spec for Amazon Bedrock and paste it into a local file named bedrock-idx.yaml:You can now use the rvl command in a terminal to connect to the database and create a new index from the YAML schema definition.Set the REDIS_URL environment variable that incorporates the username, password, host, port, and full paths to all three required TLS cert files:Now, run the command to create the index. Watch for a success message:rvl index create -s bedrock-idx.yamlValidate that the index was created successfully using the listall or info commands:rvl index info -s bedrock-idx.yamlTo use the Redis CLI, download Redis on your local machine. Connect to your database with the CLI including TLS certs:Execute the snippet below to create an index named bedrock-idx:At this point, everything should be ready. It’s time to integrate your Redis deployment with Amazon Bedrock as a knowledgebase for RAG.✅ Redis database deployed✅ AWS secret created✅ Vector index createdOn the Amazon Bedrock knowledgebase configuration screen select your source Amazon S3 bucket. Make sure Redis Enterprise Cloud is selected as your vector database. Plug in your database Endpoint URL, Credentials secret ARN, and Vector index name.Finally, plug in user-defined name for your Index field mapping and Amazon Bedrock-created Metadata field mapping. These are created automatically by Amazon Bedrock when processing your source data from Amazon S3.Ready to begin your generative AI journey with Redis Enterprise Cloud and Amazon Bedrock? Start by visiting the Amazon Bedrock page, in the AWS console, and read the AWS announcement here.Have a use case or example you want to highlight? Kick the tires and open a PR with a community example in our github repo.Want to go deeper after trying out this integration or need additional assistance? We’re here to help. Contact a Redis expert today.Test the full power of Redis Enterprise with no upfront costs or commitments. Our new 14-day free trial comes with everything Redis Enterprise has to offer and is available in AWS Marketplace. Get started for free today!"
